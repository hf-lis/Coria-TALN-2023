id,title,abstract,A.1,B.1,B.2,B.3,B.4,B.5,B.6,B.7,B.8,C.0,C.1,C.2,C.3,C.4,C.5,D.0,D.1,D.2,D.3,D.4,E.0,E.1,E.2,E.3,E.4,E.5,F.0,F.1,F.2,F.3,F.4,G.0,G.1,G.2,G.3,G.4,H.0,H.1,H.2,H.3,H.4,H.5,I.0,I.1,I.2,I.3,I.4,I.5,I.6,I.7,J.1,J.2,J.3,J.4,J.5,J.6,J.7,K.2,K.3,K.4,K.5,K.6,K.7,K.8
http://arxiv.org/abs/cs/9809122v1,Practical algorithms for on-line sampling,"One of the core applications of machine learning to knowledge discovery
consists on building a function (a hypothesis) from a given amount of data (for
instance a decision tree or a neural network) such that we can use it
afterwards to predict new instances of the data. In this paper, we focus on a
particular situation where we assume that the hypothesis we want to use for
prediction is very simple, and thus, the hypotheses class is of feasible size.
We study the problem of how to determine which of the hypotheses in the class
is almost the best one. We present two on-line sampling algorithms for
selecting hypotheses, give theoretical bounds on the number of necessary
examples, and analize them exprimentally. We compare them with the simple batch
sampling approach commonly used and show that in most of the situations our
algorithms use much fewer number of examples.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.01874v1,Smile and Laugh Expressions Detection Based on Local Minimum Key Points,"In this paper, a smile and laugh facial expression is presented based on
dimension reduction and description process of the key points. The paper has
two main objectives; the first is to extract the local critical points in terms
of their apparent features, and the second is to reduce the system's dependence
on training inputs. To achieve these objectives, three different scenarios on
extracting the features are proposed. First of all, the discrete parts of a
face are detected by local binary pattern method that is used to extract a set
of global feature vectors for texture classification considering various
regions of an input-image face. Then, in the first scenario and with respect to
the correlation changes of adjacent pixels on the texture of a mouth area, a
set of local key points are extracted using the Harris corner detector. In the
second scenario, the dimension reduction of the extracted points of first
scenario provided by principal component analysis algorithm leading to
reduction in computational costs and overall complexity without loss of
performance and flexibility, etc.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0806.2925v2,Neural networks in 3D medical scan visualization,"For medical volume visualization, one of the most important tasks is to
reveal clinically relevant details from the 3D scan (CT, MRI ...), e.g. the
coronary arteries, without obscuring them with less significant parts. These
volume datasets contain different materials which are difficult to extract and
visualize with 1D transfer functions based solely on the attenuation
coefficient. Multi-dimensional transfer functions allow a much more precise
classification of data which makes it easier to separate different surfaces
from each other. Unfortunately, setting up multi-dimensional transfer functions
can become a fairly complex task, generally accomplished by trial and error.
This paper explains neural networks, and then presents an efficient way to
speed up visualization process by semi-automatic transfer function generation.
We describe how to use neural networks to detect distinctive features shown in
the 2D histogram of the volume data and how to use this information for data
classification.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.03360v1,"Cadence: A Practical Time-series Partitioning Algorithm for Unlabeled
  IoT Sensor Streams","Timeseries partitioning is an essential step in most machine-learning driven,
sensor-based IoT applications. This paper introduces a sample-efficient,
robust, time-series segmentation model and algorithm. We show that by learning
a representation specifically with the segmentation objective based on maximum
mean discrepancy (MMD), our algorithm can robustly detect time-series events
across different applications. Our loss function allows us to infer whether
consecutive sequences of samples are drawn from the same distribution (null
hypothesis) and determines the change-point between pairs that reject the null
hypothesis (i.e., come from different distributions). We demonstrate its
applicability in a real-world IoT deployment for ambient-sensing based activity
recognition. Moreover, while many works on change-point detection exist in the
literature, our model is significantly simpler and matches or outperforms
state-of-the-art methods. We can fully train our model in 9-93 seconds on
average with little variation in hyperparameters for data across different
applications.",0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0001027v1,Pattern Discovery and Computational Mechanics,"Computational mechanics is a method for discovering, describing and
quantifying patterns, using tools from statistical physics. It constructs
optimal, minimal models of stochastic processes and their underlying causal
structures. These models tell us about the intrinsic computation embedded
within a process---how it stores and transforms information. Here we summarize
the mathematics of computational mechanics, especially recent optimality and
uniqueness results. We also expound the principles and motivations underlying
computational mechanics, emphasizing its connections to the minimum description
length principle, PAC theory, and other aspects of machine learning.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.00387v2,Highway Networks,"There is plenty of theoretical and empirical evidence that depth of neural
networks is a crucial ingredient for their success. However, network training
becomes more difficult with increasing depth and training of very deep networks
remains an open problem. In this extended abstract, we introduce a new
architecture designed to ease gradient-based training of very deep networks. We
refer to networks with this architecture as highway networks, since they allow
unimpeded information flow across several layers on ""information highways"". The
architecture is characterized by the use of gating units which learn to
regulate the flow of information through a network. Highway networks with
hundreds of layers can be trained directly using stochastic gradient descent
and with a variety of activation functions, opening up the possibility of
studying extremely deep and efficient architectures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1802.00756v3,Infinitary and Cyclic Proof Systems for Transitive Closure Logic,"Transitive closure logic is a known extension of first-order logic obtained
by introducing a transitive closure operator. While other extensions of
first-order logic with inductive definitions are a priori parametrized by a set
of inductive definitions, the addition of the transitive closure operator
uniformly captures all finitary inductive definitions. In this paper we present
an infinitary proof system for transitive closure logic which is an infinite
descent-style counterpart to the existing (explicit induction) proof system for
the logic. We show that, as for similar systems for first-order logic with
inductive definitions, our infinitary system is complete for the standard
semantics and subsumes the explicit system. Moreover, the uniformity of the
transitive closure operator allows semantically meaningful complete
restrictions to be defined using simple syntactic criteria. Consequently, the
restriction to regular infinitary (i.e. cyclic) proofs provides the basis for
an effective system for automating inductive reasoning.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.7879v2,"Kahler: An Implementation of Discrete Exterior Calculus on Hermitian
  Manifolds","This paper details the techniques and algorithms implemented in Kahler, a
Python library that implements discrete exterior calculus on arbitrary
Hermitian manifolds. Borrowing techniques and ideas first implemented in PyDEC,
Kahler provides a uniquely general framework for computation using discrete
exterior calculus. Manifolds can have arbitrary dimension, topology, bilinear
Hermitian metrics, and embedding dimension. Kahler comes equipped with tools
for generating triangular meshes in arbitrary dimensions with arbitrary
topology. Kahler can also generate discrete sharp operators and implement de
Rham maps. Computationally intensive tasks are automatically parallelized over
the number of cores detected. The program itself is written in Cython--a
superset of the Python language that is translated to C and compiled for extra
speed. Kahler is applied to several example problems: normal modes of a
vibrating membrane, electromagnetic resonance in a cavity, the quantum harmonic
oscillator, and the Dirac-Kahler equation. Convergence is demonstrated on
random meshes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1512.03696v2,"Characterisation of limit measures of higher-dimensional cellular
  automata","We consider the typical asymptotic behaviour of cellular automata of higher
dimension (greater than 2). That is, we take an initial configuration at random
according to a Bernoulli (i.i.d) probability measure, iterate some cellular
automaton, and consider the (set of) limit probability measure(s) as time tends
to infinity. In this paper, we prove that limit measures that can be reached by
higher-dimensional cellular automata are completely characterised by
computability conditions, as in the one-dimensional case. This implies that
cellular automata have the same variety and complexity of typical asymptotic
behaviours as Turing machines, and that any nontrivial property in this regard
is undecidable (Rice-type theorem). These results extend to connected sets of
limit measures and Ces\`aro mean convergence. The main tool is the
implementation of arbitrary computation in the time evolution of a cellular
automata in such a way that it emerges and self-organises from a random
configuration.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1503.03613v1,On the Impossibility of Learning the Missing Mass,"This paper shows that one cannot learn the probability of rare events without
imposing further structural assumptions. The event of interest is that of
obtaining an outcome outside the coverage of an i.i.d. sample from a discrete
distribution. The probability of this event is referred to as the ""missing
mass"". The impossibility result can then be stated as: the missing mass is not
distribution-free PAC-learnable in relative error. The proof is
semi-constructive and relies on a coupling argument using a dithered geometric
distribution. This result formalizes the folklore that in order to predict rare
events, one necessarily needs distributions with ""heavy tails"".",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.08425v1,"xSDK Foundations: Toward an Extreme-scale Scientific Software
  Development Kit","Extreme-scale computational science increasingly demands multiscale and
multiphysics formulations. Combining software developed by independent groups
is imperative: no single team has resources for all predictive science and
decision support capabilities. Scientific libraries provide high-quality,
reusable software components for constructing applications with improved
robustness and portability. However, without coordination, many libraries
cannot be easily composed. Namespace collisions, inconsistent arguments, lack
of third-party software versioning, and additional difficulties make
composition costly.
  The Extreme-scale Scientific Software Development Kit (xSDK) defines
community policies to improve code quality and compatibility across
independently developed packages (hypre, PETSc, SuperLU, Trilinos, and
Alquimia) and provides a foundation for addressing broader issues in software
interoperability, performance portability, and sustainability. The xSDK
provides turnkey installation of member software and seamless combination of
aggregate capabilities, and it marks first steps toward extreme-scale
scientific software ecosystems from which future applications can be composed
rapidly with assured quality and scalability.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0310061v1,"Local-search techniques for propositional logic extended with
  cardinality constraints","We study local-search satisfiability solvers for propositional logic extended
with cardinality atoms, that is, expressions that provide explicit ways to
model constraints on cardinalities of sets. Adding cardinality atoms to the
language of propositional logic facilitates modeling search problems and often
results in concise encodings. We propose two ``native'' local-search solvers
for theories in the extended language. We also describe techniques to reduce
the problem to standard propositional satisfiability and allow us to use
off-the-shelf SAT solvers. We study these methods experimentally. Our general
finding is that native solvers designed specifically for the extended language
perform better than indirect methods relying on SAT solvers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1006.1549v1,"Extending scientific computing system with structural quantum
  programming capabilities","We present a basic high-level structures used for developing quantum
programming languages. The presented structures are commonly used in many
existing quantum programming languages and we use quantum pseudo-code based on
QCL quantum programming language to describe them. We also present the
implementation of introduced structures in GNU Octave language for scientific
computing. Procedures used in the implementation are available as a package
quantum-octave, providing a library of functions, which facilitates the
simulation of quantum computing. This package allows also to incorporate
high-level programming concepts into the simulation in GNU Octave and Matlab.
As such it connects features unique for high-level quantum programming
languages, with the full palette of efficient computational routines commonly
available in modern scientific computing systems. To present the major features
of the described package we provide the implementation of selected quantum
algorithms. We also show how quantum errors can be taken into account during
the simulation of quantum algorithms using quantum-octave package. This is
possible thanks to the ability to operate on density matrices.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0207066v1,Polynomial Time Data Reduction for Dominating Set,"Dealing with the NP-complete Dominating Set problem on undirected graphs, we
demonstrate the power of data reduction by preprocessing from a theoretical as
well as a practical side. In particular, we prove that Dominating Set
restricted to planar graphs has a so-called problem kernel of linear size,
achieved by two simple and easy to implement reduction rules. Moreover, having
implemented our reduction rules, first experiments indicate the impressive
practical potential of these rules. Thus, this work seems to open up a new and
prospective way how to cope with one of the most important problems in graph
theory and combinatorial optimization.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.12552v1,Cirquent calculus in a nutshell,"This paper is a brief and informal presentation of cirquent calculus, a novel
proof system for resource-conscious logics. As such, it is a refinement of
sequent calculus with mechanisms that allow to explicitly account for the
possibility of sharing of subexpressions/subresources between different
expressions/resources. This is achieved by dealing with circuit-style
constructs, termed cirquents, instead of formulas, sequents or other tree-like
structures. The approach exhibits greater expressiveness, flexibility and
efficiency compared to the more traditional proof-theoretic approaches.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1610.05141v2,"Efficient Random Sampling -- Parallel, Vectorized, Cache-Efficient, and
  Online","We consider the problem of sampling $n$ numbers from the range
$\{1,\ldots,N\}$ without replacement on modern architectures. The main result
is a simple divide-and-conquer scheme that makes sequential algorithms more
cache efficient and leads to a parallel algorithm running in expected time
$\mathcal{O}(n/p+\log p)$ on $p$ processors, i.e., scales to massively parallel
machines even for moderate values of $n$. The amount of communication between
the processors is very small (at most $\mathcal{O}(\log p)$) and independent of
the sample size. We also discuss modifications needed for load balancing,
online sampling, sampling with replacement, Bernoulli sampling, and
vectorization on SIMD units or GPUs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1903.02835v1,"Only Connect, Securely","The lattice model proposed by Denning in her seminal work provided secure
information flow analyses with an intuitive and uniform mathematical
foundation. Different organisations, however, may employ quite different
security lattices. In this paper, we propose a connection framework that
permits different organisations to exchange information while maintaining both
security of information flows as well as their autonomy in formulating and
maintaining security policy. Our prescriptive framework is based on the
rigorous mathematical framework of Lagois connections given by Melton, together
with a simple operational model for transferring object data between domains.
The merit of this formulation is that it is simple, minimal, adaptable and
intuitive, and provides a formal framework for establishing secure information
flow across autonomous interacting organisations. We show that our framework is
semantically sound, by proving that the connections proposed preserve standard
correctness notions such as non-interference.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.12198v1,"Sorting Big Data by Revealed Preference with Application to College
  Ranking","When ranking big data observations such as colleges in the United States,
diverse consumers reveal heterogeneous preferences. The objective of this paper
is to sort out a linear ordering for these observations and to recommend
strategies to improve their relative positions in the ranking. A properly
sorted solution could help consumers make the right choices, and governments
make wise policy decisions. Previous researchers have applied exogenous
weighting or multivariate regression approaches to sort big data objects,
ignoring their variety and variability. By recognizing the diversity and
heterogeneity among both the observations and the consumers, we instead apply
endogenous weighting to these contradictory revealed preferences. The outcome
is a consistent steady-state solution to the counterbalance equilibrium within
these contradictions. The solution takes into consideration the spillover
effects of multiple-step interactions among the observations. When information
from data is efficiently revealed in preferences, the revealed preferences
greatly reduce the volume of the required data in the sorting process. The
employed approach can be applied in many other areas, such as sports team
ranking, academic journal ranking, voting, and real effective exchange rates.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.15104v1,Social Influences in Recommendation Systems,"Social networking sites such as Flickr and Facebook allow users to share
content with family, friends, and interest groups. Also, tags can often assign
to resources. In the previous research using few association rules FAR, we have
seen that high-quality and efficient association-based tag recommendation is
possible, but the set-up that we considered was very generic and did not take
social information into account. The proposed method in the previous paper,
FAR, in particular, exhibited a favorable trade-off between recommendation
quality and runtime. Unfortunately, recommendation quality is unlikely to be
optimal because the algorithms are not aware of any social information that may
be available. Two proposed approaches take a more social view on tag
recommendation regarding the issue: social contact variants and social groups
of interest. The user data is varied and used as a source of associations. The
adoption of social contact variants has two approaches. The first social
variant is User-centered Knowledge, to contrast Collective Knowledge. It
improves tag recommendation by grouping historic tag data according to friend
relationships and interests. The second variant is dubbed 'social batched
personomy' and attempts to address both quality and scalability issues by
processing queries in batches instead of individually, such as done in a
conventional personomy approach. For the social group of interest, 'community
batched personomy' is proposed to provide better accuracy groups of
recommendation systems in contrast also to Collective Knowledge. By taking
social information into account can enhance the performance of recommendation
systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1806.10748v1,"Towards automatic initialization of registration algorithms using
  simulated endoscopy images","Registering images from different modalities is an active area of research in
computer aided medical interventions. Several registration algorithms have been
developed, many of which achieve high accuracy. However, these results are
dependent on many factors, including the quality of the extracted features or
segmentations being registered as well as the initial alignment. Although
several methods have been developed towards improving segmentation algorithms
and automating the segmentation process, few automatic initialization
algorithms have been explored. In many cases, the initial alignment from which
a registration is initiated is performed manually, which interferes with the
clinical workflow. Our aim is to use scene classification in endoscopic
procedures to achieve coarse alignment of the endoscope and a preoperative
image of the anatomy. In this paper, we show using simulated scenes that a
neural network can predict the region of anatomy (with respect to a
preoperative image) that the endoscope is located in by observing a single
endoscopic video frame. With limited training and without any hyperparameter
tuning, our method achieves an accuracy of 76.53 (+/-1.19)%. There are several
avenues for improvement, making this a promising direction of research. Code is
available at https://github.com/AyushiSinha/AutoInitialization.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1502.07118v4,Local Linearizability,"The semantics of concurrent data structures is usually given by a sequential
specification and a consistency condition. Linearizability is the most popular
consistency condition due to its simplicity and general applicability.
Nevertheless, for applications that do not require all guarantees offered by
linearizability, recent research has focused on improving performance and
scalability of concurrent data structures by relaxing their semantics.
  In this paper, we present local linearizability, a relaxed consistency
condition that is applicable to container-type concurrent data structures like
pools, queues, and stacks. While linearizability requires that the effect of
each operation is observed by all threads at the same time, local
linearizability only requires that for each thread T, the effects of its local
insertion operations and the effects of those removal operations that remove
values inserted by T are observed by all threads at the same time. We
investigate theoretical and practical properties of local linearizability and
its relationship to many existing consistency conditions. We present a generic
implementation method for locally linearizable data structures that uses
existing linearizable data structures as building blocks. Our implementations
show performance and scalability improvements over the original building blocks
and outperform the fastest existing container-type implementations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1812.10748v1,"Malicious Software Detection and Classification utilizing
  Temporal-Graphs of System-call Group Relations","In this work we propose a graph-based model that, utilizing relations between
groups of System-calls, distinguishes malicious from benign software samples
and classifies the detected malicious samples to one of a set of known malware
families. More precisely, given a System-call Dependency Graph (ScDG) that
depicts the malware's behavior, we first transform it to a more abstract
representation, utilizing the indexing of System-calls to a set of groups of
similar functionality, constructing thus an abstract and mutation-tolerant
graph that we call Group Relation Graph (GrG); then, we construct another graph
representation, which we call Coverage Graph (CvG), that depicts the dominating
relations between the nodes of a GrG graph. Based on the research so far in the
field, we pointed out that behavior-based graph representations had not
leveraged the aspect of the temporal evolution of the graph. Hence, the novelty
of our work is that, preserving the initial representations of GrG and CvG
graphs, we focus on augmenting the potentials of theses graphs by adding
further features that enhance its abilities on detecting and further
classifying to a known malware family an unknown malware sample. To that end,
we construct periodical instances of the graph that represent its temporal
evolution concerning its structural modifications, creating another graph
representation that we call Temporal Graphs. In this paper, we present the
theoretical background behind our approach, discuss the current technological
status on malware detection and classification and demonstrate the overall
architecture of our proposed detection and classification model alongside with
its underlying main principles and its structural key-components.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/cs/0504078v1,Adaptive Online Prediction by Following the Perturbed Leader,"When applying aggregating strategies to Prediction with Expert Advice, the
learning rate must be adaptively tuned. The natural choice of
sqrt(complexity/current loss) renders the analysis of Weighted Majority
derivatives quite complicated. In particular, for arbitrary weights there have
been no results proven so far. The analysis of the alternative ""Follow the
Perturbed Leader"" (FPL) algorithm from Kalai & Vempala (2003) (based on
Hannan's algorithm) is easier. We derive loss bounds for adaptive learning rate
and both finite expert classes with uniform weights and countable expert
classes with arbitrary weights. For the former setup, our loss bounds match the
best known results so far, while for the latter our results are new.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.11281v3,Robust Stochastic Bayesian Games for Behavior Space Coverage,"A key challenge in multi-agent systems is the design of intelligent agents
solving real-world tasks in close interaction with other agents (e.g. humans),
thereby being confronted with a variety of behavioral variations and limited
knowledge about the true behaviors of observed agents. The practicability of
existing works addressing this challenge is being limited due to using finite
sets of hypothesis for behavior prediction, the lack of a hypothesis design
process ensuring coverage over all behavioral variations and
sample-inefficiency when modeling continuous behavioral variations. In this
work, we present an approach to this challenge based on a new framework of
Robust Stochastic Bayesian Games (RSBGs). An RSBG defines hypothesis sets by
partitioning the physically feasible, continuous behavior space of the other
agents. It combines the optimality criteria of the Robust Markov Decision
Process (RMDP) and the Stochastic Bayesian Game (SBG) to exponentially reduce
the sample complexity for planning with hypothesis sets defined over continuous
behavior spaces. Our approach outperforms the baseline algorithms in two
experiments modeling time-varying intents and large multidimensional behavior
spaces, while achieving the same performance as a planner with knowledge of the
true behaviors of other agents.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1105.6160v1,"SENVM: Server Environment Monitoring and Controlling System for a Small
  Data Center Using Wireless Sensor Network","In recent years, efficient energy utilization becomes an essential
requirement for data centers, especially in data centers of world-leading
companies, where ""Green Data Center"" defines a new term for an
environment-concerned data center. Solutions to change existing a data center
to the green one may vary. In the big company, high-cost approaches including
re-planning server rooms, changing air-conditioners, buying low-powered
servers, and equipping sophisticating environmental control equipments are
possible, but not for small to medium enterprises (SMEs) and academic sectors
which have limited budget. In this paper, we propose a novel system, SENVM,
used to monitor and control air temperature in a server room to be in
appropriate condition, not too cold, where very unnecessary cooling leads to
unnecessary extra electricity expenses, and also inefficient in energy
utilization. With implementing on an emerging technology, Wireless Sensor
Network (WSN), Green Data Center is feasible to every small data center with no
wiring installation, easy deployment, and low maintenance fee. In addition, the
prototype of the system has been tested, and the first phase of the project is
deployed in a real-world data center.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
http://arxiv.org/abs/1806.01005v2,Path Throughput Importance Weights,"Many Monte Carlo light transport simulations use multiple importance sampling
(MIS) to weight between different path sampling strategies. We propose to use
the path throughput to compute the MIS weights instead of the commonly used
probability density per area measure. This new formulation is equivalent to the
previous approach and results in the same weights as well as implementation.
However, it is more intuitive and can help in understanding the effects of
modifications to the weight function. We show some examples of required
modifications which are often neglected in implementations. Also, our new
perspective might help to derive MIS strategies for new samplers in the future.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0502039v1,Efficient Parallel Simulations of Asynchronous Cellular Arrays,"A definition for a class of asynchronous cellular arrays is proposed. An
example of such asynchrony would be independent Poisson arrivals of cell
iterations. The Ising model in the continuous time formulation of Glauber falls
into this class. Also proposed are efficient parallel algorithms for simulating
these asynchronous cellular arrays. In the algorithms, one or several cells are
assigned to a processing element (PE), local times for different PEs can be
different. Although the standard serial algorithm by Metropolis, Rosenbluth,
Rosenbluth, Teller, and Teller can simulate such arrays, it is usually believed
to be without an efficient parallel counterpart. However, the proposed parallel
algorithms contradict this belief proving to be both efficient and able to
perform the same task as the standard algorithm. The results of experiments
with the new algorithms are encouraging: the speed-up is greater than 16 using
25 PEs on a shared memory MIMD bus computer, and greater than 1900 using 2**14
PEs on a SIMD computer. The algorithm by Bortz, Kalos, and Lebowitz can be
incorporated in the proposed parallel algorithms, further contributing to
speed-up. [In this paper I invented the update-cites-of-local-time-minima
parallel simulation scheme. Now the scheme is becoming popular. Many misprints
of the original 1987 Complex Systems publication are corrected here.-B.L.]",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.09397v2,"Semi-Supervised Clustering via Information-Theoretic Markov Chain
  Aggregation","We connect the problem of semi-supervised clustering to constrained Markov
aggregation, i.e., the task of partitioning the state space of a Markov chain.
We achieve this connection by considering every data point in the dataset as an
element of the Markov chain's state space, by defining the transition
probabilities between states via similarities between corresponding data
points, and by incorporating semi-supervision information as hard constraints
in a Hartigan-style algorithm. The introduced Constrained Markov Clustering
(CoMaC) is an extension of a recent information-theoretic framework for
(unsupervised) Markov aggregation to the semi-supervised case. Instantiating
CoMaC for certain parameter settings further generalizes two previous
information-theoretic objectives for unsupervised clustering. Our results
indicate that CoMaC is competitive with the state-of-the-art.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1604.01504v5,"Integrating NOE and RDC using sum-of-squares relaxation for protein
  structure determination","We revisit the problem of protein structure determination from geometrical
restraints from NMR, using convex optimization. It is well-known that the
NP-hard distance geometry problem of determining atomic positions from pairwise
distance restraints can be relaxed into a convex semidefinite program. Often
the NOE distance restraints are too imprecise and sparse for accurate structure
determination. Residual dipolar coupling (RDC) measurements provide additional
geometric information on the angles between atom-pair directions and axes of
the principal-axis-frame. The optimization problem involving RDC is highly
non-convex and requires a good initialization even within the simulated
annealing framework. In this paper, we model the protein backbone as an
articulated structure composed of rigid units. Determining the rotation of each
rigid unit gives the full protein structure. We propose solving the non-convex
optimization problems using the sum-of-squares (SOS) hierarchy. The two
algorithms - RDC-SOS and RDC-NOE-SOS, have polynomial time complexity in the
number of amino-acid residues and run efficiently on a standard desktop. In
many instances, the proposed methods exactly recover the solution to the
original non-convex optimization problem. We introduce a statistical tool, the
Cramer-Rao bound (CRB), to provide an information theoretic bound on the
highest resolution one can hope to achieve when determining protein structure
from noisy measurements using any methodology. Our simulation results show that
when the RDC measurements are corrupted by Gaussian noise of realistic
variance, both SOS based algorithms attain the CRB. We successfully apply our
method in a divide-and-conquer fashion to determine the structure of ubiquitin
from experimental NOE and RDC measurements, achieving more accurate and faster
reconstructions compared to the current state of the art.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1012.2694v1,The 2-Center Problem in Three Dimensions,"Let P be a set of n points in R^3. The 2-center problem for P is to find two
congruent balls of minimum radius whose union covers P. We present two
randomized algorithms for computing a 2-center of P. The first algorithm runs
in O(n^3 log^5 n) expected time, and the second algorithm runs in O((n^2 log^5
n) /(1-r*/r_0)^3) expected time, where r* is the radius of the 2-center balls
of P and r_0 is the radius of the smallest enclosing ball of P. The second
algorithm is faster than the first one as long as r* is not too close to r_0,
which is equivalent to the condition that the centers of the two covering balls
be not too close to each other.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.12032v2,"StRDAN: Synthetic-to-Real Domain Adaptation Network for Vehicle
  Re-Identification","Vehicle re-identification aims to obtain the same vehicles from vehicle
images. This is challenging but essential for analyzing and predicting traffic
flow in the city. Although deep learning methods have achieved enormous
progress for this task, their large data requirement is a critical shortcoming.
Therefore, we propose a synthetic-to-real domain adaptation network (StRDAN)
framework, which can be trained with inexpensive large-scale synthetic and real
data to improve performance. The StRDAN training method combines domain
adaptation and semi-supervised learning methods and their associated losses.
StRDAN offers significant improvement over the baseline model, which can only
be trained using real data, for VeRi and CityFlow-ReID datasets, achieving 3.1%
and 12.9% improved mean average precision, respectively.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.07157v1,Watset: Automatic Induction of Synsets from a Graph of Synonyms,"This paper presents a new graph-based approach that induces synsets using
synonymy dictionaries and word embeddings. First, we build a weighted graph of
synonyms extracted from commonly available resources, such as Wiktionary.
Second, we apply word sense induction to deal with ambiguous words. Finally, we
cluster the disambiguated version of the ambiguous input graph into synsets.
Our meta-clustering approach lets us use an efficient hard clustering algorithm
to perform a fuzzy clustering of the graph. Despite its simplicity, our
approach shows excellent results, outperforming five competitive
state-of-the-art methods in terms of F-score on three gold standard datasets
for English and Russian derived from large-scale manually constructed lexical
resources.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1407.5030v4,To Reach or not to Reach? Efficient Algorithms for Total-Payoff Games,"Quantitative games are two-player zero-sum games played on directed weighted
graphs. Total-payoff games (that can be seen as a refinement of the
well-studied mean-payoff games) are the variant where the payoff of a play is
computed as the sum of the weights. Our aim is to describe the first
pseudo-polynomial time algorithm for total-payoff games in the presence of
arbitrary weights. It consists of a non-trivial application of the value
iteration paradigm. Indeed, it requires to study, as a milestone, a refinement
of these games, called min-cost reachability games, where we add a reachability
objective to one of the players. For these games, we give an efficient value
iteration algorithm to compute the values and optimal strategies (when they
exist), that runs in pseudo-polynomial time. We also propose heuristics
allowing one to possibly speed up the computations in both cases.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1901.01797v1,Baker game and polynomial-time approximation schemes,"Baker devised a technique to obtain approximation schemes for many
optimization problems restricted to planar graphs; her technique was later
extended to more general graph classes. In particular, using the Baker's
technique and the minor structure theorem, Dawar et al. gave Polynomial-Time
Approximation Schemes (PTAS) for all monotone optimization problems expressible
in the first-order logic when restricted to a proper minor-closed class of
graphs. We define a Baker game formalizing the notion of repeated application
of Baker's technique interspersed with vertex removal, prove that monotone
optimization problems expressible in the first-order logic admit PTAS when
restricted to graph classes in which the Baker game can be won in a constant
number of rounds, and prove without use of the minor structure theorem that all
proper minor-closed classes of graphs have this property.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0209024v2,"Errors in Low and Lapsley's article ""Optimization Flow Control, I: Basic
  Algorithm and Convergence""","In the note two errors in Low and Lapsley's article ""Optimization Flow
Control, I: Basic Algorithm and Convergence"", ""IEEE/ACM Transactions on
Networking"", 7(6), pp. 861-874, 1999, are shown. Because of these errors the
proofs of both theorems presented in the article are incomplete and some
assessments are wrong.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.05404v1,"Symmetries in Reversible Programming: From Symmetric Rig Groupoids to
  Reversible Programming Languages","The $\mathit{\Pi}$ family of reversible programming languages for boolean
circuits is presented as a syntax of combinators witnessing type isomorphisms
of algebraic datatypes. In this paper, we give a denotational semantics for
this language, using the language of weak groupoids \`a la Homotopy Type
Theory, and show how to derive an equational theory for it, presented by
2-combinators witnessing equivalences of reversible circuits. We establish a
correspondence between the syntactic groupoid of the language and a formally
presented univalent subuniverse of finite types. The correspondence relates
1-combinators to 1-paths, and 2-combinators to 2-paths in the universe, which
is shown to be sound and complete for both levels, establishing full
abstraction and adequacy. We extend the already established Curry-Howard
correspondence for $\mathit{\Pi}$ to a Curry-Howard-Lambek correspondence
between Reversible Logic, Reversible Programming Languages, and Symmetric Rig
Groupoids, by showing that the syntax of $\mathit{\Pi}$ is presented by the
free symmetric rig groupoid, given by finite sets and permutations. Our proof
uses techniques from the theory of group presentations and rewriting systems to
solve the word problem for symmetric groups. Using the formalisation of our
results, we show how to perform normalisation-by-evaluation, verification, and
synthesis of reversible logic gates, motivated by examples from quantum
computing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1210.3344v1,Galois correspondence for counting quantifiers,"We introduce a new type of closure operator on the set of relations,
max-implementation, and its weaker analog max-quantification. Then we show that
approximation preserving reductions between counting constraint satisfaction
problems (#CSPs) are preserved by these two types of closure operators.
Together with some previous results this means that the approximation
complexity of counting CSPs is determined by partial clones of relations that
additionally closed under these new types of closure operators. Galois
correspondence of various kind have proved to be quite helpful in the study of
the complexity of the CSP. While we were unable to identify a Galois
correspondence for partial clones closed under max-implementation and
max-quantification, we obtain such results for slightly different type of
closure operators, k-existential quantification. This type of quantifiers are
known as counting quantifiers in model theory, and often used to enhance first
order logic languages. We characterize partial clones of relations closed under
k-existential quantification as sets of relations invariant under a set of
partial functions that satisfy the condition of k-subset surjectivity. Finally,
we give a description of Boolean max-co-clones, that is, sets of relations on
{0,1} closed under max-implementations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1101.4733v1,An Algebra of Synchronous Scheduling Interfaces,"In this paper we propose an algebra of synchronous scheduling interfaces
which combines the expressiveness of Boolean algebra for logical and functional
behaviour with the min-max-plus arithmetic for quantifying the non-functional
aspects of synchronous interfaces. The interface theory arises from a
realisability interpretation of intuitionistic modal logic (also known as
Curry-Howard-Isomorphism or propositions-as-types principle). The resulting
algebra of interface types aims to provide a general setting for specifying
type-directed and compositional analyses of worst-case scheduling bounds. It
covers synchronous control flow under concurrent, multi-processing or
multi-threading execution and permits precise statements about exactness and
coverage of the analyses supporting a variety of abstractions. The paper
illustrates the expressiveness of the algebra by way of some examples taken
from network flow problems, shortest-path, task scheduling and worst-case
reaction times in synchronous programming.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1410.4235v1,"Convolution, Separation and Concurrency","A notion of convolution is presented in the context of formal power series
together with lifting constructions characterising algebras of such series,
which usually are quantales. A number of examples underpin the universality of
these constructions, the most prominent ones being separation logics, where
convolution is separating conjunction in an assertion quantale; interval
logics, where convolution is the chop operation; and stream interval functions,
where convolution is used for analysing the trajectories of dynamical or
real-time systems. A Hoare logic is constructed in a generic fashion on the
power series quantale, which applies to each of these examples. In many cases,
commutative notions of convolution have natural interpretations as concurrency
operations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0307058v1,Efficient Instrumentation for Performance Profiling,"Performance profiling consists of tracing a software system during execution
and then analyzing the obtained traces. However, traces themselves affect the
performance of the system distorting its execution. Therefore, there is a need
to minimize the effect of the tracing on the underlying system's performance.
To achieve this, the trace set needs to be optimized according to the
performance profiling problem being solved. Our position is that such
minimization can be achieved only by adding the software trace design and
implementation to the overall software development process. In such a process,
the performance analyst supplies the knowledge of performance measurement
requirements, while the software developer supplies the knowledge of the
software. Both of these are needed for an optimal trace placement.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.04530v1,"Discovering Textual Structures: Generative Grammar Induction using
  Template Trees","Natural language generation provides designers with methods for automatically
generating text, e.g. for creating summaries, chatbots and game content. In
practise, text generators are often either learned and hard to interpret, or
created by hand using techniques such as grammars and templates. In this paper,
we introduce a novel grammar induction algorithm for learning interpretable
grammars for generative purposes, called Gitta. We also introduce the novel
notion of template trees to discover latent templates in corpora to derive
these generative grammars. By using existing human-created grammars, we found
that the algorithm can reasonably approximate these grammars using only a few
examples. These results indicate that Gitta could be used to automatically
learn interpretable and easily modifiable grammars, and thus provide a stepping
stone for human-machine co-creation of generative models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1205.4672v1,"Timing and Code Size Optimization on Achieving Full Parallelism in
  Uniform Nested Loops","Multidimensional Retiming is one of the most important optimization
techniques to improve timing parameters of nested loops. It consists in
exploring the iterative and recursive structures of loops to redistribute
computation nodes on cycle periods, and thus to achieve full parallelism.
However, this technique introduces a large overhead in a loop generation due to
the loop transformation. The provided solutions are generally characterized by
an important cycle number and a great code size. It represents the most
limiting factors while implementing them in embedded systems. In this paper, we
present a new Multidimensional Retiming technique, called ""Optimal
Multidimensional Retiming"" (OMDR). It reveals the timing and data dependency
characteristics of nodes, to minimize the overhead. The experimental results
show that the average improvement on the execution time of the nested loops by
our technique is 19.31% compared to the experiments provided by an existent
Multidimensional Retiming Technique. The average code size is reduced by 43.53%
compared to previous experiments.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0205053v1,"Sotto Voce: Exploring the Interplay of Conversation and Mobile Audio
  Spaces","In addition to providing information to individual visitors, electronic
guidebooks have the potential to facilitate social interaction between visitors
and their companions. However, many systems impede visitor interaction. By
contrast, our electronic guidebook, Sotto Voce, has social interaction as a
primary design goal. The system enables visitors to share audio information -
specifically, they can hear each other's guidebook activity using a
technologically mediated audio eavesdropping mechanism. We conducted a study of
visitors using Sotto Voce while touring a historic house. The results indicate
that visitors are able to use the system effectively, both as a conversational
resource and as an information appliance. More surprisingly, our results
suggest that the technologically mediated audio often cohered the visitors'
conversation and activity to a far greater degree than audio delivered through
the open air.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/cs/0404013v1,Tycoon: A Distributed Market-based Resource Allocation System,"P2P clusters like the Grid and PlanetLab enable in principle the same
statistical multiplexing efficiency gains for computing as the Internet
provides for networking. The key unsolved problem is resource allocation.
Existing solutions are not economically efficient and require high latency to
acquire resources. We designed and implemented Tycoon, a market based
distributed resource allocation system based on an Auction Share scheduling
algorithm. Preliminary results show that Tycoon achieves low latency and high
fairness while providing incentives for truth-telling on the part of strategic
users.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1608.01835v3,Stable-Unstable Semantics: Beyond NP with Normal Logic Programs,"Standard answer set programming (ASP) targets at solving search problems from
the first level of the polynomial time hierarchy (PH). Tackling search problems
beyond NP using ASP is less straightforward. The class of disjunctive logic
programs offers the most prominent way of reaching the second level of the PH,
but encoding respective hard problems as disjunctive programs typically
requires sophisticated techniques such as saturation or meta-interpretation.
The application of such techniques easily leads to encodings that are
inaccessible to non-experts. Furthermore, while disjunctive ASP solvers often
rely on calls to a (co-)NP oracle, it may be difficult to detect from the input
program where the oracle is being accessed. In other formalisms, such as
Quantified Boolean Formulas (QBFs), the interface to the underlying oracle is
more transparent as it is explicitly recorded in the quantifier prefix of a
formula. On the other hand, ASP has advantages over QBFs from the modeling
perspective. The rich high-level languages such as ASP-Core-2 offer a wide
variety of primitives that enable concise and natural encodings of search
problems. In this paper, we present a novel logic programming--based modeling
paradigm that combines the best features of ASP and QBFs. We develop so-called
combined logic programs in which oracles are directly cast as (normal) logic
programs themselves. Recursive incarnations of this construction enable logic
programming on arbitrarily high levels of the PH. We develop a proof-of-concept
implementation for our new paradigm.
  This paper is under consideration for acceptance in TPLP.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.00295v1,Iranis: A Large-scale Dataset of Farsi License Plate Characters,"Providing huge amounts of data is a fundamental demand when dealing with Deep
Neural Networks (DNNs). Employing these algorithms to solve computer vision
problems resulted in the advent of various image datasets to feed the most
common visual imagery deep structures, known as Convolutional Neural Networks
(CNNs). In this regard, some datasets can be found that contain hundreds or
even thousands of images for license plate detection and optical character
recognition purposes. However, no publicly available image dataset provides
such data for the recognition of Farsi characters used in car license plates.
The gap has to be filled due to the numerous advantages of developing accurate
deep learning-based systems for law enforcement and surveillance purposes. This
paper introduces a large-scale dataset that includes images of numbers and
characters used in Iranian car license plates. The dataset, named Iranis,
contains more than 83,000 images of Farsi numbers and letters collected from
real-world license plate images captured by various cameras. The variety of
instances in terms of camera shooting angle, illumination, resolution, and
contrast make the dataset a proper choice for training DNNs. Dataset images are
manually annotated for object detection and image classification. Finally, and
to build a baseline for Farsi character recognition, the paper provides a
performance analysis using a YOLO v.3 object detector.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.09851v1,Clique Cover of Graphs with Bounded Degeneracy,"Structural parameters of graph (such as degeneracy and arboricity) had rarely
been considered when designing algorithms for $\textit{(edge) clique cover}$
problems. Taking degeneracy of graph into account, we present a greedy
framework and two fixed-parameter tractable algorithms for $\textit{clique
cover}$ problems. We introduce a set theoretic concept and demonstrate its use
in the computations of different objectives of $\textit{clique cover}$.
Furthermore, we show efficacy of our algorithms in practice.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0304021v1,Model Checking for a Class of Weighted Automata,"A large number of different model checking approaches has been proposed
during the last decade. The different approaches are applicable to different
model types including untimed, timed, probabilistic and stochastic models. This
paper presents a new framework for model checking techniques which includes
some of the known approaches, but enlarges the class of models for which model
checking can be applied to the general class of weighted automata. The approach
allows an easy adaption of model checking to models which have not been
considered yet for this purpose. Examples for those new model types for which
model checking can be applied are max/plus or min/plus automata which are well
established models to describe different forms of dynamic systems and
optimization problems. In this context, model checking can be used to verify
temporal or quantitative properties of a system. The paper first presents
briefly our class of weighted automata, as a very general model type. Then
Valued Computational Tree Logic (CTL$) is introduced as a natural extension of
the well known branching time logic CTL. Afterwards, algorithms to check a
weighted automaton according to a CTL$ formula are presented. As a last result,
a bisimulation is presented for weighted automata and for CTL$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.02273v1,"VLEngagement: A Dataset of Scientific Video Lectures for Evaluating
  Population-based Engagement","With the emergence of e-learning and personalised education, the production
and distribution of digital educational resources have boomed. Video lectures
have now become one of the primary modalities to impart knowledge to masses in
the current digital age. The rapid creation of video lecture content challenges
the currently established human-centred moderation and quality assurance
pipeline, demanding for more efficient, scalable and automatic solutions for
managing learning resources. Although a few datasets related to engagement with
educational videos exist, there is still an important need for data and
research aimed at understanding learner engagement with scientific video
lectures. This paper introduces VLEngagement, a novel dataset that consists of
content-based and video-specific features extracted from publicly available
scientific video lectures and several metrics related to user engagement. We
introduce several novel tasks related to predicting and understanding
context-agnostic engagement in video lectures, providing preliminary baselines.
This is the largest and most diverse publicly available dataset to our
knowledge that deals with such tasks. The extraction of Wikipedia topic-based
features also allows associating more sophisticated Wikipedia based features to
the dataset to improve the performance in these tasks. The dataset, helper
tools and example code snippets are available publicly at
https://github.com/sahanbull/context-agnostic-engagement",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1011.5435v1,A system for coarse-grained location-based synchronisation,"This paper describes a system for supporting coarse-grained location-based
synchronisation. This type of synchronisation may occur when people need only
some awareness about the location of others within the specific context of an
on-going activity. We have identified a number of reference scenarios for this
type of synchronisation and we have implemented and deployed a prototype to
evaluate the type of support provided. The results of the evaluation suggest a
good acceptance of the overall concept, indicating that this might be a
valuable approach for many of the indicated scenarios, possibly replacing or
complementing existing synchronisation practices.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1
http://arxiv.org/abs/1301.5293v2,Approximately counting semismooth integers,"An integer $n$ is $(y,z)$-semismooth if $n=pm$ where $m$ is an integer with
all prime divisors $\le y$ and $p$ is 1 or a prime $\le z$. arge quantities of
semismooth integers are utilized in modern integer factoring algorithms, such
as the number field sieve, that incorporate the so-called large prime variant.
Thus, it is useful for factoring practitioners to be able to estimate the value
of $\Psi(x,y,z)$, the number of $(y,z)$-semismooth integers up to $x$, so that
they can better set algorithm parameters and minimize running times, which
could be weeks or months on a cluster supercomputer. In this paper, we explore
several algorithms to approximate $\Psi(x,y,z)$ using a generalization of
Buchstab's identity with numeric integration.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1309.5130v1,A Comparison of Well-Quasi Orders on Trees,"Well-quasi orders such as homeomorphic embedding are commonly used to ensure
termination of program analysis and program transformation, in particular
supercompilation.
  We compare eight well-quasi orders on how discriminative they are and their
computational complexity. The studied well-quasi orders comprise two very
simple examples, two examples from literature on supercompilation and four new
proposed by the author.
  We also discuss combining several well-quasi orders to get well-quasi orders
of higher discriminative power. This adds 19 more well-quasi orders to the
list.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.07056v1,"Emulating Human Conversations using Convolutional Neural Network-based
  IR","Conversational agents (""bots"") are beginning to be widely used in
conversational interfaces. To design a system that is capable of emulating
human-like interactions, a conversational layer that can serve as a fabric for
chat-like interaction with the agent is needed. In this paper, we introduce a
model that employs Information Retrieval by utilizing convolutional deep
structured semantic neural network-based features in the ranker to present
human-like responses in ongoing conversation with a user. In conversations,
accounting for context is critical to the retrieval model; we show that our
context-sensitive approach using a Convolutional Deep Structured Semantic Model
(cDSSM) with character trigrams significantly outperforms several conventional
baselines in terms of the relevance of responses retrieved.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0809.0922v3,Superposition for Fixed Domains,"Superposition is an established decision procedure for a variety of
first-order logic theories represented by sets of clauses. A satisfiable
theory, saturated by superposition, implicitly defines a minimal term-generated
model for the theory. Proving universal properties with respect to a saturated
theory directly leads to a modification of the minimal model's term-generated
domain, as new Skolem functions are introduced. For many applications, this is
not desired.
  Therefore, we propose the first superposition calculus that can explicitly
represent existentially quantified variables and can thus compute with respect
to a given domain. This calculus is sound and refutationally complete in the
limit for a first-order fixed domain semantics. For saturated Horn theories and
classes of positive formulas, we can even employ the calculus to prove
properties of the minimal model itself, going beyond the scope of known
superposition-based approaches.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.00534v1,"Empirical evaluation of shallow and deep learning classifiers for Arabic
  sentiment analysis","This work presents a detailed comparison of the performance of deep learning
models such as convolutional neural networks (CNN), long short-term memory
(LSTM), gated recurrent units (GRU), their hybrids, and a selection of shallow
learning classifiers for sentiment analysis of Arabic reviews. Additionally,
the comparison includes state-of-the-art models such as the transformer
architecture and the araBERT pre-trained model. The datasets used in this study
are multi-dialect Arabic hotel and book review datasets, which are some of the
largest publicly available datasets for Arabic reviews. Results showed deep
learning outperforming shallow learning for binary and multi-label
classification, in contrast with the results of similar work reported in the
literature. This discrepancy in outcome was caused by dataset size as we found
it to be proportional to the performance of deep learning models. The
performance of deep and shallow learning techniques was analyzed in terms of
accuracy and F1 score. The best performing shallow learning technique was
Random Forest followed by Decision Tree, and AdaBoost. The deep learning models
performed similarly using a default embedding layer, while the transformer
model performed best when augmented with araBERT.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0002016v3,SLT-Resolution for the Well-Founded Semantics,"Global SLS-resolution and SLG-resolution are two representative mechanisms
for top-down evaluation of the well-founded semantics of general logic
programs. Global SLS-resolution is linear for query evaluation but suffers from
infinite loops and redundant computations. In contrast, SLG-resolution resolves
infinite loops and redundant computations by means of tabling, but it is not
linear. The principal disadvantage of a non-linear approach is that it cannot
be implemented using a simple, efficient stack-based memory structure nor can
it be easily extended to handle some strictly sequential operators such as cuts
in Prolog.
  In this paper, we present a linear tabling method, called SLT-resolution, for
top-down evaluation of the well-founded semantics. SLT-resolution is a
substantial extension of SLDNF-resolution with tabling. Its main features
include: (1) It resolves infinite loops and redundant computations while
preserving the linearity. (2) It is terminating, and sound and complete w.r.t.
the well-founded semantics for programs with the bounded-term-size property
with non-floundering queries. Its time complexity is comparable with
SLG-resolution and polynomial for function-free logic programs. (3) Because of
its linearity for query evaluation, SLT-resolution bridges the gap between the
well-founded semantics and standard Prolog implementation techniques. It can be
implemented by an extension to any existing Prolog abstract machines such as
WAM or ATOAM.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.08264v4,"M2Lens: Visualizing and Explaining Multimodal Models for Sentiment
  Analysis","Multimodal sentiment analysis aims to recognize people's attitudes from
multiple communication channels such as verbal content (i.e., text), voice, and
facial expressions. It has become a vibrant and important research topic in
natural language processing. Much research focuses on modeling the complex
intra- and inter-modal interactions between different communication channels.
However, current multimodal models with strong performance are often
deep-learning-based techniques and work like black boxes. It is not clear how
models utilize multimodal information for sentiment predictions. Despite recent
advances in techniques for enhancing the explainability of machine learning
models, they often target unimodal scenarios (e.g., images, sentences), and
little research has been done on explaining multimodal models. In this paper,
we present an interactive visual analytics system, M2Lens, to visualize and
explain multimodal models for sentiment analysis. M2Lens provides explanations
on intra- and inter-modal interactions at the global, subset, and local levels.
Specifically, it summarizes the influence of three typical interaction types
(i.e., dominance, complement, and conflict) on the model predictions. Moreover,
M2Lens identifies frequent and influential multimodal features and supports the
multi-faceted exploration of model behaviors from language, acoustic, and
visual modalities. Through two case studies and expert interviews, we
demonstrate our system can help users gain deep insights into the multimodal
models for sentiment analysis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0803.4253v1,Combinatorial Explorations in Su-Doku,"Su-Doku, a popular combinatorial puzzle, provides an excellent testbench for
heuristic explorations. Several interesting questions arise from its
deceptively simple set of rules. How many distinct Su-Doku grids are there? How
to find a solution to a Su-Doku puzzle? Is there a unique solution to a given
Su-Doku puzzle? What is a good estimation of a puzzle's difficulty? What is the
minimum puzzle size (the number of ""givens"")?
  This paper explores how these questions are related to the well-known
alldifferent constraint which emerges in a wide variety of Constraint
Satisfaction Problems (CSP) and compares various algorithmic approaches based
on different formulations of Su-Doku.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0311025v1,"Fine-Grained Authorization for Job Execution in the Grid: Design and
  Implementation","In this paper we describe our work on enabling fine-grained authorization for
resource usage and management. We address the need of virtual organizations to
enforce their own polices in addition to those of the resource owners, in
regard to both resource consumption and job management. To implement this
design, we propose changes and extensions to the Globus Toolkit's version 2
resource management mechanism. We describe the prototype and the policy
language that we designed to express fine-grained policies, and we present an
analysis of our solution.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1102.4423v1,Solving k-Set Agreement with Stable Skeleton Graphs,"In this paper we consider the k-set agreement problem in distributed
message-passing systems using a round-based approach: Both synchrony of
communication and failures are captured just by means of the messages that
arrive within a round, resulting in round-by-round communication graphs that
can be characterized by simple communication predicates. We introduce the weak
communication predicate PSources(k) and show that it is tight for k-set
agreement, in the following sense: We (i) prove that there is no algorithm for
solving (k-1)-set agreement in systems characterized by PSources(k), and (ii)
present a novel distributed algorithm that achieves k-set agreement in runs
where PSources(k) holds. Our algorithm uses local approximations of the stable
skeleton graph, which reflects the underlying perpetual synchrony of a run. We
prove that this approximation is correct in all runs, regardless of the
communication predicate, and show that graph-theoretic properties of the stable
skeleton graph can be used to solve k-set agreement if PSources(k) holds.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1711.00795v1,"Talos: Neutralizing Vulnerabilities with Security Workarounds for Rapid
  Response","Considerable delays often exist between the discovery of a vulnerability and
the issue of a patch. One way to mitigate this window of vulnerability is to
use a configuration workaround, which prevents the vulnerable code from being
executed at the cost of some lost functionality -- but only if one is
available. Since program configurations are not specifically designed to
mitigate software vulnerabilities, we find that they only cover 25.2% of
vulnerabilities.
  To minimize patch delay vulnerabilities and address the limitations of
configuration workarounds, we propose Security Workarounds for Rapid Response
(SWRRs), which are designed to neutralize security vulnerabilities in a timely,
secure, and unobtrusive manner. Similar to configuration workarounds, SWRRs
neutralize vulnerabilities by preventing vulnerable code from being executed at
the cost of some lost functionality. However, the key difference is that SWRRs
use existing error-handling code within programs, which enables them to be
mechanically inserted with minimal knowledge of the program and minimal
developer effort. This allows SWRRs to achieve high coverage while still being
fast and easy to deploy.
  We have designed and implemented Talos, a system that mechanically
instruments SWRRs into a given program, and evaluate it on five popular Linux
server programs. We run exploits against 11 real-world software vulnerabilities
and show that SWRRs neutralize the vulnerabilities in all cases. Quantitative
measurements on 320 SWRRs indicate that SWRRs instrumented by Talos can
neutralize 75.1% of all potential vulnerabilities and incur a loss of
functionality similar to configuration workarounds in 71.3% of those cases. Our
overall conclusion is that automatically generated SWRRs can safely mitigate
2.1x more vulnerabilities, while only incurring a loss of functionality
comparable to that of traditional configuration workarounds.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0807.4478v1,"An Image-Based Sensor System for Autonomous Rendez-Vous with
  Uncooperative Satellites","In this paper are described the image processing algorithms developed by
SENER, Ingenieria y Sistemas to cope with the problem of image-based,
autonomous rendez-vous (RV) with an orbiting satellite. The methods developed
have a direct application in the OLEV (Orbital Life Extension Extension
Vehicle) mission. OLEV is a commercial mission under development by a
consortium formed by Swedish Space Corporation, Kayser-Threde and SENER, aimed
to extend the operational life of geostationary telecommunication satellites by
supplying them control, navigation and guidance services. OLEV is planned to
use a set of cameras to determine the angular position and distance to the
client satellite during the complete phases of rendez-vous and docking, thus
enabling the operation with satellites not equipped with any specific
navigational aid to provide support during the approach. The ability to operate
with un-equipped client satellites significantly expands the range of
applicability of the system under development, compared to other competing
video technologies already tested in previous spatial missions, such as the
ones described here below.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1612.06018v2,Self-Correcting Models for Model-Based Reinforcement Learning,"When an agent cannot represent a perfectly accurate model of its
environment's dynamics, model-based reinforcement learning (MBRL) can fail
catastrophically. Planning involves composing the predictions of the model;
when flawed predictions are composed, even minor errors can compound and render
the model useless for planning. Hallucinated Replay (Talvitie 2014) trains the
model to ""correct"" itself when it produces errors, substantially improving MBRL
with flawed models. This paper theoretically analyzes this approach,
illuminates settings in which it is likely to be effective or ineffective, and
presents a novel error bound, showing that a model's ability to self-correct is
more tightly related to MBRL performance than one-step prediction error. These
results inspire an MBRL algorithm for deterministic MDPs with performance
guarantees that are robust to model class limitations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1411.5635v2,Justifying Answer Sets using Argumentation,"An answer set is a plain set of literals which has no further structure that
would explain why certain literals are part of it and why others are not. We
show how argumentation theory can help to explain why a literal is or is not
contained in a given answer set by defining two justification methods, both of
which make use of the correspondence between answer sets of a logic program and
stable extensions of the Assumption-Based Argumentation (ABA) framework
constructed from the same logic program. Attack Trees justify a literal in
argumentation-theoretic terms, i.e. using arguments and attacks between them,
whereas ABA-Based Answer Set Justifications express the same justification
structure in logic programming terms, that is using literals and their
relationships. Interestingly, an ABA-Based Answer Set Justification corresponds
to an admissible fragment of the answer set in question, and an Attack Tree
corresponds to an admissible fragment of the stable extension corresponding to
this answer set.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0607016v2,An Analysis of Arithmetic Constraints on Integer Intervals,"Arithmetic constraints on integer intervals are supported in many constraint
programming systems. We study here a number of approaches to implement
constraint propagation for these constraints. To describe them we introduce
integer interval arithmetic. Each approach is explained using appropriate proof
rules that reduce the variable domains. We compare these approaches using a set
of benchmarks. For the most promising approach we provide results that
characterize the effect of constraint propagation. This is a full version of
our earlier paper, cs.PL/0403016.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1904.11738v1,"Deep-IRT: Make Deep Learning Based Knowledge Tracing Explainable Using
  Item Response Theory","Deep learning based knowledge tracing model has been shown to outperform
traditional knowledge tracing model without the need for human-engineered
features, yet its parameters and representations have long been criticized for
not being explainable. In this paper, we propose Deep-IRT which is a synthesis
of the item response theory (IRT) model and a knowledge tracing model that is
based on the deep neural network architecture called dynamic key-value memory
network (DKVMN) to make deep learning based knowledge tracing explainable.
Specifically, we use the DKVMN model to process the student's learning
trajectory and estimate the student ability level and the item difficulty level
over time. Then, we use the IRT model to estimate the probability that a
student will answer an item correctly using the estimated student ability and
the item difficulty. Experiments show that the Deep-IRT model retains the
performance of the DKVMN model, while it provides a direct psychological
interpretation of both students and items.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1112.5605v1,A Study of CAPTCHAs for Securing Web Services,"Atomizing various Web activities by replacing human to human interactions on
the Internet has been made indispensable due to its enormous growth. However,
bots also known as Web-bots which have a malicious intend and pretending to be
humans pose a severe threat to various services on the Internet that implicitly
assume a human interaction. Accordingly, Web service providers before allowing
access to such services use various Human Interaction Proof's (HIPs) to
authenticate that the user is a human and not a bot. Completely Automated
Public Turing test to tell Computers and Humans Apart (CAPTCHA) is a class of
HIPs tests and are based on Artificial Intelligence. These tests are easier for
humans to qualify and tough for bots to simulate. Several Web services use
CAPTCHAs as a defensive mechanism against automated Web-bots. In this paper, we
review the existing CAPTCHA schemes that have been proposed or are being used
to protect various Web services. We classify them in groups and compare them
with each other in terms of security and usability. We present general method
used to generate and break text-based and image-based CAPTCHAs. Further, we
discuss various security and usability issues in CAPTCHA design and provide
guidelines for improving their robustness and usability.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0
http://arxiv.org/abs/2105.09114v1,"Explainable Tsetlin Machine framework for fake news detection with
  credibility score assessment","The proliferation of fake news, i.e., news intentionally spread for
misinformation, poses a threat to individuals and society. Despite various
fact-checking websites such as PolitiFact, robust detection techniques are
required to deal with the increase in fake news. Several deep learning models
show promising results for fake news classification, however, their black-box
nature makes it difficult to explain their classification decisions and
quality-assure the models. We here address this problem by proposing a novel
interpretable fake news detection framework based on the recently introduced
Tsetlin Machine (TM). In brief, we utilize the conjunctive clauses of the TM to
capture lexical and semantic properties of both true and fake news text.
Further, we use the clause ensembles to calculate the credibility of fake news.
For evaluation, we conduct experiments on two publicly available datasets,
PolitiFact and GossipCop, and demonstrate that the TM framework significantly
outperforms previously published baselines by at least $5\%$ in terms of
accuracy, with the added benefit of an interpretable logic-based
representation. Further, our approach provides higher F1-score than BERT and
XLNet, however, we obtain slightly lower accuracy. We finally present a case
study on our model's explainability, demonstrating how it decomposes into
meaningful words and their negations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.07510v2,Plug-and-Play gradient-based denoisers applied to CT image enhancement,"Blur and noise corrupting Computed Tomography (CT) images can hide or distort
small but important details, negatively affecting the diagnosis. In this paper,
we present a novel gradient-based Plug-and-Play algorithm, constructed on the
Half-Quadratic Splitting scheme, and we apply it to restore CT images. In
particular, we consider different schemes encompassing external and internal
denoisers as priors, defined on the image gradient domain. The internal prior
is based on the Total Variation functional. The external denoiser is
implemented by a deep Convolutional Neural Network (CNN) trained on the
gradient domain (and not on the image one, as in state-of-the-art works). We
also prove a general fixed-point convergence theorem under weak assumptions on
both internal and external denoisers. The experiments confirm the effectiveness
of the proposed framework in restoring blurred noisy CT images, both in
simulated and real medical settings. The achieved enhancements in the restored
images are really remarkable, if compared to the results of many
state-of-the-art methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.12513v3,On gray-box modeling for virtual flow metering,"A virtual flow meter (VFM) enables continuous prediction of flow rates in
petroleum production systems. The predicted flow rates may aid the daily
control and optimization of a petroleum asset. Gray-box modeling is an approach
that combines mechanistic and data-driven modeling. The objective is to create
a computationally feasible VFM for use in real-time applications, with high
prediction accuracy and scientifically consistent behavior. This article
investigates five different gray-box model types in an industrial case study
using real, historical production data from 10 petroleum wells, spanning at
most four years of production. The results are diverse with an oil flow rate
prediction error in the range of 1.8%-40.6%. Further, the study casts light
upon the nontrivial task of balancing learning from both physics and data.
Consequently, providing general recommendations towards the suitability of
different hybrid models is challenging. Nevertheless, the results are promising
and indicate that gray-box VFMs may reduce the prediction error of a
mechanistic VFM while remaining scientifically consistent. The findings
motivate further experimentation with gray-box VFM models and suggest several
future research directions to improve upon the performance and scientific
consistency.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1607.06344v3,Solving equations and optimization problems with uncertainty,"We study the problem of detecting zeros of continuous functions that are
known only up to an error bound, extending the earlier theoretical work with
explicit algorithms and experiments with an implementation. More formally, the
robustness of zero of a continuous map $f: X\to \mathbb{R}^n$ is the maximal
$r>0$ such that each $g:X\to\mathbb{R}^n$ with $\|f-g\|_\infty\le r$ has a
zero. We develop and implement an efficient algorithm approximating the
robustness of zero. Further, we show how to use the algorithm for approximating
worst-case optima in optimization problems in which the feasible domain is
defined by equations that are only known approximately.
  An important ingredient is an algorithm for deciding the topological
extension problem based on computing cohomological obstructions to
extendability and their persistence. We describe an explicit algorithm for the
primary and secondary obstruction, two stages of a sequence of algorithms with
increasing complexity. We provide experimental evidence that for random
Gaussian fields, the primary obstruction---a much less computationally
demanding test than the secondary obstruction---is typically sufficient for
approximating robustness of zero.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.12601v2,"Vacuum-formed 3D printed electronics: fabrication of thin, rigid and
  free-form interactive surfaces","Vacuum-forming is a common manufacturing technique for constructing thin
plastic shell products by pressing heated plastic sheets onto a mold using
atmospheric pressure. Vacuum-forming is ubiquitous in packaging and casing
products in industry spanning fast moving consumer goods to connected devices.
Integrating advanced functionality, which may include sensing, computation and
communication, within thin structures is desirable for various next-generation
interactive devices. Hybrid additive manufacturing techniques like
thermoforming are becoming popular for prototyping freeform electronics given
its design flexibility, speed and cost-effectiveness. In this paper, we present
a new hybrid method for constructing thin, rigid and free-form interconnected
surfaces via fused deposition modelling (FDM) 3D printing and vacuum-forming.
While 3D printing a mold for vacuum-forming has been explored by many,
utilising 3D printing to construct sheet materials has remains unexplored. 3D
printing the sheet material allows embedding conductive traces within thin
layers of the substrate, which can be vacuum-formed but remain conductive and
insulated. We characterise the behaviour of the vacuum-formed 3D printed sheet,
analyse the electrical performance of 3D printed traces after vacuum-forming,
and showcase a range of examples constructed using the technique. We
demonstrate a new design interface specifically for designing conformal
interconnects, which allows designers to draw conductive patterns in 3D and
export pre-distorted sheet models ready to be 3D printed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1706.09715v1,Constrained Type Families,"We present an approach to support partiality in type-level computation
without compromising expressiveness or type safety. Existing frameworks for
type-level computation either require totality or implicitly assume it. For
example, type families in Haskell provide a powerful, modular means of defining
type-level computation. However, their current design implicitly assumes that
type families are total, introducing nonsensical types and significantly
complicating the metatheory of type families and their extensions. We propose
an alternative design, using qualified types to pair type-level computations
with predicates that capture their domains. Our approach naturally captures the
intuitive partiality of type families, simplifying their metatheory. As
evidence, we present the first complete proof of consistency for a language
with closed type families.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1310.1177v2,"Clustering on Multiple Incomplete Datasets via Collective Kernel
  Learning","Multiple datasets containing different types of features may be available for
a given task. For instance, users' profiles can be used to group users for
recommendation systems. In addition, a model can also use users' historical
behaviors and credit history to group users. Each dataset contains different
information and suffices for learning. A number of clustering algorithms on
multiple datasets were proposed during the past few years. These algorithms
assume that at least one dataset is complete. So far as we know, all the
previous methods will not be applicable if there is no complete dataset
available. However, in reality, there are many situations where no dataset is
complete. As in building a recommendation system, some new users may not have a
profile or historical behaviors, while some may not have a credit history.
Hence, no available dataset is complete. In order to solve this problem, we
propose an approach called Collective Kernel Learning to infer hidden sample
similarity from multiple incomplete datasets. The idea is to collectively
completes the kernel matrices of incomplete datasets by optimizing the
alignment of the shared instances of the datasets. Furthermore, a clustering
algorithm is proposed based on the kernel matrix. The experiments on both
synthetic and real datasets demonstrate the effectiveness of the proposed
approach. The proposed clustering algorithm outperforms the comparison
algorithms by as much as two times in normalized mutual information.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.05519v1,Guiding Next-Step Hint Generation Using Automated Tests,"Learning basic programming with Scratch can be hard for novices and tutors
alike: Students may not know how to advance when solving a task, teachers may
face classrooms with many raised hands at a time, and the problem is
exacerbated when novices are on their own in online or virtual lessons. It is
therefore desirable to generate next-step hints automatically to provide
individual feedback for students who are stuck, but current approaches rely on
the availability of multiple hand-crafted or hand-selected sample solutions
from which to draw valid hints, and have not been adapted for Scratch.
Automated testing provides an opportunity to automatically select suitable
candidate solutions for hint generation, even from a pool of student solutions
using different solution approaches and varying in quality. In this paper we
present Catnip, the first next-step hint generation approach for Scratch, which
extends existing data-driven hint generation approaches with automated testing.
Evaluation of Catnip on a dataset of student Scratch programs demonstrates that
the generated hints point towards functional improvements, and the use of
automated tests allows the hints to be better individualized for the chosen
solution path.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1701.04928v1,Bringing Impressionism to Life with Neural Style Transfer in Come Swim,"Neural Style Transfer is a striking, recently-developed technique that uses
neural networks to artistically redraw an image in the style of a source style
image. This paper explores the use of this technique in a production setting,
applying Neural Style Transfer to redraw key scenes in 'Come Swim' in the style
of the impressionistic painting that inspired the film. We document how the
technique can be driven within the framework of an iterative creative process
to achieve a desired look, and propose a mapping of the broad parameter space
to a key set of creative controls. We hope that this mapping can provide
insights into priorities for future research.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1712.09710v1,On low for speed oracles,"Relativizing computations of Turing machines to an oracle is a central
concept in the theory of computation, both in complexity theory and in
computability theory(!). Inspired by lowness notions from computability theory,
Allender introduced the concept of ""low for speed"" oracles. An oracle A is low
for speed if relativizing to A has essentially no effect on computational
complexity, meaning that if a decidable language can be decided in time $f(n)$
with access to oracle A, then it can be decided in time poly(f(n)) without any
oracle. The existence of non-computable such A's was later proven by Bayer and
Slaman, who even constructed a computably enumerable one, and exhibited a
number of properties of these oracles as well as interesting connections with
computability theory. In this paper, we pursue this line of research, answering
the questions left by Bayer and Slaman and give further evidence that the
structure of the class of low for speed oracles is a very rich one.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1310.7935v3,The Unreasonable Fundamental Incertitudes Behind Bitcoin Mining,"Bitcoin is a ""crypto currency"", a decentralized electronic payment scheme
based on cryptography which has recently gained excessive popularity.
Scientific research on bitcoin is less abundant. A paper at Financial
Cryptography 2012 conference explains that it is a system which ""uses no fancy
cryptography"", and is ""by no means perfect"". It depends on a well-known
cryptographic standard SHA-256. In this paper we revisit the cryptographic
process which allows one to make money by producing bitcoins. We reformulate
this problem as a Constrained Input Small Output (CISO) hashing problem and
reduce the problem to a pure block cipher problem. We estimate the speed of
this process and we show that the cost of this process is less than it seems
and it depends on a certain cryptographic constant which we estimated to be at
most 1.86. These optimizations enable bitcoin miners to save tens of millions
of dollars per year in electricity bills. Miners who set up mining operations
face many economic incertitudes such as high volatility. In this paper we point
out that there are fundamental incertitudes which depend very strongly on the
bitcoin specification. The energy efficiency of bitcoin miners have already
been improved by a factor of about 10,000, and we claim that further
improvements are inevitable. Better technology is bound to be invented, would
it be quantum miners. More importantly, the specification is likely to change.
A major change have been proposed in May 2013 at Bitcoin conference in San
Diego by Dan Kaminsky. However, any sort of change could be flatly rejected by
the community which have heavily invested in mining with the current
technology. Another question is the reward halving scheme in bitcoin. The
current bitcoin specification mandates a strong 4-year cyclic property. We find
this property totally unreasonable and harmful and explain why and how it needs
to be changed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1503.04018v3,"On the Coverability Problem for Pushdown Vector Addition Systems in One
  Dimension","Does the trace language of a given vector addition system (VAS) intersect
with a given context-free language? This question lies at the heart of several
verification questions involving recursive programs with integer parameters. In
particular, it is equivalent to the coverability problem for VAS that operate
on a pushdown stack. We show decidability in dimension one, based on an
analysis of a new model called grammar-controlled vector addition systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0601010v1,Multi-Map Orbit Hopping Chaotic Stream Cipher,"In this paper we propose a multi-map orbit hopping chaotic stream cipher that
utilizes the idea of spread spectrum mechanism for secure digital
communications and fundamental chaos characteristics of mixing, unpredictable,
and extremely sensitive to initial conditions. The design, key and subkeys, and
detail implementation of the system are addressed. A variable number of well
studied chaotic maps form a map bank. And the key determines how the system
hops between multiple orbits, and it also determines the number of maps, the
number of orbits for each map, and the number of sample points for each orbits.
A detailed example is provided.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1711.01250v3,"The Robustness of LWPP and WPP, with an Application to Graph
  Reconstruction","We show that the counting class LWPP [FFK94] remains unchanged even if one
allows a polynomial number of gap values rather than one. On the other hand, we
show that it is impossible to improve this from polynomially many gap values to
a superpolynomial number of gap values by relativizable proof techniques.
  The first of these results implies that the Legitimate Deck Problem (from the
study of graph reconstruction) is in LWPP (and thus low for PP, i.e., $\rm
PP^{\mbox{Legitimate Deck}} = PP$) if the weakened version of the
Reconstruction Conjecture holds in which the number of nonisomorphic preimages
is assumed merely to be polynomially bounded. This strengthens the 1992 result
of K\""{o}bler, Sch\""{o}ning, and Tor\'{a}n [KST92] that the Legitimate Deck
Problem is in LWPP if the Reconstruction Conjecture holds, and provides
strengthened evidence that the Legitimate Deck Problem is not NP-hard.
  We additionally show on the one hand that our main LWPP robustness result
also holds for WPP, and also holds even when one allows both the rejection- and
acceptance- gap-value targets to simultaneously be polynomial-sized lists; yet
on the other hand, we show that for the #P-based analog of LWPP the behavior
much differs in that, in some relativized worlds, even two target values
already yield a richer class than one value does. Despite that nonrobustness
result for a #P-based class, we show that the #P-based ""exact counting"" class
$\rm C_{=}P$ remains unchanged even if one allows a polynomial number of target
values for the number of accepting paths of the machine.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1509.00016v4,Localization in Seeded PageRank,"Seeded PageRank is an important network analysis tool for identifying and
studying regions nearby a given set of nodes, which are called seeds. The
seeded PageRank vector is the stationary distribution of a random walk that
randomly resets at the seed nodes. Intuitively, this vector is concentrated
nearby the given seeds, but is mathematically non-zero for all nodes in a
connected graph. We study this concentration, or localization, and show a
sublinear upper bound on the number of entries required to approximate seeded
PageRank on all graphs with a natural type of skewed-degree sequence---similar
to those that arise in many real-world networks. Experiments with both
real-world and synthetic graphs give further evidence to the idea that the
degree sequence of a graph has a major influence on the localization behavior
of seeded PageRank. Moreover, we establish that this localization is
non-trivial by showing that complete-bipartite graphs produce seeded PageRank
vectors that cannot be approximated with a sublinear number of non-zeros.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0806.0909v1,Outage and Local Throughput and Capacity of Random Wireless Networks,"Outage probabilities and single-hop throughput are two important performance
metrics that have been evaluated for certain specific types of wireless
networks. However, there is a lack of comprehensive results for larger classes
of networks, and there is no systematic approach that permits the convenient
comparison of the performance of networks with different geometries and levels
of randomness.
  The uncertainty cube is introduced to categorize the uncertainty present in a
network. The three axes of the cube represent the three main potential sources
of uncertainty in interference-limited networks: the node distribution, the
channel gains (fading), and the channel access (set of transmitting nodes). For
the performance analysis, a new parameter, the so-called {\em spatial
contention}, is defined. It measures the slope of the outage probability in an
ALOHA network as a function of the transmit probability $p$ at $p=0$. Outage is
defined as the event that the signal-to-interference ratio (SIR) is below a
certain threshold in a given time slot. It is shown that the spatial contention
is sufficient to characterize outage and throughput in large classes of
wireless networks, corresponding to different positions on the uncertainty
cube. Existing results are placed in this framework, and new ones are derived.
  Further, interpreting the outage probability as the SIR distribution, the
ergodic capacity of unit-distance links is determined and compared to the
throughput achievable for fixed (yet optimized) transmission rates.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1501.05141v2,An Algebra to Merge Heterogeneous Classifiers,"In distributed classification, each learner observes its environment and
deduces a classifier. As a learner has only a local view of its environment,
classifiers can be exchanged among the learners and integrated, or merged, to
improve accuracy. However, the operation of merging is not defined for most
classifiers. Furthermore, the classifiers that have to be merged may be of
different types in settings such as ad-hoc networks in which several
generations of sensors may be creating classifiers. We introduce decision
spaces as a framework for merging possibly different classifiers. We formally
study the merging operation as an algebra, and prove that it satisfies a
desirable set of properties. The impact of time is discussed for the two main
data mining settings. Firstly, decision spaces can naturally be used with
non-stationary distributions, such as the data collected by sensor networks, as
the impact of a model decays over time. Secondly, we introduce an approach for
stationary distributions, such as homogeneous databases partitioned over
different learners, which ensures that all models have the same impact. We also
present a method that uses storage flexibly to achieve different types of decay
for non-stationary distributions. Finally, we show that the algebraic approach
developed for merging can also be used to analyze the behaviour of other
operators.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.08754v3,"Using Minimum Path Cover to Boost Dynamic Programming on DAGs: Co-Linear
  Chaining Extended","Aligning sequencing reads on graph representations of genomes is an important
ingredient of pan-genomics. Such approaches typically find a set of local
anchors that indicate plausible matches between substrings of a read to
subpaths of the graph. These anchor matches are then combined to form a
(semi-local) alignment of the complete read on a subpath. Co-linear chaining is
an algorithmically rigorous approach to combine the anchors. It is a well-known
approach for the case of two sequences as inputs. Here we extend the approach
so that one of the inputs can be a directed acyclic graph (DAGs), e.g. a
splicing graph in transcriptomics or a variant graph in pan-genomics.
  This extension to DAGs turns out to have a tight connection to the minimum
path cover problem, asking for a minimum-cardinality set of paths that cover
all the nodes of a DAG. We study the case when the size $k$ of a minimum path
cover is small, which is often the case in practice. First, we propose an
algorithm for finding a minimum path cover of a DAG $(V,E)$ in $O(k|E|\log|V|)$
time, improving all known time-bounds when $k$ is small and the DAG is not too
dense. Second, we introduce a general technique for extending dynamic
programming (DP) algorithms from sequences to DAGs. This is enabled by our
minimum path cover algorithm, and works by mimicking the DP algorithm for
sequences on each path of the minimum path cover. This technique generally
produces algorithms that are slower than their counterparts on sequences only
by a factor $k$. Our technique can be applied, for example, to the classical
longest increasing subsequence and longest common subsequence problems,
extended to labeled DAGs. Finally, we apply this technique to the co-linear
chaining problem. We also implemented the new co-linear chaining approach.
Experiments on splicing graphs show that the new method is efficient also in
practice.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.08172v1,Algorithms and Complexity for Variants of Covariates Fine Balance,"We study here several variants of the covariates fine balance problem where
we generalize some of these problems and introduce a number of others. We
present here a comprehensive complexity study of the covariates problems
providing polynomial time algorithms, or a proof of NP-hardness. The polynomial
time algorithms described are mostly combinatorial and rely on network flow
techniques. In addition we present several fixed-parameter tractable results
for problems where the number of covariates and the number of levels of each
covariate are seen as a parameter.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1911.00630v2,Predicting Weather Uncertainty with Deep Convnets,"Modern weather forecast models perform uncertainty quantification using
ensemble prediction systems, which collect nonparametric statistics based on
multiple perturbed simulations. To provide accurate estimation, dozens of such
computationally intensive simulations must be run. We show that deep neural
networks can be used on a small set of numerical weather simulations to
estimate the spread of a weather forecast, significantly reducing computational
cost. To train the system, we both modify the 3D U-Net architecture and explore
models that incorporate temporal data. Our models serve as a starting point to
improve uncertainty quantification in current real-time weather forecasting
systems, which is vital for predicting extreme events.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0908.1407v1,"Generalized Analysis of a Distributed Energy Efficient Algorithm for
  Change Detection","An energy efficient distributed Change Detection scheme based on Page's CUSUM
algorithm was presented in \cite{icassp}. In this paper we consider a
nonparametric version of this algorithm. In the algorithm in \cite{icassp},
each sensor runs CUSUM and transmits only when the CUSUM is above some
threshold. The transmissions from the sensors are fused at the physical layer.
The channel is modeled as a Multiple Access Channel (MAC) corrupted with noise.
The fusion center performs another CUSUM to detect the change. In this paper,
we generalize the algorithm to also include nonparametric CUSUM and provide a
unified analysis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.07378v1,CLaRO: a Data-driven CNL for Specifying Competency Questions,"Competency Questions (CQs) for an ontology and similar artefacts aim to
provide insights into the contents of an ontology and to demarcate its scope.
The absence of a controlled natural language, tooling and automation to support
the authoring of CQs has hampered their effective use in ontology development
and evaluation. The few question templates that exists are based on informal
analyses of a small number of CQs and have limited coverage of question types
and sentence constructions. We aim to fill this gap by proposing a
template-based CNL to author CQs, called CLaRO. For its design, we exploited a
new dataset of 234 CQs that had been processed automatically into 106 patterns,
which we analysed and used to design a template-based CNL, with an additional
CNL model and XML serialisation. The CNL was evaluated with a subset of
questions from the original dataset and with two sets of newly sourced CQs. The
coverage of CLaRO, with its 93 main templates and 41 linguistic variants, is
about 90% for unseen questions. CLaRO has the potential to facilitate
streamlining formalising ontology content requirements and, given that about
one third of the competency questions in the test sets turned out to be invalid
questions, assist in writing good questions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1902.08728v2,Designing for Participation and Change in Digital Institutions,"Whether we recognize it or not, the Internet is rife with exciting and
original institutional forms that are transforming social organization on and
offline. Issues of governance in these Internet platforms and other digital
institutions have posed a challenge for software engineers, many of whom have
little exposure to the relevant history or theory of institutional design.
Here, we offer one useful framework with an aim to stimulate dialogue between
computer scientists and political scientists. The dominant guiding practices
for the design of digital institutions to date in human-computer interaction,
computer-supported cooperative work, and the tech industry at large have been
an incentive-focused behavioral engineering paradigm, a collection of
atheoretical approaches such as A/B-testing, and incremental issue-driven
software engineering. One institutional analysis framework that has been useful
in the design of traditional institutions is the body of resource governance
literature known as the ""Ostrom Workshop"". A key finding of this literature
that has yet to be broadly incorporated in the design of many digital
institutions is the importance of including participatory change process
mechanisms in what is called a ""constitutional layer"" of institutional
design---in other words, defining rules that allow and facilitate diverse
stakeholder participation in the ongoing process of institutional design
change. We explore to what extent this consideration is met or could be better
met in three varied cases of digital institutions: cryptocurrencies, cannabis
informatics, and amateur Minecraft server governance. Examining such highly
varied cases allows us to demonstrate the broad relevance of constitutional
layers in many different types of digital institutions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/cs/0208020v1,Using the DIFF Command for Natural Language Processing,"Diff is a software program that detects differences between two data sets and
is useful in natural language processing. This paper shows several examples of
the application of diff. They include the detection of differences between two
different datasets, extraction of rewriting rules, merging of two different
datasets, and the optimal matching of two different data sets. Since diff comes
with any standard UNIX system, it is readily available and very easy to use.
Our studies showed that diff is a practical tool for research into natural
language processing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1908.00041v3,FaVeST: Fast Vector Spherical Harmonic Transforms,"Vector spherical harmonics on the unit sphere of $\mathbb{R}^3$ have broad
applications in geophysics, quantum mechanics and astrophysics. In the
representation of a tangent vector field, one needs to evaluate the expansion
and the Fourier coefficients of vector spherical harmonics. In this paper, we
develop fast algorithms (FaVeST) for vector spherical harmonic transforms on
these evaluations. The forward FaVeST evaluates the Fourier coefficients and
has a computational cost proportional to $N\log \sqrt{N}$ for $N$ number of
evaluation points. The adjoint FaVeST which evaluates a linear combination of
vector spherical harmonics with a degree up to $\sqrt{M}$ for $M$ evaluation
points has cost proportional to $M\log\sqrt{M}$. Numerical examples of
simulated tangent fields illustrate the accuracy, efficiency and stability of
FaVeST.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1204.2712v1,Learning to Rank Query Recommendations by Semantic Similarities,"Logs of the interactions with a search engine show that users often
reformulate their queries. Examining these reformulations shows that
recommendations that precise the focus of a query are helpful, like those based
on expansions of the original queries. But it also shows that queries that
express some topical shift with respect to the original query can help user
access more rapidly the information they need. We propose a method to identify
from the query logs of past users queries that either focus or shift the
initial query topic. This method combines various click-based, topic-based and
session based ranking strategies and uses supervised learning in order to
maximize the semantic similarities between the query and the recommendations,
while at the same diversifying them. We evaluate our method using the
query/click logs of a Japanese web search engine and we show that the
combination of the three methods proposed is significantly better than any of
them taken individually.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1210.6963v5,"Schulze and Ranked-Pairs Voting are Fixed-Parameter Tractable to Bribe,
  Manipulate, and Control","Schulze and ranked-pairs elections have received much attention recently, and
the former has quickly become a quite widely used election system. For many
cases these systems have been proven resistant to bribery, control, or
manipulation, with ranked pairs being particularly praised for being NP-hard
for all three of those. Nonetheless, the present paper shows that with respect
to the number of candidates, Schulze and ranked-pairs elections are
fixed-parameter tractable to bribe, control, and manipulate: we obtain uniform,
polynomial-time algorithms whose degree does not depend on the number of
candidates. We also provide such algorithms for some weighted variants of these
problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0710.5338v1,Weighted Random Popular Matchings,"For a set A of n applicants and a set I of m items, we consider a problem of
computing a matching of applicants to items, i.e., a function M mapping A to I;
here we assume that each applicant $x \in A$ provides a preference list on
items in I. We say that an applicant $x \in A$ prefers an item p than an item q
if p is located at a higher position than q in its preference list, and we say
that x prefers a matching M over a matching M' if x prefers M(x) over M'(x).
For a given matching problem A, I, and preference lists, we say that M is more
popular than M' if the number of applicants preferring M over M' is larger than
that of applicants preferring M' over M, and M is called a popular matching if
there is no other matching that is more popular than M. Here we consider the
situation that A is partitioned into $A_{1},A_{2},...,A_{k}$, and that each
$A_{i}$ is assigned a weight $w_{i}>0$ such that w_{1}>w_{2}>...>w_{k}>0$. For
such a matching problem, we say that M is more popular than M' if the total
weight of applicants preferring M over M' is larger than that of applicants
preferring M' over M, and we call M an k-weighted popular matching if there is
no other matching that is more popular than M. In this paper, we analyze the
2-weighted matching problem, and we show that (lower bound) if
$m/n^{4/3}=o(1)$, then a random instance of the 2-weighted matching problem
with $w_{1} \geq 2w_{2}$ has a 2-weighted popular matching with probability
o(1); and (upper bound) if $n^{4/3}/m = o(1)$, then a random instance of the
2-weighted matching problem with $w_{1} \geq 2w_{2}$ has a 2-weighted popular
matching with probability 1-o(1).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1401.6359v5,Subsumption Checking in Conjunctive Coalgebraic Fixpoint Logics,"While reasoning in a logic extending a complete Boolean basis is coNP-hard,
restricting to conjunctive fragments of modal languages sometimes allows for
tractable reasoning even in the presence of greatest fixpoints. One such
example is the EL family of description logics; here, efficient reasoning is
based on satisfaction checking in suitable small models that characterize
formulas in terms of simulations. It is well-known, though, that not every
conjunctive modal language has a tractable reasoning problem. Natural questions
are then how common such tractable fragments are and how to identify them. In
this work we provide sufficient conditions for tractability in a general way by
considering unlabeled tableau rules for a given modal logic. We work in the
framework of coalgebraic logic as a unifying semantic setting. Apart from
recovering known results for description logics such as EL and FL0, we obtain
new ones for conjunctive fragments of relational and non-relational modal
logics with greatest fixpoints. Most notably we find tractable fragments of
game logic and the alternating-time mu-calculus.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0412066v1,"From Feature Extraction to Classification: A multidisciplinary Approach
  applied to Portuguese Granites","The purpose of this paper is to present a complete methodology based on a
multidisciplinary approach, that goes from the extraction of features till the
classification of a set of different portuguese granites. The set of tools to
extract the features that characterise polished surfaces of the granites is
mainly based on mathematical morphology. The classification methodology is
based on a genetic algorithm capable of search the input feature space used by
the nearest neighbour rule classifier. Results show that is adequate to perform
feature reduction and simultaneous improve the recognition rate. Moreover, the
present methodology represents a robust strategy to understand the proper
nature of the images treated, and their discriminant features. KEYWORDS:
Portuguese grey granites, feature extraction, mathematical morphology, feature
reduction, genetic algorithms, nearest neighbour rule classifiers (k-NNR).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.03761v1,"Exploring the Personal Informatics Analysis Gap: ""There's a Lot of
  Bacon""","Personal informatics research helps people track personal data for the
purposes of self-reflection and gaining self-knowledge. This field, however,
has predominantly focused on the data collection and insight-generation
elements of self-tracking, with less attention paid to flexible data analysis.
As a result, this inattention has led to inflexible analytic pipelines that do
not reflect or support the diverse ways people want to engage with their data.
This paper contributes a review of personal informatics and visualization
research literature to expose a gap in our knowledge for designing flexible
tools that assist people engaging with and analyzing personal data in personal
contexts, what we call the personal informatics analysis gap. We explore this
gap through a multistage longitudinal study on how asthmatics engage with
personal air quality data, and we report how participants: were motivated by
broad and diverse goals; exhibited patterns in the way they explored their
data; engaged with their data in playful ways; discovered new insights through
serendipitous exploration; and were reluctant to use analysis tools on their
own. These results present new opportunities for visual analysis research and
suggest the need for fundamental shifts in how and what we design when
supporting personal data analysis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1111.5189v1,"A Frame Rate Optimization Framework For Improving Continuity In Video
  Streaming","This paper aims to reduce the prebuffering requirements, while maintaining
continuity, for video streaming. Current approaches do this by making use of
adaptive media playout (AMP) to reduce the playout rate. However, this
introduces playout distortion to the viewers and increases the viewing latency.
We approach this by proposing a frame rate optimization framework that adjusts
both the encoder frame generation rate and the decoder playout frame rate.
Firstly, we model this problem as the joint adjustment of the encoder frame
generation interval and the decoder playout frame interval. This model is used
with a discontinuity penalty virtual buffer to track the accumulated difference
between the receiving frame interval and the playout frame interval. We then
apply Lyapunov optimization to the model to systematically derive a pair of
decoupled optimization policies. We show that the occupancy of the
discontinuity penalty virtual buffer is correlated to the video discontinuity
and that this framework produces a very low playout distortion in addition to a
significant reduction in the prebuffering requirements compared to existing
approaches. Secondly, we introduced a delay constraint into the framework by
using a delay accumulator virtual buffer. Simulation results show that the the
delay constrained framework provides a superior tradeoff between the video
quality and the delay introduced compared to the existing approach. Finally, we
analyzed the impact of delayed feedback between the receiver and the sender on
the optimization policies. We show that the delayed feedbacks have a minimal
impact on the optimization policies.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0703112v1,"User-level DSM System for Modern High-Performance Interconnection
  Networks","In this paper, we introduce a new user-level DSM system which has the ability
to directly interact with underlying interconnection networks. The DSM system
provides the application programmer a flexible API to program parallel
applications either using shared memory semantics over physically distributed
memory or to use an efficient remote memory demand paging technique. We also
introduce a new time slice based memory consistency protocol which is used by
the DSM system. We present preliminary results from our implementation on a
small Opteron Linux cluster interconnected over Myrinet.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1602.03924v1,Modeling Human Ad Hoc Coordination,"Whether in groups of humans or groups of computer agents, collaboration is
most effective between individuals who have the ability to coordinate on a
joint strategy for collective action. However, in general a rational actor will
only intend to coordinate if that actor believes the other group members have
the same intention. This circular dependence makes rational coordination
difficult in uncertain environments if communication between actors is
unreliable and no prior agreements have been made. An important normative
question with regard to coordination in these ad hoc settings is therefore how
one can come to believe that other actors will coordinate, and with regard to
systems involving humans, an important empirical question is how humans arrive
at these expectations. We introduce an exact algorithm for computing the
infinitely recursive hierarchy of graded beliefs required for rational
coordination in uncertain environments, and we introduce a novel mechanism for
multiagent coordination that uses it. Our algorithm is valid in any environment
with a finite state space, and extensions to certain countably infinite state
spaces are likely possible. We test our mechanism for multiagent coordination
as a model for human decisions in a simple coordination game using existing
experimental data. We then explore via simulations whether modeling humans in
this way may improve human-agent collaboration.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0403041v1,A Theory of Computation Based on Quantum Logic (I),"The (meta)logic underlying classical theory of computation is Boolean
(two-valued) logic. Quantum logic was proposed by Birkhoff and von Neumann as a
logic of quantum mechanics more than sixty years ago. The major difference
between Boolean logic and quantum logic is that the latter does not enjoy
distributivity in general. The rapid development of quantum computation in
recent years stimulates us to establish a theory of computation based on
quantum logic. The present paper is the first step toward such a new theory and
it focuses on the simplest models of computation, namely finite automata. It is
found that the universal validity of many properties of automata depend heavily
upon the distributivity of the underlying logic. This indicates that these
properties does not universally hold in the realm of quantum logic. On the
other hand, we show that a local validity of them can be recovered by imposing
a certain commutativity to the (atomic) statements about the automata under
consideration. This reveals an essential difference between the classical
theory of computation and the computation theory based on quantum logic.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1411.7542v1,"Scalability of using Restricted Boltzmann Machines for Combinatorial
  Optimization","Estimation of Distribution Algorithms (EDAs) require flexible probability
models that can be efficiently learned and sampled. Restricted Boltzmann
Machines (RBMs) are generative neural networks with these desired properties.
We integrate an RBM into an EDA and evaluate the performance of this system in
solving combinatorial optimization problems with a single objective. We assess
how the number of fitness evaluations and the CPU time scale with problem size
and with problem complexity. The results are compared to the Bayesian
Optimization Algorithm, a state-of-the-art EDA. Although RBM-EDA requires
larger population sizes and a larger number of fitness evaluations, it
outperforms BOA in terms of CPU times, in particular if the problem is large or
complex. RBM-EDA requires less time for model building than BOA. These results
highlight the potential of using generative neural networks for combinatorial
optimization.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.08359v1,End-2-End COVID-19 Detection from Breath & Cough Audio,"Our main contributions are as follows: (I) We demonstrate the first attempt
to diagnose COVID-19 using end-to-end deep learning from a crowd-sourced
dataset of audio samples, achieving ROC-AUC of 0.846; (II) Our model, the
COVID-19 Identification ResNet, (CIdeR), has potential for rapid scalability,
minimal cost and improving performance as more data becomes available. This
could enable regular COVID-19 testing at apopulation scale; (III) We introduce
a novel modelling strategy using a custom deep neural network to diagnose
COVID-19 from a joint breath and cough representation; (IV) We release our four
stratified folds for cross parameter optimisation and validation on a standard
public corpus and details on the models for reproducibility and future
reference.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1207.5211v5,Can Quantum Communication Speed Up Distributed Computation?,"The focus of this paper is on {\em quantum distributed} computation, where we
investigate whether quantum communication can help in {\em speeding up}
distributed network algorithms. Our main result is that for certain fundamental
network problems such as minimum spanning tree, minimum cut, and shortest
paths, quantum communication {\em does not} help in substantially speeding up
distributed algorithms for these problems compared to the classical setting.
  In order to obtain this result, we extend the technique of Das Sarma et al.
[SICOMP 2012] to obtain a uniform approach to prove non-trivial lower bounds
for quantum distributed algorithms for several graph optimization (both exact
and approximate versions) as well as verification problems, some of which are
new even in the classical setting, e.g. tight randomized lower bounds for
Hamiltonian cycle and spanning tree verification, answering an open problem of
Das Sarma et al., and a lower bound in terms of the weight aspect ratio,
matching the upper bounds of Elkin [STOC 2004]. Our approach introduces the
{\em Server model} and {\em Quantum Simulation Theorem} which together provide
a connection between distributed algorithms and communication complexity. The
Server model is the standard two-party communication complexity model augmented
with additional power; yet, most of the hardness in the two-party model is
carried over to this new model. The Quantum Simulation Theorem carries this
hardness further to quantum distributed computing. Our techniques, except the
proof of the hardness in the Server model, require very little knowledge in
quantum computing, and this can help overcoming a usual impediment in proving
bounds on quantum distributed algorithms.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1102.5043v1,"uRbAn: A Multipath Routing based Architecture with Energy and Mobility
  Management for Quality of Service Support in Mobile Ad hoc Networks","Designing a wireless node that supports quality of service (QoS) in a mobile
ad hoc network is a challenging task. In this paper, we propose an architecture
of a wireless node that may be used to form a mobile ad hoc network that
supports QoS. We discuss the core functionalities required for such a node and
how those functionalities can be incorporated. A feature of our architecture is
that the node has the ability to utilize multiple paths, if available, for the
provision of QoS. However, in the absence of multiple paths it can utilize the
resources provided by a single path between the source and the destination. We
follow a modular approach where each module is expanded iteratively. We compare
the features of our architecture with the existing architectures proposed in
the literature. Our architecture has provisions of energy and mobility
management and it can be customized to design a system-on-chip (SoC).",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0202007v1,"Steady State Resource Allocation Analysis of the Stochastic Diffusion
  Search","This article presents the long-term behaviour analysis of Stochastic
Diffusion Search (SDS), a distributed agent-based system for best-fit pattern
matching. SDS operates by allocating simple agents into different regions of
the search space. Agents independently pose hypotheses about the presence of
the pattern in the search space and its potential distortion. Assuming a
compositional structure of hypotheses about pattern matching agents perform an
inference on the basis of partial evidence from the hypothesised solution.
Agents posing mutually consistent hypotheses about the pattern support each
other and inhibit agents with inconsistent hypotheses. This results in the
emergence of a stable agent population identifying the desired solution.
Positive feedback via diffusion of information between the agents significantly
contributes to the speed with which the solution population is formed. The
formulation of the SDS model in terms of interacting Markov Chains enables its
characterisation in terms of the allocation of agents, or computational
resources. The analysis characterises the stationary probability distribution
of the activity of agents, which leads to the characterisation of the solution
population in terms of its similarity to the target pattern.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1506.01165v1,Image Retrieval System Base on EMD Similarity Measure and S-Tree,"The paper approaches the binary signature for each image based on the
percentage of the pixels in each color images, at the same time the paper
builds a similar measure between images based on EMD (Earth Mover's Distance).
Besides, the paper proceeded to create the S-tree based on the similar measure
EMD to store the image's binary signatures to quickly query image signature
data. From there, the paper build an image retrieval algorithm and CBIR
(Content-Based Image Retrieval) based on a similar measure EMD and S-tree.
Based on this theory, the paper proceeded to build application and experimental
assessment of the process of querying image on the database system which have
over 10,000 images.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.12822v5,Algorithms and Complexity on Indexing Founder Graphs,"We study the problem of matching a string in a labeled graph. Previous
research has shown that unless the Orthogonal Vectors Hypothesis (OVH) is
false, one cannot solve this problem in strongly sub-quadratic time, nor index
the graph in polynomial time to answer queries efficiently (Equi et al. ICALP
2019, SOFSEM 2021). These conditional lower-bounds cover even deterministic
graphs with binary alphabet, but there naturally exist also graph classes that
are easy to index: E.g. Wheeler graphs (Gagie et al. Theor. Comp. Sci. 2017)
cover graphs admitting a Burrows-Wheeler transform -based indexing scheme.
However, it is NP-complete to recognize if a graph is a Wheeler graph (Gibney,
Thankachan, ESA 2019).
  We propose an approach to alleviate the construction bottleneck of Wheeler
graphs. Rather than starting from an arbitrary graph, we study graphs induced
from multiple sequence alignments (MSAs). Elastic degenerate strings (Bernadini
et al. SPIRE 2017, ICALP 2019) can be seen as such graphs, and we introduce
here their generalization: elastic founder graphs. We first prove that even
such induced graphs are hard to index under OVH. Then we introduce two
subclasses, repeat-free and semi-repeat-free graphs, that are easy to index. We
give a linear time algorithm to construct a repeat-free non-elastic founder
graph from a gapless MSA, and (parameterized) near-linear time algorithms to
construct semi-repeat-free (repeat-free, respectively) elastic founder graphs
from general MSAs. Finally, we show that repeat-free elastic founder graphs
admit a reduction to Wheeler graphs in polynomial time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1012.0009v3,Time-Varying Graphs and Dynamic Networks,"The past few years have seen intensive research efforts carried out in some
apparently unrelated areas of dynamic systems -- delay-tolerant networks,
opportunistic-mobility networks, social networks -- obtaining closely related
insights. Indeed, the concepts discovered in these investigations can be viewed
as parts of the same conceptual universe; and the formal models proposed so far
to express some specific concepts are components of a larger formal description
of this universe. The main contribution of this paper is to integrate the vast
collection of concepts, formalisms, and results found in the literature into a
unified framework, which we call TVG (for time-varying graphs). Using this
framework, it is possible to express directly in the same formalism not only
the concepts common to all those different areas, but also those specific to
each. Based on this definitional work, employing both existing results and
original observations, we present a hierarchical classification of TVGs; each
class corresponds to a significant property examined in the distributed
computing literature. We then examine how TVGs can be used to study the
evolution of network properties, and propose different techniques, depending on
whether the indicators for these properties are a-temporal (as in the majority
of existing studies) or temporal. Finally, we briefly discuss the introduction
of randomness in TVGs.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1109.2434v1,Expressiveness of Communication in Answer Set Programming,"Answer set programming (ASP) is a form of declarative programming that allows
to succinctly formulate and efficiently solve complex problems. An intuitive
extension of this formalism is communicating ASP, in which multiple ASP
programs collaborate to solve the problem at hand. However, the expressiveness
of communicating ASP has not been thoroughly studied. In this paper, we present
a systematic study of the additional expressiveness offered by allowing ASP
programs to communicate. First, we consider a simple form of communication
where programs are only allowed to ask questions to each other. For the most
part, we deliberately only consider simple programs, i.e. programs for which
computing the answer sets is in P. We find that the problem of deciding whether
a literal is in some answer set of a communicating ASP program using simple
communication is NP-hard. In other words: we move up a step in the polynomial
hierarchy due to the ability of these simple ASP programs to communicate and
collaborate. Second, we modify the communication mechanism to also allow us to
focus on a sequence of communicating programs, where each program in the
sequence may successively remove some of the remaining models. This mimics a
network of leaders, where the first leader has the first say and may remove
models that he or she finds unsatisfactory. Using this particular communication
mechanism allows us to capture the entire polynomial hierarchy. This means, in
particular, that communicating ASP could be used to solve problems that are
above the second level of the polynomial hierarchy, such as some forms of
abductive reasoning as well as PSPACE-complete problems such as STRIPS
planning.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1310.3580v4,"Online Coordinated Charging Decision Algorithm for Electric Vehicles
  without Future Information","The large-scale integration of plug-in electric vehicles (PEVs) to the power
grid spurs the need for efficient charging coordination mechanisms. It can be
shown that the optimal charging schedule smooths out the energy consumption
over time so as to minimize the total energy cost. In practice, however, it is
hard to smooth out the energy consumption perfectly, because the future PEV
charging demand is unknown at the moment when the charging rate of an existing
PEV needs to be determined. In this paper, we propose an Online cooRdinated
CHARging Decision (ORCHARD) algorithm, which minimizes the energy cost without
knowing the future information. Through rigorous proof, we show that ORCHARD is
strictly feasible in the sense that it guarantees to fulfill all charging
demands before due time. Meanwhile, it achieves the best known competitive
ratio of 2.39. To further reduce the computational complexity of the algorithm,
we propose a novel reduced-complexity algorithm to replace the standard convex
optimization techniques used in ORCHARD. Through extensive simulations, we show
that the average performance gap between ORCHARD and the offline optimal
solution, which utilizes the complete future information, is as small as 14%.
By setting a proper speeding factor, the average performance gap can be further
reduced to less than 6%.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0412045v1,Validating a Web Service Security Abstraction by Typing,"An XML web service is, to a first approximation, an RPC service in which
requests and responses are encoded in XML as SOAP envelopes, and transported
over HTTP. We consider the problem of authenticating requests and responses at
the SOAP-level, rather than relying on transport-level security. We propose a
security abstraction, inspired by earlier work on secure RPC, in which the
methods exported by a web service are annotated with one of three security
levels: none, authenticated, or both authenticated and encrypted. We model our
abstraction as an object calculus with primitives for defining and calling web
services. We describe the semantics of our object calculus by translating to a
lower-level language with primitives for message passing and cryptography. To
validate our semantics, we embed correspondence assertions that specify the
correct authentication of requests and responses. By appeal to the type theory
for cryptographic protocols of Gordon and Jeffrey's Cryptyc, we verify the
correspondence assertions simply by typing. Finally, we describe an
implementation of our semantics via custom SOAP headers.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0801.1516v1,An Evaluation of a Structured Spreadsheet Development Methodology,"This paper presents the results of an empirical evaluation of the quality of
a structured methodology for the development of spreadsheet models, proposed in
numerous previous papers by Rajalingham K, Knight B and Chadwick D et al. This
paper also describes an improved version of their methodology, supported by
appropriate examples. The principal objective of a structured and disciplined
methodology for the construction of spreadsheet models is to reduce the
occurrence of user-generated errors in the models. The evaluation of the
effectiveness of the methodology has been carried out based on a number of
real-life experiments. The results of these experiments demonstrate the
methodology's potential for improved integrity control and enhanced
comprehensibility of spreadsheet models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,1
http://arxiv.org/abs/2005.07456v3,Cross-lingual Transfer of Sentiment Classifiers,"Word embeddings represent words in a numeric space so that semantic relations
between words are represented as distances and directions in the vector space.
Cross-lingual word embeddings transform vector spaces of different languages so
that similar words are aligned. This is done by constructing a mapping between
vector spaces of two languages or learning a joint vector space for multiple
languages. Cross-lingual embeddings can be used to transfer machine learning
models between languages, thereby compensating for insufficient data in
less-resourced languages. We use cross-lingual word embeddings to transfer
machine learning prediction models for Twitter sentiment between 13 languages.
We focus on two transfer mechanisms that recently show superior transfer
performance. The first mechanism uses the trained models whose input is the
joint numerical space for many languages as implemented in the LASER library.
The second mechanism uses large pretrained multilingual BERT language models.
Our experiments show that the transfer of models between similar languages is
sensible, even with no target language data. The performance of cross-lingual
models obtained with the multilingual BERT and LASER library is comparable, and
the differences are language-dependent. The transfer with CroSloEngual BERT,
pretrained on only three languages, is superior on these and some closely
related languages.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/cs/0608049v2,"Solving non-uniqueness in agglomerative hierarchical clustering using
  multidendrograms","In agglomerative hierarchical clustering, pair-group methods suffer from a
problem of non-uniqueness when two or more distances between different clusters
coincide during the amalgamation process. The traditional approach for solving
this drawback has been to take any arbitrary criterion in order to break ties
between distances, which results in different hierarchical classifications
depending on the criterion followed. In this article we propose a
variable-group algorithm that consists in grouping more than two clusters at
the same time when ties occur. We give a tree representation for the results of
the algorithm, which we call a multidendrogram, as well as a generalization of
the Lance and Williams' formula which enables the implementation of the
algorithm in a recursive way.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1207.5419v1,Basic Network Creation Games with Communication Interests,"Network creation games model the creation and usage costs of networks formed
by a set of selfish peers. Each peer has the ability to change the network in a
limited way, e.g., by creating or deleting incident links. In doing so, a peer
can reduce its individual communication cost. Typically, these costs are
modeled by the maximum or average distance in the network. We introduce a
generalized version of the basic network creation game (BNCG). In the BNCG (by
Alon et al., SPAA 2010), each peer may replace one of its incident links by a
link to an arbitrary peer. This is done in a selfish way in order to minimize
either the maximum or average distance to all other peers. That is, each peer
works towards a network structure that allows himself to communicate
efficiently with all other peers. However, participants of large networks are
seldom interested in all peers. Rather, they want to communicate efficiently
only with a small subset of peers. Our model incorporates these (communication)
interests explicitly. In the MAX-version, each node tries to minimize its
maximum distance to nodes it is interested in.
  Given peers with interests and a communication network forming a tree, we
prove several results on the structure and quality of equilibria in our model.
For the MAX-version, we give an upper worst case bound of O(\sqrt{n}) for the
private costs in an equilibrium of n peers. Moreover, we give an equilibrium
for a circular interest graph where a node has private cost \Omega(\sqrt{n}),
showing that our bound is tight. This example can be extended such that we get
a tight bound of \Theta(\sqrt{n}) for the price of anarchy. For the case of
general communication networks we show the price of anarchy to be \Theta(n).
Additionally, we prove an interesting connection between a maximum independent
set in the interest graph and the private costs of the peers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0503066v1,"A Practical Approach for Circuit Routing on Dynamic Reconfigurable
  Devices","Management of communication by on-line routing in new FPGAs with a large
amount of logic resources and partial reconfigurability is a new challenging
problem. A Network-on-Chip
  (NoC) typically uses packet routing mechanism, which has often unsafe data
transfers, and network interface overhead. In this paper, circuit routing for
such dynamic NoCs is investigated, and a practical 1-dimensional network with
an efficient routing algorithm is proposed and implemented. Also, this concept
has been extended to the 2-dimensional case. The implementation results show
the low area overhead and high performance of this network.",0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.09333v1,"StreamBlocks: A compiler for heterogeneous dataflow computing (technical
  report)","To increase performance and efficiency, systems use FPGAs as reconfigurable
accelerators. A key challenge in designing these systems is partitioning
computation between processors and an FPGA. An appropriate division of labor
may be difficult to predict in advance and require experiments and
measurements. When an investigation requires rewriting part of the system in a
new language or with a new programming model, its high cost can retard the
study of different configurations. A single-language system with an appropriate
programming model and compiler that targets both platforms simplifies this
exploration to a simple recompile with new compiler directives.
  This work introduces StreamBlocks, an open-source compiler and runtime that
uses the CAL dataflow programming language to partition computations across
heterogeneous (CPU/accelerator) platforms. Because of the dataflow model's
semantics and the CAL language, StreamBlocks can exploit both thread
parallelism in multi-core CPUs and the inherent parallelism of FPGAs.
StreamBlocks supports exploring the design space with a profile-guided tool
that helps identify the best hardware-software partitions.",0,0,0,0,1,0,1,0,1,0,0,0,0,0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.05413v2,Formal Methods in Railways: a Systematic Mapping Study,"Formal methods are mathematically-based techniques for the rigorous
development of software-intensive systems. The railway signaling domain is a
field in which formal methods have traditionally been applied, with several
success stories. This article reports on a mapping study that surveys the
landscape of research on applications of formal methods to the development of
railway systems. Our main results are as follows: (i) we identify a total of
328 primary studies relevant to our scope published between 1989 and 2020, of
which 44% published during the last 5 years and 24% involving industry; (ii)
the majority of studies are evaluated through Examples (41%) and Experience
Reports (38%), while full-fledged Case Studies are limited (1.5%); (iii) Model
checking is the most commonly adopted technique (47%), followed by simulation
(27%) and theorem proving (19.5%); (iv) the dominant languages are UML (18%)
and B (15%), while frequently used tools are ProB (9%), NuSMV (8%) and UPPAAL
(7%); however, a diverse landscape of languages and tools is employed;
  (v) the majority of systems are interlocking products (40%), followed by
models of high-level control logic (27%);
  (vi) most of the studies focus on the Architecture (66%) and Detailed Design
(45%) development phases. Based on these findings, we highlight current
research gaps and expected actions. In particular, the need to focus on more
empirically sound research methods, such as Case Studies and Controlled
Experiments, and to lower the degree of abstraction, by applying formal methods
and tools to development phases that are closer to software development. Our
study contributes with an empirically based perspective on the future of
research and practice in formal methods applications for railways.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.04310v2,Categorical structures for type theory in univalent foundations,"In this paper, we analyze and compare three of the many algebraic structures
that have been used for modeling dependent type theories: categories with
families, split type-categories, and representable maps of presheaves. We study
these in univalent type theory, where the comparisons between them can be given
more elementarily than in set-theoretic foundations. Specifically, we construct
maps between the various types of structures, and show that assuming the
Univalence axiom, some of the comparisons are equivalences.
  We then analyze how these structures transfer along (weak and strong)
equivalences of categories, and, in particular, show how they descend from a
category (not assumed univalent/saturated) to its Rezk completion. To this end,
we introduce relative universes, generalizing the preceding notions, and study
the transfer of such relative universes along suitable structure.
  We work throughout in (intensional) dependent type theory; some results, but
not all, assume the univalence axiom. All the material of this paper has been
formalized in Coq, over the UniMath library.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1911.09852v2,Developments in Multi-Agent Fair Allocation,"Fairness is becoming an increasingly important concern when designing
markets, allocation procedures, and computer systems. I survey some recent
developments in the field of multi-agent fair allocation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.00706v1,GPU-Accelerated Mobile Multi-view Style Transfer,"An estimated 60% of smartphones sold in 2018 were equipped with multiple rear
cameras, enabling a wide variety of 3D-enabled applications such as 3D Photos.
The success of 3D Photo platforms (Facebook 3D Photo, Holopix, etc) depend on a
steady influx of user generated content. These platforms must provide simple
image manipulation tools to facilitate content creation, akin to traditional
photo platforms. Artistic neural style transfer, propelled by recent
advancements in GPU technology, is one such tool for enhancing traditional
photos. However, naively extrapolating single-view neural style transfer to the
multi-view scenario produces visually inconsistent results and is prohibitively
slow on mobile devices. We present a GPU-accelerated multi-view style transfer
pipeline which enforces style consistency between views with on-demand
performance on mobile platforms. Our pipeline is modular and creates high
quality depth and parallax effects from a stereoscopic image pair.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.01358v1,A Generalized Method for Proving Polynomial Calculus Degree Lower Bounds,"We study the problem of obtaining lower bounds for polynomial calculus (PC)
and polynomial calculus resolution (PCR) on proof degree, and hence by
[Impagliazzo et al. '99] also on proof size. [Alekhnovich and Razborov '03]
established that if the clause-variable incidence graph of a CNF formula F is a
good enough expander, then proving that F is unsatisfiable requires high PC/PCR
degree. We further develop the techniques in [AR03] to show that if one can
""cluster"" clauses and variables in a way that ""respects the structure"" of the
formula in a certain sense, then it is sufficient that the incidence graph of
this clustered version is an expander. As a corollary of this, we prove that
the functional pigeonhole principle (FPHP) formulas require high PC/PCR degree
when restricted to constant-degree expander graphs. This answers an open
question in [Razborov '02], and also implies that the standard CNF encoding of
the FPHP formulas require exponential proof size in polynomial calculus
resolution. Thus, while Onto-FPHP formulas are easy for polynomial calculus, as
shown in [Riis '93], both FPHP and Onto-PHP formulas are hard even when
restricted to bounded-degree expanders.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.12592v1,"Understanding and Accelerating EM Algorithm's Convergence by Fair
  Competition Principle and Rate-Verisimilitude Function","Why can the Expectation-Maximization (EM) algorithm for mixture models
converge? Why can different initial parameters cause various convergence
difficulties? The Q-L synchronization theory explains that the observed data
log-likelihood L and the complete data log-likelihood Q are positively
correlated; we can achieve maximum L by maximizing Q. According to this theory,
the Deterministic Annealing EM (DAEM) algorithm's authors make great efforts to
eliminate locally maximal Q for avoiding L's local convergence. However, this
paper proves that in some cases, Q may and should decrease for L to increase;
slow or local convergence exists only because of small samples and unfair
competition. This paper uses marriage competition to explain different
convergence difficulties and proposes the Fair Competition Principle (FCP) with
an initialization map for improving initializations. It uses the
rate-verisimilitude function, extended from the rate-distortion function, to
explain the convergence of the EM and improved EM algorithms. This convergence
proof adopts variational and iterative methods that Shannon et al. used for
analyzing rate-distortion functions. The initialization map can vastly save
both algorithms' running times for binary Gaussian mixtures. The FCP and the
initialization map are useful for complicated mixtures but not sufficient; we
need further studies for specific methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,0,0,0,1,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1608.04374v2,A Geometric Framework for Convolutional Neural Networks,"In this paper, a geometric framework for neural networks is proposed. This
framework uses the inner product space structure underlying the parameter set
to perform gradient descent not in a component-based form, but in a
coordinate-free manner. Convolutional neural networks are described in this
framework in a compact form, with the gradients of standard --- and
higher-order --- loss functions calculated for each layer of the network. This
approach can be applied to other network structures and provides a basis on
which to create new networks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1503.09006v2,"Fast, Multicore-Scalable, Low-Fragmentation Memory Allocation through
  Large Virtual Memory and Global Data Structures","We demonstrate that general-purpose memory allocation involving many threads
on many cores can be done with high performance, multicore scalability, and low
memory consumption. For this purpose, we have designed and implemented scalloc,
a concurrent allocator that generally performs and scales in our experiments
better than other allocators while using less memory, and is still competitive
otherwise. The main ideas behind the design of scalloc are: uniform treatment
of small and big objects through so-called virtual spans, efficiently and
effectively reclaiming free memory through fast and scalable global data
structures, and constant-time (modulo synchronization) allocation and
deallocation operations that trade off memory reuse and spatial locality
without being subject to false sharing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0509061v3,"Guarantees for the Success Frequency of an Algorithm for Finding
  Dodgson-Election Winners","In the year 1876 the mathematician Charles Dodgson, who wrote fiction under
the now more famous name of Lewis Carroll, devised a beautiful voting system
that has long fascinated political scientists. However, determining the winner
of a Dodgson election is known to be complete for the \Theta_2^p level of the
polynomial hierarchy. This implies that unless P=NP no polynomial-time solution
to this problem exists, and unless the polynomial hierarchy collapses to NP the
problem is not even in NP. Nonetheless, we prove that when the number of voters
is much greater than the number of candidates--although the number of voters
may still be polynomial in the number of candidates--a simple greedy algorithm
very frequently finds the Dodgson winners in such a way that it ``knows'' that
it has found them, and furthermore the algorithm never incorrectly declares a
nonwinner to be a winner.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.04618v1,From H&M to Gap for Lightweight BWT Merging,"Recently, Holt and McMillan [Bionformatics 2014, ACM-BCB 2014] have proposed
a simple and elegant algorithm to merge the Burrows-Wheeler transforms of a
family of strings. In this paper we show that the H&M algorithm can be improved
so that, in addition to merging the BWTs, it can also merge the Longest Common
Prefix (LCP) arrays. The new algorithm, called Gap because of how it operates,
has the same asymptotic cost as the H&M algorithm and requires additional space
only for storing the LCP values.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0405067v1,Note on Counting Eulerian Circuits,"We show that the problem of counting the number of Eulerian circuits in an
undirected graph is complete for the class #P.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1707.03623v1,"The detector principle of constructing artificial neural networks as an
  alternative to the connectionist paradigm","Artificial neural networks (ANN) are inadequate to biological neural
networks. This inadequacy is manifested in the use of the obsolete model of the
neuron and the connectionist paradigm of constructing ANN. The result of this
inadequacy is the existence of many shortcomings of the ANN and the problems of
their practical implementation. The alternative principle of ANN construction
is proposed in the article. This principle was called the detector principle.
The basis of the detector principle is the consideration of the binding
property of the input signals of a neuron. A new model of the neuron-detector,
a new approach to teaching ANN - counter training and a new approach to the
formation of the ANN architecture are used in this principle.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1608.06249v1,A Survey on Honeypot Software and Data Analysis,"In this survey, we give an extensive overview on honeypots. This includes not
only honeypot software but also methodologies to analyse honeypot data.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1210.2864v1,Lightweight compilation of (C)LP to JavaScript,"We present and evaluate a compiler from Prolog (and extensions) to JavaScript
which makes it possible to use (constraint) logic programming to develop the
client side of web applications while being compliant with current industry
standards. Targeting JavaScript makes (C)LP programs executable in virtually
every modern computing device with no additional software requirements from the
point of view of the user. In turn, the use of a very high-level language
facilitates the development of high-quality, complex software. The compiler is
a back end of the Ciao system and supports most of its features, including its
module system and its rich language extension mechanism based on packages. We
present an overview of the compilation process and a detailed description of
the run-time system, including the support for modular compilation into
separate JavaScript code. We demonstrate the maturity of the compiler by
testing it with complex code such as a CLP(FD) library written in Prolog with
attributed variables. Finally, we validate our proposal by measuring the
performance of some LP and CLP(FD) benchmarks running on top of major
JavaScript engines.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.07319v2,"The Impact of Heterogeneity and Geometry on the Proof Complexity of
  Random Satisfiability","Satisfiability is considered the canonical NP-complete problem and is used as
a starting point for hardness reductions in theory, while in practice heuristic
SAT solving algorithms can solve large-scale industrial SAT instances very
efficiently. This disparity between theory and practice is believed to be a
result of inherent properties of industrial SAT instances that make them
tractable. Two characteristic properties seem to be prevalent in the majority
of real-world SAT instances, heterogeneous degree distribution and locality. To
understand the impact of these two properties on SAT, we study the proof
complexity of random k-SAT models that allow to control heterogeneity and
locality. Our findings show that heterogeneity alone does not make SAT easy as
heterogeneous random k-SAT instances have superpolynomial resolution size. This
implies intractability of these instances for modern SAT-solvers. On the other
hand, modeling locality with an underlying geometry leads to small
unsatisfiable subformulas, which can be found within polynomial time.
  A key ingredient for the result on geometric random k-SAT can be found in the
complexity of higher-order Voronoi diagrams. As an additional technical
contribution, we show a linear upper bound on the number of non-empty Voronoi
regions, that holds for points with random positions in a very general setting.
In particular, it covers arbitrary p-norms, higher dimensions, and weights
affecting the area of influence of each point multiplicatively. This is in
stark contrast to quadratic lower bounds for the worst case.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1305.3932v3,Inferring the Origin Locations of Tweets with Quantitative Confidence,"Social Internet content plays an increasingly critical role in many domains,
including public health, disaster management, and politics. However, its
utility is limited by missing geographic information; for example, fewer than
1.6% of Twitter messages (tweets) contain a geotag. We propose a scalable,
content-based approach to estimate the location of tweets using a novel yet
simple variant of gaussian mixture models. Further, because real-world
applications depend on quantified uncertainty for such estimates, we propose
novel metrics of accuracy, precision, and calibration, and we evaluate our
approach accordingly. Experiments on 13 million global, comprehensively
multi-lingual tweets show that our approach yields reliable, well-calibrated
results competitive with previous computationally intensive methods. We also
show that a relatively small number of training data are required for good
estimates (roughly 30,000 tweets) and models are quite time-invariant
(effective on tweets many weeks newer than the training set). Finally, we show
that toponyms and languages with small geographic footprint provide the most
useful location signals.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1210.2035v1,Synthesis of Reactive Protocols for Vehicle-to-Vehicle Communication,"We present a synthesis method for communication protocols for active safety
applications that satisfy certain formal specifications on quality of service
requirements. The protocols are developed to provide reliable communication
services for automobile active safety applications. The synthesis method
transforms a specification into a distributed implementation of senders and
receivers that together satisfy the quality of service requirements by
transmitting messages over an unreliable medium. We develop a specification
language and an execution model for the implementations, and demonstrate the
viability of our method by developing a protocol for a traffic scenario in
which a car runs a red light at a busy intersection.",0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1906.03424v3,An Automaton Group with PSPACE-Complete Word Problem,"We construct an automaton group with a PSPACE-complete word problem, proving
a conjecture due to Steinberg. Additionally, the constructed group has a
provably more difficult, namely EXPSPACE-complete, compressed word problem and
acts over a binary alphabet. Thus, it is optimal in terms of the alphabet size.
Our construction directly simulates the computation of a Turing machine in an
automaton group and, therefore, seems to be quite versatile. It combines two
ideas: the first one is a construction used by D'Angeli, Rodaro and the first
author to obtain an inverse automaton semigroup with a PSPACE-complete word
problem and the second one is to utilize a construction used by Barrington to
simulate Boolean circuits of bounded degree and logarithmic depth in the group
of even permutations over five elements.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1812.08585v2,"Intertemporal Connections Between Query Suggestions and Search Engine
  Results for Politics Related Queries","This short paper deals with the combination and comparison of two data
sources: Search engine results and query suggestions for 16 terms related to
political candidates and parties. The data was collected before the federal
election in Germany in September 2017 for a period of two months. The rank
biased overlap (RBO) statistic is used to measure the similarity of the
top-weighted rankings. For each search term and for both the search results and
query auto-completions we study the stability of the rankings over time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1705.07767v3,The language of Stratified Sets is confluent and strongly normalising,"We study the properties of the language of Stratified Sets (first-order logic
with $\in$ and a stratification condition) as used in TST, TZT, and (with
stratifiability instead of stratification) in Quine's NF. We find that the
syntax forms a nominal algebra for substitution and that stratification and
stratifiability imply confluence and strong normalisation under rewrites
corresponding naturally to $\beta$-conversion.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.05632v1,"CODIC: A Low-Cost Substrate for Enabling Custom In-DRAM Functionalities
  and Optimizations","DRAM is the dominant main memory technology used in modern computing systems.
Computing systems implement a memory controller that interfaces with DRAM via
DRAM commands. DRAM executes the given commands using internal components
(e.g., access transistors, sense amplifiers) that are orchestrated by DRAM
internal timings, which are fixed foreach DRAM command. Unfortunately, the use
of fixed internal timings limits the types of operations that DRAM can perform
and hinders the implementation of new functionalities and custom mechanisms
that improve DRAM reliability, performance and energy. To overcome these
limitations, we propose enabling programmable DRAM internal timings for
controlling in-DRAM components. To this end, we design CODIC, a new low-cost
DRAM substrate that enables fine-grained control over four previously fixed
internal DRAM timings that are key to many DRAM operations. We implement CODIC
with only minimal changes to the DRAM chip and the DDRx interface. To
demonstrate the potential of CODIC, we propose two new CODIC-based security
mechanisms that outperform state-of-the-art mechanisms in several ways: (1) a
new DRAM Physical Unclonable Function (PUF) that is more robust and has
significantly higher throughput than state-of-the-art DRAM PUFs, and (2) the
first cold boot attack prevention mechanism that does not introduce any
performance or energy overheads at runtime.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1709.07358v1,"Non-Depth-First Search against Independent Distributions on an AND-OR
  Tree","Suzuki and Niida (Ann. Pure. Appl. Logic, 2015) showed the following results
on independent distributions (IDs) on an AND-OR tree, where they took only
depth-first algorithms into consideration. (1) Among IDs such that probability
of the root having value 0 is fixed as a given r such that 0 < r < 1, if d is a
maximizer of cost of the best algorithm then d is an independent and identical
distribution (IID). (2) Among all IDs, if d is a maximizer of cost of the best
algorithm then d is an IID. In the case where non-depth-first algorithms are
taken into consideration, the counter parts of (1) and (2) are left open in the
above work. Peng et al. (Inform. Process. Lett., 2017) extended (1) and (2) to
multi-branching trees, where in (2) they put an additional hypothesis on IDs
that probability of the root having value 0 is neither 0 nor 1. We give
positive answers for the two questions of Suzuki-Niida. A key to the proof is
that if ID d achieves the equilibrium among IDs then we can chose an algorithm
of the best cost against d from depth-first algorithms. In addition, we extend
the result of Peng et al. to the case where non-depth-first algorithms are
taken into consideration.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.02976v1,Growing and Evolving 3D Prints,"Design - especially of physical objects - can be understood as creative acts
solving practical problems. In this paper we describe a biologically-inspired
developmental model as the basis of a generative form-finding system. Using
local interactions between cells in a two-dimensional environment, then
capturing the state of the system at every time step, complex three-dimensional
(3D) forms can be generated by the system. Unlike previous systems, our method
is capable of directly producing 3D printable objects, eliminating intermediate
transformations and manual manipulation often necessary to ensure the 3D form
is printable. We devise fitness measures for optimising 3D printability and
aesthetic complexity and use a Covariance Matrix Adaptation Evolutionary
Strategies algorithm (CMA-ES) to find 3D forms that are both aesthetically
interesting and physically printable using fused deposition modelling printing
techniques. We investigate the system's capabilities by evolving and 3D
printing objects at different levels of structural consistency, and assess the
quality of the fitness measures presented to explore the design space of our
generative system. We find that by evolving first for aesthetic complexity,
then evolving for structural consistency until the form is 'just printable',
gives the best results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1302.2015v2,Persistence modules: Algebra and algorithms,"Persistent homology was shown by Carlsson and Zomorodian to be homology of
graded chain complexes with coefficients in the graded ring $\kk[t]$. As such,
the behavior of persistence modules -- graded modules over $\kk[t]$ is an
important part in the analysis and computation of persistent homology.
  In this paper we present a number of facts about persistence modules; ranging
from the well-known but under-utilized to the reconstruction of techniques to
work in a purely algebraic approach to persistent homology. In particular, the
results we present give concrete algorithms to compute the persistent homology
of a simplicial complex with torsion in the chain complex.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9908013v1,Collective Intelligence for Control of Distributed Dynamical Systems,"We consider the El Farol bar problem, also known as the minority game (W. B.
Arthur, ``The American Economic Review'', 84(2): 406--411 (1994), D. Challet
and Y.C. Zhang, ``Physica A'', 256:514 (1998)). We view it as an instance of
the general problem of how to configure the nodal elements of a distributed
dynamical system so that they do not ``work at cross purposes'', in that their
collective dynamics avoids frustration and thereby achieves a provided global
goal. We summarize a mathematical theory for such configuration applicable when
(as in the bar problem) the global goal can be expressed as minimizing a global
energy function and the nodes can be expressed as minimizers of local free
energy functions. We show that a system designed with that theory performs
nearly optimally for the bar problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0405007v1,"""In vivo"" spam filtering: A challenge problem for data mining","Spam, also known as Unsolicited Commercial Email (UCE), is the bane of email
communication. Many data mining researchers have addressed the problem of
detecting spam, generally by treating it as a static text classification
problem. True in vivo spam filtering has characteristics that make it a rich
and challenging domain for data mining. Indeed, real-world datasets with these
characteristics are typically difficult to acquire and to share. This paper
demonstrates some of these characteristics and argues that researchers should
pursue in vivo spam filtering as an accessible domain for investigating them.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.02264v3,Improving axial resolution in SIM using deep learning,"Structured Illumination Microscopy is a widespread methodology to image live
and fixed biological structures smaller than the diffraction limits of
conventional optical microscopy. Using recent advances in image up-scaling
through deep learning models, we demonstrate a method to reconstruct 3D SIM
image stacks with twice the axial resolution attainable through conventional
SIM reconstructions. We further evaluate our method for robustness to noise &
generalisability to varying observed specimens, and discuss potential adaptions
of the method to further improvements in resolution.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1811.01837v4,TzK Flow - Conditional Generative Model,"We introduce TzK (pronounced ""task""), a conditional probability flow-based
model that exploits attributes (e.g., style, class membership, or other side
information) in order to learn tight conditional prior around manifolds of the
target observations. The model is trained via approximated ML, and offers
efficient approximation of arbitrary data sample distributions (similar to GAN
and flow-based ML), and stable training (similar to VAE and ML), while avoiding
variational approximations. TzK exploits meta-data to facilitate a bottleneck,
similar to autoencoders, thereby producing a low-dimensional representation.
Unlike autoencoders, the bottleneck does not limit model expressiveness,
similar to flow-based ML. Supervised, unsupervised, and semi-supervised
learning are supported by replacing missing observations with samples from
learned priors. We demonstrate TzK by training jointly on MNIST and Omniglot
datasets with minimal preprocessing, and weak supervision, with results
comparable to state-of-the-art.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0911.1393v5,Most tensor problems are NP-hard,"We prove that multilinear (tensor) analogues of many efficiently computable
problems in numerical linear algebra are NP-hard. Our list here includes:
determining the feasibility of a system of bilinear equations, deciding whether
a 3-tensor possesses a given eigenvalue, singular value, or spectral norm;
approximating an eigenvalue, eigenvector, singular vector, or the spectral
norm; and determining the rank or best rank-1 approximation of a 3-tensor.
Furthermore, we show that restricting these problems to symmetric tensors does
not alleviate their NP-hardness. We also explain how deciding nonnegative
definiteness of a symmetric 4-tensor is NP-hard and how computing the
combinatorial hyperdeterminant of a 4-tensor is NP-, #P-, and VNP-hard. We
shall argue that our results provide another view of the boundary separating
the computational tractability of linear/convex problems from the
intractability of nonlinear/nonconvex ones.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1208.5317v2,"Characterizing Weighted MSO for Trees by Branching Transitive Closure
  Logics","We introduce the branching transitive closure operator on weighted monadic
second-order logic formulas where the branching corresponds in a natural way to
the branching inherent in trees. For arbitrary commutative semirings, we prove
that weighted monadic second order logics on trees is equivalent to the
definability by formulas which start with one of the following operators: (i) a
branching transitive closure or (ii) an existential second-order quantifier
followed by one universal first-order quantifier; in both cases the operator is
applied to step-formulas over (a) Boolean first-order logic enriched by modulo
counting or (b) Boolean monadic-second order logic.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1007.1611v2,"A Constant-Factor Approximation for Wireless Capacity Maximization with
  Power Control in the SINR Model","In modern wireless networks, devices are able to set the power for each
transmission carried out. Experimental but also theoretical results indicate
that such power control can improve the network capacity significantly. We
study this problem in the physical interference model using SINR constraints.
  In the SINR capacity maximization problem, we are given n pairs of senders
and receivers, located in a metric space (usually a so-called fading metric).
The algorithm shall select a subset of these pairs and choose a power level for
each of them with the objective of maximizing the number of simultaneous
communications. This is, the selected pairs have to satisfy the SINR
constraints with respect to the chosen powers.
  We present the first algorithm achieving a constant-factor approximation in
fading metrics. The best previous results depend on further network parameters
such as the ratio of the maximum and the minimum distance between a sender and
its receiver. Expressed only in terms of n, they are (trivial) Omega(n)
approximations.
  Our algorithm still achieves an O(log n) approximation if we only assume to
have a general metric space rather than a fading metric. Furthermore, by using
standard techniques the algorithm can also be used in single-hop and multi-hop
scheduling scenarios. Here, we also get polylog(n) approximations.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1805.06699v1,Dual parameterization of Weighted Coloring,"Given a graph $G$, a proper $k$-coloring of $G$ is a partition $c =
(S_i)_{i\in [1,k]}$ of $V(G)$ into $k$ stable sets $S_1,\ldots, S_{k}$. Given a
weight function $w: V(G) \to \mathbb{R}^+$, the weight of a color $S_i$ is
defined as $w(i) = \max_{v \in S_i} w(v)$ and the weight of a coloring $c$ as
$w(c) = \sum_{i=1}^{k}w(i)$. Guan and Zhu [Inf. Process. Lett., 1997] defined
the weighted chromatic number of a pair $(G,w)$, denoted by $\sigma(G,w)$, as
the minimum weight of a proper coloring of $G$. The problem of determining
$\sigma(G,w)$ has received considerable attention during the last years, and
has been proved to be notoriously hard: for instance, it is NP-hard on split
graphs, unsolvable on $n$-vertex trees in time $n^{o(\log n)}$ unless the ETH
fails, and W[1]-hard on forests parameterized by the size of a largest tree. In
this article we provide some positive results for the problem, by considering
its so-called dual parameterization: given a vertex-weighted graph $(G,w)$ and
an integer $k$, the question is whether $\sigma(G,w) \leq \sum_{v \in V(G)}
w(v) - k$. We prove that this problem is FPT by providing an algorithm running
in time $9^k \cdot n^{O(1)}$, and it is easy to see that no algorithm in time
$2^{o(k)} \cdot n^{O(1)}$ exists under the ETH. On the other hand, we present a
kernel with at most $(2^{k-1}+1) (k-1)$ vertices, and we rule out the existence
of polynomial kernels unless ${\sf NP} \subseteq {\sf coNP} / {\sf poly}$, even
on split graphs with only two different weights. Finally, we identify some
classes of graphs on which the problem admits a polynomial kernel, in
particular interval graphs and subclasses of split graphs, and in the latter
case we present lower bounds on the degrees of the polynomials.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0310052v1,On secret sharing for graphs,"In the paper we discuss how to share the secrets, that are graphs. So, far
secret sharing schemes were designed to work with numbers. As the first step,
we propose conditions for ""graph to number"" conversion methods. Hence, the
existing schemes can be used, without weakening their properties. Next, we show
how graph properties can be used to extend capabilities of secret sharing
schemes. This leads to proposal of using such properties for number based
secret sharing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.07252v2,"Using Containers to Create More Interactive Online Training and
  Education Materials","Containers are excellent hands-on learning environments for computing topics
because they are customizable, portable, and reproducible. The Cornell
University Center for Advanced Computing has developed the Cornell Virtual
Workshop in high performance computing topics for many years, and we have
always sought to make the materials as rich and interactive as possible. Toward
the goal of building a more hands-on experimental learning experience directly
into web-based online training environments, we developed the Cornell Container
Runner Service, which allows online content developers to build container-based
interactive edit and run commands directly into their web pages. Using
containers along with CCRS has the potential to increase learner engagement and
outcomes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0
http://arxiv.org/abs/1805.04165v3,Erasure Correction for Noisy Radio Networks,"The radio network model is a well-studied model of wireless, multi-hop
networks. However, radio networks make the strong assumption that messages are
delivered deterministically. The recently introduced noisy radio network model
relaxes this assumption by dropping messages independently at random.
  In this work we quantify the relative computational power of noisy radio
networks and classic radio networks. In particular, given a non-adaptive
protocol for a fixed radio network we show how to reliably simulate this
protocol if noise is introduced with a multiplicative cost of
$\mathrm{poly}(\log \Delta, \log \log n)$ rounds where $n$ is the number nodes
in the network and $\Delta$ is the max degree. Moreover, we demonstrate that,
even if the simulated protocol is not non-adaptive, it can be simulated with a
multiplicative $O(\Delta \log ^2 \Delta)$ cost in the number of rounds. Lastly,
we argue that simulations with a multiplicative overhead of $o(\log \Delta)$
are unlikely to exist by proving that an $\Omega(\log \Delta)$ multiplicative
round overhead is necessary under certain natural assumptions.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.13679v3,A tempered subdiffusive Black-Scholes model,"In this paper, we focus on the tempered subdiffusive Black-Scholes model. The
main part of our work consists of the finite difference method as a numerical
approach to the option pricing in the considered model. We derive the governing
fractional differential equation and the related weighted numerical scheme. The
proposed method has the $2-\alpha$ order of accuracy with respect to time,
where $\alpha\in(0,1)$ is the subdiffusion parameter, and $2$ with respect to
space. Furthermore, we provide the stability and convergence analysis. Finally,
we present some numerical results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0707.1432v1,Sequential products in effect categories,"A new categorical framework is provided for dealing with multiple arguments
in a programming language with effects, for example in a language with
imperative features. Like related frameworks (Monads, Arrows, Freyd
categories), we distinguish two kinds of functions. In addition, we also
distinguish two kinds of equations. Then, we are able to define a kind of
product, that generalizes the usual categorical product. This yields a powerful
tool for deriving many results about languages with effects.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1607.07698v2,Domains and Random Variables,"The aim of this paper is to establish a theory of random variables on
domains. Domain theory is a fundamental component of theoretical computer
science, providing mathematical models of computational processes. Random
variables are the mainstay of probability theory. Since computational models
increasingly involve probabilistic aspects, it's only natural to explore the
relationship between these two areas. Our main results show how to cast results
about random variables using a domain-theoretic approach. The pay-off is an
extension of the results from probability measures to sub-probability measures.
We also use our approach to extend the class of domains for which we can
classify the domain structure of the space of sub-probability measures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1607.07483v1,"Collision-Free Poisson Motion Planning in Ultra High-Dimensional
  Molecular Conformation Spaces","The function of protein, RNA, and DNA is modulated by fast, dynamic exchanges
between three-dimensional conformations. Conformational sampling of
biomolecules with exact and nullspace inverse kinematics, using rotatable bonds
as revolute joints and non-covalent interactions as holonomic constraints, can
accurately characterize these native ensembles. However, sampling biomolecules
remains challenging owing to their ultra-high dimensional configuration spaces,
and the requirement to avoid (self-) collisions, which results in low
acceptance rates.
  Here, we present two novel mechanisms to overcome these limitations. First,
we introduced temporary constraints between near-colliding links. The resulting
constraint varieties instantaneously redirect the search for collision-free
conformations, and couple motions between distant parts of the linkage. Second,
we adapted a randomized Poisson-disk motion planner, which prevents local
oversampling and widens the search, to ultra-high dimensions. We evaluated our
algorithm on several model systems. Our contributions apply to general
high-dimensional motion planning problems in static and dynamic environments
with obstacles.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0906.3923v4,Bayesian Forecasting of WWW Traffic on the Time Varying Poisson Model,"Traffic forecasting from past observed traffic data with small calculation
complexity is one of important problems for planning of servers and networks.
Focusing on World Wide Web (WWW) traffic as fundamental investigation, this
paper would deal with Bayesian forecasting of network traffic on the time
varying Poisson model from a viewpoint from statistical decision theory. Under
this model, we would show that the estimated forecasting value is obtained by
simple arithmetic calculation and expresses real WWW traffic well from both
theoretical and empirical points of view.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0105036v2,Disjunctive Logic Programs with Inheritance,"The paper proposes a new knowledge representation language, called DLP<,
which extends disjunctive logic programming (with strong negation) by
inheritance. The addition of inheritance enhances the knowledge modeling
features of the language providing a natural representation of default
reasoning with exceptions.
  A declarative model-theoretic semantics of DLP< is provided, which is shown
to generalize the Answer Set Semantics of disjunctive logic programs.
  The knowledge modeling features of the language are illustrated by encoding
classical nonmonotonic problems in DLP<.
  The complexity of DLP< is analyzed, proving that inheritance does not cause
any computational overhead, as reasoning in DLP< has exactly the same
complexity as reasoning in disjunctive logic programming. This is confirmed by
the existence of an efficient translation from DLP< to plain disjunctive logic
programming. Using this translation, an advanced KR system supporting the DLP<
language has been implemented on top of the DLV system and has subsequently
been integrated into DLV.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1504.06484v1,Recent Advances in Real Geometric Reasoning,"In the 1930s Tarski showed that real quantifier elimination was possible, and
in 1975 Collins gave a remotely practicable method, albeit with
doubly-exponential complexity, which was later shown to be inherent. We discuss
some of the recent major advances in Collins method: such as an alternative
approach based on passing via the complexes, and advances which come closer to
""solving the question asked"" rather than ""solving all problems to do with these
polynomials"".",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.07211v2,Fast Generation of Unlabelled Free Trees using Weight Sequences,"In this paper, we introduce a new representation for ordered trees, the
weight sequence representation. We then use this to construct new
representations for both rooted trees and free trees, namely the canonical
weight sequence representation. We construct algorithms for generating the
weight sequence representations for all rooted and free trees of order n, and
then add a number of modifications to improve the efficiency of the algorithms.
Python implementations of the algorithms incorporate further improvements by
using generators to avoid having to store the long lists of trees returned by
the recursive calls, as well as caching the lists for rooted trees of small
order, thereby eliminating many of the recursive calls. We further show how the
algorithm can be modifed to generate adjacency list and adjacency matrix
representations for free trees. We compared the run-times of our Python
implementation for generating free trees with the Python implementation of the
well-known WROM algorithm taken from NetworkX. The implementation of our
algorithm is over four times as fast as the implementation of the WROM
algorithm. The run-times for generating adjacency lists and matrices are
somewhat longer than those for weight sequences, but are still over three times
as fast as the corresponding implementations of the WROM algorithm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1611.00598v1,"A bioinformatics system for searching Co-Occurrence based on
  Co-Operational Formation with Advanced Method (COCOFAM)","Literature analysis is a key step in obtaining background information in
biomedical research. However, it is difficult for researchers to obtain
knowledge of their interests in an efficient manner because of the massive
amount of the published biomedical literature. Therefore, efficient and
systematic search strategies are required, which allow ready access to the
substantial amount of literature. In this paper, we propose a novel search
system, named Co-Occurrence based on Co-Operational Formation with Advanced
Method(COCOFAM) which is suitable for the large-scale literature analysis.
COCOFAM is based on integrating both Spark for local clusters and a global job
scheduler to gather crowdsourced co-occurrence data on global clusters. It will
allow users to obtain information of their interests from the substantial
amount of literature.",0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0505084v2,An explicit formula for the number of tunnels in digital objects,"An important concept in digital geometry for computer imagery is that of
tunnel. In this paper we obtain a formula for the number of tunnels as a
function of the number of the object vertices, pixels, holes, connected
components, and 2x2 grid squares. It can be used to test for tunnel-freedom a
digital object, in particular a digital curve.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0412078v1,The vertex-transitive TLF-planar graphs,"We consider the class of the topologically locally finite (in short TLF)
planar vertex-transitive graphs, a class containing in particular all the
one-ended planar Cayley graphs and the normal transitive tilings. We
characterize these graphs with a finite local representation and a special kind
of finite state automaton named labeling scheme. As a result, we are able to
enumerate and describe all TLF-planar vertex-transitive graphs of any given
degree. Also, we are able decide to whether any TLF-planar transitive graph is
Cayley or not.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1503.08880v1,A composite constraints approach to declarative agent-based modeling,"Agent-based models (ABMs) are ubiquitous in research and industry. Currently,
simulating ABMs involves at least some imperative (step-by-step) computer
instructions. An alternative approach is declarative programming, in which a
set of requirements is described at a high level of abstraction. Here we
describe a fully declarative approach to the automated construction of
simulations for ABMs. In this framework, logic for ABM simulations is
encapsulated into predefined components. The user specifies a set of
requirements describing the desired functionality. Additionally, each component
has a set of consistency requirements. The framework iteratively seeks a
simulation design that satisfies both user and system requirements. This
approach allows the user to omit most details from the simulation
specification, simplifying simulation design.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.16925v2,"Ethical Analysis on the Application of Neurotechnology for Human
  Augmentation in Physicians and Surgeons","With the shortage of physicians and surgeons and increase in demand worldwide
due to situations such as the COVID-19 pandemic, there is a growing interest in
finding solutions to help address the problem. A solution to this problem would
be to use neurotechnology to provide them augmented cognition, senses and
action for optimal diagnosis and treatment. Consequently, doing so can
negatively impact them and others. We argue that applying neurotechnology for
human enhancement in physicians and surgeons can cause injustices, and harm to
them and patients. In this paper, we will first describe the augmentations and
neurotechnologies that can be used to achieve the relevant augmentations for
physicians and surgeons. We will then review selected ethical concerns
discussed within literature, discuss the neuroengineering behind using
neurotechnology for augmentation purposes, then conclude with an analysis on
outcomes and ethical issues of implementing human augmentation via
neurotechnology in medical and surgical practice.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,0,0,1,0,0,0,0
http://arxiv.org/abs/1107.0399v4,"Vision-Based Navigation I: A navigation filter for fusing
  DTM/correspondence updates","An algorithm for pose and motion estimation using corresponding features in
images and a digital terrain map is proposed. Using a Digital Terrain (or
Digital Elevation) Map (DTM/DEM) as a global reference enables recovering the
absolute position and orientation of the camera. In order to do this, the DTM
is used to formulate a constraint between corresponding features in two
consecutive frames. The utilization of data is shown to improve the robustness
and accuracy of the inertial navigation algorithm. Extended Kalman filter was
used to combine results of inertial navigation algorithm and proposed
vision-based navigation algorithm. The feasibility of this algorithms is
established through numerical simulations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1604.00545v3,The AGI Containment Problem,"There is considerable uncertainty about what properties, capabilities and
motivations future AGIs will have. In some plausible scenarios, AGIs may pose
security risks arising from accidents and defects. In order to mitigate these
risks, prudent early AGI research teams will perform significant testing on
their creations before use. Unfortunately, if an AGI has human-level or greater
intelligence, testing itself may not be safe; some natural AGI goal systems
create emergent incentives for AGIs to tamper with their test environments,
make copies of themselves on the internet, or convince developers and operators
to do dangerous things. In this paper, we survey the AGI containment problem -
the question of how to build a container in which tests can be conducted safely
and reliably, even on AGIs with unknown motivations and capabilities that could
be dangerous. We identify requirements for AGI containers, available
mechanisms, and weaknesses that need to be addressed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.08609v1,On the Hardness of Red-Blue Pebble Games,"Red-blue pebble games model the computation cost of a two-level memory
hierarchy. We present various hardness results in different red-blue pebbling
variants, with a focus on the oneshot model. We first study the relationship
between previously introduced red-blue pebble models (base, oneshot, nodel). We
also analyze a new variant (compcost) to obtain a more realistic model of
computation. We then prove that red-blue pebbling is NP-hard in all of these
model variants. Furthermore, we show that in the oneshot model, a
$\delta$-approximation algorithm for $\delta<2$ is only possible if the unique
games conjecture is false. Finally, we show that greedy algorithms are not good
candidates for approximation, since they can return significantly worse
solutions than the optimum.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0905.0451v1,Maximum Flow in Directed Planar Graphs with Vertex Capacities,"In this paper we present an O(n log n) algorithm for finding a maximum flow
in a directed planar graph, where the vertices are subject to capacity
constraints, in addition to the arcs. If the source and the sink are on the
same face, then our algorithm can be implemented in O(n) time.
  For general (not planar) graphs, vertex capacities do not make the problem
more difficult, as there is a simple reduction that eliminates vertex
capacities. However, this reduction does not preserve the planarity of the
graph. The essence of our algorithm is a different reduction that does preserve
the planarity, and can be implemented in linear time. For the special case of
undirected planar graph, an algorithm with the same time complexity was
recently claimed, but we show that it has a flaw.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1202.4626v11,The erny conjecture,"A word $w$ of letters on edges of underlying graph $\Gamma$ of deterministic
finite automaton (DFA) is called synchronizing if $w$ sends all states of the
automaton to a unique state. J. \v{C}erny discovered in 1964 a sequence of
$n$-state complete DFA possessing a minimal synchronizing word of length
$(n-1)^2$. The hypothesis, well known today as the \v{C}erny conjecture, claims
that it is also precise upper bound on the length of such a word for a complete
DFA. The hypothesis was formulated in 1966 by Starke. The problem has motivated
great and constantly growing number of investigations and generalizations. To
prove the conjecture, we use algebra w on a special class of row monomial
matrices (one unit and rest zeros in every row), induced by words in the
alphabet of labels on edges. These matrices generate a space with respect to
the mentioned operation. The proof is based on connection between length of
words $u$ and dimension of the space generated by solutions $L_x$ of matrix
equation $M_uL_x=M_s$ for synchronizing word $s$, as well as on the relation
between ranks of $M_u$ and $L_x$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0306134v2,The Complexity of Boolean Constraint Isomorphism,"In 1978, Schaefer proved his famous dichotomy theorem for generalized
satisfiability problems. He defined an infinite number of propositional
satisfiability problems (nowadays usually called Boolean constraint
satisfaction problems) and showed that all these satisfiability problems are
either in P or NP-complete. In recent years, similar results have been obtained
for quite a few other problems for Boolean constraints.Almost all of these
problems are variations of the satisfiability problem.
  In this paper, we address a problem that is not a variation of
satisfiability, namely, the isomorphism problem for Boolean constraints.
Previous work by B\""ohler et al. showed that the isomorphism problem is either
coNP-hard or reducible to the graph isomorphism problem (a problem that is in
NP, but not known to be NP-hard), thus distinguishing a hard case and an easier
case. However, they did not classify which cases are truly easy, i.e., in P.
This paper accomplishes exactly that. It shows that Boolean constraint
isomorphism is coNP-hard (and GI-hard), or equivalent to graph isomorphism, or
in P, and it gives simple criteria to determine which case holds.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9909012v1,Certificate Revocation Paradigms,"Research in the field of electronic signature confirmation has been active
for some 20 years now. Unfortunately present certificate-based solutions also
come from that age when no-one knew about online data transmission. The
official standardized X.509 framework also depends heavily on offline
operations, one of the most complicated ones being certificate revocation
handling. This is done via huge Certificate Revocation Lists which are both
inconvenient and expencive. Several improvements to these lists are proposed
and in this report we try to analyze them briefly. We conclude that although it
is possible to do better than in the original X.509 setting, none of the
solutions presented this far is good enough.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1802.05405v3,Putting a bug in ML: The moth olfactory network learns to read MNIST,"We seek to (i) characterize the learning architectures exploited in
biological neural networks for training on very few samples, and (ii) port
these algorithmic structures to a machine learning context. The Moth Olfactory
Network is among the simplest biological neural systems that can learn, and its
architecture includes key structural elements and mechanisms widespread in
biological neural nets, such as cascaded networks, competitive inhibition, high
intrinsic noise, sparsity, reward mechanisms, and Hebbian plasticity. These
structural biological elements, in combination, enable rapid learning.
  MothNet is a computational model of the Moth Olfactory Network, closely
aligned with the moth's known biophysics and with in vivo electrode data
collected from moths learning new odors. We assign this model the task of
learning to read the MNIST digits. We show that MothNet successfully learns to
read given very few training samples (1 to 10 samples per class). In this
few-samples regime, it outperforms standard machine learning methods such as
nearest-neighbors, support-vector machines, and neural networks (NNs), and
matches specialized one-shot transfer-learning methods but without the need for
pre-training. The MothNet architecture illustrates how algorithmic structures
derived from biological brains can be used to build alternative NNs that may
avoid some of the learning rate limitations of current engineered NNs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1607.04004v2,Filter Design for Generalized Frequency-Division Multiplexing,"In this paper, optimal filter design for generalized frequency-division
multiplexing (GFDM) is considered under two design criteria: rate maximization
and out-of-band (OOB) emission minimization. First, the problem of GFDM filter
optimization for rate maximization is formulated by expressing the transmission
rate of GFDM as a function of GFDM filter coefficients. It is shown that
Dirichlet filters are rate-optimal in additive white Gaussian noise (AWGN)
channels with no carrier frequency offset (CFO) under linear zero-forcing (ZF)
or minimum mean-square error (MMSE) receivers, but in general channels
perturbed by CFO a properly designed nontrivial GFDM filter can yield better
performance than Dirichlet filters by adjusting the subcarrier waveform to cope
with the channel-induced CFO. Next, the problem of GFDM filter design for OOB
emission minimization is formulated by expressing the power spectral density
(PSD) of the GFDM transmit signal as a function of GFDM filter coefficients,
and it is shown that the OOB emission can be reduced significantly by designing
the GFDM filter properly. Finally, joint design of GFDM filter and window for
the two design criteria is considered.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.08445v1,Improved Oracles for Time-Dependent Road Networks,"A novel landmark-based oracle (CFLAT) is presented, which provides
earliest-arrival-time route plans in time-dependent road networks. To our
knowledge, this is the first oracle that preprocesses combinatorial structures
(collections of time-stamped min-travel-time-path trees) rather than
travel-time functions. The preprocessed data structure is exploited by a new
query algorithm (CFCA) which computes (and pays for it), apart from
earliest-arrival-time estimations, the actual connecting path that preserves
the theoretical approximation guarantees. To make it practical and tackle the
main burden of landmark-based oracles (the large preprocessing requirements),
CFLAT is extensively engineered. A thorough experimental evaluation on two
real-world benchmark instances shows that CFLAT achieves a significant
improvement on preprocessing, approximation guarantees and query-times, in
comparison to previous landmark-based oracles, whose query algorithms do not
account for the path construction. It also achieves competitive query-time
performance and approximation guarantees compared to state-of-art speedup
heuristics for time-dependent road networks, whose query-times in most cases do
not account for path construction.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.10156v1,Learning to Persuade on the Fly: Robustness Against Ignorance,"We study a repeated persuasion setting between a sender and a receiver, where
at each time $t$, the sender observes a payoff-relevant state drawn
independently and identically from an unknown prior distribution, and shares
state information with the receiver, who then myopically chooses an action. As
in the standard setting, the sender seeks to persuade the receiver into
choosing actions that are aligned with the sender's preference by selectively
sharing information about the state. However, in contrast to the standard
models, the sender does not know the prior, and has to persuade while gradually
learning the prior on the fly.
  We study the sender's learning problem of making persuasive action
recommendations to achieve low regret against the optimal persuasion mechanism
with the knowledge of the prior distribution. Our main positive result is an
algorithm that, with high probability, is persuasive across all rounds and
achieves $O(\sqrt{T\log T})$ regret, where $T$ is the horizon length. The core
philosophy behind the design of our algorithm is to leverage robustness against
the sender's ignorance of the prior. Intuitively, at each time our algorithm
maintains a set of candidate priors, and chooses a persuasion scheme that is
simultaneously persuasive for all of them. To demonstrate the effectiveness of
our algorithm, we further prove that no algorithm can achieve regret better
than $\Omega(\sqrt{T})$, even if the persuasiveness requirements were
significantly relaxed. Therefore, our algorithm achieves optimal regret for the
sender's learning problem up to terms logarithmic in $T$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1002.0411v1,Face Identification by SIFT-based Complete Graph Topology,"This paper presents a new face identification system based on Graph Matching
Technique on SIFT features extracted from face images. Although SIFT features
have been successfully used for general object detection and recognition, only
recently they were applied to face recognition. This paper further investigates
the performance of identification techniques based on Graph matching topology
drawn on SIFT features which are invariant to rotation, scaling and
translation. Face projections on images, represented by a graph, can be matched
onto new images by maximizing a similarity function taking into account spatial
distortions and the similarities of the local features. Two graph based
matching techniques have been investigated to deal with false pair assignment
and reducing the number of features to find the optimal feature set between
database and query face SIFT features. The experimental results, performed on
the BANCA database, demonstrate the effectiveness of the proposed system for
automatic face identification.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0010008v1,The Light Lexicographic path Ordering,"We introduce syntactic restrictions of the lexicographic path ordering to
obtain the Light Lexicographic Path Ordering. We show that the light
lexicographic path ordering leads to a characterisation of the functions
computable in space bounded by a polynomial in the size of the inputs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1504.08308v3,"Efficient Image-Space Extraction and Representation of 3D Surface
  Topography","Surface topography refers to the geometric micro-structure of a surface and
defines its tactile characteristics (typically in the sub-millimeter range).
High-resolution 3D scanning techniques developed recently enable the 3D
reconstruction of surfaces including their surface topography. In his paper, we
present an efficient image-space technique for the extraction of surface
topography from high-resolution 3D reconstructions. Additionally, we filter
noise and enhance topographic attributes to obtain an improved representation
for subsequent topography classification. Comprehensive experiments show that
the our representation captures well topographic attributes and significantly
improves classification performance compared to alternative 2D and 3D
representations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0402032v1,Fitness inheritance in the Bayesian optimization algorithm,"This paper describes how fitness inheritance can be used to estimate fitness
for a proportion of newly sampled candidate solutions in the Bayesian
optimization algorithm (BOA). The goal of estimating fitness for some candidate
solutions is to reduce the number of fitness evaluations for problems where
fitness evaluation is expensive. Bayesian networks used in BOA to model
promising solutions and generate the new ones are extended to allow not only
for modeling and sampling candidate solutions, but also for estimating their
fitness. The results indicate that fitness inheritance is a promising concept
in BOA, because population-sizing requirements for building appropriate models
of promising solutions lead to good fitness estimates even if only a small
proportion of candidate solutions is evaluated using the actual fitness
function. This can lead to a reduction of the number of actual fitness
evaluations by a factor of 30 or more.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0705.4604v1,Temporal Runtime Verification using Monadic Difference Logic,"In this paper we present an algorithm for performing runtime verification of
a bounded temporal logic over timed runs. The algorithm consists of three
elements. First, the bounded temporal formula to be verified is translated into
a monadic first-order logic over difference inequalities, which we call monadic
difference logic. Second, at each step of the timed run, the monadic difference
formula is modified by computing a quotient with the state and time of that
step. Third, the resulting formula is checked for being a tautology or being
unsatisfiable by a decision procedure for monadic difference logic.
  We further provide a simple decision procedure for monadic difference logic
based on the data structure Difference Decision Diagrams. The algorithm is
complete in a very strong sense on a subclass of temporal formulae
characterized as homogeneously monadic and it is approximate on other formulae.
The approximation comes from the fact that not all unsatisfiable or
tautological formulae are recognised at the earliest possible time of the
runtime verification.
  Contrary to existing approaches, the presented algorithms do not work by
syntactic rewriting but employ efficient decision structures which make them
applicable in real applications within for instance business software.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1406.0285v1,Block-Structured Supermarket Models,"Supermarket models are a class of parallel queueing networks with an adaptive
control scheme that play a key role in the study of resource management of,
such as, computer networks, manufacturing systems and transportation networks.
When the arrival processes are non-Poisson and the service times are
non-exponential, analysis of such a supermarket model is always limited,
interesting, and challenging.
  This paper describes a supermarket model with non-Poisson inputs: Markovian
Arrival Processes (MAPs) and with non-exponential service times: Phase-type
(PH) distributions, and provides a generalized matrix-analytic method which is
first combined with the operator semigroup and the mean-field limit. When
discussing such a more general supermarket model, this paper makes some new
results and advances as follows: (1) Providing a detailed probability analysis
for setting up an infinite-dimensional system of differential vector equations
satisfied by the expected fraction vector, where ""the invariance of environment
factors"" is given as an important result. (2) Introducing the phase-type
structure to the operator semigroup and to the mean-field limit, and a
Lipschitz condition can be obtained by means of a unified matrix-differential
algorithm. (3) The matrix-analytic method is used to compute the fixed point
which leads to performance computation of this system. Finally, we use some
numerical examples to illustrate how the performance measures of this
supermarket model depend on the non-Poisson inputs and on the non-exponential
service times. Thus the results of this paper give new highlight on
understanding influence of non-Poisson inputs and of non-exponential service
times on performance measures of more general supermarket models.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1203.3724v2,"Static Analysis of Run-Time Errors in Embedded Real-Time Parallel C
  Programs","We present a static analysis by Abstract Interpretation to check for run-time
errors in parallel and multi-threaded C programs. Following our work on
Astr\'ee, we focus on embedded critical programs without recursion nor dynamic
memory allocation, but extend the analysis to a static set of threads
communicating implicitly through a shared memory and explicitly using a finite
set of mutual exclusion locks, and scheduled according to a real-time
scheduling policy and fixed priorities. Our method is thread-modular. It is
based on a slightly modified non-parallel analysis that, when analyzing a
thread, applies and enriches an abstract set of thread interferences. An
iterator then re-analyzes each thread in turn until interferences stabilize. We
prove the soundness of our method with respect to the sequential consistency
semantics, but also with respect to a reasonable weakly consistent memory
semantics. We also show how to take into account mutual exclusion and thread
priorities through a partitioning over an abstraction of the scheduler state.
We present preliminary experimental results analyzing an industrial program
with our prototype, Th\'es\'ee, and demonstrate the scalability of our
approach.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.05041v2,Blends in Maple,"A blend of two Taylor series for the same smooth real- or complex-valued
function of a single variable can be useful for approximation. We use an
explicit formula for a two-point Hermite interpolational polynomial to
construct such blends. We show a robust Maple implementation that can stably
and efficiently evaluate blends using linear-cost Horner form, evaluate their
derivatives to arbitrary order at the same time, or integrate a blend exactly.
The implementation is suited for use with evalhf. We provide a top-level user
interface and efficient module exports for programmatic use. This work was
presented at the Maple Conference 2020. See www.maplesoft.com/mapleconference",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1906.06600v1,A Statistical View on Synthetic Aperture Imaging for Occlusion Removal,"Synthetic apertures find applications in many fields, such as radar, radio
telescopes, microscopy, sonar, ultrasound, LiDAR, and optical imaging. They
approximate the signal of a single hypothetical wide aperture sensor with
either an array of static small aperture sensors or a single moving small
aperture sensor. Common sense in synthetic aperture sampling is that a dense
sampling pattern within a wide aperture is required to reconstruct a clear
signal. In this article we show that there exists practical limits to both,
synthetic aperture size and number of samples for the application of occlusion
removal. This leads to an understanding on how to design synthetic aperture
sampling patterns and sensors in a most optimal and practically efficient way.
We apply our findings to airborne optical sectioning which uses camera drones
and synthetic aperture imaging to computationally remove occluding vegetation
or trees for inspecting ground surfaces.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0503091v1,Resource Bounded Unprovability of Computational Lower Bounds,"This paper introduces new notions of asymptotic proofs,
PT(polynomial-time)-extensions, PTM(polynomial-time Turing
machine)-omega-consistency, etc. on formal theories of arithmetic including PA
(Peano Arithmetic). This paper shows that P not= NP (more generally, any
super-polynomial-time lower bound in PSPACE) is unprovable in a
PTM-omega-consistent theory T, where T is a consistent PT-extension of PA. This
result gives a unified view to the existing two major negative results on
proving P not= NP, Natural Proofs and relativizable proofs, through the two
manners of characterization of PTM-omega-consistency. We also show that the
PTM-omega-consistency of T cannot be proven in any PTM-omega-consistent theory
S, where S is a consistent PT-extension of T.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1310.6303v1,Simulation Over One-counter Nets is PSPACE-Complete,"One-counter nets (OCN) are Petri nets with exactly one unbounded place. They
are equivalent to a subclass of one-counter automata with just a weak test for
zero. Unlike many other semantic equivalences, strong and weak simulation
preorder are decidable for OCN, but the computational complexity was an open
problem. We show that both strong and weak simulation preorder on OCN are
PSPACE-complete.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.13544v1,"Improving Human Decisions by Adjusting the Alerting Thresholds for
  Computer Alerting Tools According to User and Task Characteristics","Objective: To investigate whether performance (number of correct decisions)
of humans supported by a computer alerting tool can be improved by tailoring
the tool's alerting threshold (sensitivity/specificity combination) according
to user ability and task difficulty. Background: Many researchers implicitly
assume that for each tool there exists a single ideal threshold. But research
shows the effects of alerting tools on decision errors to vary depending on
variables such as user ability and task difficulty. These findings motivated
our investigation. Method: Forty-seven participants edited text passages, aided
by a simulated spell-checker tool. We experimentally manipulated passage
difficulty and tool alerting threshold, measured participants' editing and
dictation ability, and counted participants' decision errors (false positives +
false negatives). We investigated whether alerting threshold, user ability,
task difficulty and their interactions affected error count. Results: Which
alerting threshold better helped a user depended on an interaction between user
ability and passage difficulty. Some effects were large: for higher ability
users, a more sensitive threshold reduced errors by 30%, on the easier
passages. Participants were not significantly more likely to prefer the
alerting threshold with which they performed better. Conclusion: Adjusting
alerting thresholds for individual users' ability and task difficulty could
substantially improve effects of automated alerts on user decisions. This
potential deserves further investigation. Improvement size and rules for
adjustment will be application-specific. Application: Computer alerting tools
have critical roles in many domains. Designers should assess potential benefits
of adjustable alerting thresholds for their specific CAT application. Guidance
for choosing thresholds will be essential for realizing these benefits in
practice.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,1,0,1,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.04863v2,"Learning to Utilize Correlated Auxiliary Noise: A Possible Quantum
  Advantage","This paper has two messages. First, we demonstrate that neural networks that
process noisy data can learn to exploit, when available, access to auxiliary
noise that is correlated with the noise on the data. In effect, the network
learns to use the correlated auxiliary noise as an approximate key to decipher
its noisy input data. Second, we show that, for this task, the scaling behavior
with increasing noise is such that future quantum machines could possess an
advantage. In particular, decoherence generates correlated auxiliary noise in
the environment. The new approach could, therefore, help enable future quantum
machines by providing machine-learned quantum error correction.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1610.02374v1,"Unified Control and Data Flow Diagrams Applied to Software Engineering
  and other Systems","More often than not, there is a need to understand the structure of complex
computer code: what functions and in what order they are called, how
information travels around static, input, and output variables, what depends on
what. As a rule, executable code and data are scattered among multiple files
and even multiple modules. Information is transmitted among variables which
often change names. These tangled relations greatly complicate the development,
maintenance, and redevelopment of code, its analysis for complexity and its
robustness. As of now, there is no tool which is capable of presenting the
real-life, useful diagram of actual code. Conventional flowcharts fail.
Proposed is the method which overcomes these difficulties. The main idea is
that functionality of software can be described through flows of control, which
is essentially flows of time, and flows of data. These are inseparable. The
second idea is to follow very strict system boundaries and distinctions with
respect to modules, functions, blocks, and operators, as well as data holders,
showing them all as subsystems, in other words, by clearly expressing the
system structure when every piece of executable code and every variable may
have its own graphical representation. The third is defining timelines as the
entities clearly separated from the connected blocks of code. Timelines allow
presentation of nesting of the control flow as deep as necessary. As a proof of
concept, the same methods successfully describe production systems. Keywords:
flowchart, UML, software diagram, visual programming, extreme programming,
extreme modeling, control flow, data flow.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.08126v1,"Testing the Quantitative Spacetime Hypothesis using Artificial Narrative
  Comprehension (I) : Bootstrapping Meaning from Episodic Narrative viewed as a
  Feature Landscape","The problem of extracting important and meaningful parts of a sensory data
stream, without prior training, is studied for symbolic sequences, by using
textual narrative as a test case. This is part of a larger study concerning the
extraction of concepts from spacetime processes, and their knowledge
representations within hybrid symbolic-learning `Artificial Intelligence'. Most
approaches to text analysis make extensive use of the evolved human sense of
language and semantics. In this work, streams are parsed without knowledge of
semantics, using only measurable patterns (size and time) within the changing
stream of symbols -- as an event `landscape'. This is a form of interferometry.
Using lightweight procedures that can be run in just a few seconds on a single
CPU, this work studies the validity of the Semantic Spacetime Hypothesis, for
the extraction of concepts as process invariants. This `semantic preprocessor'
may then act as a front-end for more sophisticated long-term graph-based
learning techniques. The results suggest that what we consider important and
interesting about sensory experience is not solely based on higher reasoning,
but on simple spacetime process cues, and this may be how cognitive processing
is bootstrapped in the beginning.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.11875v2,Bayesian Conditional GAN for MRI Brain Image Synthesis,"As a powerful technique in medical imaging, image synthesis is widely used in
applications such as denoising, super resolution and modality transformation
etc. Recently, the revival of deep neural networks made immense progress in the
field of medical imaging. Although many deep leaning based models have been
proposed to improve the image synthesis accuracy, the evaluation of the model
uncertainty, which is highly important for medical applications, has been a
missing part. In this work, we propose to use Bayesian conditional generative
adversarial network (GAN) with concrete dropout to improve image synthesis
accuracy. Meanwhile, an uncertainty calibration approach is involved in the
whole pipeline to make the uncertainty generated by Bayesian network
interpretable. The method is validated with the T1w to T2w MR image translation
with a brain tumor dataset of 102 subjects. Compared with the conventional
Bayesian neural network with Monte Carlo dropout, results of the proposed
method reach a significant lower RMSE with a p-value of 0.0186. Improvement of
the calibration of the generated uncertainty by the uncertainty recalibration
method is also illustrated.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.07228v1,Tableaux for Free Logics with Descriptions,"The paper provides a tableau approach to definite descriptions. We focus on
several formalizations of the so-called minimal free description theory (MFD)
usually formulated axiomatically in the setting of free logic. We consider five
analytic tableau systems corresponding to different kinds of free logic,
including the logic of definedness applied in computer science and constructive
mathematics for dealing with partial functions (here called negative quasi-free
logic). The tableau systems formalise MFD based on PFL (positive free logic),
NFL (negative free logic), PQFL and NQFL (the quasi-free counterparts of the
former ones). Also the logic NQFLm is taken into account, which is equivalent
to NQFL, but whose language does not comprise the existence predicate. It is
shown that all tableaux are sound and complete with respect to the semantics of
these logics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0801.4268v1,Protecting Spreadsheets Against Fraud,"Previous research on spreadsheet risks has predominantly focussed on errors
inadvertently introduced by spreadsheet writers i.e. it focussed on the
end-user aspects of spreadsheet development. When analyzing a faulty
spreadsheet, one might not be able to determine whether a particular error
(fault) has been made by mistake or with fraudulent intentions. However, the
fences protecting against fraudulent errors have to be different from those
shielding against inadvertent mistakes. Faults resulting from errors committed
inadvertently can be prevented ab initio by tools that notify the spreadsheet
writer about potential problems whereas faults that are introduced on purpose
have to be discovered by auditors without the cooperation of their originators.
Even worse, some spreadsheet writers will do their best to conceal fraudulent
parts of their spreadsheets from auditors. In this paper we survey the
available means for fraud protection by contrasting approaches suitable for
spreadsheets with those known from fraud protection for conventional software.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,1
http://arxiv.org/abs/2103.12095v2,"Am I fit for this physical activity? Neural embedding of physical
  conditioning from inertial sensors","Inertial Measurement Unit (IMU) sensors are present in everyday devices such
as smartphones and fitness watches. As a result, the array of health-related
research and applications that tap onto this data has been growing, but little
attention has been devoted to the prediction of an individual's heart rate (HR)
from IMU data, when undergoing a physical activity. Would that be even
possible? If so, this could be used to design personalized sets of aerobic
exercises, for instance. In this work, we show that it is viable to obtain
accurate HR predictions from IMU data using Recurrent Neural Networks, provided
only access to HR and IMU data from a short-lived, previously executed
activity. We propose a novel method for initializing an RNN's hidden state
vectors, using a specialized network that attempts to extract an embedding of
the physical conditioning (PCE) of a subject. We show that using a
discriminator in the training phase to help the model learn whether two PCEs
belong to the same individual further reduces the prediction error. We evaluate
the proposed model when predicting the HR of 23 subjects performing a variety
of physical activities from IMU data available in public datasets (PAMAP2,
PPG-DaLiA). For comparison, we use as baselines the only model specifically
proposed for this task and an adapted state-of-the-art model for Human Activity
Recognition (HAR), a closely related task. Our method, PCE-LSTM, yields over
10% lower mean absolute error. We demonstrate empirically that this error
reduction is in part due to the use of the PCE. Last, we use the two datasets
(PPG-DaLiA, WESAD) to show that PCE-LSTM can also be successfully applied when
photoplethysmography (PPG) sensors are available, outperforming the
state-of-the-art deep learning baselines by more than 30%.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1408.1928v1,"Microtask crowdsourcing for disease mention annotation in PubMed
  abstracts","Identifying concepts and relationships in biomedical text enables knowledge
to be applied in computational analyses. Many biological natural language
process (BioNLP) projects attempt to address this challenge, but the state of
the art in BioNLP still leaves much room for improvement. Progress in BioNLP
research depends on large, annotated corpora for evaluating information
extraction systems and training machine learning models. Traditionally, such
corpora are created by small numbers of expert annotators often working over
extended periods of time. Recent studies have shown that workers on microtask
crowdsourcing platforms such as Amazon's Mechanical Turk (AMT) can, in
aggregate, generate high-quality annotations of biomedical text. Here, we
investigated the use of the AMT in capturing disease mentions in PubMed
abstracts. We used the NCBI Disease corpus as a gold standard for refining and
benchmarking our crowdsourcing protocol. After several iterations, we arrived
at a protocol that reproduced the annotations of the 593 documents in the
training set of this gold standard with an overall F measure of 0.872
(precision 0.862, recall 0.883). The output can also be tuned to optimize for
precision (max = 0.984 when recall = 0.269) or recall (max = 0.980 when
precision = 0.436). Each document was examined by 15 workers, and their
annotations were merged based on a simple voting method. In total 145 workers
combined to complete all 593 documents in the span of 1 week at a cost of $.06
per abstract per worker. The quality of the annotations, as judged with the F
measure, increases with the number of workers assigned to each task such that
the system can be tuned to balance cost against quality. These results
demonstrate that microtask crowdsourcing can be a valuable tool for generating
well-annotated corpora in BioNLP.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.12978v1,Equivalence of Hidden Markov Models with Continuous Observations,"We consider Hidden Markov Models that emit sequences of observations that are
drawn from continuous distributions. For example, such a model may emit a
sequence of numbers, each of which is drawn from a uniform distribution, but
the support of the uniform distribution depends on the state of the Hidden
Markov Model. Such models generalise the more common version where each
observation is drawn from a finite alphabet. We prove that one can determine in
polynomial time whether two Hidden Markov Models with continuous observations
are equivalent.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1306.2815v1,I/O-Efficient Planar Range Skyline and Attrition Priority Queues,"In the planar range skyline reporting problem, we store a set P of n 2D
points in a structure such that, given a query rectangle Q = [a_1, a_2] x [b_1,
b_2], the maxima (a.k.a. skyline) of P \cap Q can be reported efficiently. The
query is 3-sided if an edge of Q is grounded, giving rise to two variants:
top-open (b_2 = \infty) and left-open (a_1 = -\infty) queries.
  All our results are in external memory under the O(n/B) space budget, for
both the static and dynamic settings:
  * For static P, we give structures that answer top-open queries in O(log_B n
+ k/B), O(loglog_B U + k/B), and O(1 + k/B) I/Os when the universe is R^2, a U
x U grid, and a rank space grid [O(n)]^2, respectively (where k is the number
of reported points). The query complexity is optimal in all cases.
  * We show that the left-open case is harder, such that any linear-size
structure must incur \Omega((n/B)^e + k/B) I/Os for a query. We show that this
case is as difficult as the general 4-sided queries, for which we give a static
structure with the optimal query cost O((n/B)^e + k/B).
  * We give a dynamic structure that supports top-open queries in O(log_2B^e
(n/B) + k/B^1-e) I/Os, and updates in O(log_2B^e (n/B)) I/Os, for any e
satisfying 0 \le e \le 1. This leads to a dynamic structure for 4-sided queries
with optimal query cost O((n/B)^e + k/B), and amortized update cost O(log
(n/B)).
  As a contribution of independent interest, we propose an I/O-efficient
version of the fundamental structure priority queue with attrition (PQA). Our
PQA supports FindMin, DeleteMin, and InsertAndAttrite all in O(1) worst case
I/Os, and O(1/B) amortized I/Os per operation.
  We also add the new CatenateAndAttrite operation that catenates two PQAs in
O(1) worst case and O(1/B) amortized I/Os. This operation is a non-trivial
extension to the classic PQA of Sundar, even in internal memory.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1501.03641v2,On Computability and Triviality of Well Groups,"The concept of well group in a special but important case captures
homological properties of the zero set of a continuous map $f:K\to R^n$ on a
compact space K that are invariant with respect to perturbations of f. The
perturbations are arbitrary continuous maps within $L_\infty$ distance r from f
for a given r>0. The main drawback of the approach is that the computability of
well groups was shown only when dim K=n or n=1.
  Our contribution to the theory of well groups is twofold: on the one hand we
improve on the computability issue, but on the other hand we present a range of
examples where the well groups are incomplete invariants, that is, fail to
capture certain important robust properties of the zero set.
  For the first part, we identify a computable subgroup of the well group that
is obtained by cap product with the pullback of the orientation of R^n by f. In
other words, well groups can be algorithmically approximated from below. When f
is smooth and dim K<2n-2, our approximation of the (dim K-n)th well group is
exact.
  For the second part, we find examples of maps $f,f': K\to R^n$ with all well
groups isomorphic but whose perturbations have different zero sets. We discuss
on a possible replacement of the well groups of vector valued maps by an
invariant of a better descriptive power and computability status.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.15296v1,Fact or Factitious? Contextualized Opinion Spam Detection,"In this paper we perform an analytic comparison of a number of techniques
used to detect fake and deceptive online reviews. We apply a number machine
learning approaches found to be effective, and introduce our own approach by
fine-tuning state of the art contextualised embeddings. The results we obtain
show the potential of contextualised embeddings for fake review detection, and
lay the groundwork for future research in this area.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1504.01563v1,"""How much?"" Is Not Enough - An Analysis of Open Budget Initiatives","A worldwide movement towards the publication of Open Government Data is
taking place, and budget data is one of the key elements pushing this trend.
Its importance is mostly related to transparency, but publishing budget data,
combined with other actions, can also improve democratic participation, allow
comparative analysis of governments and boost data-driven business. However,
the lack of standards and common evaluation criteria still hinders the
development of appropriate tools and the materialization of the appointed
benefits. In this paper, we present a model to analyse government initiatives
to publish budget data. We identify the main features of these initiatives with
a double objective: (i) to drive a structured analysis, relating some
dimensions to their possible impacts, and (ii) to derive characterization
attributes to compare initiatives based on each dimension. We define use
perspectives and analyse some initiatives using this model. We conclude that,
in order to favour use perspectives, special attention must be given to user
feedback, semantics standards and linking possibilities.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1306.0225v10,"Convergence Analysis and Parallel Computing Implementation for the
  Multiagent Coordination Optimization Algorithm","In this report, a novel variation of Particle Swarm Optimization (PSO)
algorithm, called Multiagent Coordination Optimization (MCO), is implemented in
a parallel computing way for practical use by introducing MATLAB built-in
function ""parfor"" into MCO. Then we rigorously analyze the global convergence
of MCO by means of semistability theory. Besides sharing global optimal
solutions with the PSO algorithm, the MCO algorithm integrates cooperative
swarm behavior of multiple agents into the update formula by sharing velocity
and position information between neighbors to improve its performance.
Numerical evaluation of the parallel MCO algorithm is provided in the report by
running the proposed algorithm on supercomputers in the High Performance
Computing Center at Texas Tech University. In particular, the optimal value and
consuming time are compared with PSO and serial MCO by solving several
benchmark functions in the literature, respectively. Based on the simulation
results, the performance of the parallel MCO is not only superb compared with
PSO for solving many nonlinear, noncovex optimization problems, but also is of
high efficiency by saving the computational time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.01253v1,"New efficient time-stepping schemes for the anisotropic phase-field
  dendritic crystal growth model","In this paper, we propose and analyze a first-order and a second-order
time-stepping schemes for the anisotropic phase-field dendritic crystal growth
model. The proposed schemes are based on an auxiliary variable approach for the
Allen-Cahn equation and delicate treatment of the terms coupling the Allen-Cahn
equation and temperature equation. The idea of the former is to introduce
suitable auxiliary variables to facilitate construction of high order stable
schemes for a large class of gradient flows. We propose a new technique to
treat the coupling terms involved in the crystal growth model and introduce
suitable stabilization terms to result in totally decoupled schemes, which
satisfy a discrete energy law without affecting the convergence order. A
delicate implementation demonstrates that the proposed schemes can be realized
in a very efficient way. That is, it only requires solving four linear elliptic
equations and a simple algebraic equation at each time step. A detailed
comparison with existing schemes is given, and the advantage of the new schemes
are emphasized. As far as we know this is the first second-order scheme that is
totally decoupled, linear, unconditionally stable for the dendritic crystal
growth model with variable mobility parameter.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.05223v2,Guarded Cubical Type Theory: Path Equality for Guarded Recursion,"This paper improves the treatment of equality in guarded dependent type
theory (GDTT), by combining it with cubical type theory (CTT). GDTT is an
extensional type theory with guarded recursive types, which are useful for
building models of program logics, and for programming and reasoning with
coinductive types. We wish to implement GDTT with decidable type-checking,
while still supporting non-trivial equality proofs that reason about the
extensions of guarded recursive constructions. CTT is a variation of
Martin-L\""of type theory in which the identity type is replaced by abstract
paths between terms. CTT provides a computational interpretation of functional
extensionality, is conjectured to have decidable type checking, and has an
implemented type-checker. Our new type theory, called guarded cubical type
theory, provides a computational interpretation of extensionality for guarded
recursive types. This further expands the foundations of CTT as a basis for
formalisation in mathematics and computer science. We present examples to
demonstrate the expressivity of our type theory, all of which have been checked
using a prototype type-checker implementation, and present semantics in a
presheaf category.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.13486v1,When Creators Meet the Metaverse: A Survey on Computational Arts,"The metaverse, enormous virtual-physical cyberspace, has brought
unprecedented opportunities for artists to blend every corner of our physical
surroundings with digital creativity. This article conducts a comprehensive
survey on computational arts, in which seven critical topics are relevant to
the metaverse, describing novel artworks in blended virtual-physical realities.
The topics first cover the building elements for the metaverse, e.g., virtual
scenes and characters, auditory, textual elements. Next, several remarkable
types of novel creations in the expanded horizons of metaverse cyberspace have
been reflected, such as immersive arts, robotic arts, and other user-centric
approaches fuelling contemporary creative outputs. Finally, we propose several
research agendas: democratising computational arts, digital privacy, and safety
for metaverse artists, ownership recognition for digital artworks,
technological challenges, and so on. The survey also serves as introductory
material for artists and metaverse technologists to begin creations in the
realm of surrealistic cyberspace.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.04049v1,A Human Dimension of Hacking: Social Engineering through Social Media,"Social engineering through social media channels targeting organizational
employees is emerging as one of the most challenging information security
threats. Social engineering defies traditional security efforts due to the
method of attack relying on human naivet\'e or error. The vast amount of
information now made available to social engineers through online social
networks is facilitating methods of attack which rely on some form of human
error to enable infiltration into company networks. While, paramount to
organisational information security objectives is the introduction of relevant
comprehensive policy and guideline, perspectives and practices vary from global
region to region. This paper identifies such regional variations and then
presents a detailed investigation on information security outlooks and
practices, surrounding social media, in Australian organisations (both public
and private). Results detected disparate views and practices, suggesting
further work is needed to achieve effective protection against security threats
arsing due to social media adoption.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1702.05425v2,Exact clustering in linear time,"The time complexity of data clustering has been viewed as fundamentally
quadratic, slowing with the number of data items, as each item is compared for
similarity to preceding items. Clustering of large data sets has been
infeasible without resorting to probabilistic methods or to capping the number
of clusters. Here we introduce MIMOSA, a novel class of algorithms which
achieve linear time computational complexity on clustering tasks. MIMOSA
algorithms mark and match partial-signature keys in a hash table to obtain
exact, error-free cluster retrieval. Benchmark measurements, on clustering a
data set of 10,000,000 news articles by news topic, found that a MIMOSA
implementation finished more than four orders of magnitude faster than a
standard centroid implementation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1605.00406v2,"A Tight Lower Bound for the Weights of Maximum Weight Matching in
  Bipartite Graphs","Let $\Ga$ be the collection of all weighted bipartite graphs each having
$\sigma$ and $m$, as the size of a vertex partition and the total weight,
respectively. We give a tight lower bound $\lceil \frac{m-\sigma}{\sigma}
\rceil+1$ for the set $\{\textit{Wt}(\textit{mwm}(G))~|~G \in \Ga\}$ which
denotes the collection of weights of maximum weight bipartite matchings of all
graphs in $\Ga$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1406.1953v2,Automatic Extraction of Protein Interaction in Literature,"Protein-protein interaction extraction is the key precondition of the
construction of protein knowledge network, and it is very important for the
research in the biomedicine. This paper extracted directional protein-protein
interaction from the biological text, using the SVM-based method. Experiments
were evaluated on the LLL05 corpus with good results. The results show that
dependency features are import for the protein-protein interaction extraction
and features related to the interaction word are effective for the interaction
direction judgment. At last, we analyzed the effects of different features and
planed for the next step.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1509.01277v2,"A Dataset for Improved RGBD-based Object Detection and Pose Estimation
  for Warehouse Pick-and-Place","An important logistics application of robotics involves manipulators that
pick-and-place objects placed in warehouse shelves. A critical aspect of this
task corre- sponds to detecting the pose of a known object in the shelf using
visual data. Solving this problem can be assisted by the use of an RGB-D
sensor, which also provides depth information beyond visual data. Nevertheless,
it remains a challenging problem since multiple issues need to be addressed,
such as low illumination inside shelves, clutter, texture-less and reflective
objects as well as the limitations of depth sensors. This paper provides a new
rich data set for advancing the state-of-the-art in RGBD- based 3D object pose
estimation, which is focused on the challenges that arise when solving
warehouse pick- and-place tasks. The publicly available data set includes
thousands of images and corresponding ground truth data for the objects used
during the first Amazon Picking Challenge at different poses and clutter
conditions. Each image is accompanied with ground truth information to assist
in the evaluation of algorithms for object detection. To show the utility of
the data set, a recent algorithm for RGBD-based pose estimation is evaluated in
this paper. Based on the measured performance of the algorithm on the data set,
various modifications and improvements are applied to increase the accuracy of
detection. These steps can be easily applied to a variety of different
methodologies for object pose detection and improve performance in the domain
of warehouse pick-and-place.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.00882v2,k-apices of minor-closed graph classes. I. Bounding the obstructions,"Let ${\cal G}$ be a minor-closed graph class. We say that a graph $G$ is a
$k$-apex of ${\cal G}$ if $G$ contains a set $S$ of at most $k$ vertices such
that $G\setminus S$ belongs to ${\cal G}.$ We denote by ${\cal A}_k ({\cal G})$
the set of all graphs that are $k$-apices of ${\cal G}.$ We prove that every
graph in the obstruction set of ${\cal A}_k ({\cal G}),$ i.e., the
minor-minimal set of graphs not belonging to ${\cal A}_k ({\cal G}),$ has size
at most $2^{2^{2^{2^{{\sf poly}(k)}}}},$ where ${\sf poly}$ is a polynomial
function whose degree depends on the size of the minor-obstructions of ${\cal
G}.$ This bound drops to $2^{2^{{\sf poly}(k)}}$ when ${\cal G}$ excludes some
apex graph as a minor.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1212.0892v3,An Intuitive Approach to Inertial Sensor Bias Estimation,"A simple approach to gyro and accelerometer bias estimation is proposed. It
does not involve Kalman filtering or similar formal techniques. Instead, it is
based on physical intuition and exploits a duality between gimbaled and
strapdown inertial systems. The estimation problem is decoupled into two
separate stages. At the first stage, inertial system attitude errors are
corrected by means of a feedback from an external aid. In the presence of
uncompensated biases, the steady-state feedback rebalances those biases and can
be used to estimate them. At the second stage, the desired bias estimates are
expressed in a closed form in terms of the feedback signal. The estimator has
only three tunable parameters and is easy to implement and use. The tests
proved the feasibility of the proposed approach for the estimation of low-cost
MEMS inertial sensor biases on a moving land vehicle.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1703.03831v1,"User Longevity and Engagement in Mobile Multiplayer Sports Management
  Games","Mobile games are extremely popular and engage millions of people every day.
Even if they are often quite simple, their development features a high degree
of difficulty and requires close attention to both achieve a high satisfaction
from users and grant a return to the developers. In this paper we propose a
model that analyzes users' playing session time in order to evaluate and
maximize the longevity of games, even during the first phases of their
development.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0808.0962v1,"Verification of Peterson's Algorithm for Leader Election in a
  Unidirectional Asynchronous Ring Using NuSMV","The finite intrinsic nature of the most distributed algorithms gives us this
ability to use model checking tools for verification of this type of
algorithms. In this paper, I attempt to use NuSMV as a model checking tool for
verifying necessary properties of Peterson's algorithm for leader election
problem in a unidirectional asynchronous ring topology. Peterson's algorithm
for an asynchronous ring supposes that each node in the ring has a unique ID
and also a queue for dealing with storage problem. By considering that the
queue can have any combination of values, a constructed model for a ring with
only four nodes will have more than a billion states. Although it seems that
model checking is not a feasible approach for this problem, I attempt to use
several effective limiting assumptions for hiring formal model checking
approach without losing the correct functionality of the Peterson's algorithm.
These enforced limiting assumptions target the degree of freedom in the model
checking process and significantly decrease the CPU time, memory usage and the
total number of page faults. By deploying these limitations, the number of
nodes can be increased from four to eight in the model checking process with
NuSMV.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1712.06481v3,"Inductive $k$-independent graphs and $c$-colorable subgraphs in
  scheduling: A review","Inductive $k$-independent graphs generalize chordal graphs and have recently
been advocated in the context of interference-avoiding wireless communication
scheduling. The NP-hard problem of finding maximum-weight induced $c$-colorable
subgraphs, which is a generalization of finding maximum independent sets,
naturally occurs when selecting $c$ sets of pairwise non-conflicting jobs
(modeled as graph vertices). We investigate the parameterized complexity of
this problem on inductive $k$-independent graphs. We show that the Independent
Set problem is W[1]-hard even on 2-simplicial 3-minoes---a subclass of
inductive 2-independent graphs. In contrast, we prove that the more general
Maximum $c$-Colorable Subgraph problem is fixed-parameter tractable on
edge-wise unions of cluster and chordal graphs, which are 2-simplicial. In both
cases, the parameter is the solution size. Aside from this, we survey other
graph classes between inductive 1-inductive and inductive 2-inductive graphs
with applications in scheduling.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.12998v2,"Including Sparse Production Knowledge into Variational Autoencoders to
  Increase Anomaly Detection Reliability","Digitalization leads to data transparency for production systems that we can
benefit from with data-driven analysis methods like neural networks. For
example, automated anomaly detection enables saving resources and optimizing
the production. We study using rarely occurring information about labeled
anomalies into Variational Autoencoder neural network structures to overcome
information deficits of supervised and unsupervised approaches. This method
outperforms all other models in terms of accuracy, precision, and recall. We
evaluate the following methods: Principal Component Analysis, Isolation Forest,
Classifying Neural Networks, and Variational Autoencoders on seven time series
datasets to find the best performing detection methods. We extend this idea to
include more infrequently occurring meta information about production
processes. This use of sparse labels, both of anomalies or production data,
allows to harness any additional information available for increasing anomaly
detection performance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.06761v1,Curricular Complexity Versus Quality of Computer Science Programs,"In this research paper we describe a study that involves measuring the
complexities of undergraduate curricula offered by computer science
departments, and then comparing them to the quality of these departments, where
quality is determined by a metric-based ranking system. The study objective was
to determine whether or not a relationship exists between the quality of
computer science departments and the complexity of the curricula they offer.
The relationship between curricular complexity and program quality was
previously investigated for the case of undergraduate electrical engineering
programs, with surprising results. It was found that if the US News & World
Report Best Undergraduate Programs ranking is used as a proxy for quality, then
a statistically significant difference in curricular complexities exists
between higher and lower quality electrical engineering programs. Furthermore,
it was found that higher quality electrical engineering programs tend to have
lower complexity curricula, and vice versa. In the study reported in this
paper, a sufficient amount of data was collected in order to determine that an
inverse relationship between program quality and curricular complexity also
exists in undergraduate computer science departments. This brings up an
interesting question regarding the extent to which this phenomenon exists
across the spectrum of STEM disciplines.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0
http://arxiv.org/abs/1508.05056v2,"Diving Deep into Sentiment: Understanding Fine-tuned CNNs for Visual
  Sentiment Prediction","Visual media are powerful means of expressing emotions and sentiments. The
constant generation of new content in social networks highlights the need of
automated visual sentiment analysis tools. While Convolutional Neural Networks
(CNNs) have established a new state-of-the-art in several vision problems,
their application to the task of sentiment analysis is mostly unexplored and
there are few studies regarding how to design CNNs for this purpose. In this
work, we study the suitability of fine-tuning a CNN for visual sentiment
prediction as well as explore performance boosting techniques within this deep
learning setting. Finally, we provide a deep-dive analysis into a benchmark,
state-of-the-art network architecture to gain insight about how to design
patterns for CNNs on the task of visual sentiment prediction.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1707.00106v5,"Simultaneous Consecutive Ones Submatrix and Editing Problems : Classical
  Complexity \& Fixed-Parameter Tractable Results","A binary matrix $M$ has the consecutive ones property ($C1P$) for rows (resp.
columns) if there is a permutation of its columns (resp. rows) that arranges
the ones consecutively in all the rows (resp. columns). If $M$ has the $C1P$
for rows and columns, then $M$ is said to have the simultaneous consecutive
ones property ($SC1P$). In this article, we consider the classical complexity
and fixed-parameter tractability of $(a)$ Simultaneous Consecutive Ones
Submatrix ($SC1S$) and $(b)$ Simultaneous Consecutive Ones Editing ($SC1E$)
[Oswald et al., Theoretical Comp. Sci. 410(21-23):1986-1992,
\hyperref[references]{2009}] problems. We show that the decision versions of
$SC1S$ and $SC1E$ problems are NP-complete. We consider the parameterized
versions of $SC1S$ and $SC1E$ problems with $d$, being the solution size, as
the parameter. Given a binary matrix $M$ and a positive integer $d$,
$d$-$SC1S$-$R$, $d$-$SC1S$-$C$, and $d$-$SC1S$-$RC$ problems decide whether
there exists a set of rows, columns, and rows as well as columns, respectively,
of size at most $d$, whose deletion results in a matrix with the $SC1P$. The
$d$-$SC1P$-$0E$, $d$-$SC1P$-$1E$, and $d$-$SC1P$-$01E$ problems decide whether
there exists a set of $0$-entries, $1$-entries, and $0$-entries as well as
$1$-entries, respectively, of size at most $d$, whose flipping results in a
matrix with the \vspace{0.095 in} $SC1P$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1706.09557v1,Machine listening intelligence,"This manifesto paper will introduce machine listening intelligence, an
integrated research framework for acoustic and musical signals modelling, based
on signal processing, deep learning and computational musicology.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0312003v1,Hybrid LQG-Neural Controller for Inverted Pendulum System,"The paper presents a hybrid system controller, incorporating a neural and an
LQG controller. The neural controller has been optimized by genetic algorithms
directly on the inverted pendulum system. The failure free optimization process
stipulated a relatively small region of the asymptotic stability of the neural
controller, which is concentrated around the regulation point. The presented
hybrid controller combines benefits of a genetically optimized neural
controller and an LQG controller in a single system controller. High quality of
the regulation process is achieved through utilization of the neural
controller, while stability of the system during transient processes and a wide
range of operation are assured through application of the LQG controller. The
hybrid controller has been validated by applying it to a simulation model of an
inherently unstable system of inverted pendulum.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1210.6052v1,"Leveraging Peer Centrality in the Design of Socially-Informed
  Peer-to-Peer Systems","Social applications mine user social graphs to improve performance in search,
provide recommendations, allow resource sharing and increase data privacy. When
such applications are implemented on a peer-to-peer (P2P) architecture, the
social graph is distributed on the P2P system: the traversal of the social
graph translates into a socially-informed routing in the peer-to-peer layer. In
this work we introduce the model of a projection graph that is the result of
decentralizing a social graph onto a peer-to-peer network. We focus on three
social network metrics: degree, node betweenness and edge betweenness
centrality and analytically formulate the relation between metrics in the
social graph and in the projection graph. Through experimental evaluation on
real networks, we demonstrate that when mapping user communities of sizes up to
50-150 users on each peer, the association between the properties of the social
graph and the projection graph is high, and thus the properties of the
(dynamic) projection graph can be inferred from the properties of the (slower
changing) social graph. Furthermore, we demonstrate with two application
scenarios on large-scale social networks the usability of the projection graph
in designing social search applications and unstructured P2P overlays.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1407.3008v3,Bigtable Merge Compaction,"NoSQL databases are widely used for massive data storage and real-time web
applications. Yet important aspects of these data structures are not well
understood. For example, NoSQL databases write most of their data to a
collection of files on disk, meanwhile periodically compacting subsets of these
files. A compaction policy must choose which files to compact, and when to
compact them, without knowing the future workload. Although these choices can
affect computational efficiency by orders of magnitude, existing literature
lacks tools for designing and analyzing online compaction policies --- policies
are now chosen largely by trial and error.
  Here we introduce tools for the design and analysis of compaction policies
for Google Bigtable, propose new policies, give average-case and worst-case
competitive analyses, and present preliminary empirical benchmarks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0705.0751v1,Approximate textual retrieval,"An approximate textual retrieval algorithm for searching sources with high
levels of defects is presented. It considers splitting the words in a query
into two overlapping segments and subsequently building composite regular
expressions from interlacing subsets of the segments. This procedure reduces
the probability of missed occurrences due to source defects, yet diminishes the
retrieval of irrelevant, non-contextual occurrences.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.14160v1,"Design and Test of an adaptive augmented reality interface to manage
  systems to assist critical missions","We present a user interface (UI) based on augmented reality (AR) with
head-mounted display (HMD) for improving situational awareness during critical
operation and improve human efficiency on operations. The UI displays
contextual information as well as accepts orders given from the headset to
control unmanned aerial vehicles (UAVs) for assisting the rescue team. We
established experiments where people had been put in a stressful situation and
are asked to resolve a complex mission using a headset and a computer.
Comparing both technologies, our results show that augmented reality has the
potential to be an important tool to help those involved in the emergency
situation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0911.4513v1,A framework for protein and membrane interactions,"We introduce the BioBeta Framework, a meta-model for both protein-level and
membrane-level interactions of living cells. This formalism aims to provide a
formal setting where to encode, compare and merge models at different
abstraction levels; in particular, higher-level (e.g. membrane) activities can
be given a formal biological justification in terms of low-level (i.e.,
protein) interactions. A BioBeta specification provides a protein signature
together a set of protein reactions, in the spirit of the kappa-calculus.
Moreover, the specification describes when a protein configuration triggers one
of the only two membrane interaction allowed, that is ""pinch"" and ""fuse"". In
this paper we define the syntax and semantics of BioBeta, analyse its
properties, give it an interpretation as biobigraphical reactive systems, and
discuss its expressivity by comparing with kappa-calculus and modelling
significant examples. Notably, BioBeta has been designed after a bigraphical
metamodel for the same purposes. Hence, each instance of the calculus
corresponds to a bigraphical reactive system, and vice versa (almost).
Therefore, we can inherith the rich theory of bigraphs, such as the automatic
construction of labelled transition systems and behavioural congruences.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1508.04725v2,Partitioning graphs into induced subgraphs,"We study the Induced $H$ Partition problem from the parameterized complexity
point of view. In the Induced $H$ Partition problem the task is to partition
vertices of a graph $G$ into sets $V_1,V_2,\dots,V_n$ such that the graph $H$
is isomorphic to the subgraph of $G$ induced by each set $V_i$ for $i =
1,2,\dots,n.$ The pattern graph $H$ is fixed.
  For the parametrization we consider three distinct structural parameters of
the graph $G$ - namely the tree-width, the neighborhood diversity, and the
modular-width. For the parametrization by the neighborhood diversity we obtain
an FPT algorithm for every graph $H.$ For the parametrization by the tree-width
we obtain an FPT algorithm for every connected graph $H.$ Finally, for the
parametrization by the modular-width we derive an FPT algorithm for every prime
graph $H.$",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0708.2173v2,Provenance as Dependency Analysis,"Provenance is information recording the source, derivation, or history of
some information. Provenance tracking has been studied in a variety of
settings; however, although many design points have been explored, the
mathematical or semantic foundations of data provenance have received
comparatively little attention. In this paper, we argue that dependency
analysis techniques familiar from program analysis and program slicing provide
a formal foundation for forms of provenance that are intended to show how (part
of) the output of a query depends on (parts of) its input. We introduce a
semantic characterization of such dependency provenance, show that this form of
provenance is not computable, and provide dynamic and static approximation
techniques.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.6359v1,An Empirical Study on Refactoring Activity,"This paper reports an empirical study on refactoring activity in three Java
software systems. We investigated some questions on refactoring activity, to
confirm or disagree on conclusions that have been drawn from previous empirical
studies. Unlike previous empirical studies, our study found that it is not
always true that there are more refactoring activities before major project
release date than after. In contrast, we were able to confirm that software
developers perform different types of refactoring operations on test code and
production code, specific developers are responsible for refactorings in the
project, refactoring edits are not very well tested. Further, floss refactoring
is more popular among the developers, refactoring activity is frequent in the
projects, majority of bad smells once occurred they persist up to the latest
version of the system. By confirming assumptions by other researchers we can
have greater confidence that those research conclusions are generalizable.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2104.13488v1,Text Generation with Deep Variational GAN,"Generating realistic sequences is a central task in many machine learning
applications. There has been considerable recent progress on building deep
generative models for sequence generation tasks. However, the issue of
mode-collapsing remains a main issue for the current models. In this paper we
propose a GAN-based generic framework to address the problem of mode-collapse
in a principled approach. We change the standard GAN objective to maximize a
variational lower-bound of the log-likelihood while minimizing the
Jensen-Shanon divergence between data and model distributions. We experiment
our model with text generation task and show that it can generate realistic
text with high diversity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0703007v2,Intensional properties of polygraphs,"We present polygraphic programs, a subclass of Albert Burroni's polygraphs,
as a computational model, showing how these objects can be seen as first-order
functional programs. We prove that the model is Turing complete. We use
polygraphic interpretations, a termination proof method introduced by the
second author, to characterize polygraphic programs that compute in polynomial
time. We conclude with a characterization of polynomial time functions and
non-deterministic polynomial time functions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1304.5678v1,Analytic Feature Selection for Support Vector Machines,"Support vector machines (SVMs) rely on the inherent geometry of a data set to
classify training data. Because of this, we believe SVMs are an excellent
candidate to guide the development of an analytic feature selection algorithm,
as opposed to the more commonly used heuristic methods. We propose a
filter-based feature selection algorithm based on the inherent geometry of a
feature set. Through observation, we identified six geometric properties that
differ between optimal and suboptimal feature sets, and have statistically
significant correlations to classifier performance. Our algorithm is based on
logistic and linear regression models using these six geometric properties as
predictor variables. The proposed algorithm achieves excellent results on high
dimensional text data sets, with features that can be organized into a handful
of feature types; for example, unigrams, bigrams or semantic structural
features. We believe this algorithm is a novel and effective approach to
solving the feature selection problem for linear SVMs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0710.3979v2,"Toward Trusted Sharing of Network Packet Traces Using Anonymization:
  Single-Field Privacy/Analysis Tradeoffs","Network data needs to be shared for distributed security analysis.
Anonymization of network data for sharing sets up a fundamental tradeoff
between privacy protection versus security analysis capability. This
privacy/analysis tradeoff has been acknowledged by many researchers but this is
the first paper to provide empirical measurements to characterize the
privacy/analysis tradeoff for an enterprise dataset. Specifically we perform
anonymization options on single-fields within network packet traces and then
make measurements using intrusion detection system alarms as a proxy for
security analysis capability. Our results show: (1) two fields have a zero sum
tradeoff (more privacy lessens security analysis and vice versa) and (2) eight
fields have a more complex tradeoff (that is not zero sum) in which both
privacy and analysis can both be simultaneously accomplished.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1601.07009v1,Greedy reduction of navigation time in random search processes,"Random search processes are instrumental in studying and understanding
navigation properties of complex networks, food search strategies of animals,
diffusion control of molecular processes in biological cells, and improving web
search engines. An essential part of random search processes and their
applications are various forms of (continuous or discrete time) random walk
models. The efficiency of a random search strategy in complex networks is
measured with the mean first passage time between two nodes or, more generally,
with the mean first passage time between two subsets of the vertex set. In this
paper we formulate a problem of adding a set of $k$ links between the two
subsets of the vertex set that optimally reduce the mean first passage time
between the sets. We demonstrate that the mean first passage time between two
sets is non-increasing and supermodular set function defined over the set of
links between the two sets. This allows us to use two greedy algorithms that
approximately solve the problem and we compare their performance against
several standard link prediction algorithms. We find that the proposed greedy
algorithms are better at choosing the links that reduce the navigation time
between the two sets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.07917v1,"The Design and Implementation of a Verified File System with End-to-End
  Data Integrity","Despite significant research and engineering efforts, many of today's
important computer systems suffer from bugs. To increase the reliability of
software systems, recent work has applied formal verification to certify the
correctness of such systems, with recent successes including certified file
systems and certified cryptographic protocols, albeit using quite different
proof tactics and toolchains. Unifying these concepts, we present the first
certified file system that uses cryptographic primitives to protect itself
against tampering. Our certified file system defends against adversaries that
might wish to tamper with the raw disk. Such an ""untrusted storage"" threat
model captures the behavior of storage devices that might silently return
erroneous bits as well as adversaries who might have limited access to a disk,
perhaps while in transit. In this paper, we present IFSCQ, a certified
cryptographic file system with strong integrity guarantees. IFSCQ combines and
extends work on cryptographic file systems and formally certified file systems
to prove that our design is correct. It is the first certified file system that
is secure against strong adversaries that can maliciously corrupt on-disk data
and metadata, including attempting to roll back the disk to earlier versions of
valid data. IFSCQ achieves this by constructing a Merkle hash tree of the whole
disk, and by proving that tampered disk blocks will always be detected if they
ever occur. We demonstrate that IFSCQ runs with reasonable overhead while
detecting several kinds of attacks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1706.05670v2,Recognizing hyperelliptic graphs in polynomial time,"Recently, a new set of multigraph parameters was defined, called
""gonalities"". Gonality bears some similarity to treewidth, and is a relevant
graph parameter for problems in number theory and multigraph algorithms.
Multigraphs of gonality 1 are trees. We consider so-called ""hyperelliptic
graphs"" (multigraphs of gonality 2) and provide a safe and complete sets of
reduction rules for such multigraphs, showing that for three of the flavors of
gonality, we can recognize hyperelliptic graphs in O(n log n+m) time, where n
is the number of vertices and m the number of edges of the multigraph.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.08715v1,"Cain: Automatic Code Generation for Simultaneous Convolutional Kernels
  on Focal-plane Sensor-processors","Focal-plane Sensor-processors (FPSPs) are a camera technology that enable low
power, high frame rate computation, making them suitable for edge computation.
Unfortunately, these devices' limited instruction sets and registers make
developing complex algorithms difficult. In this work, we present Cain - a
compiler that targets SCAMP-5, a general-purpose FPSP - which generates code
from multiple convolutional kernels. As an example, given the convolutional
kernels for an MNIST digit recognition neural network, Cain produces code that
is half as long, when compared to the other available compilers for SCAMP-5.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1101.5200v1,"Proceedings Third International Workshop on Classical Logic and
  Computation","The fact that classical mathematical proofs of simply existential statements
can be read as programs was established by Goedel and Kreisel half a century
ago. But the possibility of extracting useful computational content from
classical proofs was taken seriously only from the 1990s on when it was
discovered that proof interpretations based on Goedel's and Kreisel's ideas can
provide new nontrivial algorithms and numerical results, and the Curry-Howard
correspondence can be extended to classical logic via programming concepts such
as continuations and control operators.
  The workshop series ""Classical Logic and Computation"" aims to support a
fruitful exchange of ideas between the various lines of research on
computational aspects of classical logic. This volume contains the abstracts of
the invited lectures and the accepted contributed papers of the third CL&C
workshop which was held jointly with the workshop ""Program Extraction and
Constructive Mathematics"" at the University of Brno in August 21-22, 2010, as a
satellite of CSL and MFCS. The workshops were held in honour of Helmut
Schwichtenberg who became ""professor emeritus"" in September 2010.
  The topics of the papers include the foundations, optimizations and
applications of proof interpretations such as Hilbert's epsilon substitution
method, Goedel's functional interpretation, learning based realizability and
negative translations as well as special calculi and theories capturing
computational and complexity-theoretic aspects of classical logic such as the
lambda-mu-calculus, applicative theories, sequent-calculi, resolution and
cut-elimination",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1410.4963v1,On Succinct Representations of Binary Trees,"We observe that a standard transformation between \emph{ordinal} trees
(arbitrary rooted trees with ordered children) and binary trees leads to
interesting succinct binary tree representations. There are four symmetric
versions of these transformations. Via these transformations we get four
succinct representations of $n$-node binary trees that use $2n + n/(\log
n)^{O(1)}$ bits and support (among other operations) navigation, inorder
numbering, one of pre- or post-order numbering, subtree size and lowest common
ancestor (LCA) queries. The ability to support inorder numbering is crucial for
the well-known range-minimum query (RMQ) problem on an array $A$ of $n$ ordered
values. While this functionality, and more, is also supported in $O(1)$ time
using $2n + o(n)$ bits by Davoodi et al.'s (\emph{Phil. Trans. Royal Soc. A}
\textbf{372} (2014)) extension of a representation by Farzan and Munro
(\emph{Algorithmica} \textbf{6} (2014)), their \emph{redundancy}, or the $o(n)$
term, is much larger, and their approach may not be suitable for practical
implementations.
  One of these transformations is related to the Zaks' sequence (S.~Zaks,
\emph{Theor. Comput. Sci.} \textbf{10} (1980)) for encoding binary trees, and
we thus provide the first succinct binary tree representation based on Zaks'
sequence. Another of these transformations is equivalent to Fischer and Heun's
(\emph{SIAM J. Comput.} \textbf{40} (2011)) \minheap\ structure for this
problem. Yet another variant allows an encoding of the Cartesian tree of $A$ to
be constructed from $A$ using only $O(\sqrt{n} \log n)$ bits of working space.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.06544v2,Parallel-in-Time Simulation of Power Converters Using Multirate PDEs,"This paper presents an efficient numerical algorithm for the simulation of
pulse-width modulated power converters via parallelization in time domain. The
method applies the multirate partial differential equation approach on the
coarse grid of the (two-grid) parallel-in-time algorithm Parareal. Performance
of the proposed approach is illustrated via its application to a DC-DC
converter.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1806.04411v2,Named Entity Recognition with Extremely Limited Data,"Traditional information retrieval treats named entity recognition as a
pre-indexing corpus annotation task, allowing entity tags to be indexed and
used during search. Named entity taggers themselves are typically trained on
thousands or tens of thousands of examples labeled by humans.
  However, there is a long tail of named entities classes, and for these cases,
labeled data may be impossible to find or justify financially. We propose
exploring named entity recognition as a search task, where the named entity
class of interest is a query, and entities of that class are the relevant
""documents"". What should that query look like? Can we even perform NER-style
labeling with tens of labels? This study presents an exploration of CRF-based
NER models with handcrafted features and of how we might transform them into
search queries.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0308009v3,"The Generalized Riemann or Henstock Integral Underpinning Multivariate
  Data Analysis: Application to Faint Structure Finding in Price Processes","Practical data analysis involves many implicit or explicit assumptions about
the good behavior of the data, and excludes consideration of various
potentially pathological or limit cases. In this work, we present a new general
theory of data, and of data processing, to bypass some of these assumptions.
The new framework presented is focused on integration, and has direct
applicability to expectation, distance, correlation, and aggregation. In a case
study, we seek to reveal faint structure in financial data. Our new foundation
for data encoding and handling offers increased justification for our
conclusions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.00218v2,A Non-Deterministic Multiset Query Language,"We develop a multiset query and update language executable in a term
rewriting system. Its most remarkable feature, besides non-standard approach to
quantification and introduction of fresh values, is non-determinism - a query
result is not uniquely determined by the database. We argue that this feature
is very useful, e.g., in modelling user choices during simulation or
reachability analysis of a data-centric business process - the intended
application of our work. Query evaluation is implemented by converting the
query into a terminating term rewriting system and normalizing the initial term
which encapsulates the current database. A normal form encapsulates a query
result. We prove that our language can express any relational algebra query.
Finally, we present a simple business process specification framework (and an
example specification). Both syntax and semantics of our query language is
implemented in Maude.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1408.5389v1,Computing Multi-Relational Sufficient Statistics for Large Databases,"Databases contain information about which relationships do and do not hold
among entities. To make this information accessible for statistical analysis
requires computing sufficient statistics that combine information from
different database tables. Such statistics may involve any number of {\em
positive and negative} relationships. With a naive enumeration approach,
computing sufficient statistics for negative relationships is feasible only for
small databases. We solve this problem with a new dynamic programming algorithm
that performs a virtual join, where the requisite counts are computed without
materializing join tables. Contingency table algebra is a new extension of
relational algebra, that facilitates the efficient implementation of this
M\""obius virtual join operation. The M\""obius Join scales to large datasets
(over 1M tuples) with complex schemas. Empirical evaluation with seven
benchmark datasets showed that information about the presence and absence of
links can be exploited in feature selection, association rule mining, and
Bayesian network learning.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1309.4958v4,Approximation of smallest linear tree grammar,"A simple linear-time algorithm for constructing a linear context-free tree
grammar of size O(rg + r g log (n/r g))for a given input tree T of size n is
presented, where g is the size of a minimal linear context-free tree grammar
for T, and r is the maximal rank of symbols in T (which is a constant in many
applications). This is the first example of a grammar-based tree compression
algorithm with a good, i.e. logarithmic in terms of the size of the input tree,
approximation ratio. The analysis of the algorithm uses an extension of the
recompression technique from strings to trees.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1406.1368v3,Peeling potatoes near-optimally in near-linear time,"We consider the following geometric optimization problem: find a convex
polygon of maximum area contained in a given simple polygon $P$ with $n$
vertices. We give a randomized near-linear-time $(1-\varepsilon)$-approximation
algorithm for this problem: in $O(n( \log^2 n + (1/\varepsilon^3) \log n +
1/\varepsilon^4))$ time we find a convex polygon contained in $P$ that, with
probability at least $2/3$, has area at least $(1-\varepsilon)$ times the area
of an optimal solution. We also obtain similar results for the variant of
computing a convex polygon inside $P$ with maximum perimeter.
  To achieve these results we provide new results in geometric probability. The
first result is a bound relating the probability that two points chosen
uniformly at random inside $P$ are mutually visible and the area of the largest
convex body inside $P$. The second result is a bound on the expected value of
the difference between the perimeter of any planar convex body $K$ and the
perimeter of the convex hull of a uniform random sample inside $K$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1010.5537v2,Using entropy measures for comparison of software traces,"The analysis of execution paths (also known as software traces) collected
from a given software product can help in a number of areas including software
testing, software maintenance and program comprehension. The lack of a scalable
matching algorithm operating on detailed execution paths motivates the search
for an alternative solution.
  This paper proposes the use of word entropies for the classification of
software traces. Using a well-studied defective software as an example, we
investigate the application of both Shannon and extended entropies
(Landsberg-Vedral, R\'{e}nyi and Tsallis) to the classification of traces
related to various software defects. Our study shows that using entropy
measures for comparisons gives an efficient and scalable method for comparing
traces. The three extended entropies, with parameters chosen to emphasize rare
events, all perform similarly and are superior to the Shannon entropy.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.09646v1,"A Systematic Literature Review of Automated Query Reformulations in
  Source Code Search","Software developers often fix critical bugs to ensure the reliability of
their software. They might also need to add new features to their software at a
regular interval to stay competitive in the market. These bugs and features are
reported as change requests (i.e., technical documents written by software
users). Developers consult these documents to implement the required changes in
the software code. As a part of change implementation, they often choose a few
important keywords from a change request as an ad hoc query. Then they execute
the query with a code search engine (e.g., Lucene) and attempt to find out the
exact locations within the software code that need to be changed.
Unfortunately, even experienced developers often fail to choose the right
queries. As a consequence, the developers often experience difficulties in
detecting the appropriate locations within the code and spend the majority of
their time in numerous trials and errors. There have been many studies that
attempt to support developers in constructing queries by automatically
reformulating their ad hoc queries. In this systematic literature review, we
carefully select 70 primary studies on query reformulations from 2,970
candidate studies, perform an in-depth qualitative analysis using the Grounded
Theory approach, and then answer six important research questions. Our
investigation has reported several major findings. First, to date, eight major
methodologies (e.g., term weighting, query-term co-occurrence analysis,
thesaurus lookup) have been adopted in query reformulation. Second, the
existing studies suffer from several major limitations (e.g., lack of
generalizability, vocabulary mismatch problem, weak evaluation, the extra
burden on the developers) that might prevent their wide adoption. Finally, we
discuss several open issues in search query reformulations and suggest multiple
future research opportunities.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1511.03086v1,The CTU Prague Relational Learning Repository,"The aim of the CTU Prague Relational Learning Repository is to support
machine learning research with multi-relational data. The repository currently
contains 50 SQL databases hosted on a public MySQL server located at
relational.fit.cvut.cz. A searchable meta-database provides metadata (e.g., the
number of tables in the database, the number of rows and columns in the tables,
the number of foreign key constraints between tables).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0208025v1,"Efficient Micro-Mobility using Intra-domain Multicast-based Mechanisms
  (M&M)","One of the most important metrics in the design of IP mobility protocols is
the handover performance. The current Mobile IP (MIP) standard has been shown
to exhibit poor handover performance. Most other work attempts to modify MIP to
slightly improve its efficiency, while others propose complex techniques to
replace MIP. Rather than taking these approaches, we instead propose a new
architecture for providing efficient and smooth handover, while being able to
co-exist and inter-operate with other technologies. Specifically, we propose an
intra-domain multicast-based mobility architecture, where a visiting mobile is
assigned a multicast address to use while moving within a domain. Efficient
handover is achieved using standard multicast join/prune mechanisms. Two
approaches are proposed and contrasted. The first introduces the concept
proxy-based mobility, while the other uses algorithmic mapping to obtain the
multicast address of visiting mobiles. We show that the algorithmic mapping
approach has several advantages over the proxy approach, and provide mechanisms
to support it. Network simulation (using NS-2) is used to evaluate our scheme
and compare it to other routing-based micro-mobility schemes - CIP and HAWAII.
The proactive handover results show that both M&M and CIP shows low handoff
delay and packet reordering depth as compared to HAWAII. The reason for M&M's
comparable performance with CIP is that both use bi-cast in proactive handover.
The M&M, however, handles multiple border routers in a domain, where CIP fails.
We also provide a handover algorithm leveraging the proactive path setup
capability of M&M, which is expected to outperform CIP in case of reactive
handover.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1703.02992v1,A Manifold Approach to Learning Mutually Orthogonal Subspaces,"Although many machine learning algorithms involve learning subspaces with
particular characteristics, optimizing a parameter matrix that is constrained
to represent a subspace can be challenging. One solution is to use Riemannian
optimization methods that enforce such constraints implicitly, leveraging the
fact that the feasible parameter values form a manifold. While Riemannian
methods exist for some specific problems, such as learning a single subspace,
there are more general subspace constraints that offer additional flexibility
when setting up an optimization problem, but have not been formulated as a
manifold.
  We propose the partitioned subspace (PS) manifold for optimizing matrices
that are constrained to represent one or more subspaces. Each point on the
manifold defines a partitioning of the input space into mutually orthogonal
subspaces, where the number of partitions and their sizes are defined by the
user. As a result, distinct groups of features can be learned by defining
different objective functions for each partition. We illustrate the properties
of the manifold through experiments on multiple dataset analysis and domain
adaptation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1910.11672v2,Rare Event Simulation for non-Markovian repairable Fault Trees,"Dynamic Fault Trees (DFT) are widely adopted in industry to assess the
dependability of safety-critical equipment. Since many systems are too large to
be studied numerically, DFTs dependability is often analysed using Monte Carlo
simulation. A bottleneck here is that many simulation samples are required in
the case of rare events, e.g. in highly reliable systems where components fail
seldomly. Rare Event Simulation (RES) provides techniques to reduce the number
of samples in the case of rare events. We present a RES technique based on
importance splitting, to study failures in highly reliable DFTs. Whereas RES
usually requires meta-information from an expert, our method is fully
automatic: by cleverly exploiting the fault tree structure we extract the
so-called importance function. We handle DFTs with Markovian and non-Markovian
failure and repair distributions (for which no numerical methods exist) and
show the efficiency of our approach on several case studies.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.02280v1,"Generating Code with Polymorphic let: A Ballad of Value Restriction,
  Copying and Sharing","Getting polymorphism and effects such as mutation to live together in the
same language is a tale worth telling, under the recurring refrain of copying
vs. sharing. We add new stanzas to the tale, about the ordeal to generate code
with polymorphism and effects, and be sure it type-checks. Generating
well-typed-by-construction polymorphic let-expressions is impossible in the
Hindley-Milner type system: even the author believed that.
  The polymorphic-let generator turns out to exist. We present its derivation
and the application for the lightweight implementation of quotation via a novel
and unexpectedly simple source-to-source transformation to code-generating
combinators.
  However, generating let-expressions with polymorphic functions demands more
than even the relaxed value restriction can deliver. We need a new deal for
let-polymorphism in ML. We conjecture the weaker restriction and implement it
in a practically-useful code-generation library. Its formal justification is
formulated as the research program.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1810.06169v2,"Traffic Signs in the Wild: Highlights from the IEEE Video and Image
  Processing Cup 2017 Student Competition [SP Competitions]","Robust and reliable traffic sign detection is necessary to bring autonomous
vehicles onto our roads. State-of-the-art algorithms successfully perform
traffic sign detection over existing databases that mostly lack severe
challenging conditions. VIP Cup 2017 competition focused on detecting such
traffic signs under challenging conditions. To facilitate such task and
competition, we introduced a video dataset denoted as CURE-TSD that includes a
variety of challenging conditions. The goal of this challenge was to implement
traffic sign detection algorithms that can robustly perform under such
challenging conditions. In this article, we share an overview of the VIP Cup
2017 experience including competition setup, teams, technical approaches,
participation statistics, and competition experience through finalist teams
members' and organizers' eyes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.13346v1,"Parallel-beam X-ray CT datasets of apples with internal defects and
  label balancing for machine learning","We present three parallel-beam tomographic datasets of 94 apples with
internal defects along with defect label files. The datasets are prepared for
development and testing of data-driven, learning-based image reconstruction,
segmentation and post-processing methods. The three versions are a noiseless
simulation; simulation with added Gaussian noise, and with scattering noise.
The datasets are based on real 3D X-ray CT data and their subsequent volume
reconstructions. The ground truth images, based on the volume reconstructions,
are also available through this project. Apples contain various defects, which
naturally introduce a label bias. We tackle this by formulating the bias as an
optimization problem. In addition, we demonstrate solving this problem with two
methods: a simple heuristic algorithm and through mixed integer quadratic
programming. This ensures the datasets can be split into test, training or
validation subsets with the label bias eliminated. Therefore the datasets can
be used for image reconstruction, segmentation, automatic defect detection, and
testing the effects of (as well as applying new methodologies for removing)
label bias in machine learning.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.09000v5,"The parameterized complexity of finding secluded solutions to some
  classical optimization problems on graphs","This work studies the parameterized complexity of finding secluded solutions
to classical combinatorial optimization problems on graphs such as finding
minimum s-t separators, feedback vertex sets, dominating sets, maximum
independent sets, and vertex deletion problems for hereditary graph properties:
Herein, one searches not only to minimize or maximize the size of the solution,
but also to minimize the size of its neighborhood. This restriction has
applications in secure routing and community detection.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.07060v1,"Algorithm for computing semi-Fourier sequences of expressions involving
  exponentiations and integrations","We provide an algorithm for computing semi-Fourier sequences for expressions
constructed from arithmetic operations, exponentiations and integrations. The
semi-Fourier sequence is a relaxed version of Fourier sequence for polynomials
(expressions made of additions and multiplications).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.02565v1,"Modeling Data Reuse in Deep Neural Networks by Taking Data-Types into
  Cognizance","In recent years, researchers have focused on reducing the model size and
number of computations (measured as ""multiply-accumulate"" or MAC operations) of
DNNs. The energy consumption of a DNN depends on both the number of MAC
operations and the energy efficiency of each MAC operation. The former can be
estimated at design time; however, the latter depends on the intricate data
reuse patterns and underlying hardware architecture. Hence, estimating it at
design time is challenging. This work shows that the conventional approach to
estimate the data reuse, viz. arithmetic intensity, does not always correctly
estimate the degree of data reuse in DNNs since it gives equal importance to
all the data types. We propose a novel model, termed ""data type aware weighted
arithmetic intensity"" ($DI$), which accounts for the unequal importance of
different data types in DNNs. We evaluate our model on 25 state-of-the-art DNNs
on two GPUs. We show that our model accurately models data-reuse for all
possible data reuse patterns for different types of convolution and different
types of layers. We show that our model is a better indicator of the energy
efficiency of DNNs. We also show its generality using the central limit
theorem.",0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1611.01287v1,Managing Quality Requirements Using Activity-Based Quality Models,"Managing requirements on quality aspects is an important issue in the
development of software systems. Difficulties arise from expressing them
appropriately what in turn results from the difficulty of the concept of
quality itself. Building and using quality models is an approach to handle the
complexity of software quality. A novel kind of quality models uses the
activities performed on and with the software as an explicit dimension. These
quality models are a well-suited basis for managing quality requirements from
elicitation over refinement to assurance. The paper proposes such an approach
and shows its applicability in an automotive case study.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1601.05330v2,Towards a Theory of Affect and Software Developers' Performance,"For more than thirty years, it has been claimed that a way to improve
software developers' productivity and software quality is to focus on people.
The underlying assumption seems to be that ""happy and satisfied software
developers perform better"". More specifically, affects-emotions and moods-have
an impact on cognitive activities and the working performance of individuals.
Development tasks are undertaken heavily through cognitive processes, yet
software engineering research (SE) lacks theory on affects and their impact on
software development activities. This PhD dissertation supports the advocates
of studying the human and social aspects of SE and the psychology of
programming. This dissertation aims to theorize on the link between affects and
software development performance. A mixed method approach was employed, which
comprises studies of the literature in psychology and SE, quantitative
experiments, and a qualitative study, for constructing a multifaceted theory of
the link between affects and programming performance. The theory explicates the
linkage between affects and analytical problem-solving performance of
developers, their software development task productivity, and the process
behind the linkage. The results are novel in the domains of SE and psychology,
and they fill an important lack that had been raised by both previous research
and by practitioners. The implications of this PhD lie in setting out the basic
building blocks for researching and understanding the affect of software
developers, and how it is related to software development performance. Overall,
the evidence hints that happy software developers perform better in analytic
problem solving, are more productive while developing software, are prone to
share their feelings in order to let researchers and managers understand them,
and are susceptible to interventions for enhancing their affects on the job.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1805.04129v1,"A Proposal for Outlier and Noise Detection in Public Officials'
  Affidavits","Outlier and noise detection processes are highly useful in the quality
assessment of any kind of database. Such processes may have novel civic and
public applications in the detection of anomalies in public data. The purpose
of this work is to explore the possibilities of experimentation with,
validation and application of hybrid outlier and noise detection procedures in
public officials' affidavit systems currently available in Argentina.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2110.14096v1,Towards Robust Bisimulation Metric Learning,"Learned representations in deep reinforcement learning (DRL) have to extract
task-relevant information from complex observations, balancing between
robustness to distraction and informativeness to the policy. Such stable and
rich representations, often learned via modern function approximation
techniques, can enable practical application of the policy improvement theorem,
even in high-dimensional continuous state-action spaces. Bisimulation metrics
offer one solution to this representation learning problem, by collapsing
functionally similar states together in representation space, which promotes
invariance to noise and distractors. In this work, we generalize value function
approximation bounds for on-policy bisimulation metrics to non-optimal policies
and approximate environment dynamics. Our theoretical results help us identify
embedding pathologies that may occur in practical use. In particular, we find
that these issues stem from an underconstrained dynamics model and an unstable
dependence of the embedding norm on the reward signal in environments with
sparse rewards. Further, we propose a set of practical remedies: (i) a norm
constraint on the representation space, and (ii) an extension of prior
approaches with intrinsic rewards and latent space regularization. Finally, we
provide evidence that the resulting method is not only more robust to sparse
reward functions, but also able to solve challenging continuous control tasks
with observational distractions, where prior methods fail.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1809.00828v2,"Robust and parallel scalable iterative solutions for large-scale finite
  cell analyses","The finite cell method is a highly flexible discretization technique for
numerical analysis on domains with complex geometries. By using a non-boundary
conforming computational domain that can be easily meshed, automatized
computations on a wide range of geometrical models can be performed.
Application of the finite cell method, and other immersed methods, to large
real-life and industrial problems is often limited due to the conditioning
problems associated with these methods. These conditioning problems have caused
researchers to resort to direct solution methods, which signifi- cantly limit
the maximum size of solvable systems. Iterative solvers are better suited for
large-scale computations than their direct counterparts due to their lower
memory requirements and suitability for parallel computing. These benefits can,
however, only be exploited when systems are properly conditioned. In this
contribution we present an Additive-Schwarz type preconditioner that enables
efficient and parallel scalable iterative solutions of large-scale multi-level
hp-refined finite cell analyses.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1404.5584v2,"Polynomial time vertex enumeration of convex polytopes of bounded
  branch-width","Over the last years the vertex enumeration problem of polyhedra has seen a
revival in the study of metabolic networks, which increased the demand for
efficient vertex enumeration algorithms for high-dimensional polyhedra given by
inequalities. It is a famous and long standing open question in polyhedral
theory and computational geometry whether the vertices of a polytope (bounded
polyhedron), described by a set of linear constraints, can be enumerated in
total polynomial time. In this paper we apply the concept of
branch-decomposition to the vertex enumeration problem of polyhedra $P = \{x :
Ax = b, x \geq 0\}$. For this purpose, we introduce the concept of $k$-module
and show how it relates to the separators of the linear matroid generated by
the columns of $A$. We then use this to present a total polynomial time
algorithm for polytopes $P$ for which the branch-width of the linear matroid
generated by $A$ is bounded by a constant $k$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0105004v1,Parallel implementation of the TRANSIMS micro-simulation,"This paper describes the parallel implementation of the TRANSIMS traffic
micro-simulation. The parallelization method is domain decomposition, which
means that each CPU of the parallel computer is responsible for a different
geographical area of the simulated region. We describe how information between
domains is exchanged, and how the transportation network graph is partitioned.
An adaptive scheme is used to optimize load balancing. We then demonstrate how
computing speeds of our parallel micro-simulations can be systematically
predicted once the scenario and the computer architecture are known. This makes
it possible, for example, to decide if a certain study is feasible with a
certain computing budget, and how to invest that budget. The main ingredients
of the prediction are knowledge about the parallel implementation of the
micro-simulation, knowledge about the characteristics of the partitioning of
the transportation network graph, and knowledge about the interaction of these
quantities with the computer system. In particular, we investigate the
differences between switched and non-switched topologies, and the effects of 10
Mbit, 100 Mbit, and Gbit Ethernet. keywords: Traffic simulation, parallel
computing, transportation planning, TRANSIMS",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.0847v1,Reconfiguration in bounded bandwidth and treedepth,"We show that several reconfiguration problems known to be PSPACE-complete
remain so even when limited to graphs of bounded bandwidth. The essential step
is noticing the similarity to very limited string rewriting systems, whose
ability to directly simulate Turing Machines is classically known. This
resolves a question posed open in [Bonsma P., 2012]. On the other hand, we show
that a large class of reconfiguration problems becomes tractable on graphs of
bounded treedepth, and that this result is in some sense tight.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1803.06752v3,Scalar and Vectorial mu-calculus with Atoms,"We study an extension of modal $\mu$-calculus to sets with atoms and we study
its basic properties. Model checking is decidable on orbit-finite structures,
and a correspondence to parity games holds. On the other hand, satisfiability
becomes undecidable. We also show expressive limitations of atom-enriched
$\mu$-calculi, and explain how their expressive power depends on the structure
of atoms used, and on the choice between basic or vectorial syntax.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1202.1309v2,A Decidable Fragment of Strategy Logic,"Strategy Logic (SL, for short) has been recently introduced by Mogavero,
Murano, and Vardi as a useful formalism for reasoning explicitly about
strategies, as first-order objects, in multi-agent concurrent games. This logic
turns to be very powerful, subsuming all major previously studied modal logics
for strategic reasoning, including ATL, ATL*, and the like. Unfortunately, due
to its expressiveness, SL has a non-elementarily decidable model-checking
problem and a highly undecidable satisfiability problem, specifically,
$\Sigma_{1}^{1}$-Hard. In order to obtain a decidable sublogic, we introduce
and study here One-Goal Strategy Logic (SL[1G], for short). This logic is a
syntactic fragment of SL, strictly subsuming ATL*, which encompasses formulas
in prenex normal form having a single temporal goal at a time, for every
strategy quantification of agents. SL[1G] is known to have an elementarily
decidable model-checking problem. Here we prove that, unlike SL, it has the
bounded tree-model property and its satisfiability problem is decidable in
2ExpTime, thus not harder than the one for ATL*.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.04356v1,A Social Search Model for Large Scale Social Networks,"With the rise of social networks, information on the internet is no longer
solely organized by web pages. Rather, content is generated and shared among
users and organized around their social relations on social networks. This
presents new challenges to information retrieval systems. On a social network
search system, the generation of result sets not only needs to consider keyword
matches, like a traditional web search engine does, but it also needs to take
into account the searcher's social connections and the content's visibility
settings. Besides, search ranking should be able to handle both textual
relevance and the rich social interaction signals from the social network. In
this paper, we present our solution to these two challenges by first
introducing a social retrieval mechanism, and then investigate novel deep
neural networks for the ranking problem. The retrieval system treats social
connections as indexing terms, and generates meaningful results sets by biasing
towards close social connections in a constrained optimization fashion. The
result set is then ranked by a deep neural network that handles textual and
social relevance in a two-tower approach, in which personalization and textual
relevance are addressed jointly. The retrieval mechanism is deployed on
Facebook and is helping billions of users finding postings from their
connections efficiently. Based on the postings being retrieved, we evaluate our
two-tower neutral network, and examine the importance of personalization and
textual signals in the ranking problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1901.07993v2,"Parallelization and scalability analysis of inverse factorization using
  the Chunks and Tasks programming model","We present three methods for distributed memory parallel inverse
factorization of block-sparse Hermitian positive definite matrices. The three
methods are a recursive variant of the AINV inverse Cholesky algorithm,
iterative refinement, and localized inverse factorization, respectively. All
three methods are implemented using the Chunks and Tasks programming model,
building on the distributed sparse quad-tree matrix representation and parallel
matrix-matrix multiplication in the publicly available Chunks and Tasks Matrix
Library (CHTML). Although the algorithms are generally applicable, this work
was mainly motivated by the need for efficient and scalable inverse
factorization of the basis set overlap matrix in large scale electronic
structure calculations. We perform various computational tests on overlap
matrices for quasi-linear Glutamic Acid-Alanine molecules and three-dimensional
water clusters discretized using the standard Gaussian basis set STO-3G with up
to more than 10 million basis functions. We show that for such matrices the
computational cost increases only linearly with system size for all the three
methods. We show both theoretically and in numerical experiments that the
methods based on iterative refinement and localized inverse factorization
outperform previous parallel implementations in weak scaling tests where the
system size is increased in direct proportion to the number of processes. We
show also that compared to the method based on pure iterative refinement the
localized inverse factorization requires much less communication.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1807.07824v1,"A man with a computer face (to the 80th anniversary of Ivan Edward
  Sutherland)","The article presents the main milestones of the science and technology
biography of Ivan Edward Sutherland. The influence of the family and the school
on the development of its research competencies is shown, and little-known
biographical facts explaining the evolution of his scientific interests is
presented: from dynamic object-oriented graphic systems through systems of
virtual reality to asynchronous circuits.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
http://arxiv.org/abs/1907.00713v1,"Verifying that a compiler preserves concurrent value-dependent
  information-flow security","It is common to prove by reasoning over source code that programs do not leak
sensitive data. But doing so leaves a gap between reasoning and reality that
can only be filled by accounting for the behaviour of the compiler. This task
is complicated when programs enforce value-dependent information-flow security
properties (in which classification of locations can vary depending on values
in other locations) and complicated further when programs exploit
shared-variable concurrency.
  Prior work has formally defined a notion of concurrency-aware refinement for
preserving value-dependent security properties. However, that notion is
considerably more complex than standard refinement definitions typically
applied in the verification of semantics preservation by compilers. To date it
remains unclear whether it can be applied to a realistic compiler, because
there exist no general decomposition principles for separating it into smaller,
more familiar, proof obligations.
  In this work, we provide such a decomposition principle, which we show can
almost halve the complexity of proving secure refinement. Further, we
demonstrate its applicability to secure compilation, by proving in Isabelle/HOL
the preservation of value-dependent security by a proof-of-concept compiler
from an imperative While language to a generic RISC-style assembly language,
for programs with shared-memory concurrency mediated by locking primitives.
Finally, we execute our compiler in Isabelle on a While language model of the
Cross Domain Desktop Compositor, demonstrating to our knowledge the first use
of a compiler verification result to carry an information-flow security
property down to the assembly-level model of a non-trivial concurrent program.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9907032v2,Clausal Temporal Resolution,"In this article, we examine how clausal resolution can be applied to a
specific, but widely used, non-classical logic, namely discrete linear temporal
logic. Thus, we first define a normal form for temporal formulae and show how
arbitrary temporal formulae can be translated into the normal form, while
preserving satisfiability. We then introduce novel resolution rules that can be
applied to formulae in this normal form, provide a range of examples and
examine the correctness and complexity of this approach is examined and. This
clausal resolution approach. Finally, we describe related work and future
developments concerning this work.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.13796v1,Explainable Multi-class Classification of Medical Data,"Machine Learning applications have brought new insights into a secondary
analysis of medical data. Machine Learning helps to develop new drugs, define
populations susceptible to certain illnesses, identify predictors of many
common diseases. At the same time, Machine Learning results depend on
convolution of many factors, including feature selection, class (im)balance,
algorithm preference, and performance metrics. In this paper, we present
explainable multi-class classification of a large medical data set. We in
details discuss knowledge-based feature engineering, data set balancing, best
model selection, and parameter tuning. Six algorithms are used in this study:
Support Vector Machine (SVM), Na\""ive Bayes, Gradient Boosting, Decision Trees,
Random Forest, and Logistic Regression. Our empirical evaluation is done on the
UCI Diabetes 130-US hospitals for years 1999-2008 dataset, with the task to
classify patient hospital re-admission stay into three classes: 0 days, <30
days, or > 30 days. Our results show that using 23 medication features in
learning experiments improves Recall of five out of the six applied learning
algorithms. This is a new result that expands the previous studies conducted on
the same data. Gradient Boosting and Random Forest outperformed other
algorithms in terms of the three-class classification Accuracy.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1503.01145v2,"Design for Online Deliberative Processes and Technologies: Towards a
  Multidisciplinary Research Agenda","There has been rapidly growing interest in studying and designing online
deliberative processes and technologies. This SIG aims at providing a venue for
continuous and constructive dialogue between social, political and cognitive
sciences as well as computer science, HCI, and CSCW. Through an online
community and a modified version of world cafe discussions, we contribute to
the definition of the theoretical building blocks, the identification of a
research agenda for the CHI community, and the network of individuals from
academia, industry, and the public sector who share interests in different
aspects of online deliberation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/cs/0602099v1,Towards Applicative Relational Programming,"Functional programming comes in two flavours: one where ``functions are
first-class citizens'' (we call this applicative) and one which is based on
equations (we call this declarative). In relational programming clauses play
the role of equations. Hence Prolog is declarative. The purpose of this paper
is to provide in relational programming a mathematical basis for the relational
analog of applicative functional programming. We use the cylindric semantics of
first-order logic due to Tarski and provide a new notation for the required
cylinders that we call tables. We define the Table/Relation Algebra with
operators sufficient to translate Horn clauses into algebraic form. We
establish basic mathematical properties of these operators. We show how
relations can be first-class citizens, and devise mechanisms for modularity,
for local scoping of predicates, and for exporting/importing relations between
programs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0805.0120v1,Nonnegative Matrix Factorization via Rank-One Downdate,"Nonnegative matrix factorization (NMF) was popularized as a tool for data
mining by Lee and Seung in 1999. NMF attempts to approximate a matrix with
nonnegative entries by a product of two low-rank matrices, also with
nonnegative entries. We propose an algorithm called rank-one downdate (R1D) for
computing a NMF that is partly motivated by singular value decomposition. This
algorithm computes the dominant singular values and vectors of adaptively
determined submatrices of a matrix. On each iteration, R1D extracts a rank-one
submatrix from the dataset according to an objective function. We establish a
theoretical result that maximizing this objective function corresponds to
correctly classifying articles in a nearly separable corpus. We also provide
computational experiments showing the success of this method in identifying
features in realistic datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0207060v1,"Preferred well-founded semantics for logic programming by alternating
  fixpoints: Preliminary report","We analyze the problem of defining well-founded semantics for ordered logic
programs within a general framework based on alternating fixpoint theory. We
start by showing that generalizations of existing answer set approaches to
preference are too weak in the setting of well-founded semantics. We then
specify some informal yet intuitive criteria and propose a semantical framework
for preference handling that is more suitable for defining well-founded
semantics for ordered logic programs. The suitability of the new approach is
convinced by the fact that many attractive properties are satisfied by our
semantics. In particular, our semantics is still correct with respect to
various existing answer sets semantics while it successfully overcomes the
weakness of their generalization to well-founded semantics. Finally, we
indicate how an existing preferred well-founded semantics can be captured
within our semantical framework.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0607059v1,Creation and Growth of Components in a Random Hypergraph Process,"Denote by an $\ell$-component a connected $b$-uniform hypergraph with $k$
edges and $k(b-1) - \ell$ vertices. We prove that the expected number of
creations of $\ell$-component during a random hypergraph process tends to 1 as
$\ell$ and $b$ tend to $\infty$ with the total number of vertices $n$ such that
$\ell = o(\sqrt[3]{\frac{n}{b}})$. Under the same conditions, we also show that
the expected number of vertices that ever belong to an $\ell$-component is
approximately $12^{1/3} (b-1)^{1/3} \ell^{1/3} n^{2/3}$. As an immediate
consequence, it follows that with high probability the largest $\ell$-component
during the process is of size $O((b-1)^{1/3} \ell^{1/3} n^{2/3})$. Our results
give insight about the size of giant components inside the phase transition of
random hypergraphs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0601021v1,Lighting Control using Pressure-Sensitive Touchpads,"We introduce a novel approach to control physical lighting parameters by
means of a pressure-sensitive touchpad. The two-dimensional area of the
touchpad is subdivided into 5 virtual sliders, each controlling the intensity
of a color (red, green, blue, yellow, and white). The physical interaction
methodology is modeled directly after ubiquitous mechanical sliders and dimmers
which tend to be used for intensity/volume control. Our abstraction to a
pressure-sensitive touchpad provides advantages and introduces additional
benefits over such existing devices.",0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1906.11032v1,Cognitive Systems Approach to Smart Cities,"In our connected world, services are expected to be delivered at speed
through multiple means with seamless communication. To put it in day to day
conversational terms, 'there is an app for it' attitude prevails. Several
technologies are needed to meet this growing demand and indeed these
technologies are being developed. The first noteworthy is Internet of Things
(IoT), which is in itself coupled technologies to deliver seamless
communication with 'anywhere, anytime' as an underlying objective. The
'anywhere, anytime' service delivery paradigm requires a new type of smart
systems in developing these services with better capabilities to interact with
the human user, such as personalisation, affect state recognition, etc. Here
enter cognitive systems, where AI meets cognitive sciences (e.g. cognitive
psychology, linguistics, social cognition, etc.). In this paper we will examine
the requirements imposed by smart cities development, e.g. intelligent
logistics, sensor networks and domestic appliances connectivity, data streams
and media delivery, to mention but few. Then we will explore how cognitive
systems can meet the challenges these requirements present to the development
of new systems. Throughout our discussion here, examples from our recent and
current projects will be given supplemented by examples from the literature.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/cs/0406023v1,Notions of Equivalence in Software Design,"Design methods in information systems frequently create software descriptions
using formal languages. Nonetheless, most software designers prefer to describe
software using natural languages. This distinction is not simply a matter of
convenience. Natural languages are not the same as formal languages; in
particular, natural languages do not follow the notions of equivalence used by
formal languages. In this paper, we show both the existence and coexistence of
different notions of equivalence by extending the no-tion of oracles used in
formal languages. This allows distinctions to be made between the trustworthy
oracles assumed by formal languages and the untrust-worthy oracles used by
natural languages. By examin-ing the notion of equivalence, we hope to
encourage designers of software to rethink the place of ambiguity in software
design.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1701.06828v1,"Security and Privacy of performing Data Analytics in the cloud - A
  three-way handshake of Technology, Policy, and Management","Cloud platform came into existence primarily to accelerate IT delivery and to
promote innovation. To this point, it has performed largely well to the
expectations of technologists, businesses and customers. The service aspect of
this technology has paved the road for a faster set up of infrastructure and
related goals for both startups and established organizations. This has further
led to quicker delivery of many user-friendly applications to the market while
proving to be a commercially viable option to companies with limited resources.
On the technology front, the creation and adoption of this ecosystem has
allowed easy collection of massive data from various sources at one place,
where the place is sometimes referred as just the cloud. Efficient data mining
can be performed on raw data to extract potentially useful information, which
was not possible at this scale before. Targeted advertising is a common example
that can help businesses. Despite these promising offerings, concerns around
security and privacy of user information suppressed wider acceptance and an
all-encompassing deployment of the cloud platform. In this paper, we discuss
security and privacy concerns that occur due to data exchanging hands between a
cloud servicer provider (CSP) and the primary cloud user - the data collector,
from the content generator. We offer solutions that encompass technology,
policy and sound management of the cloud service asserting that this approach
has the potential to provide a holistic solution.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1904.09433v2,"Can Machine Learning Model with Static Features be Fooled: an
  Adversarial Machine Learning Approach","The widespread adoption of smartphones dramatically increases the risk of
attacks and the spread of mobile malware, especially on the Android platform.
Machine learning-based solutions have been already used as a tool to supersede
signature-based anti-malware systems. However, malware authors leverage
features from malicious and legitimate samples to estimate statistical
difference in-order to create adversarial examples. Hence, to evaluate the
vulnerability of machine learning algorithms in malware detection, we propose
five different attack scenarios to perturb malicious applications (apps). By
doing this, the classification algorithm inappropriately fits the discriminant
function on the set of data points, eventually yielding a higher
misclassification rate. Further, to distinguish the adversarial examples from
benign samples, we propose two defense mechanisms to counter attacks. To
validate our attacks and solutions, we test our model on three different
benchmark datasets. We also test our methods using various classifier
algorithms and compare them with the state-of-the-art data poisoning method
using the Jacobian matrix. Promising results show that generated adversarial
samples can evade detection with a very high probability. Additionally, evasive
variants generated by our attack models when used to harden the developed
anti-malware system improves the detection rate up to 50% when using the
Generative Adversarial Network (GAN) method.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.06645v2,A Generating-Extension-Generator for Machine Code,"The problem of ""debloating"" programs for security and performance purposes
has begun to see increased attention. Of particular interest in many
environments is debloating commodity off-the-shelf (COTS) software, which is
most commonly made available to end users as stripped binaries (i.e., neither
source code nor symbol-table/debugging information is available). Toward this
end, we created a system, called GenXGen[MC], that specializes stripped
binaries.
  Many aspects of the debloating problem can be addressed via techniques from
the literature on partial evaluation. However, applying such techniques to
real-world programs, particularly stripped binaries, involves non-trivial
state-management manipulations that have never been addressed in a completely
satisfactory manner in previous systems. In particular, a partial evaluator
needs to be able to (i) save and restore arbitrary program states, and (ii)
determine whether a program state is equal to one that arose earlier. Moreover,
to specialize stripped binaries, the system must also be able to handle program
states consisting of memory that is undifferentiated beyond the standard coarse
division into regions for the stack, the heap, and global data.
  This paper presents a new approach to state management in a program
specializer. The technique has been incorporated into GenXGen[MC], a novel tool
for producing machine-code generating extensions. Our experiments show that our
solution to issue (i) significantly decreases the space required to represent
program states, and our solution to issue (ii) drastically improves the time
for producing a specialized program (as much as 13,000x speedup).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0507013v2,An O(n log n)-Time Algorithm for the Restricted Scaffold Assignment,"The assignment problem takes as input two finite point sets S and T and
establishes a correspondence between points in S and points in T, such that
each point in S maps to exactly one point in T, and each point in T maps to at
least one point in S. In this paper we show that this problem has an O(n log
n)-time solution, provided that the points in S and T are restricted to lie on
a line (linear time, if S and T are presorted).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0312010v1,Designing of a Community-based Translation Center,"Interfaces that support multi-lingual content can reach a broader community.
We wish to extend the reach of CITIDEL, a digital library for computing
education materials, to support multiple languages. By doing so, we hope that
it will increase the number of users, and in turn the number of resources. This
paper discusses three approaches to translation (automated translation,
developer-based, and community-based), and a brief evaluation of these
approaches. It proposes a design for an online community translation center
where volunteers help translate interface components and educational materials
available in CITIDEL.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.08914v1,"MuBiNN: Multi-Level Binarized Recurrent Neural Network for EEG signal
  Classification","Recurrent Neural Networks (RNN) are widely used for learning sequences in
applications such as EEG classification. Complex RNNs could be hardly deployed
on wearable devices due to their computation and memory-intensive processing
patterns. Generally, reduction in precision leads much more efficiency and
binarized RNNs are introduced as energy-efficient solutions. However, naive
binarization methods lead to significant accuracy loss in EEG classification.
In this paper, we propose a multi-level binarized LSTM, which significantly
reduces computations whereas ensuring an accuracy pretty close to the full
precision LSTM. Our method reduces the delay of the 3-bit LSTM cell operation
47* with less than 0.01% accuracy loss.",0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1403.3969v1,Game Theory Explorer - Software for the Applied Game Theorist,"This paper presents the ""Game Theory Explorer"" software tool to create and
analyze games as models of strategic interaction. A game in extensive or
strategic form is created and nicely displayed with a graphical user interface
in a web browser. State-of-the-art algorithms then compute all Nash equilibria
of the game after a mouseclick. In tutorial fashion, we present how the program
is used, and the ideas behind its main algorithms. We report on experiences
with the architecture of the software and its development as an open-source
project.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1304.5870v2,"Parameterized Complexity of the Anchored k-Core Problem for Directed
  Graphs","Bhawalkar, Kleinberg, Lewi, Roughgarden, and Sharma [ICALP 2012] introduced
the Anchored k-Core problem, where the task is for a given graph G and integers
b, k, and p to find an induced subgraph H with at least p vertices (the core)
such that all but at most b vertices (called anchors) of H are of degree at
least k. In this paper, we extend the notion of k-core to directed graphs and
provide a number of new algorithmic and complexity results for the directed
version of the problem. We show that
  - The decision version of the problem is NP-complete for every k>=1 even if
the input graph is restricted to be a planar directed acyclic graph of maximum
degree at most k+2.
  - The problem is fixed parameter tractable (FPT) parameterized by the size of
the core p for k=1, and W[1]-hard for k>=2.
  - When the maximum degree of the graph is at most \Delta, the problem is FPT
parameterized by p+\Delta if k>= \Delta/2.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.0981v1,Templet: a Markup Language for Concurrent Programming,"In this paper we propose a new approach to the description of a network of
interacting processes in a traditional programming language. Special
programming languages or extensions to sequential languages are usually
designed to express the semantics of concurrent execution. Using libraries in
C++, Java, C#, and other languages is more practical way of concurrent
programming. However, this method leads to an increase in workload of a manual
coding. Besides, stock compilers can not detect semantic errors related to the
programming model in such libraries. The new markup language and a special
technique of automatic programming based on the marked code can solve these
problems. The article provides a detailed specification of the markup language
without discussing its implementation details. The language is used for
programming of current and prospective multi-core and many-core systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0603086v1,Matching Edges in Images ; Application to Face Recognition,"This communication describes a representation of images as a set of edges
characterized by their position and orientation. This representation allows the
comparison of two images and the computation of their similarity. The first
step in this computation of similarity is the seach of a geometrical basis of
the two dimensional space where the two images are represented simultaneously
after transformation of one of them. Presently, this simultaneous
representation takes into account a shift and a scaling ; it may be extended to
rotations or other global geometrical transformations. An elementary
probabilistic computation shows that a sufficient but not excessive number of
trials (a few tens) ensures that the exhibition of this common basis is
guaranteed in spite of possible errors in the detection of edges. When this
first step is performed, the search of similarity between the two images
reduces to counting the coincidence of edges in the two images. The approach
may be applied to many problems of pattern matching ; it was checked on face
recognition.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1210.6209v1,Characteristic of partition-circuit matroid through approximation number,"Rough set theory is a useful tool to deal with uncertain, granular and
incomplete knowledge in information systems. And it is based on equivalence
relations or partitions. Matroid theory is a structure that generalizes linear
independence in vector spaces, and has a variety of applications in many
fields. In this paper, we propose a new type of matroids, namely,
partition-circuit matroids, which are induced by partitions. Firstly, a
partition satisfies circuit axioms in matroid theory, then it can induce a
matroid which is called a partition-circuit matroid. A partition and an
equivalence relation on the same universe are one-to-one corresponding, then
some characteristics of partition-circuit matroids are studied through rough
sets. Secondly, similar to the upper approximation number which is proposed by
Wang and Zhu, we define the lower approximation number. Some characteristics of
partition-circuit matroids and the dual matroids of them are investigated
through the lower approximation number and the upper approximation number.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1311.0663v1,"De-Virtualizing Social Events: Understanding the Gap between Online and
  Offline Participation for Event Invitations","One growing use of computer-based communication media is for gathering people
to initiate or sustain social events. Although the use of computer-mediated
communication and social network sites such as Facebook for event promotion is
becoming popular, online participation in an event does not always translate to
offline attendance. In this paper, we report on an interview study of 31
participants that examines how people handle online event invitations and what
influences their online and offline participation. The results show that
people's event participation is shaped by their social perceptions of the
event's nature (e.g., public or private), their relationships to others (e.g.,
the strength of their connections to other invitees), and the medium used to
communicate event information (e.g., targeted invitation via email or spam
communication via Facebook event page). By exploring how people decide whether
to participate online or offline, the results illuminate the sophisticated
nature of the mechanisms that affect participation and have design implications
that can bridge virtual and real attendance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1105.1743v1,"Abstracting Abstract Machines: A Systematic Approach to Higher-Order
  Program Analysis","Predictive models are fundamental to engineering reliable software systems.
However, designing conservative, computable approximations for the behavior of
programs (static analyses) remains a difficult and error-prone process for
modern high-level programming languages. What analysis designers need is a
principled method for navigating the gap between semantics and analytic models:
analysis designers need a method that tames the interaction of complex
languages features such as higher-order functions, recursion, exceptions,
continuations, objects and dynamic allocation.
  We contribute a systematic approach to program analysis that yields novel and
transparently sound static analyses. Our approach relies on existing
derivational techniques to transform high-level language semantics into
low-level deterministic state-transition systems (with potentially infinite
state spaces). We then perform a series of simple machine refactorings to
obtain a sound, computable approximation, which takes the form of a
non-deterministic state-transition systems with finite state spaces. The
approach scales up uniformly to enable program analysis of realistic language
features, including higher-order functions, tail calls, conditionals, side
effects, exceptions, first-class continuations, and even garbage collection.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1801.05972v2,WYFIWYG: Investigating Effective User Support in Aerial Videography,"Tools for quadrotor trajectory design have enabled single videographers to
create complex aerial video shots that previously required dedicated hardware
and several operators. We build on this prior work by studying film-maker's
working practices which informed a system design that brings expert workflows
closer to end-users. For this purpose, we propose WYFIWYG, a new quadrotor
camera tool which (i) allows to design a video solely via specifying its
frames, (ii) encourages the exploration of the scene prior to filming and (iii)
allows to continuously frame a camera target according to compositional
intentions. Furthermore, we propose extensions to an existing algorithm,
generating more intuitive angular camera motions and producing spatially and
temporally smooth trajectories. Finally, we conduct a user study where we
evaluate how end-users work with current videography tools. We conclude by
summarizing the findings of work as implications for the design of UIs and
algorithms of quadrotor camera tools.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1803.09002v3,"Socio-spatial Self-organizing Maps: Using Social Media to Assess
  Relevant Geographies for Exposure to Social Processes","Social media offers a unique window into attitudes like racism and
homophobia, exposure to which are important, hard to measure and understudied
social determinants of health. However, individual geo-located observations
from social media are noisy and geographically inconsistent. Existing areas by
which exposures are measured, like Zip codes, average over irrelevant
administratively-defined boundaries. Hence, in order to enable studies of
online social environmental measures like attitudes on social media and their
possible relationship to health outcomes, first there is a need for a method to
define the collective, underlying degree of social media attitudes by region.
To address this, we create the Socio-spatial-Self organizing map, ""SS-SOM""
pipeline to best identify regions by their latent social attitude from Twitter
posts. SS-SOMs use neural embedding for text-classification, and augment
traditional SOMs to generate a controlled number of non-overlapping,
topologically-constrained and topically-similar clusters. We find that not only
are SS-SOMs robust to missing data, the exposure of a cohort of men who are
susceptible to multiple racism and homophobia-linked health outcomes, changes
by up to 42% using SS-SOM measures as compared to using Zip code-based
measures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.10353v1,FlexClock: Generic Clock Reconfiguration for Low-end IoT Devices,"Clock configuration within constrained general-purpose microcontrollers takes
a key role in tuning performance, power consumption, and timing accuracy of
applications in the Internet of Things (IoT). Subsystems governing the
underlying clock tree must nonetheless cope with a huge parameter space,
complex dependencies, and dynamic constraints. Manufacturers expose the
underlying functions in very diverse ways, which leads to specialized
implementations of low portability. In this paper, we propose FlexClock, an
approach for generic online clock reconfiguration on constrained IoT devices.
We argue that (costly) generic clock configuration of general purpose computers
and powerful mobile devices need to slim down to the lower end of the device
spectrum. In search of a generalized solution, we identify recurring patterns
and building blocks, which we use to decompose clock trees into independent,
reusable components. With this segmentation we derive an abstract
representation of vendor-specific clock trees, which then can be dynamically
reconfigured at runtime. We evaluate our implementation on common hardware. Our
measurements demonstrate how FlexClock significantly improves peak power
consumption and energy efficiency by enabling dynamic voltage and frequency
scaling (DVFS) in a platform-agnostic way.",0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1202.2131v1,Temporal Analysis of Literary and Programming Prose,"Literary works reference a variety of globally shared themes including
well-known people, events, and time periods. It is particularly interesting to
locate patterns that are either invariant across time or exhibit a
characteristic change across time, as they could imply something important
about society that those works record. This paper suggests the use of Google
n-gram viewer as a fast prototyping method for examining time-based properties
over a rich sample of literary prose. Using this method, we find that some
repeating periods of time, like Sunday, are referenced disproportionally,
allowing us to pose questions such as why a day like Thursday is so unpopular.
Furthermore, by treating software as a work of prose, we can apply a similar
analysis to open-source software repositories and explore time-based relations
in commit logs. Doing a simple statistical analysis on a few temporal keywords
in the log records, we reinforce and weaken a few beliefs on how college
students approach open source software. Finally, we help readers working on
their own temporal analysis by comparing the fundamental differences between
literary works and code repositories, and suggest blog or wiki as
recently-emerging works.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.04838v1,Block Pruning For Faster Transformers,"Pre-training has improved model accuracy for both classification and
generation tasks at the cost of introducing much larger and slower models.
Pruning methods have proven to be an effective way of reducing model size,
whereas distillation methods are proven for speeding up inference. We introduce
a block pruning approach targeting both small and fast models. Our approach
extends structured methods by considering blocks of any size and integrates
this structure into the movement pruning paradigm for fine-tuning. We find that
this approach learns to prune out full components of the underlying model, such
as attention heads. Experiments consider classification and generation tasks,
yielding among other results a pruned model that is a 2.4x faster, 74% smaller
BERT on SQuAD v1, with a 1% drop on F1, competitive both with distilled models
in speed and pruned models in size.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.03298v1,"Automation Of Transiting Exoplanet Detection, Identification and
  Habitability Assessment Using Machine Learning Approaches","We are at a unique timeline in the history of human evolution where we may be
able to discover earth-like planets around stars outside our solar system where
conditions can support life or even find evidence of life on those planets.
With the launch of several satellites in recent years by NASA, ESA, and other
major space agencies, an ample amount of datasets are at our disposal which can
be utilized to train machine learning models that can automate the arduous
tasks of exoplanet detection, its identification, and habitability
determination. Automating these tasks can save a considerable amount of time
and minimize human errors due to manual intervention. To achieve this aim, we
first analyze the light intensity curves from stars captured by the Kepler
telescope to detect the potential curves that exhibit the characteristics of an
existence of a possible planetary system. For this detection, along with
training conventional models, we propose a stacked GBDT model that can be
trained on multiple representations of the light signals simultaneously.
Subsequently, we address the automation of exoplanet identification and
habitability determination by leveraging several state-of-art machine learning
and ensemble approaches. The identification of exoplanets aims to distinguish
false positive instances from the actual instances of exoplanets whereas the
habitability assessment groups the exoplanet instances into different clusters
based on their habitable characteristics. Additionally, we propose a new metric
called Adequate Thermal Adequacy (ATA) score to establish a potential linear
relationship between habitable and non-habitable instances. Experimental
results suggest that the proposed stacked GBDT model outperformed the
conventional models in detecting transiting exoplanets. Furthermore, the
incorporation of ATA scores in habitability classification enhanced the
performance of models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.09346v2,Conelikes and Ranker Comparisons,"For every fixed class of regular languages, there is a natural hierarchy of
increasingly more general problems: Firstly, the membership problem asks
whether a given language belongs to the fixed class of languages. Secondly, the
separation problem asks for two given languages whether they can be separated
by a language from the fixed class. And thirdly, the covering problem is a
generalization of separation problem to more than two given languages. Most
instances of such problems were solved by the connection of regular languages
and finite monoids. Both the membership problem and the separation problem were
also extended to ordered monoids. The computation of pointlikes can be
interpreted as the algebraic counterpart of the covering problem. In this
paper, we consider the extension the computation of pointlikes to ordered
monoids. This leads to the notion of conelikes for the corresponding algebraic
framework.
  We apply this framework to the Trotter-Weil hierarchy and both the full and
the half levels of the $\text{FO}^2$ quantifier alternation hierarchy. As a
consequence, we solve the covering problem for the resulting subvarieties of
$\mathbf{DA}$. An important combinatorial tool are uniform ranker
characterizations for all subvarieties under consideration; these
characterizations stem from order comparisons of ranker positions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0912.1457v2,A survey on algorithmic aspects of modular decomposition,"The modular decomposition is a technique that applies but is not restricted
to graphs. The notion of module naturally appears in the proofs of many graph
theoretical theorems. Computing the modular decomposition tree is an important
preprocessing step to solve a large number of combinatorial optimization
problems. Since the first polynomial time algorithm in the early 70's, the
algorithmic of the modular decomposition has known an important development.
This paper survey the ideas and techniques that arose from this line of
research.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1310.4780v3,Effective Quotation: relating approaches to language-integrated query,"Language-integrated query techniques have been explored in a number of
different language designs. We consider two different, type-safe approaches
employed by Links and F#. Both approaches provide rich dynamic query generation
capabilities, and thus amount to a form of heterogeneous staged computation,
but to date there has been no formal investigation of their relative
expressiveness. We present two core calculi Eff and Quot, respectively
capturing the essential aspects of language-integrated querying using effects
in Links and quotation in LINQ. We show via translations from Eff to Quot and
back that the two approaches are equivalent in expressiveness. Based on the
translation from Eff to Quot, we extend a simple Links compiler to handle
queries.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.01944v1,Logics of First-Order Constraints -- A Category Independent Approach,"Reflecting our experiences in areas, like Algebraic Specifications, Abstract
Model Theory, Graph Transformations, and Model Driven Software Engineering
(MDSE), we present a general, category independent approach to Logics of
First-Order Constraints (LFOC). Traditional First-Order Logic, Description
Logic and the sketch framework are discussed as examples. We use the concept of
institution [Diaconescu08,GoguenBurstall92] as a guideline to describe LFOC's.
The main result states that any choice of the six parameters, we are going to
describe, gives us a corresponding ""institution of constraints"" at hand. The
""presentations"" for an institution of constraints can be characterized as
""first-order sketches"". As a corresponding variant of the ""sketch-entailments""
in [Makkai97], we finally introduce ""sketch rules"" to equip LFOC's with the
necessary expressive power.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.14215v2,"Deep Residual 3D U-Net for Joint Segmentation and Texture Classification
  of Nodules in Lung","In this work we present a method for lung nodules segmentation, their texture
classification and subsequent follow-up recommendation from the CT image of
lung. Our method consists of neural network model based on popular U-Net
architecture family but modified for the joint nodule segmentation and its
texture classification tasks and an ensemble-based model for the follow-up
recommendation. This solution was evaluated within the LNDb medical imaging
challenge and produced the best nodule segmentation result on the final
leaderboard.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0308005v2,Disabled Access for Museum Websites,"Physical disabled access is something that most museums consider very
seriously. Indeed, there are normally legal requirements to do so. However,
online disabled access is still a relatively novel field. Most museums have not
yet considered the issues in depth. The Human-Computer Interface for their
websites is normally tested with major browsers, but not with specialist
browsers or against the relevant accessibility and validation standards. We
consider the current state of the art in this area and mention an accessibility
survey of some museum websites.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0
http://arxiv.org/abs/1311.0536v3,"The SPARQL2XQuery Interoperability Framework. Utilizing Schema Mapping,
  Schema Transformation and Query Translation to Integrate XML and the Semantic
  Web","The Web of Data is an open environment consisting of a great number of large
inter-linked RDF datasets from various domains. In this environment,
organizations and companies adopt the Linked Data practices utilizing Semantic
Web (SW) technologies, in order to publish their data and offer SPARQL
endpoints (i.e., SPARQL-based search services). On the other hand, the dominant
standard for information exchange in the Web today is XML. The SW and XML
worlds and their developed infrastructures are based on different data models,
semantics and query languages. Thus, it is crucial to develop interoperability
mechanisms that allow the Web of Data users to access XML datasets, using
SPARQL, from their own working environments. It is unrealistic to expect that
all the existing legacy data (e.g., Relational, XML, etc.) will be transformed
into SW data. Therefore, publishing legacy data as Linked Data and providing
SPARQL endpoints over them has become a major research challenge. In this
direction, we introduce the SPARQL2XQuery Framework which creates an
interoperable environment, where SPARQL queries are automatically translated to
XQuery queries, in order to access XML data across the Web. The SPARQL2XQuery
Framework provides a mapping model for the expression of OWL-RDF/S to XML
Schema mappings as well as a method for SPARQL to XQuery translation. To this
end, our Framework supports both manual and automatic mapping specification
between ontologies and XML Schemas. In the automatic mapping specification
scenario, the SPARQL2XQuery exploits the XS2OWL component which transforms XML
Schemas into OWL ontologies. Finally, extensive experiments have been conducted
in order to evaluate the schema transformation, mapping generation, query
translation and query evaluation efficiency, using both real and synthetic
datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0906.1953v3,An Exponential Time 2-Approximation Algorithm for Bandwidth,"The bandwidth of a graph G on n vertices is the minimum b such that the
vertices of G can be labeled from 1 to n such that the labels of every pair of
adjacent vertices differ by at most b.
  In this paper, we present a 2-approximation algorithm for the bandwidth
problem that takes worst-case O(1.9797^n) time and uses polynomial space. This
improves both the previous best 2- and 3-approximation algorithms of Cygan et
al. which have an O(3^n) and O(2^n) worst-case time bounds, respectively. Our
algorithm is based on constructing bucket decompositions of the input graph. A
bucket decomposition partitions the vertex set of a graph into ordered sets
(called buckets) of (almost) equal sizes such that all edges are either
incident to vertices in the same bucket or to vertices in two consecutive
buckets. The idea is to find the smallest bucket size for which there exists a
bucket decomposition. The algorithm uses a simple divide-and-conquer strategy
along with dynamic programming to achieve this improved time bound.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.11881v1,"Introspective Visuomotor Control: Exploiting Uncertainty in Deep
  Visuomotor Control for Failure Recovery","End-to-end visuomotor control is emerging as a compelling solution for robot
manipulation tasks. However, imitation learning-based visuomotor control
approaches tend to suffer from a common limitation, lacking the ability to
recover from an out-of-distribution state caused by compounding errors. In this
paper, instead of using tactile feedback or explicitly detecting the failure
through vision, we investigate using the uncertainty of a policy neural
network. We propose a novel uncertainty-based approach to detect and recover
from failure cases. Our hypothesis is that policy uncertainties can implicitly
indicate the potential failures in the visuomotor control task and that robot
states with minimum uncertainty are more likely to lead to task success. To
recover from high uncertainty cases, the robot monitors its uncertainty along a
trajectory and explores possible actions in the state-action space to bring
itself to a more certain state. Our experiments verify this hypothesis and show
a significant improvement on task success rate: 12% in pushing, 15% in
pick-and-reach and 22% in pick-and-place.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0007012v1,Using Learning-based Filters to Detect Rule-based Filtering Obsolescence,"For years, Caisse des Depots et Consignations has produced information
filtering applications. To be operational, these applications require high
filtering performances which are achieved by using rule-based filters. With
this technique, an administrator has to tune a set of rules for each topic.
However, filters become obsolescent over time. The decrease of their
performances is due to diachronic polysemy of terms that involves a loss of
precision and to diachronic polymorphism of concepts that involves a loss of
recall.
  To help the administrator to maintain his filters, we have developed a method
which automatically detects filtering obsolescence. It consists in making a
learning-based control filter using a set of documents which have already been
categorised as relevant or not relevant by the rule-based filter. The idea is
to supervise this filter by processing a differential comparison of its
outcomes with those of the control one.
  This method has many advantages. It is simple to implement since the training
set used by the learning is supplied by the rule-based filter. Thus, both the
making and the use of the control filter are fully automatic. With automatic
detection of obsolescence, learning-based filtering finds a rich application
which offers interesting prospects.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1906.06761v1,Self-organized inductive reasoning with NeMuS,"Neural Multi-Space (NeMuS) is a weighted multi-space representation for a
portion of first-order logic designed for use with machine learning and neural
network methods. It was demonstrated that it can be used to perform reasoning
based on regions forming patterns of refutation and also in the process of
inductive learning in ILP-like style. Initial experiments were carried out to
investigate whether a self-organizing the approach is suitable to generate
similar concept regions according to the attributes that form such concepts. We
present the results and make an analysis of the suitability of the method in
the process of inductive learning with NeMuS.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1706.08504v1,"The Bernays-Schnfinkel-Ramsey Fragment with Bounded Difference
  Constraints over the Reals is Decidable","First-order linear real arithmetic enriched with uninterpreted predicate
symbols yields an interesting modeling language. However, satisfiability of
such formulas is undecidable, even if we restrict the uninterpreted predicate
symbols to arity one. In order to find decidable fragments of this language, it
is necessary to restrict the expressiveness of the arithmetic part. One
possible path is to confine arithmetic expressions to difference constraints of
the form $x - y \mathrel{\#} c$, where $\#$ ranges over the standard relations
$<, \leq, =, \neq, \geq, >$ and $x,y$ are universally quantified. However, it
is known that combining difference constraints with uninterpreted predicate
symbols yields an undecidable satisfiability problem again. In this paper, it
is shown that satisfiability becomes decidable if we in addition bound the
ranges of universally quantified variables. As bounded intervals over the reals
still comprise infinitely many values, a trivial instantiation procedure is not
sufficient to solve the problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1210.0613v1,"On Multiplicative Linear Logic, Modality and Quantum Circuits","A logical system derived from linear logic and called QMLL is introduced and
shown able to capture all unitary quantum circuits. Conversely, any proof is
shown to compute, through a concrete GoI interpretation, some quantum circuits.
The system QMLL, which enjoys cut-elimination, is obtained by endowing
multiplicative linear logic with a quantum modality.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0405093v2,Computerized Face Detection and Recognition,"This publication presents methods for face detection, analysis and
recognition: fast normalized cross-correlation (fast correlation coefficient)
between multiple templates based face pre-detection method, method for
detection of exact face contour based on snakes and Generalized Gradient Vector
Flow field, method for combining recognition algorithms based on Cumulative
Match Characteristics in order to increase recognition speed and accuracy, and
face recognition method based on Principal Component Analysis of the Wavelet
Packet Decomposition allowing to use PCA - based recognition method with large
number of training images. For all the methods are presented experimental
results and comparisons of speed and accuracy with large face databases.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1708.02380v1,Ghera: A Repository of Android App Vulnerability Benchmarks,"Security of mobile apps affects the security of their users. This has fueled
the development of techniques to automatically detect vulnerabilities in mobile
apps and help developers secure their apps; specifically, in the context of
Android platform due to openness and ubiquitousness of the platform. Despite a
slew of research efforts in this space, there is no comprehensive repository of
up-to-date and lean benchmarks that contain most of the known Android app
vulnerabilities and, consequently, can be used to rigorously evaluate both
existing and new vulnerability detection techniques and help developers learn
about Android app vulnerabilities. In this paper, we describe Ghera, an open
source repository of benchmarks that capture 25 known vulnerabilities in
Android apps (as pairs of exploited/benign and exploiting/malicious apps). We
also present desirable characteristics of vulnerability benchmarks and
repositories that we uncovered while creating Ghera.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/cs/0203026v1,"Conformal Geometry, Euclidean Space and Geometric Algebra","Projective geometry provides the preferred framework for most implementations
of Euclidean space in graphics applications. Translations and rotations are
both linear transformations in projective geometry, which helps when it comes
to programming complicated geometrical operations. But there is a fundamental
weakness in this approach - the Euclidean distance between points is not
handled in a straightforward manner. Here we discuss a solution to this
problem, based on conformal geometry. The language of geometric algebra is best
suited to exploiting this geometry, as it handles the interior and exterior
products in a single, unified framework. A number of applications are
discussed, including a compact formula for reflecting a line off a general
spherical surface.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.01747v2,Information Complexity and Generalization Bounds,"We present a unifying picture of PAC-Bayesian and mutual information-based
upper bounds on the generalization error of randomized learning algorithms. As
we show, Tong Zhang's information exponential inequality (IEI) gives a general
recipe for constructing bounds of both flavors. We show that several important
results in the literature can be obtained as simple corollaries of the IEI
under different assumptions on the loss function. Moreover, we obtain new
bounds for data-dependent priors and unbounded loss functions. Optimizing the
bounds gives rise to variants of the Gibbs algorithm, for which we discuss two
practical examples for learning with neural networks, namely, Entropy- and
PAC-Bayes- SGD. Further, we use an Occam's factor argument to show a
PAC-Bayesian bound that incorporates second-order curvature information of the
training loss.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1010.3367v2,Spectrally degenerate graphs: Hereditary case,"It is well known that the spectral radius of a tree whose maximum degree is D
cannot exceed 2sqrt{D-1}. Similar upper bound holds for arbitrary planar
graphs, whose spectral radius cannot exceed sqrt{8D}+10, and more generally,
for all d-degenerate graphs, where the corresponding upper bound is sqrt{4dD}.
Following this, we say that a graph G is spectrally d-degenerate if every
subgraph H of G has spectral radius at most sqrt{d.Delta(H)}. In this paper we
derive a rough converse of the above-mentioned results by proving that each
spectrally d-degenerate graph G contains a vertex whose degree is at most
4dlog_2(D/d) (if D>=2d). It is shown that the dependence on D in this upper
bound cannot be eliminated, as long as the dependence on d is subexponential.
It is also proved that the problem of deciding if a graph is spectrally
d-degenerate is co-NP-complete.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1403.1866v2,"An Expressive Model for the Web Infrastructure: Definition and
  Application to the BrowserID SSO System","The web constitutes a complex infrastructure and as demonstrated by numerous
attacks, rigorous analysis of standards and web applications is indispensable.
  Inspired by successful prior work, in particular the work by Akhawe et al. as
well as Bansal et al., in this work we propose a formal model for the web
infrastructure. While unlike prior works, which aim at automatic analysis, our
model so far is not directly amenable to automation, it is much more
comprehensive and accurate with respect to the standards and specifications. As
such, it can serve as a solid basis for the analysis of a broad range of
standards and applications.
  As a case study and another important contribution of our work, we use our
model to carry out the first rigorous analysis of the BrowserID system (a.k.a.
Mozilla Persona), a recently developed complex real-world single sign-on system
that employs technologies such as AJAX, cross-document messaging, and HTML5 web
storage. Our analysis revealed a number of very critical flaws that could not
have been captured in prior models. We propose fixes for the flaws, formally
state relevant security properties, and prove that the fixed system in a
setting with a so-called secondary identity provider satisfies these security
properties in our model. The fixes for the most critical flaws have already
been adopted by Mozilla and our findings have been rewarded by the Mozilla
Security Bug Bounty Program.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1708.03183v2,"Automated Tiling of Unstructured Mesh Computations with Application to
  Seismological Modelling","Sparse tiling is a technique to fuse loops that access common data, thus
increasing data locality. Unlike traditional loop fusion or blocking, the loops
may have different iteration spaces and access shared datasets through indirect
memory accesses, such as A[map[i]] -- hence the name ""sparse"". One notable
example of such loops arises in discontinuous-Galerkin finite element methods,
because of the computation of numerical integrals over different domains (e.g.,
cells, facets). The major challenge with sparse tiling is implementation -- not
only is it cumbersome to understand and synthesize, but it is also onerous to
maintain and generalize, as it requires a complete rewrite of the bulk of the
numerical computation. In this article, we propose an approach to extend the
applicability of sparse tiling based on raising the level of abstraction.
Through a sequence of compiler passes, the mathematical specification of a
problem is progressively lowered, and eventually sparse-tiled C for-loops are
generated. Besides automation, we advance the state-of-the-art by introducing:
a revisited, more efficient sparse tiling algorithm; support for
distributed-memory parallelism; a range of fine-grained optimizations for
increased run-time performance; implementation in a publicly-available library,
SLOPE; and an in-depth study of the performance impact in Seigen, a real-world
elastic wave equation solver for seismological problems, which shows speed-ups
up to 1.28x on a platform consisting of 896 Intel Broadwell cores.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.05937v1,Formalization of Phase Ordering,"Phasers pose an interesting synchronization mechanism that generalizes many
collective synchronization patterns seen in parallel programming languages,
including barriers, clocks, and point-to-point synchronization using latches or
semaphores. This work characterizes scheduling constraints on phaser
operations, by relating the execution state of two tasks that operate on the
same phaser. We propose a formalization of Habanero phasers,
May-Happen-In-Parallel, and Happens-Before relations for phaser operations, and
show that these relations conform with the semantics. Our formalization and
proofs are fully mechanized using the Coq proof assistant, and are available
online.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1204.3436v1,"Explaining Adaptation in Genetic Algorithms With Uniform Crossover: The
  Hyperclimbing Hypothesis","The hyperclimbing hypothesis is a hypothetical explanation for adaptation in
genetic algorithms with uniform crossover (UGAs). Hyperclimbing is an
intuitive, general-purpose, non-local search heuristic applicable to discrete
product spaces with rugged or stochastic cost functions. The strength of this
heuristic lie in its insusceptibility to local optima when the cost function is
deterministic, and its tolerance for noise when the cost function is
stochastic. Hyperclimbing works by decimating a search space, i.e. by
iteratively fixing the values of small numbers of variables. The hyperclimbing
hypothesis holds that UGAs work by implementing efficient hyperclimbing. Proof
of concept for this hypothesis comes from the use of a novel analytic technique
involving the exploitation of algorithmic symmetry. We have also obtained
experimental results that show that a simple tweak inspired by the
hyperclimbing hypothesis dramatically improves the performance of a UGA on
large, random instances of MAX-3SAT and the Sherrington Kirkpatrick Spin
Glasses problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1603.03110v1,"Change management: Implementation and benefits of the change control in
  the information technology enviroment","In the competitive environment, companies have given increasing importance to
the IT sector and the resources it delivers as strategic. As a result, IT
becomes a living being within the company. This sector is being subject to
continuous changes in this scenario. These changes can occur within the own IT
sector or whether IT to other sectors of the company. For both scenarios, it is
important to have a good change control to avoid unnecessary trouble and
expense. This paper aims to show through a case study, the benefits and results
obtained with the implementation of a process of managing and controlling
changes in the information technology environment of a large government company
in Brazil.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1612.04790v2,"A 17/12-Approximation Algorithm for 2-Vertex-Connected Spanning
  Subgraphs on Graphs with Minimum Degree At Least 3","We obtain a polynomial-time 17/12-approximation algorithm for the
minimum-cost 2-vertex-connected spanning subgraph problem, restricted to graphs
of minimum degree at least 3. Our algorithm uses the framework of
ear-decompositions for approximating connectivity problems, which was
previously used in algorithms for finding the smallest 2-edge-connected
spanning subgraph by Cheriyan, Seb\H{o} and Szigeti (SIAM J.Discrete Math.
2001) who gave a 17/12-approximation algorithm for this problem, and by
Seb\H{o} and Vygen (Combinatorica 2014), who improved the approximation ratio
to 4/3.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0404042v2,"Extraction of topological features from communication network
  topological patterns using self-organizing feature maps","Different classes of communication network topologies and their
representation in the form of adjacency matrix and its eigenvalues are
presented. A self-organizing feature map neural network is used to map
different classes of communication network topological patterns. The neural
network simulation results are reported.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0808.2602v2,Easily testable logical networks based on a 'widened long flip-flop',"The article describes an attempt to solve at once three basic problems
arising at testing a complex digital equipment for defects: 1) the problem of
an exponential increasing of the complexity of testing the equipment with the
complexity of the equipment; 2) the problem of testing of the tester; 3) the
problem of a mutual masking of defects. The proposed solution is nothing more
than using certain limitations for connections between usual logical gates.
Arbitrary multiple stuck-at-faults are supposed as defects.",0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.00086v2,ArchiveSafe: Mass-Leakage-Resistant Storage from Proof-of-Work,"Data breaches-mass leakage of stored information-are a major security
concern. Encryption can provide confidentiality, but encryption depends on a
key which, if compromised, allows the attacker to decrypt everything,
effectively instantly. Security of encrypted data thus becomes a question of
protecting the encryption keys. In this paper, we propose using keyless
encryption to construct a mass leakage resistant archiving system, where
decryption of a file is only possible after the requester, whether an
authorized user or an adversary, completes a proof of work in the form of
solving a cryptographic puzzle. This proposal is geared towards protection of
infrequently-accessed archival data, where any one file may not require too
much work to decrypt, decryption of a large number of files-mass
leakage-becomes increasingly expensive for an attacker. We present a prototype
implementation realized as a user-space file system driver for Linux. We report
experimental results of system behaviour under different file sizes and puzzle
difficulty levels. Our keyless encryption technique can be added as a layer on
top of traditional encryption: together they provide strong security against
adversaries without the key and resistance against mass decryption by an
attacker.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.08340v2,"Return migration of German-affiliated researchers: Analyzing departure
  and return by gender, cohort, and discipline using Scopus bibliometric data
  1996-2020","The international migration of researchers is an important dimension of
scientific mobility, and has been the subject of considerable policy debate.
However, tracking the migration life courses of researchers is challenging due
to data limitations. In this study, we use Scopus bibliometric data on eight
million publications from 1.1 million researchers who have published at least
once with an affiliation address from Germany in 1996-2020. We construct the
partial life histories of published researchers in this period and explore both
their out-migration and the subsequent return of a subset of this group: the
returnees. Our analyses shed light on the career stages and gender disparities
between researchers who remain in Germany, those who emigrate, and those who
eventually return. We find that the return migration streams are even more
gender imbalanced, which points to the need for additional efforts to encourage
female researchers to come back to Germany. We document a slightly declining
trend in return migration among more recent cohorts of researchers who left
Germany, which, for most disciplines, was associated with a decrease in the
German collaborative ties of these researchers. Moreover, we find that the
gender disparities for the most gender imbalanced disciplines are unlikely to
be mitigated by return migration given the gender compositions of the cohorts
of researchers who have left Germany and of those who have returned. This
analysis uncovers new dimensions of migration among scholars by investigating
the return migration of published researchers, which is critical for the
development of science policy.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2012.05567v3,"Debiased-CAM to mitigate image perturbations with faithful visual
  explanations of machine learning","Model explanations such as saliency maps can improve user trust in AI by
highlighting important features for a prediction. However, these become
distorted and misleading when explaining predictions of images that are subject
to systematic error (bias) by perturbations and corruptions. Furthermore, the
distortions persist despite model fine-tuning on images biased by different
factors (blur, color temperature, day/night). We present Debiased-CAM to
recover explanation faithfulness across various bias types and levels by
training a multi-input, multi-task model with auxiliary tasks for explanation
and bias level predictions. In simulation studies, the approach not only
enhanced prediction accuracy, but also generated highly faithful explanations
about these predictions as if the images were unbiased. In user studies,
debiased explanations improved user task performance, perceived truthfulness
and perceived helpfulness. Debiased training can provide a versatile platform
for robust performance and explanation faithfulness for a wide range of
applications with data biases.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1204.6445v3,A Complete Dichotomy Rises from the Capture of Vanishing Signatures,"We prove a complexity dichotomy theorem for Holant problems over an arbitrary
set of complex-valued symmetric constraint functions F on Boolean variables.
This extends and unifies all previous dichotomies for Holant problems on
symmetric constraint functions (taking values without a finite modulus). We
define and characterize all symmetric vanishing signatures. They turned out to
be essential to the complete classification of Holant problems. The dichotomy
theorem has an explicit tractability criterion expressible in terms of
holographic transformations. A Holant problem defined by a set of constraint
functions F is solvable in polynomial time if it satisfies this tractability
criterion, and is #P-hard otherwise. The tractability criterion can be
intuitively stated as follows: A set F is tractable if (1) every function in F
has arity at most two, or (2) F is transformable to an affine type, or (3) F is
transformable to a product type, or (4) F is vanishing, combined with the right
type of binary functions, or (5) F belongs to a special category of vanishing
type Fibonacci gates. The proof of this theorem utilizes many previous
dichotomy theorems on Holant problems and Boolean #CSP. Holographic
transformations play an indispensable role as both a proof technique and in the
statement of the tractability criterion.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.07482v1,Decoding CNN based Object Classifier Using Visualization,"This paper investigates how working of Convolutional Neural Network (CNN) can
be explained through visualization in the context of machine perception of
autonomous vehicles. We visualize what type of features are extracted in
different convolution layers of CNN that helps to understand how CNN gradually
increases spatial information in every layer. Thus, it concentrates on region
of interests in every transformation. Visualizing heat map of activation helps
us to understand how CNN classifies and localizes different objects in image.
This study also helps us to reason behind low accuracy of a model helps to
increase trust on object detection module.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1908.04923v1,Type-two Iteration with Bounded Query Revision,"Motivated by recent results of Kapron and Steinberg (LICS 2018) we introduce
new forms of iteration on length in the setting of applied lambda-calculi for
higher-type poly-time computability. In particular, in a type-two setting, we
consider functionals which capture iteration on input length which bound
interaction with the type-one input parameter, by restricting to a constant
either the number of times the function parameter may return a value of
increasing size, or the number of times the function parameter may be applied
to an argument of increasing size. We prove that for any constant bound, the
iterators obtained are equivalent, with respect to lambda-definability over
type-one poly-time functions, to the recursor of Cook and Urquhart which
captures Cobham's notion of limited recursion on notation in this setting.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1908.06211v2,PAStime: Progress-aware Scheduling for Time-critical Computing,"Over-estimation of worst-case execution times (WCETs) of real-time tasks
leads to poor resource utilization. In a mixed-criticality system (MCS), the
over-provisioning of CPU time to accommodate the WCETs of highly critical tasks
may lead to degraded service for less critical tasks. In this paper, we present
PAStime, a novel approach to monitor and adapt the runtime progress of highly
time-critical applications, to allow for improved service to lower criticality
tasks. In PAStime, CPU time is allocated to time-critical tasks according to
the delays they experience as they progress through their control flow graphs.
This ensures that as much time as possible is made available to improve the
Quality-of-Service of less critical tasks, while high-criticality tasks are
compensated after their delays.
  In this paper, we integrate PAStime with Adaptive Mixed-criticality (AMC)
scheduling. The LO-mode budget of a high-criticality task is adjusted according
to the delay observed at execution checkpoints. This is the first
implementation of AMC in the scheduling framework Using LITMUS-RT, which is
extended with our PAStime runtime policy and tested with real-time Linux
applications such as object classification and detection. We observe in our
experimental evaluation that AMC-PAStime significantly improves the utilization
of the low-criticality tasks while guaranteeing service to high-criticality
tasks.",0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1007.3819v1,FO(FD): Extending classical logic with rule-based fixpoint definitions,"We introduce fixpoint definitions, a rule-based reformulation of fixpoint
constructs. The logic FO(FD), an extension of classical logic with fixpoint
definitions, is defined. We illustrate the relation between FO(FD) and FO(ID),
which is developed as an integration of two knowledge representation paradigms.
The satisfiability problem for FO(FD) is investigated by first reducing FO(FD)
to difference logic and then using solvers for difference logic. These
reductions are evaluated in the computation of models for FO(FD) theories
representing fairness conditions and we provide potential applications of
FO(FD).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0603075v1,Unmanaged Internet Protocol: Taming the Edge Network Management Crisis,"Though appropriate for core Internet infrastructure, the Internet Protocol is
unsuited to routing within and between emerging ad-hoc edge networks due to its
dependence on hierarchical, administratively assigned addresses. Existing
ad-hoc routing protocols address the management problem but do not scale to
Internet-wide networks. The promise of ubiquitous network computing cannot be
fulfilled until we develop an Unmanaged Internet Protocol (UIP), a scalable
routing protocol that manages itself automatically. UIP must route within and
between constantly changing edge networks potentially containing millions or
billions of nodes, and must still function within edge networks disconnected
from the main Internet, all without imposing the administrative burden of
hierarchical address assignment. Such a protocol appears challenging but
feasible. We propose an architecture based on self-certifying, cryptographic
node identities and a routing algorithm adapted from distributed hash tables.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0609082v1,Classifying extrema using intervals,"We present a straightforward and verified method of deciding whether the
n-dimensional point x (n>=1), such that \nabla f(x)=0, is the local minimizer,
maximizer or just a saddle point of a real-valued function f.
  The method scales linearly with dimensionality of the problem and never
produces false results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.14817v1,Uncertainty Reasoning for Probabilistic Petri Nets via Bayesian Networks,"This paper exploits extended Bayesian networks for uncertainty reasoning on
Petri nets, where firing of transitions is probabilistic. In particular,
Bayesian networks are used as symbolic representations of probability
distributions, modelling the observer's knowledge about the tokens in the net.
The observer can study the net by monitoring successful and failed steps.
  An update mechanism for Bayesian nets is enabled by relaxing some of their
restrictions, leading to modular Bayesian nets that can conveniently be
represented and modified. As for every symbolic representation, the question is
how to derive information - in this case marginal probability distributions -
from a modular Bayesian net. We show how to do this by generalizing the known
method of variable elimination.
  The approach is illustrated by examples about the spreading of diseases (SIR
model) and information diffusion in social networks. We have implemented our
approach and provide runtime results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1904.08722v1,"Quantitative Expressiveness of Instruction Sequence Classes for
  Computation on Single Bit Registers","The number of instructions of an instruction sequence is taken for its
logical SLOC, and is abbreviated with LLOC. A notion of quantitative
expressiveness is based on LLOC and in the special case of operation over a
family of single bit registers a collection of elementary properties are
established. A dedicated notion of interface is developed and is used for
stating relevant properties of classes of instruction sequences",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0411008v3,Intuitionistic computability logic,"Computability logic (CL) is a systematic formal theory of computational tasks
and resources, which, in a sense, can be seen as a semantics-based alternative
to (the syntactically introduced) linear logic. With its expressive and
flexible language, where formulas represent computational problems and ""truth""
is understood as algorithmic solvability, CL potentially offers a comprehensive
logical basis for constructive applied theories and computing systems
inherently requiring constructive and computationally meaningful underlying
logics.
  Among the best known constructivistic logics is Heyting's intuitionistic
calculus INT, whose language can be seen as a special fragment of that of CL.
The constructivistic philosophy of INT, however, has never really found an
intuitively convincing and mathematically strict semantical justification. CL
has good claims to provide such a justification and hence a materialization of
Kolmogorov's known thesis ""INT = logic of problems"". The present paper contains
a soundness proof for INT with respect to the CL semantics. A comprehensive
online source on CL is available at http://www.cis.upenn.edu/~giorgi/cl.html",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.11617v1,"A large, crowdsourced evaluation of gesture generation systems on common
  data: The GENEA Challenge 2020","Co-speech gestures, gestures that accompany speech, play an important role in
human communication. Automatic co-speech gesture generation is thus a key
enabling technology for embodied conversational agents (ECAs), since humans
expect ECAs to be capable of multi-modal communication. Research into gesture
generation is rapidly gravitating towards data-driven methods. Unfortunately,
individual research efforts in the field are difficult to compare: there are no
established benchmarks, and each study tends to use its own dataset, motion
visualisation, and evaluation methodology. To address this situation, we
launched the GENEA Challenge, a gesture-generation challenge wherein
participating teams built automatic gesture-generation systems on a common
dataset, and the resulting systems were evaluated in parallel in a large,
crowdsourced user study using the same motion-rendering pipeline. Since
differences in evaluation outcomes between systems now are solely attributable
to differences between the motion-generation methods, this enables benchmarking
recent approaches against one another in order to get a better impression of
the state of the art in the field. This paper reports on the purpose, design,
results, and implications of our challenge.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.05252v4,"OmniLytics: A Blockchain-based Secure Data Market for Decentralized
  Machine Learning","We propose OmniLytics, a blockchain-based secure data trading marketplace for
machine learning applications. Utilizing OmniLytics, many distributed data
owners can contribute their private data to collectively train an ML model
requested by some model owners, and receive compensation for data contribution.
OmniLytics enables such model training while simultaneously providing 1) model
security against curious data owners; 2) data security against the curious
model and data owners; 3) resilience to malicious data owners who provide
faulty results to poison model training; and 4) resilience to malicious model
owners who intend to evade payment. OmniLytics is implemented as a blockchain
smart contract to guarantee the atomicity of payment. In OmniLytics, a model
owner splits its model into the private and public parts and publishes the
public part on the contract. Through the execution of the contract, the
participating data owners securely aggregate their locally trained models to
update the model owner's public model and receive reimbursement through the
contract. We implement a working prototype of OmniLytics on Ethereum blockchain
and perform extensive experiments to measure its gas cost, execution time, and
model quality under various parameter combinations. For training a CNN on the
MNIST dataset, the MO is able to boost its model accuracy from 62% to 83%
within 500ms in blockchain processing time.This demonstrates the effectiveness
of OmniLytics for practical deployment.",0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1701.07123v1,"Towards Automatic Learning of Heuristics for Mechanical Transformations
  of Procedural Code","The current trends in next-generation exascale systems go towards integrating
a wide range of specialized (co-)processors into traditional supercomputers.
Due to the efficiency of heterogeneous systems in terms of Watts and FLOPS per
surface unit, opening the access of heterogeneous platforms to a wider range of
users is an important problem to be tackled. However, heterogeneous platforms
limit the portability of the applications and increase development complexity
due to the programming skills required. Program transformation can help make
programming heterogeneous systems easier by defining a step-wise transformation
process that translates a given initial code into a semantically equivalent
final code, but adapted to a specific platform. Program transformation systems
require the definition of efficient transformation strategies to tackle the
combinatorial problem that emerges due to the large set of transformations
applicable at each step of the process. In this paper we propose a machine
learning-based approach to learn heuristics to define program transformation
strategies. Our approach proposes a novel combination of reinforcement learning
and classification methods to efficiently tackle the problems inherent to this
type of systems. Preliminary results demonstrate the suitability of this
approach.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0404014v1,A Modular and Fault-Tolerant Data Transport Framework,"The High Level Trigger (HLT) of the future ALICE heavy-ion experiment has to
reduce its input data rate of up to 25 GB/s to at most 1.25 GB/s for output
before the data is written to permanent storage. To cope with these data rates
a large PC cluster system is being designed to scale to several 1000 nodes,
connected by a fast network. For the software that will run on these nodes a
flexible data transport and distribution software framework, described in this
thesis, has been developed. The framework consists of a set of separate
components, that can be connected via a common interface. This allows to
construct different configurations for the HLT, that are even changeable at
runtime. To ensure a fault-tolerant operation of the HLT, the framework
includes a basic fail-over mechanism that allows to replace whole nodes after a
failure. The mechanism will be further expanded in the future, utilizing the
runtime reconnection feature of the framework's component interface. To connect
cluster nodes a communication class library is used that abstracts from the
actual network technology and protocol used to retain flexibility in the
hardware choice. It contains already two working prototype versions for the TCP
protocol as well as SCI network adapters. Extensions can be added to the
library without modifications to other parts of the framework. Extensive tests
and measurements have been performed with the framework. Their results as well
as conclusions drawn from them are also presented in this thesis. Performance
tests show very promising results for the system, indicating that it can
fulfill ALICE's requirements concerning the data transport.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1110.3584v1,Optimal Final Carry Propagate Adder Design for Parallel Multipliers,"Based on the ASIC layout level simulation of 7 types of adder structures each
of four different sizes, i.e. a total of 28 adders, we propose expressions for
the width of each of the three regions of the final Carry Propagate Adder (CPA)
to be used in parallel multipliers. We also propose the types of adders to be
used in each region that would lead to the optimal performance of the hybrid
final adders in parallel multipliers. This work evaluates the complete
performance of the analyzed designs in terms of delay, area, power through
custom design and layout in 0.18 um CMOS process technology.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.02410v4,"Using Voice and Biofeedback to Predict User Engagement during
  Requirements Interviews","Capturing users' engagement is crucial for gathering feedback about the
features of a software product. In a market-driven context, current approaches
to collect and analyze users' feedback are based on techniques leveraging
information extracted from product reviews and social media. These approaches
are hardly applicable in bespoke software development, or in contexts in which
one needs to gather information from specific users. In such cases, companies
need to resort to face-to-face interviews to get feedback on their products. In
this paper, we propose to utilize biometric data, in terms of physiological and
voice features, to complement interviews with information about the engagement
of the user on the discussed product-relevant topics. We evaluate our approach
by interviewing users while gathering their physiological data (i.e.,
biofeedback) using an Empatica E4 wristband, and capturing their voice through
the default audio-recorder of a common laptop. Our results show that we can
predict users' engagement by training supervised machine learning algorithms on
biometric data (F1=0.72), and that voice features alone are sufficiently
effective (F1=0.71). Our work contributes with one the first studies in
requirements engineering in which biometrics are used to identify emotions.
This is also the first study in software engineering that considers voice
analysis. The usage of voice features could be particularly helpful for
emotion-aware requirements elicitation in remote communication, either
performed by human analysts or voice-based chatbots, and can also be exploited
to support the analysis of meetings in software engineering research.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9811023v1,Complexity limitations on quantum computation,"We use the powerful tools of counting complexity and generic oracles to help
understand the limitations of the complexity of quantum computation. We show
several results for the probabilistic quantum class BQP.
  1. BQP is low for PP, i.e., PP^BQP=PP.
  2. There exists a relativized world where P=BQP and the polynomial-time
hierarchy is infinite.
  3. There exists a relativized world where BQP does not have complete sets.
  4. There exists a relativized world where P=BQP but P is not equal to UP
intersect coUP and one-way functions exist. This gives a relativized answer to
an open question of Simon.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.11893v1,ParaLarH: Parallel FPGA Router based upon Lagrange Heuristics,"Routing of the nets in Field Programmable Gate Array (FPGA) design flow is
one of the most time consuming steps. Although Versatile Place and Route (VPR),
which is a commonly used algorithm for this purpose, routes effectively, it is
slow in execution. One way to accelerate this design flow is to use
parallelization. Since VPR is intrinsically sequential, a set of parallel
algorithms have been recently proposed for this purpose (ParaLaR and
ParaLarPD).
  These algorithms formulate the routing process as a Linear Program (LP) and
solve it using the Lagrange relaxation, the sub-gradient method, and the
Steiner tree algorithm. Out of the many metrics available to check the
effectiveness of routing, ParaLarPD, which is an improved version of ParaLaR,
suffers from large violations in the constraints of the LP problem (which is
related to the minimum channel width metric) as well as an easily measurable
critical path delay metric that can be improved further.
  In this paper, we introduce a set of novel Lagrange heuristics that improve
the Lagrange relaxation process. When tested on the MCNC benchmark circuits, on
an average, this leads to halving of the constraints violation, up to 10%
improvement in the minimum channel width, and up to 8% reduction in the
critical path delay as obtained from ParaLarPD. We term our new algorithm as
ParaLarH. Due to the increased work in the Lagrange relaxation process, as
compared to ParaLarPD, ParaLarH does slightly deteriorate the speedup obtained
because of parallelization, however, this aspect is easily compensated by using
more number of threads.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1507.06829v1,The Polylingual Labeled Topic Model,"In this paper, we present the Polylingual Labeled Topic Model, a model which
combines the characteristics of the existing Polylingual Topic Model and
Labeled LDA. The model accounts for multiple languages with separate topic
distributions for each language while restricting the permitted topics of a
document to a set of predefined labels. We explore the properties of the model
in a two-language setting on a dataset from the social science domain. Our
experiments show that our model outperforms LDA and Labeled LDA in terms of
their held-out perplexity and that it produces semantically coherent topics
which are well interpretable by human subjects.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.03303v1,Length of a Full Steiner Tree as a Function of Terminal Coordinates,"Given the coordinates of the terminals $ \{(x_j,y_j)\}_{j=1}^n $ of the full
Euclidean Steiner tree, its length equals $$ \left| \sum_{j=1}^n z_j U_j
\right| \, , $$ where $ \{z_j:=x_j+ \mathbf i y_j\}_{j=1}^n $ and $
\{U_j\}_{j=1}^n $ are suitably chosen $ 6 $th roots of unity. We also extend
this result for the cost of the optimal Weber networks which are topologically
equivalent to some full Steiner trees.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.07273v1,"FAIR Geovisualizations: Definitions, Challenges, and the Road Ahead","The availability of open data and of tools to create visualizations on top of
these open datasets have led to an ever-growing amount of geovisualizations on
the Web. There is thus an increasing need for techniques to make
geovisualizations FAIR - Findable, Accessible, Interoperable, and Reusable.
This article explores what it would mean for a geovisualization to be FAIR,
presents relevant approaches to FAIR geovisualizations and lists open research
questions on the road towards FAIR geovisualizations. The discussion is done
using three complementary perspectives: the computer, which stores
geovisualizations digitally; the analyst, who uses them for sensemaking; and
the developer, who creates them. The framework for FAIR geovisualizations
proposed, and the open questions identified are relevant to researchers working
on findable, accessible, interoperable, and reusable online visualizations of
geographic information.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0804.3234v2,Technical Report - Automatic Contour Extraction from 2D Neuron Images,"This work describes a novel methodology for automatic contour extraction from
2D images of 3D neurons (e.g. camera lucida images and other types of 2D
microscopy). Most contour-based shape analysis methods can not be used to
characterize such cells because of overlaps between neuronal processes. The
proposed framework is specifically aimed at the problem of contour following
even in presence of multiple overlaps. First, the input image is preprocessed
in order to obtain an 8-connected skeleton with one-pixel-wide branches, as
well as a set of critical regions (i.e., bifurcations and crossings). Next, for
each subtree, the tracking stage iteratively labels all valid pixel of
branches, up to a critical region, where it determines the suitable direction
to proceed. Finally, the labeled skeleton segments are followed in order to
yield the parametric contour of the neuronal shape under analysis. The reported
system was successfully tested with respect to several images and the results
from a set of three neuron images are presented here, each pertaining to a
different class, i.e. alpha, delta and epsilon ganglion cells, containing a
total of 34 crossings. The algorithms successfully got across all these
overlaps. The method has also been found to exhibit robustness even for images
with close parallel segments. The proposed method is robust and may be
implemented in an efficient manner. The introduction of this approach should
pave the way for more systematic application of contour-based shape analysis
methods in neuronal morphology.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1902.04055v4,On the number of pancake stacks requiring four flips to be sorted,"Using existing classification results for the 7- and 8-cycles in the pancake
graph, we determine the number of permutations that require 4 pancake flips
(prefix reversals) to be sorted. A similar characterization of the 8-cycles in
the burnt pancake graph, due to the authors, is used to derive a formula for
the number of signed permutations requiring 4 (burnt) pancake flips to be
sorted. We furthermore provide an analogous characterization of the 9-cycles in
the burnt pancake graph. Finally we present numerical evidence that polynomial
formulae exist giving the number of signed permutations that require $k$ flips
to be sorted, with $5\leq k\leq9$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1007.2584v3,Bisimulation for quantum processes,"In this paper we introduce a novel notion of probabilistic bisimulation for
quantum processes and prove that it is congruent with respect to various
process algebra combinators including parallel composition even when both
classical and quantum communications are present. We also establish some basic
algebraic laws for this bisimulation. In particular, we prove uniqueness of the
solutions to recursive equations of quantum processes, which provides a
powerful proof technique for verifying complex quantum protocols.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1607.06153v1,"Compositional Sequence Labeling Models for Error Detection in Learner
  Writing","In this paper, we present the first experiments using neural network models
for the task of error detection in learner writing. We perform a systematic
comparison of alternative compositional architectures and propose a framework
for error detection based on bidirectional LSTMs. Experiments on the CoNLL-14
shared task dataset show the model is able to outperform other participants on
detecting errors in learner writing. Finally, the model is integrated with a
publicly deployed self-assessment system, leading to performance comparable to
human annotators.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1408.3696v2,"On A Generalization of ""Eight Blocks to Madness""","We consider a puzzle such that a set of colored cubes is given as an
instance. Each cube has unit length on each edge and its surface is colored so
that what we call the Surface Color Condition is satisfied. Given a palette of
six colors, the condition requires that each face should have exactly one color
and all faces should have different colors from each other. The puzzle asks to
compose a 2x2x2 cube that satisfies the Surface Color Condition from eight
suitable cubes in the instance. Note that cubes and solutions have 30 varieties
respectively. In this paper, we give answers to three problems on the puzzle:
(i) For every subset of the 30 solutions, is there an instance that has the
subset exactly as its solution set? (ii) Create a maximum sized infeasible
instance (i.e., one having no solution). (iii) Create a minimum sized universal
instance (i.e., one having all 30 solutions). We solve the problems with the
help of a computer search. We show that the answer to (i) is no. For (ii) and
(iii), we show examples of the required instances, where their sizes are 23 and
12, respectively. The answer to (ii) solves one of the open problems that were
raised in [E.Berkove et al., ""An Analysis of the (Colored Cubes)^3 Puzzle,""
Discrete Mathematics, 308 (2008) pp. 1033-1045].",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1111.0422v1,Inclusion of Unambiguous RE#s is NP-Hard,"We show that testing inclusion between languages represented by regular
expressions with numerical occurrence indicators (RE#s) is NP-hard, even if the
expressions satisfy the requirement of ""unambiguity"", which is required for XML
Schema content model expressions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0011039v1,A Complete Characterization of Complete Intersection-Type Theories,"We characterize those intersection-type theories which yield complete
intersection-type assignment systems for lambda-calculi, with respect to the
three canonical set-theoretical semantics for intersection-types: the inference
semantics, the simple semantics and the F-semantics. These semantics arise by
taking as interpretation of types subsets of applicative structures, as
interpretation of the intersection constructor set-theoretic inclusion, and by
taking the interpretation of the arrow constructor a' la Scott, with respect to
either any possible functionality set, or the largest one, or the least one.
  These results strengthen and generalize significantly all earlier results in
the literature, to our knowledge, in at least three respects. First of all the
inference semantics had not been considered before. Secondly, the
characterizations are all given just in terms of simple closure conditions on
the preorder relation on the types, rather than on the typing judgments
themselves. The task of checking the condition is made therefore considerably
more tractable. Lastly, we do not restrict attention just to lambda-models, but
to arbitrary applicative structures which admit an interpretation function.
Thus we allow also for the treatment of models of restricted lambda-calculi.
Nevertheless the characterizations we give can be tailored just to the case of
lambda-models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1511.04750v4,"A Hierarchical Aggregation Framework for Efficient Multilevel Visual
  Exploration and Analysis","Data exploration and visualization systems are of great importance in the Big
Data era, in which the volume and heterogeneity of available information make
it difficult for humans to manually explore and analyse data. Most traditional
systems operate in an offline way, limited to accessing preprocessed (static)
sets of data. They also restrict themselves to dealing with small dataset
sizes, which can be easily handled with conventional techniques. However, the
Big Data era has realized the availability of a great amount and variety of big
datasets that are dynamic in nature; most of them offer API or query endpoints
for online access, or the data is received in a stream fashion. Therefore,
modern systems must address the challenge of on-the-fly scalable visualizations
over large dynamic sets of data, offering efficient exploration techniques, as
well as mechanisms for information abstraction and summarization. In this work,
we present a generic model for personalized multilevel exploration and analysis
over large dynamic sets of numeric and temporal data. Our model is built on top
of a lightweight tree-based structure which can be efficiently constructed
on-the-fly for a given set of data. This tree structure aggregates input
objects into a hierarchical multiscale model. Considering different exploration
scenarios over large datasets, the proposed model enables efficient multilevel
exploration, offering incremental construction and prefetching via user
interaction, and dynamic adaptation of the hierarchies based on user
preferences. A thorough theoretical analysis is presented, illustrating the
efficiency of the proposed model. The proposed model is realized in a web-based
prototype tool, called SynopsViz that offers multilevel visual exploration and
analysis over Linked Data datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.00688v2,Statistical Detection of Collective Data Fraud,"Statistical divergence is widely applied in multimedia processing, basically
due to regularity and interpretable features displayed in data. However, in a
broader range of data realm, these advantages may no longer be feasible, and
therefore a more general approach is required. In data detection, statistical
divergence can be used as a similarity measurement based on collective
features. In this paper, we present a collective detection technique based on
statistical divergence. The technique extracts distribution similarities among
data collections, and then uses the statistical divergence to detect collective
anomalies. Evaluation shows that it is applicable in the real world.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.05124v1,"Robust Active Perception via Data-association aware Belief Space
  planning","We develop a belief space planning (BSP) approach that advances the state of
the art by incorporating reasoning about data association (DA) within planning,
while considering additional sources of uncertainty. Existing BSP approaches
typically assume data association is given and perfect, an assumption that can
be harder to justify while operating, in the presence of localization
uncertainty, in ambiguous and perceptually aliased environments. In contrast,
our data association aware belief space planning (DA-BSP) approach explicitly
reasons about DA within belief evolution, and as such can better accommodate
these challenging real world scenarios. In particular, we show that due to
perceptual aliasing, the posterior belief becomes a mixture of probability
distribution functions, and design cost functions that measure the expected
level of ambiguity and posterior uncertainty. Using these and standard costs
(e.g.~control penalty, distance to goal) within the objective function, yields
a general framework that reliably represents action impact, and in particular,
capable of active disambiguation. Our approach is thus applicable to robust
active perception and autonomous navigation in perceptually aliased
environments. We demonstrate key aspects in basic and realistic simulations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1909.08230v1,Prolog Coding Guidelines: Status and Tool Support,"The importance of coding guidelines is generally accepted throughout
developers of every programming language. Naturally, Prolog makes no exception.
However, establishing coding guidelines is fraught with obstacles: Finding
common ground on kind and selection of rules is matter of debate; once found,
adhering to or enforcing rules is complicated as well, not least because of
Prolog's flexible syntax without keywords. In this paper, we evaluate the
status of coding guidelines in the Prolog community and discuss to what extent
they can be automatically verified. We implemented a linter for Prolog and
applied it to several packages to get a hold on the current state of the
community.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.14431v2,Fairness-Aware PageRank,"Algorithmic fairness has attracted significant attention in the past years.
Surprisingly, there is little work on fairness in networks. In this work, we
consider fairness for link analysis algorithms and in particular for the
celebrated PageRank algorithm. We provide definitions for fairness, and propose
two approaches for achieving fairness. The first modifies the jump vector of
the Pagerank algorithm to enfonce fairness, and the second imposes a fair
behavior per node. We also consider the problem of achieving fairness while
minimizing the utility loss with respect to the original algorithm. We present
experiments with real and synthetic graphs that examine the fairness of
Pagerank and demonstrate qualitatively and quantitatively the properties of our
algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1708.08765v1,Freeing Testers from Polluting Test Objectives,"Testing is the primary approach for detecting software defects. A major
challenge faced by testers lies in crafting efficient test suites, able to
detect a maximum number of bugs with manageable effort. To do so, they rely on
coverage criteria, which define some precise test objectives to be covered.
However, many common criteria specify a significant number of objectives that
occur to be infeasible or redundant in practice, like covering dead code or
semantically equal mutants. Such objectives are well-known to be harmful to the
design of test suites, impacting both the efficiency and precision of testers'
effort. This work introduces a sound and scalable formal technique able to
prune out a significant part of the infeasible and redundant objectives
produced by a large panel of white-box criteria. In a nutshell, we reduce this
challenging problem to proving the validity of logical assertions in the code
under test. This technique is implemented in a tool that relies on
weakest-precondition calculus and SMT solving for proving the assertions. The
tool is built on top of the Frama-C verification platform, which we carefully
tune for our specific scalability needs. The experiments reveal that the tool
can prune out up to 27% of test objectives in a program and scale to
applications of 200K lines of code.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1807.07067v1,"A Fixed-Parameter Linear-Time Algorithm to Compute Principal Typings of
  Planar Flow Networks","We present an alternative and simpler method for computing principal typings
of flow networks. When limited to planar flow networks, the method can be made
to run in fixed-parameter linear-time -- where the parameter not to be exceeded
is what is called the edge-outerplanarity of the networks' underlying graphs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1909.00021v2,"Approximating Stacked and Bidirectional Recurrent Architectures with the
  Delayed Recurrent Neural Network","Recent work has shown that topological enhancements to recurrent neural
networks (RNNs) can increase their expressiveness and representational
capacity. Two popular enhancements are stacked RNNs, which increases the
capacity for learning non-linear functions, and bidirectional processing, which
exploits acausal information in a sequence. In this work, we explore the
delayed-RNN, which is a single-layer RNN that has a delay between the input and
output. We prove that a weight-constrained version of the delayed-RNN is
equivalent to a stacked-RNN. We also show that the delay gives rise to partial
acausality, much like bidirectional networks. Synthetic experiments confirm
that the delayed-RNN can mimic bidirectional networks, solving some acausal
tasks similarly, and outperforming them in others. Moreover, we show similar
performance to bidirectional networks in a real-world natural language
processing task. These results suggest that delayed-RNNs can approximate
topologies including stacked RNNs, bidirectional RNNs, and stacked
bidirectional RNNs - but with equivalent or faster runtimes for the
delayed-RNNs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1707.06628v2,On the covering radius of small codes versus dual distance,"Tiet\""{a}v\""{a}inen's upper and lower bounds assert that for block-length-$n$
linear codes with dual distance $d$, the covering radius $R$ is at most
$\frac{n}{2}-(\frac{1}{2}-o(1))\sqrt{dn}$ and typically at least
$\frac{n}{2}-\Theta(\sqrt{dn\log{\frac{n}{d}}})$. The gap between those bounds
on $R -\frac{n}{2}$ is an $\Theta(\sqrt{\log{\frac{n}{d}}})$ factor related to
the gap between the worst covering radius given $d$ and the sphere-covering
bound. Our focus in this paper is on the case when $d = o(n)$, i.e., when the
code size is subexponential and the gap is $w(1)$. We show that up to a
constant, the gap can be eliminated by relaxing the covering requirement to
allow for missing $o(1)$ fraction of points. Namely, if the dual distance $d =
o(n)$, then for sufficiently large $d$, almost all points can be covered with
radius $R\leq\frac{n}{2}-\Theta(\sqrt{dn\log{\frac{n}{d}}})$. Compared to
random linear codes, our bound on $R-\frac{n}{2}$ is asymptotically tight up to
a factor less than $3$. We give applications to dual BCH codes. The proof
builds on the author's previous work on the weight distribution of cosets of
linear codes, which we simplify in this paper and extend from codes to
probability distributions on $\{0,1\}^n$, thus enabling the extension of the
above result to $(d-1)$-wise independent distributions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1801.09973v2,Social Event Scheduling,"A major challenge for social event organizers (e.g., event planning and
marketing companies, venues) is attracting the maximum number of participants,
since it has great impact on the success of the event, and, consequently, the
expected gains (e.g., revenue, artist/brand publicity). In this paper, we
introduce the Social Event Scheduling (SES) problem, which schedules a set of
social events considering user preferences and behavior, events' spatiotemporal
conflicts, and competing vents, in order to maximize the overall number of
attendees. We show that SES is strongly NP-hard, even in highly restricted
instances. To cope with the hardness of the SES problem we design a greedy
approximation algorithm. Finally, we evaluate our method experimentally using a
dataset from the Meetup event-based social network.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1311.3808v1,"Periodicity Extraction using Superposition of Distance Matching Function
  and One-dimensional Haar Wavelet Transform","Periodicity of a texture is one of the important visual characteristics and
is often used as a measure for textural discrimination at the structural level.
Knowledge about periodicity of a texture is very essential in the field of
texture synthesis and texture compression and also in the design of frieze and
wall papers. In this paper, we propose a method of periodicity extraction from
noisy images based on superposition of distance matching function (DMF) and
wavelet decomposition without de-noising the test images. Overall DMFs are
subjected to single-level Haar wavelet decomposition to obtain approximate and
detailed coefficients. Extracted coefficients help in determination of
periodicities in row and column directions. We illustrate the usefulness and
the effectiveness of the proposed method in a texture synthesis application.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1711.08715v1,"Interpolating between $k$-Median and $k$-Center: Approximation
  Algorithms for Ordered $k$-Median","We consider a generalization of $k$-median and $k$-center, called the {\em
ordered $k$-median} problem. In this problem, we are given a metric space
$(\mathcal{D},\{c_{ij}\})$ with $n=|\mathcal{D}|$ points, and a non-increasing
weight vector $w\in\mathbb{R}_+^n$, and the goal is to open $k$ centers and
assign each point each point $j\in\mathcal{D}$ to a center so as to minimize
$w_1\cdot\text{(largest assignment cost)}+w_2\cdot\text{(second-largest
assignment cost)}+\ldots+w_n\cdot\text{($n$-th largest assignment cost)}$. We
give an $(18+\epsilon)$-approximation algorithm for this problem. Our
algorithms utilize Lagrangian relaxation and the primal-dual schema, combined
with an enumeration procedure of Aouad and Segev. For the special case of
$\{0,1\}$-weights, which models the problem of minimizing the $\ell$ largest
assignment costs that is interesting in and of by itself, we provide a novel
reduction to the (standard) $k$-median problem showing that LP-relative
guarantees for $k$-median translate to guarantees for the ordered $k$-median
problem; this yields a nice and clean $(8.5+\epsilon)$-approximation algorithm
for $\{0,1\}$ weights.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.05300v1,Representing 3D Shapes with Probabilistic Directed Distance Fields,"Differentiable rendering is an essential operation in modern vision, allowing
inverse graphics approaches to 3D understanding to be utilized in modern
machine learning frameworks. Explicit shape representations (voxels, point
clouds, or meshes), while relatively easily rendered, often suffer from limited
geometric fidelity or topological constraints. On the other hand, implicit
representations (occupancy, distance, or radiance fields) preserve greater
fidelity, but suffer from complex or inefficient rendering processes, limiting
scalability. In this work, we endeavour to address both shortcomings with a
novel shape representation that allows fast differentiable rendering within an
implicit architecture. Building on implicit distance representations, we define
Directed Distance Fields (DDFs), which map an oriented point (position and
direction) to surface visibility and depth. Such a field can render a depth map
with a single forward pass per pixel, enable differential surface geometry
extraction (e.g., surface normals and curvatures) via network derivatives, be
easily composed, and permit extraction of classical unsigned distance fields.
Using probabilistic DDFs (PDDFs), we show how to model inherent discontinuities
in the underlying field. Finally, we apply our method to fitting single shapes,
unpaired 3D-aware generative image modelling, and single-image 3D
reconstruction tasks, showcasing strong performance with simple architectural
components via the versatility of our representation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1211.1565v1,Data Shapes and Data Transformations,"Nowadays, information management systems deal with data originating from
different sources including relational databases, NoSQL data stores, and Web
data formats, varying not only in terms of data formats, but also in the
underlying data model. Integrating data from heterogeneous data sources is a
time-consuming and error-prone engineering task; part of this process requires
that the data has to be transformed from its original form to other forms,
repeating all along the life cycle. With this report we provide a principled
overview on the fundamental data shapes tabular, tree, and graph as well as
transformations between them, in order to gain a better understanding for
performing said transformations more efficiently and effectively.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.13219v1,Towards a Formal Framework for Partial Compliance of Business Processes,"Binary ""YES-NO"" notions of process compliance are not very helpful to
managers for assessing the operational performance of their company because a
large number of cases fall in the grey area of partial compliance. Hence, it is
necessary to have ways to quantify partial compliance in terms of metrics and
be able to classify actual cases by assigning a numeric value of compliance to
them. In this paper, we formulate an evaluation framework to quantify the level
of compliance of business processes across different levels of abstraction
(such as task,trace and process level) and across multiple dimensions of each
task (such as temporal, monetary, role-, data-, and quality-related) to provide
managers more useful information about their operations and to help them
improve their decision making processes. Our approach can also add social value
by making social services provided by local, state and federal governments more
flexible and improving the lives of citizens.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0709.2961v1,Incremental Satisfiability and Implication for UTVPI Constraints,"Unit two-variable-per-inequality (UTVPI) constraints form one of the largest
class of integer constraints which are polynomial time solvable (unless P=NP).
There is considerable interest in their use for constraint solving, abstract
interpretation, spatial databases, and theorem proving. In this paper we
develop a new incremental algorithm for UTVPI constraint satisfaction and
implication checking that requires O(m + n log n + p) time and O(n+m+p) space
to incrementally check satisfiability of m UTVPI constraints on n variables and
check implication of p UTVPI constraints.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.09162v1,A New Validity Index for Fuzzy-Possibilistic C-Means Clustering,"In some complicated datasets, due to the presence of noisy data points and
outliers, cluster validity indices can give conflicting results in determining
the optimal number of clusters. This paper presents a new validity index for
fuzzy-possibilistic c-means clustering called Fuzzy-Possibilistic (FP) index,
which works well in the presence of clusters that vary in shape and density.
Moreover, FPCM like most of the clustering algorithms is susceptible to some
initial parameters. In this regard, in addition to the number of clusters, FPCM
requires a priori selection of the degree of fuzziness and the degree of
typicality. Therefore, we presented an efficient procedure for determining
their optimal values. The proposed approach has been evaluated using several
synthetic and real-world datasets. Final computational results demonstrate the
capabilities and reliability of the proposed approach compared with several
well-known fuzzy validity indices in the literature. Furthermore, to clarify
the ability of the proposed method in real applications, the proposed method is
implemented in microarray gene expression data clustering and medical image
segmentation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1711.09852v2,"Pricing Derivatives under Multiple Stochastic Factors by Localized
  Radial Basis Function Methods","We propose two localized Radial Basis Function (RBF) methods, the Radial
Basis Function Partition of Unity method (RBF-PUM) and the Radial Basis
Function generated Finite Differences method (RBF-FD), for solving financial
derivative pricing problems arising from market models with multiple stochastic
factors. We demonstrate the useful features of the proposed methods, such as
high accuracy, sparsity of the differentiation matrices, mesh-free nature and
multi-dimensional extendability, and show how to apply these methods for
solving time-dependent higher-dimensional PDEs in finance. We test these
methods on several problems that incorporate stochastic asset, volatility, and
interest rate dynamics by conducting numerical experiments. The results
illustrate the capability of both methods to solve the problems to a sufficient
accuracy within reasonable time. Both methods exhibit similar orders of
convergence, which can be further improved by a more elaborate choice of the
method parameters. Finally, we discuss the parallelization potentials of the
proposed methods and report the speedup on the example of RBF-FD.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1703.08982v3,Querying Log Data with Metric Temporal Logic (Technical Report),"We propose a novel framework for ontology-based access to temporal log data
using a datalog extension datalogMTL of a Horn fragment of the metric temporal
logic MTL. We show that datalogMTL is ExpSpace-complete even with punctual
intervals, in which case full MTL is known to be undecidable. We also prove
that nonrecursive datalogMTL is PSpace-complete for combined complexity and in
AC0 for data complexity. We demonstrate by two real-world use cases that
nonrecursive datalogMTL programs can express complex temporal concepts from
typical user queries and thereby facilitate access to temporal log data. Our
experiments with Siemens turbine data and MesoWest weather data show that
datalogMTL ontology-mediated queries are efficient and scale on large datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0009027v1,A Classification Approach to Word Prediction,"The eventual goal of a language model is to accurately predict the value of a
missing word given its context. We present an approach to word prediction that
is based on learning a representation for each word as a function of words and
linguistics predicates in its context. This approach raises a few new questions
that we address. First, in order to learn good word representations it is
necessary to use an expressive representation of the context. We present a way
that uses external knowledge to generate expressive context representations,
along with a learning method capable of handling the large number of features
generated this way that can, potentially, contribute to each prediction.
Second, since the number of words ``competing'' for each prediction is large,
there is a need to ``focus the attention'' on a smaller subset of these. We
exhibit the contribution of a ``focus of attention'' mechanism to the
performance of the word predictor. Finally, we describe a large scale
experimental study in which the approach presented is shown to yield
significant improvements in word prediction tasks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.12294v3,Topic Space Trajectories: A case study on machine learning literature,"The annual number of publications at scientific venues, for example,
conferences and journals, is growing quickly. Hence, even for researchers it
becomes harder and harder to keep track of research topics and their progress.
In this task, researchers can be supported by automated publication analysis.
Yet, many such methods result in uninterpretable, purely numerical
representations. As an attempt to support human analysts, we present topic
space trajectories, a structure that allows for the comprehensible tracking of
research topics. We demonstrate how these trajectories can be interpreted based
on eight different analysis approaches. To obtain comprehensible results, we
employ non-negative matrix factorization as well as suitable visualization
techniques. We show the applicability of our approach on a publication corpus
spanning 50 years of machine learning research from 32 publication venues. Our
novel analysis method may be employed for paper classification, for the
prediction of future research topics, and for the recommendation of fitting
conferences and journals for submitting unpublished work.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1809.07131v3,"Twisty Takens: A Geometric Characterization of Good Observations on
  Dense Trajectories","In nonlinear time series analysis and dynamical systems theory, Takens'
embedding theorem states that the sliding window embedding of a generic
observation along trajectories in a state space, recovers the region traversed
by the dynamics. This can be used, for instance, to show that sliding window
embeddings of periodic signals recover topological loops, and that sliding
window embeddings of quasiperiodic signals recover high-dimensional torii.
However, in spite of these motivating examples, Takens' theorem does not in
general prescribe how to choose such an observation function given particular
dynamics in a state space. In this work, we state conditions on observation
functions defined on compact Riemannian manifolds, that lead to successful
reconstructions for particular dynamics. We apply our theory and construct
families of time series whose sliding window embeddings trace tori, Klein
bottles, spheres, and projective planes. This greatly enriches the set of
examples of time series known to concentrate on various shapes via sliding
window embeddings, and will hopefully help other researchers in identifying
them in naturally occurring phenomena. We also present numerical experiments
showing how to recover low dimensional representations of the underlying
dynamics on state space, by using the persistent cohomology of sliding window
embeddings and Eilenberg-MacLane (i.e., circular and real projective)
coordinates.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1309.6202v1,Sentiment Analysis in the News,"Recent years have brought a significant growth in the volume of research in
sentiment analysis, mostly on highly subjective text types (movie or product
reviews). The main difference these texts have with news articles is that their
target is clearly defined and unique across the text. Following different
annotation efforts and the analysis of the issues encountered, we realised that
news opinion mining is different from that of other text types. We identified
three subtasks that need to be addressed: definition of the target; separation
of the good and bad news content from the good and bad sentiment expressed on
the target; and analysis of clearly marked opinion that is expressed
explicitly, not needing interpretation or the use of world knowledge.
Furthermore, we distinguish three different possible views on newspaper
articles - author, reader and text, which have to be addressed differently at
the time of analysing sentiment. Given these definitions, we present work on
mining opinions about entities in English language news, in which (a) we test
the relative suitability of various sentiment dictionaries and (b) we attempt
to separate positive or negative opinion from good or bad news. In the
experiments described here, we tested whether or not subject domain-defining
vocabulary should be ignored. Results showed that this idea is more appropriate
in the context of news opinion mining and that the approaches taking this into
consideration produce a better performance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1608.06559v1,Improving FPGA resilience through Partial Dynamic Reconfiguration,"This paper explores advances in reconfiguration properties of SRAM-based
FPGAs, namely Partial Dynamic Reconfiguration, to improve the resilience of
critical systems that take advantage of this technology. Commercial
of-the-shelf state-of-the-art FPGA devices use SRAM cells for the configuration
memory, which allow an increase in both performance and capacity. The fast
access times and unlimited number of writes of this technology, reduces
reconfiguration delays and extends the device lifetime but, at the same time,
makes them more sensitive to radiation effects, in the form of Single Event
Upsets. To overcome this limitation, manufacturers have proposed a few fault
tolerant approaches, which rely on space/time redundancy and configuration
memory content recovery - scrubbing. In this paper, we first present radiation
effects on these devices and investigate the applicability of the most commonly
used fault tolerant approaches, and then propose an approach to improve FPGA
resilience, through the use of a less intrusive failure prediction scrubbing.
It is expected that this approach relieves the system designer from
dependability concerns and reduces both time intrusiveness and overall power
consumption.",0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0805.1741v1,A Spreadsheet Auditing Tool Evaluated in an Industrial Context,"Amongst the large number of write-and-throw-away spreadsheets developed for
one-time use there is a rather neglected proportion of spreadsheets that are
huge, periodically used, and submitted to regular update-cycles like any
conventionally evolving valuable legacy application software. However, due to
the very nature of spreadsheets, their evolution is particularly tricky and
therefore error-prone. In our strive to develop tools and methodologies to
improve spreadsheet quality, we analysed consolidation spreadsheets of an
internationally operating company for the errors they contain. The paper
presents the results of the field audit, involving 78 spreadsheets with 60,446
non-empty cells. As a by-product, the study performed was also to validate our
analysis tools in an industrial context. The evaluated auditing tool offers the
auditor a new view on the formula structure of the spreadsheet by grouping
similar formulas into equivalence classes. Our auditing approach defines three
similarity criteria between formulae, namely copy, logical and structural
equivalence. To improve the visualization of large spreadsheets, equivalences
and data dependencies are displayed in separated windows that are interlinked
with the spreadsheet. The auditing approach helps to find irregularities in the
geometrical pattern of similar formulas.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1
http://arxiv.org/abs/1801.07388v1,Let's Dance: Learning From Online Dance Videos,"In recent years, deep neural network approaches have naturally extended to
the video domain, in their simplest case by aggregating per-frame
classifications as a baseline for action recognition. A majority of the work in
this area extends from the imaging domain, leading to visual-feature heavy
approaches on temporal data. To address this issue we introduce ""Let's Dance"",
a 1000 video dataset (and growing) comprised of 10 visually overlapping dance
categories that require motion for their classification. We stress the
important of human motion as a key distinguisher in our work given that, as we
show in this work, visual information is not sufficient to classify
motion-heavy categories. We compare our datasets' performance using imaging
techniques with UCF-101 and demonstrate this inherent difficulty. We present a
comparison of numerous state-of-the-art techniques on our dataset using three
different representations (video, optical flow and multi-person pose data) in
order to analyze these approaches. We discuss the motion parameterization of
each of them and their value in learning to categorize online dance videos.
Lastly, we release this dataset (and its three representations) for the
research community to use.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1905.04684v1,Lack of Unique Factorization as a Tool in Block Cipher Cryptanalysis,"Linear (or differential) cryptanalysis may seem dull topics for a
mathematician: they are about super simple invariants characterized by say a
word on n=64 bits with very few bits at 1, the space of possible attacks is
small, and basic principles are trivial. In contract mathematics offers an
infinitely rich world of possibilities. If so, why is that cryptographers have
ever found so few attacks on block ciphers? In this paper we argue that
black-box methods used so far to find attacks in symmetric cryptography are
inadequate and we work with a more recent white-box algebraic methodology.
Invariant attacks can be constructed explicitly through the study of roots of
the so-called Fundamental Equation (FE). We also argue that certain properties
of the ring of Boolean polynomials such as lack of unique factorization allow
for a certain type of product construction attacks to flourish. As a proof of
concept we show how to construct a complex and non-trivial attack where a
polynomial of degree 7 is an invariant for any number of rounds for a complex
block cipher.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
http://arxiv.org/abs/cs/0309039v1,Two novel evolutionary formulations of the graph coloring problem,"We introduce two novel evolutionary formulations of the problem of coloring
the nodes of a graph. The first formulation is based on the relationship that
exists between a graph's chromatic number and its acyclic orientations. It
views such orientations as individuals and evolves them with the aid of
evolutionary operators that are very heavily based on the structure of the
graph and its acyclic orientations. The second formulation, unlike the first
one, does not tackle one graph at a time, but rather aims at evolving a
`program' to color all graphs belonging to a class whose members all have the
same number of nodes and other common attributes. The heuristics that result
from these formulations have been tested on some of the Second DIMACS
Implementation Challenge benchmark graphs, and have been found to be
competitive when compared to the several other heuristics that have also been
tested on those graphs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1612.02495v2,"An initial investigation of the performance of GPU-based swept
  time-space decomposition","Simulations of physical phenomena are essential to the expedient design of
precision components in aerospace and other high-tech industries. These
phenomena are often described by mathematical models involving partial
differential equations (PDEs) without exact solutions. Modern design problems
require simulations with a level of resolution that is difficult to achieve in
a reasonable amount of time even in effectively parallelized solvers. Though
the scale of the problem relative to available computing power is the greatest
impediment to accelerating these applications, significant performance gains
can be achieved through careful attention to the details of memory accesses.
Parallelized PDE solvers are subject to a trade-off in memory management: store
the solution for each timestep in abundant, global memory with high access
costs or in a limited, private memory with low access costs that must be passed
between nodes. The GPU implementation of swept time-space decomposition
presented here mitigates this dilemma by using private (shared) memory,
avoiding internode communication, and overwriting unnecessary values. It shows
significant improvement in the execution time of the PDE solvers in one
dimension achieving speedups of 6-2x for large and small problem sizes
respectively compared to naive GPU versions and 7-300x compared to parallel CPU
versions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.02386v1,Augmented Reality for Education: A Review,"Augmented Reality, or simply AR, is the incorporation of information in
digital format that includes live footage of a certain user's real-time
environment. Also now, various universities are using Augmented Reality.
Applying the technology in the education sector can result in having a smart
campus. In line with that, this paper will discuss how Augmented Reality is
being used now in different learning areas.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.14086v3,Planar and Toroidal Morphs Made Easier,"We present simpler algorithms for two closely related morphing problems, both
based on the barycentric interpolation paradigm introduced by Floater and
Gotsman, which is in turn based on Floater's asymmetric extension of Tutte's
classical spring-embedding theorem. First, we give a much simpler algorithm to
construct piecewise-linear morphs between planar straight-line graphs.
Specifically, given isomorphic straight-line drawings $\Gamma_0$ and $\Gamma_1$
of the same 3-connected planar graph $G$, with the same convex outer face, we
construct a morph from $\Gamma_0$ to $\Gamma_1$ that consists of $O(n)$
unidirectional morphing steps, in $O(n^{1+\omega/2})$ time. Our algorithm
entirely avoids the classical edge-collapsing strategy dating back to Cairns;
instead, in each morphing step, we interpolate the pair of weights associated
with a single edge. Second, we describe a natural extension of barycentric
interpolation to geodesic graphs on the flat torus. Barycentric interpolation
cannot be applied directly in this setting, because the linear systems defining
intermediate vertex positions are not necessarily solvable. We describe a
simple scaling strategy that circumvents this issue. Computing the appropriate
scaling requires $O(n^{\omega/2})$ time, after which we can can compute the
drawing at any point in the morph in $O(n^{\omega/2})$ time. Our algorithm is
considerably simpler than the recent algorithm of Chambers et al.
(arXiv:2007.07927) and produces more natural morphs. Our techniques also yield
a simple proof of a conjecture of Connelly et al. for geodesic torus
triangulations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1512.05671v1,Characterizing the Demographics Behind the #BlackLivesMatter Movement,"The debates on minority issues are often dominated by or held among the
concerned minority: gender equality debates have often failed to engage men,
while those about race fail to effectively engage the dominant group. To test
this observation, we study the #BlackLivesMatter}movement and hashtag on
Twitter--which has emerged and gained traction after a series of events
typically involving the death of African-Americans as a result of police
brutality--and aim to quantify the population biases across user types
(individuals vs. organizations), and (for individuals) across various
demographics factors (race, gender and age). Our results suggest that more
African-Americans engage with the hashtag, and that they are also more active
than other demographic groups. We also discuss ethical caveats with broader
implications for studies on sensitive topics (e.g. discrimination, mental
health, or religion) that focus on users.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1410.1080v1,Generating abbreviations using Google Books library,"The article describes the original method of creating a dictionary of
abbreviations based on the Google Books Ngram Corpus. The dictionary of
abbreviations is designed for Russian, yet as its methodology is universal it
can be applied to any language. The dictionary can be used to define the
function of the period during text segmentation in various applied systems of
text processing. The article describes difficulties encountered in the process
of its construction as well as the ways to overcome them. A model of evaluating
a probability of first and second type errors (extraction accuracy and
fullness) is constructed. Certain statistical data for the use of abbreviations
are provided.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1002.4907v1,Twenty Questions Games Always End With Yes,"Huffman coding is often presented as the optimal solution to Twenty
Questions. However, a caveat is that Twenty Questions games always end with a
reply of ""Yes,"" whereas Huffman codewords need not obey this constraint. We
bring resolution to this issue, and prove that the average number of questions
still lies between H(X) and H(X)+1.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.14609v2,"An Investigation on Learning, Polluting, and Unlearning the Spam Emails
  for Lifelong Learning","Machine unlearning for security is studied in this context. Several spam
email detection methods exist, each of which employs a different algorithm to
detect undesired spam emails. But these models are vulnerable to attacks. Many
attackers exploit the model by polluting the data, which are trained to the
model in various ways. So to act deftly in such situations model needs to
readily unlearn the polluted data without the need for retraining. Retraining
is impractical in most cases as there is already a massive amount of data
trained to the model in the past, which needs to be trained again just for
removing a small amount of polluted data, which is often significantly less
than 1%. This problem can be solved by developing unlearning frameworks for all
spam detection models. In this research, unlearning module is integrated into
spam detection models that are based on Naive Bayes, Decision trees, and Random
Forests algorithms. To assess the benefits of unlearning over retraining, three
spam detection models are polluted and exploited by taking attackers' positions
and proving models' vulnerability. Reduction in accuracy and true positive
rates are shown in each case showing the effect of pollution on models. Then
unlearning modules are integrated into the models, and polluted data is
unlearned; on testing the models after unlearning, restoration of performance
is seen. Also, unlearning and retraining times are compared with different
pollution data sizes on all models. On analyzing the findings, it can be
concluded that unlearning is considerably superior to retraining. Results show
that unlearning is fast, easy to implement, easy to use, and effective.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.14731v1,"LeSICiN: A Heterogeneous Graph-based Approach for Automatic Legal
  Statute Identification from Indian Legal Documents","The task of Legal Statute Identification (LSI) aims to identify the legal
statutes that are relevant to a given description of Facts or evidence of a
legal case. Existing methods only utilize the textual content of Facts and
legal articles to guide such a task. However, the citation network among case
documents and legal statutes is a rich source of additional information, which
is not considered by existing models. In this work, we take the first step
towards utilising both the text and the legal citation network for the LSI
task. We curate a large novel dataset for this task, including Facts of cases
from several major Indian Courts of Law, and statutes from the Indian Penal
Code (IPC). Modeling the statutes and training documents as a heterogeneous
graph, our proposed model LeSICiN can learn rich textual and graphical
features, and can also tune itself to correlate these features. Thereafter, the
model can be used to inductively predict links between test documents (new
nodes whose graphical features are not available to the model) and statutes
(existing nodes). Extensive experiments on the dataset show that our model
comfortably outperforms several state-of-the-art baselines, by exploiting the
graphical structure along with textual features. The dataset and our codes are
available at https://github.com/Law-AI/LeSICiN.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0811.3712v1,Performance Modeling and Evaluation for Information-Driven Networks,"Information-driven networks include a large category of networking systems,
where network nodes are aware of information delivered and thus can not only
forward data packets but may also perform information processing. In many
situations, the quality of service (QoS) in information-driven networks is
provisioned with the redundancy in information. Traditional performance models
generally adopt evaluation measures suitable for packet-oriented service
guarantee, such as packet delay, throughput, and packet loss rate. These
performance measures, however, do not align well with the actual need of
information-driven networks. New performance measures and models for
information-driven networks, despite their importance, have been mainly blank,
largely because information processing is clearly application dependent and
cannot be easily captured within a generic framework. To fill the vacancy, we
present a new performance evaluation framework particularly tailored for
information-driven networks, based on the recent development of stochastic
network calculus. We analyze the QoS with respect to information delivery and
study the scheduling problem with the new performance metrics. Our analytical
framework can be used to calculate the network capacity in information delivery
and in the meantime to help transmission scheduling for a large body of systems
where QoS is stochastically guaranteed with the redundancy in information.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.03845v1,"StripBrush: A Constraint-Relaxed 3D Brush Reduces Physical Effort and
  Enhances the Quality of Spatial Drawing","Spatial drawing using ruled-surface brush strokes is a popular mode of
content creation in immersive VR, yet little is known about the usability of
existing spatial drawing interfaces or potential improvements. We address these
questions in a three-phase study. (1) Our exploratory need-finding study (N=8)
indicates that popular spatial brushes require users to perform large wrist
motions, causing physical strain. We speculate that this is partly due to
constraining users to align their 3D controllers with their intended stroke
normal orientation. (2) We designed and implemented a new brush interface that
significantly reduces the physical effort and wrist motion involved in VR
drawing, with the additional benefit of increasing drawing accuracy. We achieve
this by relaxing the normal alignment constraints, allowing users to control
stroke rulings, and estimating normals from them instead. (3) Our comparative
evaluation of StripBrush (N=17) against the traditional brush shows that
StripBrush requires significantly less physical effort and allows users to more
accurately depict their intended shapes while offering competitive ease-of-use
and speed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1701.08128v1,"Evaluating a sublinear-time algorithm for the Minimum Spanning Tree
  Weight problem","We present an implementation and an experimental evaluation of an algorithm
that, given a connected graph G (represented by adjacency lists), estimates in
sublinear time, with a relative error, the Minimum Spanning Tree Weight of G;
the original algorithm has been presented in ""Approximating the minimum
spanning tree weight in sublinear time"", by Bernard Chazelle, Ronitt Rubinfeld,
and Luca Trevisan (published with SIAM, DOI 10.1137/S0097539702403244). Since
the theoretical performances have already been shown and demonstrated in the
above-mentioned paper, our goal is, exclusively, to experimental evaluate the
algorithm and at last to present the results. Initially we discuss about some
theoretical aspects that arose while we were valuating the asymptotic
complexity of our specific implementation. Some technical insights are then
given on the implementation of the algorithm and on the dataset used in the
test phase, hence to show how the experiment has been carried out even for
reproducibility purposes; the results are then evaluated empirically and widely
discussed, comparing these with the performances of the Prim algorithm and the
Kruskal algorithm, launching several runs on a heterogeneous set of graphs and
different theoretical models for them.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0609111v2,"A State-Based Regression Formulation for Domains with Sensing
  Actions<br> and Incomplete Information","We present a state-based regression function for planning domains where an
agent does not have complete information and may have sensing actions. We
consider binary domains and employ a three-valued characterization of domains
with sensing actions to define the regression function. We prove the soundness
and completeness of our regression formulation with respect to the definition
of progression. More specifically, we show that (i) a plan obtained through
regression for a planning problem is indeed a progression solution of that
planning problem, and that (ii) for each plan found through progression, using
regression one obtains that plan or an equivalent one.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0405079v1,Higher-Order Concurrent Win32 Programming,"We present a concurrent framework for Win32 programming based on Concurrent
ML, a concurrent language with higher-order functions, static typing,
lightweight threads and synchronous communication channels. The key points of
the framework are the move from an event loop model to a threaded model for the
processing of window messages, and the decoupling of controls notifications
from the system messages. This last point allows us to derive a general way of
writing controls that leads to easy composition, and can accommodate ActiveX
Controls in a transparent way.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.07333v1,Reinforcement Learning in a Physics-Inspired Semi-Markov Environment,"Reinforcement learning (RL) has been demonstrated to have great potential in
many applications of scientific discovery and design. Recent work includes, for
example, the design of new structures and compositions of molecules for
therapeutic drugs. Much of the existing work related to the application of RL
to scientific domains, however, assumes that the available state representation
obeys the Markov property. For reasons associated with time, cost, sensor
accuracy, and gaps in scientific knowledge, many scientific design and
discovery problems do not satisfy the Markov property. Thus, something other
than a Markov decision process (MDP) should be used to plan / find the optimal
policy. In this paper, we present a physics-inspired semi-Markov RL
environment, namely the phase change environment. In addition, we evaluate the
performance of value-based RL algorithms for both MDPs and partially observable
MDPs (POMDPs) on the proposed environment. Our results demonstrate deep
recurrent Q-networks (DRQN) significantly outperform deep Q-networks (DQN), and
that DRQNs benefit from training with hindsight experience replay. Implications
for the use of semi-Markovian RL and POMDPs for scientific laboratories are
also discussed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1207.4265v1,"Spot: An accurate and efficient multi-entity device-free WLAN
  localization system","Device-free (DF) localization in WLANs has been introduced as a value-added
service that allows tracking indoor entities that do not carry any devices.
Previous work in DF WLAN localization focused on the tracking of a single
entity due to the intractability of the multi-entity tracking problem whose
complexity grows exponentially with the number of humans being tracked. In this
paper, we introduce Spot as an accurate and efficient system for multi-entity
DF detection and tracking. Spot is based on a probabilistic energy minimization
framework that combines a conditional random field with a Markov model to
capture the temporal and spatial relations between the entities' poses. A novel
cross-calibration technique is introduced to reduce the calibration overhead of
multiple entities to linear, regardless of the number of humans being tracked.
This also helps in increasing the system accuracy. We design the energy
minimization function with the goal of being efficiently solved in mind. We
show that the designed function can be mapped to a binary graph-cut problem
whose solution has a linear complexity on average and a third order polynomial
in the worst case. We further employ clustering on the estimated location
candidates to reduce outliers and obtain more accurate tracking. Experimental
evaluation in two typical testbeds, with a side-by-side comparison with the
state-of-the-art, shows that Spot can achieve a multi-entity tracking accuracy
of less than 1.1m. This corresponds to at least 36% enhancement in median
distance error over the state-of-the-art DF localization systems, which can
only track a single entity. In addition, Spot can estimate the number of
entities correctly to within one difference error. This highlights that Spot
achieves its goals of having an accurate and efficient software-only DF
tracking solution of multiple entities in indoor environments.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0206019v1,Simultaneous Embedding of a Planar Graph and Its Dual on the Grid,"Traditional representations of graphs and their duals suggest the requirement
that the dual vertices be placed inside their corresponding primal faces, and
the edges of the dual graph cross only their corresponding primal edges. We
consider the problem of simultaneously embedding a planar graph and its dual
into a small integer grid such that the edges are drawn as straight-line
segments and the only crossings are between primal-dual pairs of edges. We
provide a linear-time algorithm that simultaneously embeds a 3-connected planar
graph and its dual on a (2n-2) by (2n-2) integer grid, where n is the total
number of vertices in the graph and its dual. Furthermore our embedding
algorithm satisfies the two natural requirements mentioned above.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1504.00442v8,Refuting Unique Game Conjecture,"In this short note, the author shows that the gap problem of some $k$-CSPs
with the support of its predicate the ground of a balanced pairwise independent
distribution can be solved by a modified version of Hast's Algorithm BiLin that
calls Charikar\&Wirth's SDP algorithm for two rounds in polynomial time, when
$k$ is sufficiently large, the support of its predicate is combined by the
grounds of three biased homogeneous distributions and the three biases satisfy
certain conditions. To conclude, the author refutes Unique Game Conjecture,
assuming $P\ne NP$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1311.2702v1,Verifiable Source Code Documentation in Controlled Natural Language,"Writing documentation about software internals is rarely considered a
rewarding activity. It is highly time-consuming and the resulting documentation
is fragile when the software is continuously evolving in a multi-developer
setting. Unfortunately, traditional programming environments poorly support the
writing and maintenance of documentation. Consequences are severe as the lack
of documentation on software structure negatively impacts the overall quality
of the software product. We show that using a controlled natural language with
a reasoner and a query engine is a viable technique for verifying the
consistency and accuracy of documentation and source code. Using ACE, a
state-of-the-art controlled natural language, we present positive results on
the comprehensibility and the general feasibility of creating and verifying
documentation. As a case study, we used automatic documentation verification to
identify and fix severe flaws in the architecture of a non-trivial piece of
software. Moreover, a user experiment shows that our language is faster and
easier to learn and understand than other formal languages for software
documentation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.07748v2,"Variational Inference for Category Recommendation in E-Commerce
  platforms","Category recommendation for users on an e-Commerce platform is an important
task as it dictates the flow of traffic through the website. It is therefore
important to surface precise and diverse category recommendations to aid the
users' journey through the platform and to help them discover new groups of
items. An often understated part in category recommendation is users'
proclivity to repeat purchases. The structure of this temporal behavior can be
harvested for better category recommendations and in this work, we attempt to
harness this through variational inference. Further, to enhance the variational
inference based optimization, we initialize the optimizer at better starting
points through the well known Metapath2Vec algorithm. We demonstrate our
results on two real-world datasets and show that our model outperforms standard
baseline methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.04855v4,"Generalized Kernel Ridge Regression for Nonparametric Structural
  Functions and Semiparametric Treatment Effects","We propose a family of estimators based on kernel ridge regression for
nonparametric structural functions (also called dose response curves) and
semiparametric treatment effects. Treatment and covariates may be discrete or
continuous, and low, high, or infinite dimensional. We reduce causal estimation
and inference to combinations of kernel ridge regressions, which have closed
form solutions and are easily computed by matrix operations, unlike other
machine learning paradigms. This computational simplicity allows us to extend
the framework in two directions: from means to increments and distributions of
counterfactual outcomes; and from parameters of the full population to those of
subpopulations and alternative populations. For structural functions, we prove
uniform consistency with finite sample rates. For treatment effects, we prove
$\sqrt{n}$ consistency, Gaussian approximation, and semiparametric efficiency
with a new double spectral robustness property. We conduct simulations and
estimate average, heterogeneous, and incremental structural functions of the US
Jobs Corps training program.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1504.03483v1,Detecting and Explaining Conflicts in Attributed Feature Models,"Product configuration systems are often based on a variability model. The
development of a variability model is a time consuming and error-prone process.
Considering the ongoing development of products, the variability model has to
be adapted frequently. These changes often lead to mistakes, such that some
products cannot be derived from the model anymore, that undesired products are
derivable or that there are contradictions in the variability model. In this
paper, we propose an approach to discover and to explain contradictions in
attributed feature models efficiently in order to assist the developer with the
correction of mistakes. We use extended feature models with attributes and
arithmetic constraints, translate them into a constraint satisfaction problem
and explore those for contradictions. When a contradiction is found, the
constraints are searched for a set of contradicting relations by the
QuickXplain algorithm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1601.04943v3,"Semantics for probabilistic programming: higher-order functions,
  continuous distributions, and soft constraints","We study the semantic foundation of expressive probabilistic programming
languages, that support higher-order functions, continuous distributions, and
soft constraints (such as Anglican, Church, and Venture). We define a
metalanguage (an idealised version of Anglican) for probabilistic computation
with the above features, develop both operational and denotational semantics,
and prove soundness, adequacy, and termination. They involve measure theory,
stochastic labelled transition systems, and functor categories, but admit
intuitive computational readings, one of which views sampled random variables
as dynamically allocated read-only variables. We apply our semantics to
validate nontrivial equations underlying the correctness of certain compiler
optimisations and inference algorithms such as sequential Monte Carlo
simulation. The language enables defining probability distributions on
higher-order functions, and we study their properties.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0802.0820v2,Independence and concurrent separation logic,"A compositional Petri net-based semantics is given to a simple language
allowing pointer manipulation and parallelism. The model is then applied to
give a notion of validity to the judgements made by concurrent separation logic
that emphasizes the process-environment duality inherent in such rely-guarantee
reasoning. Soundness of the rules of concurrent separation logic with respect
to this definition of validity is shown. The independence information retained
by the Petri net model is then exploited to characterize the independence of
parallel processes enforced by the logic. This is shown to permit a refinement
operation capable of changing the granularity of atomic actions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0712.3705v1,Framework and Resources for Natural Language Parser Evaluation,"Because of the wide variety of contemporary practices used in the automatic
syntactic parsing of natural languages, it has become necessary to analyze and
evaluate the strengths and weaknesses of different approaches. This research is
all the more necessary because there are currently no genre- and
domain-independent parsers that are able to analyze unrestricted text with 100%
preciseness (I use this term to refer to the correctness of analyses assigned
by a parser). All these factors create a need for methods and resources that
can be used to evaluate and compare parsing systems. This research describes:
(1) A theoretical analysis of current achievements in parsing and parser
evaluation. (2) A framework (called FEPa) that can be used to carry out
practical parser evaluations and comparisons. (3) A set of new evaluation
resources: FiEval is a Finnish treebank under construction, and MGTS and RobSet
are parser evaluation resources in English. (4) The results of experiments in
which the developed evaluation framework and the two resources for English were
used for evaluating a set of selected parsers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.04008v2,Waypoint Routing on Bounded Treewidth Graphs,"In the \textsc{Waypoint Routing Problem} one is given an undirected
capacitated and weighted graph $G$, a source-destination pair $s,t\in V(G)$ and
a set $W\subseteq V(G)$, of \emph{waypoints}. The task is to find a walk which
starts at the source vertex $s$, visits, in any order, all waypoints, ends at
the destination vertex $t$, respects edge capacities, that is, traverses each
edge at most as many times as is its capacity, and minimizes the cost computed
as the sum of costs of traversed edges with multiplicities. We study the
problem for graphs of bounded treewidth and present a new algorithm for the
problem working in $2^{O(\mathrm{tw})}\cdot n$ time, significantly improving
upon the previously known algorithms. We also show that this running time is
optimal for the problem under Exponential Time Hypothesis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.14716v2,Prosody leaks into the memories of words,"The average predictability (aka informativity) of a word in context has been
shown to condition word duration (Seyfarth, 2014). All else being equal, words
that tend to occur in more predictable environments are shorter than words that
tend to occur in less predictable environments. One account of the
informativity effect on duration is that the acoustic details of probabilistic
reduction are stored as part of a word's mental representation. Other research
has argued that predictability effects are tied to prosodic structure in
integral ways. With the aim of assessing a potential prosodic basis for
informativity effects in speech production, this study extends past work in two
directions; it investigated informativity effects in another large language,
Mandarin Chinese, and broadened the study beyond word duration to additional
acoustic dimensions, pitch and intensity, known to index prosodic prominence.
The acoustic information of content words was extracted from a large telephone
conversation speech corpus with over 400,000 tokens and 6,000 word types spoken
by 1,655 individuals and analyzed for the effect of informativity using
frequency statistics estimated from a 431 million word subtitle corpus. Results
indicated that words with low informativity have shorter durations, replicating
the effect found in English. In addition, informativity had significant effects
on maximum pitch and intensity, two phonetic dimensions related to prosodic
prominence. Extending this interpretation, these results suggest that
predictability is closely linked to prosodic prominence, and that the lexical
representation of a word includes phonetic details associated with its average
prosodic prominence in discourse. In other words, the lexicon absorbs prosodic
influences on speech production.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0101034v1,Data Security Equals Graph Connectivity,"To protect sensitive information in a cross tabulated table, it is a common
practice to suppress some of the cells in the table. This paper investigates
four levels of data security of a two-dimensional table concerning the
effectiveness of this practice. These four levels of data security protect the
information contained in, respectively, individual cells, individual rows and
columns, several rows or columns as a whole, and a table as a whole. The paper
presents efficient algorithms and NP-completeness results for testing and
achieving these four levels of data security. All these complexity results are
obtained by means of fundamental equivalences between the four levels of data
security of a table and four types of connectivity of a graph constructed from
that table.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.07921v1,"Performance analysis of a distributed algorithm for admission control in
  wireless networks under the $2$-hop interference model","A general open problem in networking is: what are the fundamental limits to
the performance that is achievable with some given amount of resources? More
specifically, if each node in the network has information about only its
$1$-hop neighborhood, then what are the limits to performance? This problem is
considered for wireless networks where each communication link has a minimum
bandwidth quality-of-service (QoS) requirement. Links in the same vicinity
contend for the shared wireless medium. The conflict graph captures which pairs
of links interfere with each other and depends on the MAC protocol. In IEEE
802.11 MAC protocol-based networks, when communication between nodes $i$ and
$j$ takes place, the neighbors of both $i$ and $j$ remain silent. This model of
interference is called the $2$-hop interference model because the distance in
the network graph between any two links that can be simultaneously active is at
least $2$. In the admission control problem, the objective is to determine,
using only localized information, whether a given set of flow rates is
feasible.
  In the present work, a distributed algorithm is proposed for this problem,
where each node has information only about its $1$-hop neighborhood. The
worst-case performance of the distributed algorithm, i.e. the largest factor by
which the performance of this distributed algorithm is away from that of an
optimal, centralized algorithm, is analyzed. Lower and upper bounds on the
suboptimality of the distributed algorithm are obtained, and both bounds are
shown to be tight. The exact worst-case performance is obtained for some ring
topologies. While distance-$d$ distributed algorithms have been analyzed for
the $1$-hop interference model, an open problem in the literature is to extend
these results to the $K$-hop interference model, and the present work initiates
the generalization to the $K$-hop interference model.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1411.2649v3,A Primer on IPv4 Scarcity,"With the ongoing exhaustion of free address pools at the registries serving
the global demand for IPv4 address space, scarcity has become reality. Networks
in need of address space can no longer get more address allocations from their
respective registries.
  In this work we frame the fundamentals of the IPv4 address exhaustion
phenomena and connected issues. We elaborate on how the current ecosystem of
IPv4 address space has evolved since the standardization of IPv4, leading to
the rather complex and opaque scenario we face today. We outline the evolution
in address space management as well as address space use patterns, identifying
key factors of the scarcity issues. We characterize the possible solution space
to overcome these issues and open the perspective of address blocks as virtual
resources, which involves issues such as differentiation between address
blocks, the need for resource certification, and issues arising when
transferring address space between networks.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1806.09029v1,Improving Text-to-SQL Evaluation Methodology,"To be informative, an evaluation must measure how well systems generalize to
realistic unseen data. We identify limitations of and propose improvements to
current evaluations of text-to-SQL systems. First, we compare human-generated
and automatically generated questions, characterizing properties of queries
necessary for real-world applications. To facilitate evaluation on multiple
datasets, we release standardized and improved versions of seven existing
datasets and one new text-to-SQL dataset. Second, we show that the current
division of data into training and test sets measures robustness to variations
in the way questions are asked, but only partially tests how well systems
generalize to new queries; therefore, we propose a complementary dataset split
for evaluation of future work. Finally, we demonstrate how the common practice
of anonymizing variables during evaluation removes an important challenge of
the task. Our observations highlight key difficulties, and our methodology
enables effective measurement of future development.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0909.2859v1,Electric routing and concurrent flow cutting,"We investigate an oblivious routing scheme, amenable to distributed
computation and resilient to graph changes, based on electrical flow. Our main
technical contribution is a new rounding method which we use to obtain a bound
on the L1->L1 operator norm of the inverse graph Laplacian. We show how this
norm reflects both latency and congestion of electric routing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.11139v1,Sublinear quantum algorithms for estimating von Neumann entropy,"Entropy is a fundamental property of both classical and quantum systems,
spanning myriad theoretical and practical applications in physics and computer
science. We study the problem of obtaining estimates to within a multiplicative
factor $\gamma>1$ of the Shannon entropy of probability distributions and the
von Neumann entropy of mixed quantum states. Our main results are:
  $\quad\bullet$ an $\widetilde{\mathcal{O}}\left(
n^{\frac{1+\eta}{2\gamma^2}}\right)$-query quantum algorithm that outputs a
$\gamma$-multiplicative approximation of the Shannon entropy $H(\mathbf{p})$ of
a classical probability distribution $\mathbf{p} = (p_1,\ldots,p_n)$;
  $\quad\bullet$ an $\widetilde{\mathcal{O}}\left(
n^{\frac12+\frac{1+\eta}{2\gamma^2}}\right)$-query quantum algorithm that
outputs a $\gamma$-multiplicative approximation of the von Neumann entropy
$S(\rho)$ of a density matrix $\rho\in\mathbb{C}^{n\times n}$.
  In both cases, the input is assumed to have entropy bounded away from zero by
a quantity determined by the parameter $\eta>0$, since, as we prove, no
polynomial query algorithm can multiplicatively approximate the entropy of
distributions with arbitrarily low entropy. In addition, we provide
$\Omega\left(n^{\frac{1}{3\gamma^2}}\right)$ lower bounds on the query
complexity of $\gamma$-multiplicative estimation of Shannon and von Neumann
entropies.
  We work with the quantum purified query access model, which can handle both
classical probability distributions and mixed quantum states, and is the most
general input model considered in the literature.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0601045v1,"PageRank without hyperlinks: Structural re-ranking using links induced
  by language models","Inspired by the PageRank and HITS (hubs and authorities) algorithms for Web
search, we propose a structural re-ranking approach to ad hoc information
retrieval: we reorder the documents in an initially retrieved set by exploiting
asymmetric relationships between them. Specifically, we consider generation
links, which indicate that the language model induced from one document assigns
high probability to the text of another; in doing so, we take care to prevent
bias against long documents. We study a number of re-ranking criteria based on
measures of centrality in the graphs formed by generation links, and show that
integrating centrality into standard language-model-based retrieval is quite
effective at improving precision at top ranks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0812.4792v1,"On Optimal Linear Redistribution of VCG Payments in Assignment of
  Heterogeneous Objects","There are p heterogeneous objects to be assigned to n competing agents (n >
p) each with unit demand. It is required to design a Groves mechanism for this
assignment problem satisfying weak budget balance, individual rationality, and
minimizing the budget imbalance. This calls for designing an appropriate rebate
function. Our main result is an impossibility theorem which rules out linear
rebate functions with non-zero efficiency in heterogeneous object assignment.
Motivated by this theorem, we explore two approaches to get around this
impossibility. In the first approach, we show that linear rebate functions with
non-zero are possible when the valuations for the objects are correlated. In
the second approach, we show that rebate functions with non-zero efficiency are
possible if linearity is relaxed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.12498v2,"Computer Algebra in Physics: The hidden SO(4) symmetry of the hydrogen
  atom","Pauli first noticed the hidden SO(4) symmetry for the Hydrogen atom in the
early stages of quantum mechanics [1]. Departing from that symmetry, one can
recover the spectrum of a spinless hydrogen atom and the degeneracy of its
states without explicitly solving Schr\""odinger's equation [2]. In this paper,
we derive that SO(4) symmetry and spectrum using a computer algebra system
(CAS). While this problem is well known [3, 4], its solution involves several
steps of manipulating expressions with tensorial quantum operators, simplifying
them by taking into account a combination of commutator rules and Einstein's
sum rule for repeated indices. Therefore, it is an excellent model to test the
current status of CAS concerning this kind of quantum-and-tensor-algebra
computations. Generally speaking, when capable, CAS can significantly help with
manipulations that, like non-commutative tensor calculus subject to algebra
rules, are tedious, time-consuming and error-prone. The presentation also shows
a pattern of computer algebra operations that can be useful for systematically
tackling more complicated symbolic problems of this kind.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9810018v1,A Proof Theoretic View of Constraint Programming,"We provide here a proof theoretic account of constraint programming that
attempts to capture the essential ingredients of this programming style. We
exemplify it by presenting proof rules for linear constraints over interval
domains, and illustrate their use by analyzing the constraint propagation
process for the {\tt SEND + MORE = MONEY} puzzle. We also show how this
approach allows one to build new constraint solvers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0702160v1,A Quantifier-Free String Theory for ALOGTIME Reasoning,"The main contribution of this work is the definition of a quantifier-free
string theory T_1 suitable for formalizing ALOGTIME reasoning. After describing
L_1 -- a new, simple, algebraic characterization of the complexity class
ALOGTIME based on strings instead of numbers -- the theory T_1 is defined
(based on L_1), and a detailed formal development of T_1 is given.
  Then, theorems of T_1 are shown to translate into families of propositional
tautologies that have uniform polysize Frege proofs, T_1 is shown to prove the
soundness of a particular Frege system F, and F is shown to provably p-simulate
any proof system whose soundness can be proved in T_1. Finally, T_1 is compared
with other theories for ALOGTIME reasoning in the literature.
  To our knowledge, this is the first formal theory for ALOGTIME reasoning
whose basic objects are strings instead of numbers, and the first
quantifier-free theory formalizing ALOGTIME reasoning in which a direct proof
of the soundness of some Frege system has been given (in the case of
first-order theories, such a proof was first given by Arai for his theory AID).
Also, the polysize Frege proofs we give for the propositional translations of
theorems of T_1 are considerably simpler than those for other theories, and so
is our proof of the soundness of a particular F-system in T_1. Together with
the simplicity of T_1's recursion schemes, axioms, and rules these facts
suggest that T_1 is one of the most natural theories available for ALOGTIME
reasoning.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.00764v2,"Symbolic, Distributed and Distributional Representations for Natural
  Language Processing in the Era of Deep Learning: a Survey","Natural language is inherently a discrete symbolic representation of human
knowledge. Recent advances in machine learning (ML) and in natural language
processing (NLP) seem to contradict the above intuition: discrete symbols are
fading away, erased by vectors or tensors called distributed and distributional
representations. However, there is a strict link between
distributed/distributional representations and discrete symbols, being the
first an approximation of the second. A clearer understanding of the strict
link between distributed/distributional representations and symbols may
certainly lead to radically new deep learning networks. In this paper we make a
survey that aims to renew the link between symbolic representations and
distributed/distributional representations. This is the right time to
revitalize the area of interpreting how discrete symbols are represented inside
neural networks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1904.05202v1,"Development of QoS methods in the information networks with fractal
  traffic","The paper discusses actual task of ensuring the quality of services in
information networks with fractal traffic. The generalized approach to traffic
management and quality of service based on the account of multifractal
properties of the network traffic is proposed. To describe the multifractal
traffic properties, it is proposed to use the Hurst exponent, the range of
generalized Hurst exponent and coefficient of variation. Methods of preventing
of network overload in communication node, routing cost calculation and load
balancing, which based on fractal properties of traffic are presented. The
results of simulation have shown that the joint use of the proposed methods can
significantly improve the quality of service network.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1005.4540v3,Optimal Partitions in Additively Separable Hedonic Games,"We conduct a computational analysis of fair and optimal partitions in
additively separable hedonic games. We show that, for strict preferences, a
Pareto optimal partition can be found in polynomial time while verifying
whether a given partition is Pareto optimal is coNP-complete, even when
preferences are symmetric and strict. Moreover, computing a partition with
maximum egalitarian or utilitarian social welfare or one which is both Pareto
optimal and individually rational is NP-hard. We also prove that checking
whether there exists a partition which is both Pareto optimal and envy-free is
$\Sigma_{2}^{p}$-complete. Even though an envy-free partition and a Nash stable
partition are both guaranteed to exist for symmetric preferences, checking
whether there exists a partition which is both envy-free and Nash stable is
NP-complete.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1205.4564v1,Computational Complexity of Traffic Hijacking under BGP and S-BGP,"Harmful Internet hijacking incidents put in evidence how fragile the Border
Gateway Protocol (BGP) is, which is used to exchange routing information
between Autonomous Systems (ASes). As proved by recent research contributions,
even S-BGP, the secure variant of BGP that is being deployed, is not fully able
to blunt traffic attraction attacks. Given a traffic flow between two ASes, we
study how difficult it is for a malicious AS to devise a strategy for hijacking
or intercepting that flow. We show that this problem marks a sharp difference
between BGP and S-BGP. Namely, while it is solvable, under reasonable
assumptions, in polynomial time for the type of attacks that are usually
performed in BGP, it is NP-hard for S-BGP. Our study has several by-products.
E.g., we solve a problem left open in the literature, stating when performing a
hijacking in S-BGP is equivalent to performing an interception.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0704.1827v1,Transaction-Oriented Simulation In Ad Hoc Grids,"This paper analyses the possibilities of performing parallel
transaction-oriented simulations with a special focus on the space-parallel
approach and discrete event simulation synchronisation algorithms that are
suitable for transaction-oriented simulation and the target environment of Ad
Hoc Grids. To demonstrate the findings a Java-based parallel
transaction-oriented simulator for the simulation language GPSS/H is
implemented on the basis of the promising Shock Resistant Time Warp
synchronisation algorithm and using the Grid framework ProActive. The
validation of this parallel simulator shows that the Shock Resistant Time Warp
algorithm can successfully reduce the number of rolled back Transaction moves
but it also reveals circumstances in which the Shock Resistant Time Warp
algorithm can be outperformed by the normal Time Warp algorithm. The conclusion
of this paper suggests possible improvements to the Shock Resistant Time Warp
algorithm to avoid such problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.05920v1,Application of Econometric Data Analysis Methods to Physics Software,"We report an investigation of data analysis methods derived from other
disciplines, which we applied to physics software systems. They concern the
analysis of inequality, trend analysis and the analysis of diversity. The
analysis of inequality exploits statistical methods originating from
econometrics; trend analysis is typical of economics and environmental
sciences; the analysis of diversity is based on concepts derived from ecology
and treats software as an ecosystem. To the best of our knowledge, this is an
innovative exploration, as we could not find track of previous use of these
methods in the experimental physics domains within the scope of the IEEE
Nuclear Science Symposium. We applied these methods in the context of Geant4
physics validation and Geant4 maintainability assessment.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1404.1201v2,Regular Substitution Sets: A Means of Controlling E-Unification,"A method for selecting solution constructors in narrowing is presented. The
method is based on a sort discipline that describes regular sets of ground
constructor terms as sorts. It is extended to cope with regular sets of ground
substitutions, thus allowing different sorts to be computed for terms with
different variable bindings. An algorithm for computing signatures of
equationally defined functions is given that allows potentially infinite
overloading. Applications to formal program development are sketched.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1403.6652v2,DeepWalk: Online Learning of Social Representations,"We present DeepWalk, a novel approach for learning latent representations of
vertices in a network. These latent representations encode social relations in
a continuous vector space, which is easily exploited by statistical models.
DeepWalk generalizes recent advancements in language modeling and unsupervised
feature learning (or deep learning) from sequences of words to graphs. DeepWalk
uses local information obtained from truncated random walks to learn latent
representations by treating walks as the equivalent of sentences. We
demonstrate DeepWalk's latent representations on several multi-label network
classification tasks for social networks such as BlogCatalog, Flickr, and
YouTube. Our results show that DeepWalk outperforms challenging baselines which
are allowed a global view of the network, especially in the presence of missing
information. DeepWalk's representations can provide $F_1$ scores up to 10%
higher than competing methods when labeled data is sparse. In some experiments,
DeepWalk's representations are able to outperform all baseline methods while
using 60% less training data. DeepWalk is also scalable. It is an online
learning algorithm which builds useful incremental results, and is trivially
parallelizable. These qualities make it suitable for a broad class of real
world applications such as network classification, and anomaly detection.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.03095v1,Quantifying and Explaining Immutability in Scala,"Functional programming typically emphasizes programming with first-class
functions and immutable data. Immutable data types enable fault tolerance in
distributed systems, and ensure process isolation in message-passing
concurrency, among other applications. However, beyond the distinction between
reassignable and non-reassignable fields, Scala's type system does not have a
built-in notion of immutability for type definitions. As a result, immutability
is ""by-convention"" in Scala, and statistics about the use of immutability in
real-world Scala code are non-existent.
  This paper reports on the results of an empirical study on the use of
immutability in several medium-to-large Scala open-source code bases, including
Scala's standard library and the Akka actor framework. The study investigates
both shallow and deep immutability, two widely-used forms of immutability in
Scala. Perhaps most interestingly, for type definitions determined to be
mutable, explanations are provided for why neither the shallow nor the deep
immutability property holds; in turn, these explanations are aggregated into
statistics in order to determine the most common reasons for why type
definitions are mutable rather than immutable.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.08169v1,"Transformation of Python Applications into Function-as-a-Service
  Deployments","New cloud programming and deployment models pose challenges to software
application engineers who are looking, often in vain, for tools to automate any
necessary code adaptation and transformation. Function-as-a-Service interfaces
are particular non-trivial targets when considering that most cloud
applications are implemented in non-functional languages. Among the most widely
used of these languages is Python. This starting position calls for an
automated approach to transform monolithic Python code into modular FaaS units
by partially automated decomposition. Hence, this paper introduces and
evaluates Lambada, a Python module to dynamically decompose, convert and deploy
unmodified Python code into AWS Lambda functions. Beyond the tooling in the
form of a measured open source prototype implementation, the paper contributes
a description of the algorithms and code rewriting rules as blueprints for
transformations of other scripting languages.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1111.0837v5,Exponential Lower Bounds for Polytopes in Combinatorial Optimization,"We solve a 20-year old problem posed by Yannakakis and prove that there
exists no polynomial-size linear program (LP) whose associated polytope
projects to the traveling salesman polytope, even if the LP is not required to
be symmetric. Moreover, we prove that this holds also for the cut polytope and
the stable set polytope. These results were discovered through a new connection
that we make between one-way quantum communication protocols and semidefinite
programming reformulations of LPs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0204051v1,Parrondo Strategies for Artificial Traders,"On markets with receding prices, artificial noise traders may consider
alternatives to buy-and-hold. By simulating variations of the Parrondo
strategy, using real data from the Swedish stock market, we produce first
indications of a buy-low-sell-random Parrondo variation outperforming
buy-and-hold. Subject to our assumptions, buy-low-sell-random also outperforms
the traditional value and trend investor strategies. We measure the success of
the Parrondo variations not only through their performance compared to other
kinds of strategies, but also relative to varying levels of perfect
information, received through messages within a multi-agent system of
artificial traders.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1201.0216v1,Building Smart Communities with Cyber-Physical Systems,"There is a growing trend towards the convergence of cyber-physical systems
(CPS) and social computing, which will lead to the emergence of smart
communities composed of various objects (including both human individuals and
physical things) that interact and cooperate with each other. These smart
communities promise to enable a number of innovative applications and services
that will improve the quality of life. This position paper addresses some
opportunities and challenges of building smart communities characterized by
cyber-physical and social intelligence.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/cs/0312024v1,Evolution: Google vs. DRIS,"This paper gives an absolute new search system that builds the information
retrieval infrastructure for Internet. Now most search engine companies are
mainly concerned with how to make profit from company users by advertisement
and ranking prominence, but never consider what its real customers will feel.
Few web search engines can sell billions dollars just at the cost of
inconvenience of most Internet users, but not its high quality of search
service. When we have to bear the bothersome advertisements in the awful
results and have no choices, Internet as the kind of public good will surely be
undermined. If current Internet can't fully ensure our right to know, it may
need some sound improvements or a revolution.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1906.05721v1,Visual Wake Words Dataset,"The emergence of Internet of Things (IoT) applications requires intelligence
on the edge. Microcontrollers provide a low-cost compute platform to deploy
intelligent IoT applications using machine learning at scale, but have
extremely limited on-chip memory and compute capability. To deploy computer
vision on such devices, we need tiny vision models that fit within a few
hundred kilobytes of memory footprint in terms of peak usage and model size on
device storage. To facilitate the development of microcontroller friendly
models, we present a new dataset, Visual Wake Words, that represents a common
microcontroller vision use-case of identifying whether a person is present in
the image or not, and provides a realistic benchmark for tiny vision models.
Within a limited memory footprint of 250 KB, several state-of-the-art mobile
models achieve accuracy of 85-90% on the Visual Wake Words dataset. We
anticipate the proposed dataset will advance the research on tiny vision models
that can push the pareto-optimal boundary in terms of accuracy versus memory
usage for microcontroller applications.",0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0312046v1,"On the Abductive or Deductive Nature of Database Schema Validation and
  Update Processing Problems","We show that database schema validation and update processing problems such
as view updating, materialized view maintenance, integrity constraint checking,
integrity constraint maintenance or condition monitoring can be classified as
problems of either abductive or deductive nature, according to the reasoning
paradigm that inherently suites them. This is done by performing abductive and
deductive reasoning on the event rules [Oli91], a set of rules that define the
difference between consecutive database states In this way, we show that it is
possible to provide methods able to deal with all these problems as a whole. We
also show how some existing general deductive and abductive procedures may be
used to reason on the event rules. In this way, we show that these procedures
can deal with all database schema validation and update processing problems
considered in this paper.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0702088v1,"Paths Beyond Local Search: A Nearly Tight Bound for Randomized
  Fixed-Point Computation","In 1983, Aldous proved that randomization can speedup local search. For
example, it reduces the query complexity of local search over [1:n]^d from
Theta (n^{d-1}) to O (d^{1/2}n^{d/2}). It remains open whether randomization
helps fixed-point computation. Inspired by this open problem and recent
advances on equilibrium computation, we have been fascinated by the following
question:
  Is a fixed-point or an equilibrium fundamentally harder to find than a local
optimum? In this paper, we give a nearly-tight bound of Omega(n)^{d-1} on the
randomized query complexity for computing a fixed point of a discrete Brouwer
function over [1:n]^d. Since the randomized query complexity of global
optimization over [1:n]^d is Theta (n^{d}), the randomized query model over
[1:n]^d strictly separates these three important search problems: Global
optimization is harder than fixed-point computation, and fixed-point
computation is harder than local search. Our result indeed demonstrates that
randomization does not help much in fixed-point computation in the query model;
the deterministic complexity of this problem is Theta (n^{d-1}).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.6331v3,Interaction Graphs: Graphings,"In two previous papers, we exposed a combinatorial approach to the program of
Geometry of Interaction, a program initiated by Jean-Yves Girard. The strength
of our approach lies in the fact that we interpret proofs by simpler structures
- graphs - than Girard's constructions, while generalizing the latter since
they can be recovered as special cases of our setting. This third paper extends
this approach by considering a generalization of graphs named graphings, which
is in some way a geometric realization of a graph. This very general framework
leads to a number of new models of multiplicative-additive linear logic which
generalize Girard's geometry of interaction models and opens several new lines
of research. As an example, we exhibit a family of such models which account
for second-order quantification without suffering the same limitations as
Girard's models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1610.08575v1,Minimal unsatisfiability and deficiency: recent developments,"Starting with Aharoni and Linial in 1986, the deficiency delta(F) = c(F) -
n(F) >= 1 for minimally unsatisfiable clause-sets F, the difference of the
number of clauses and the number of variables, is playing an important role in
investigations into the structure of minimal unsatisfiability. The talk
belonging to this extended abstract, available at
http://cs.swan.ac.uk/~csoliver/papers.html#BORDEAUX2016 , gives a high-level
overview on recent developments.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.13583v1,A Modular First Formalisation of Combinatorial Design Theory,"Combinatorial design theory studies set systems with certain balance and
symmetry properties and has applications to computer science and elsewhere.
This paper presents a modular approach to formalising designs for the first
time using Isabelle and assesses the usability of a locale-centric approach to
formalisations of mathematical structures. We demonstrate how locales can be
used to specify numerous types of designs and their hierarchy. The resulting
library, which is concise and adaptable, includes formal definitions and proofs
for many key properties, operations, and theorems on the construction and
existence of designs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.02641v1,"Learning Stabilizing Control Policies for a Tensegrity Hopper with
  Augmented Random Search","In this paper, we consider tensegrity hopper - a novel tensegrity-based
robot, capable of moving by hopping. The paper focuses on the design of the
stabilizing control policies, which are obtained with Augmented Random Search
method. In particular, we search for control policies which allow the hopper to
maintain vertical stability after performing a single jump. It is demonstrated,
that the hopper can maintain a vertical configuration, subject to the different
initial conditions and with changing control frequency rates. In particular,
lowering control frequency from 1000Hz in training to 500Hz in execution did
not affect the success rate of the balancing task.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.09764v1,Human-agent coordination in a group formation game,"Coordination and cooperation between humans and autonomous agents in
cooperative games raises interesting questions of human decision making and
behaviour changes. Here we report our findings from a group formation game in a
small-world network of different mixes of human and agent players, aiming to
achieve connected clusters of the same colour by swapping places with
neighbouring players using non-overlapping information. In the experiments the
human players are incentivized by rewarding to prioritize their own cluster
while the model of agents' decision making is derived from our previous
experiment of purely cooperative game between human players. The experiments
were performed by grouping the players in three different setups to investigate
the overall effect of having cooperative autonomous agents within teams. We
observe that the change in the behavior of human subjects adjusts to playing
with autonomous agents by being less risk averse, while keeping the overall
performance efficient by splitting the behaviour into selfish and cooperative
in the two actions performed during the rounds of the game. Moreover, results
from two hybrid human-agent setups suggest that the group composition affects
the evolution of clusters. Our findings indicate that in purely or lesser
cooperative settings, providing more control to humans could help in maximizing
the overall performance of hybrid systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1001.0443v1,Discovering Knowledge from Multi-modal Lecture Recordings,"Educational media mining is the process of converting raw media data from
educational systems to useful information that can be used to design learning
systems, answer research questions and allow personalized learning experiences.
Knowledge discovery encompasses a wide range of techniques ranging from
database queries to more recent developments in machine learning and language
technology. Educational media mining techniques are now being used in IT
Services research worldwide. Multi-modal Lecture Recordings is one of the
important types of educational media and this paper explores the research
challenges for mining lecture recordings for the efficient personalized
learning experiences. Keywords: Educational Media Mining; Lecture Recordings,
Multimodal Information System, Personalized Learning; Online Course Ware;
Skills and Competences;",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.05924v2,Nowcasting Gentrification Using Airbnb Data,"There is a rumbling debate over the impact of gentrification: presumed
gentrifiers have been the target of protests and attacks in some cities, while
they have been welcome as generators of new jobs and taxes in others. Census
data fails to measure neighborhood change in real-time since it is usually
updated every ten years. This work shows that Airbnb data can be used to
quantify and track neighborhood changes. Specifically, we consider both
structured data (e.g. number of listings, number of reviews, listing
information) and unstructured data (e.g. user-generated reviews processed with
natural language processing and machine learning algorithms) for three major
cities, New York City (US), Los Angeles (US), and Greater London (UK). We find
that Airbnb data (especially its unstructured part) appears to nowcast
neighborhood gentrification, measured as changes in housing affordability and
demographics. Overall, our results suggest that user-generated data from online
platforms can be used to create socioeconomic indices to complement traditional
measures that are less granular, not in real-time, and more costly to obtain.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1605.06015v3,"BeatBox - HPC Simulation Environment for Biophysically and Anatomically
  Realistic Cardiac Electrophysiology","The BeatBox simulation environment combines flexible script language user
interface with the robust computational tools, in order to setup cardiac
electrophysiology in-silico experiments without re-coding at low-level, so that
cell excitation, tissue/anatomy models, stimulation protocols may be included
into a BeatBox script, and simulation run either sequentially or in parallel
(MPI) without re-compilation. BeatBox is a free software written in C language
to be run on a Unix-based platform. It provides the whole spectrum of multi
scale tissue modelling from 0-dimensional individual cell simulation,
1-dimensional fibre, 2-dimensional sheet and 3-dimensional slab of tissue, up
to anatomically realistic whole heart simulations, with run time measurements
including cardiac re-entry tip/filament tracing, ECG, local/global samples of
any variables, etc. BeatBox solvers, cell, and tissue/anatomy models
repositories are extended via robust and flexible interfaces, thus providing an
open framework for new developments in the field. In this paper we give an
overview of the BeatBox current state, together with a description of the main
computational methods and MPI parallelisation approaches.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0807.2961v1,"Perturbed affine arithmetic for invariant computation in numerical
  program analysis","We completely describe a new domain for abstract interpretation of numerical
programs. Fixpoint iteration in this domain is proved to converge to finite
precise invariants for (at least) the class of stable linear recursive filters
of any order. Good evidence shows it behaves well also for some non-linear
schemes. The result, and the structure of the domain, rely on an interesting
interplay between order and topology.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.00707v1,Efficient covering of convex domains by congruent discs,"In this paper, we consider the problem of covering a plane region with unit
discs. We present an improved upper bound and the first nontrivial lower bound
on the number of discs needed for such a covering, depending on the area and
perimeter of the region. We provide algorithms for efficient covering of convex
polygonal regions using unit discs. We show that the computational complexity
of the algorithms is pseudo-polynomial in the size of the input and the output.
We also show that these algorithms provide a constant factor approximation of
the optimal covering of the region.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1701.05419v1,"Moving to VideoKifu: the last steps toward a fully automatic
  record-keeping of a Go game","In a previous paper [ arXiv:1508.03269 ] we described the techniques we
successfully employed for automatically reconstructing the whole move sequence
of a Go game by means of a set of pictures. Now we describe how it is possible
to reconstruct the move sequence by means of a video stream (which may be
provided by an unattended webcam), possibly in real-time. Although the basic
algorithms remain the same, we will discuss the new problems that arise when
dealing with videos, with special care for the ones that could block a
real-time analysis and require an improvement of our previous techniques or
even a completely brand new approach. Eventually we present a number of
preliminary but positive experimental results supporting the effectiveness of
the software we are developing, built on the ideas here outlined.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0808.0298v1,Computing the nucleolus of weighted voting games,"Weighted voting games (WVG) are coalitional games in which an agent's
contribution to a coalition is given by his it weight, and a coalition wins if
its total weight meets or exceeds a given quota. These games model
decision-making in political bodies as well as collaboration and surplus
division in multiagent domains. The computational complexity of various
solution concepts for weighted voting games received a lot of attention in
recent years. In particular, Elkind et al.(2007) studied the complexity of
stability-related solution concepts in WVGs, namely, of the core, the least
core, and the nucleolus. While they have completely characterized the
algorithmic complexity of the core and the least core, for the nucleolus they
have only provided an NP-hardness result. In this paper, we solve an open
problem posed by Elkind et al. by showing that the nucleolus of WVGs, and, more
generally, k-vector weighted voting games with fixed k, can be computed in
pseudopolynomial time, i.e., there exists an algorithm that correctly computes
the nucleolus and runs in time polynomial in the number of players and the
maximum weight. In doing so, we propose a general framework for computing the
nucleolus, which may be applicable to a wider of class of games.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1603.06807v2,"Generating Factoid Questions With Recurrent Neural Networks: The 30M
  Factoid Question-Answer Corpus","Over the past decade, large-scale supervised learning corpora have enabled
machine learning researchers to make substantial advances. However, to this
date, there are no large-scale question-answer corpora available. In this paper
we present the 30M Factoid Question-Answer Corpus, an enormous question answer
pair corpus produced by applying a novel neural network architecture on the
knowledge base Freebase to transduce facts into natural language questions. The
produced question answer pairs are evaluated both by human evaluators and using
automatic evaluation metrics, including well-established machine translation
and sentence similarity metrics. Across all evaluation criteria the
question-generation model outperforms the competing template-based baseline.
Furthermore, when presented to human evaluators, the generated questions appear
comparable in quality to real human-generated questions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.12059v2,Constructing minimally 3-connected graphs,"A $3$-connected graph is minimally 3-connected if removal of any edge
destroys 3-connectivity. We present an algorithm for constructing minimally
3-connected graphs based on the results in (Dawes, JCTB 40, 159-168, 1986)
using two operations: adding an edge between non-adjacent vertices and
splitting a vertex. In order to test sets of vertices and edges for
3-compatibility, which depends on the cycles of the graph, we develop a method
for obtaining the cycles of $G'$ from the cycles of $G$, where $G'$ is obtained
from $G$ by one of the two operations above. We eliminate isomorphs using
certificates generated by McKay's isomorphism checker nauty. The algorithm
consecutively constructs the non-isomorphic minimally 3-connected graphs with
$n$ vertices and $m$ edges from the non-isomorphic minimally 3-connected graphs
with $n-1$ vertices and $m-2$ edges, $n-1$ vertices and $m-3$ edges, and $n-2$
vertices and $m-3$ edges.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.15664v1,Is 2NF a Stable Normal Form?,"Traditionally, it was accepted that a relational database can be normalized
step-by-step, from a set of un-normalized tables to tables in $1NF$, then to
$2NF$, then to $3NF$, then (possibly) to $BCNF$. The rule applied to a table in
$1NF$ in order to transform it to a set of tables in $2NF$ seems to be too
straightforward to pose any difficulty.
  However, we show that, depending on the set of functional dependencies, it is
impossible to reach $2NF$ and stop there; one must, in these cases, perform the
normalization from $1NF$ to $3NF$ as an indecomposable move. The minimal setup
to exhibit the phenomena requires a single composite key, and two partially
overlapping chains of transitive dependencies.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.01171v2,Expected Value of Communication for Planning in Ad Hoc Teamwork,"A desirable goal for autonomous agents is to be able to coordinate on the fly
with previously unknown teammates. Known as ""ad hoc teamwork"", enabling such a
capability has been receiving increasing attention in the research community.
One of the central challenges in ad hoc teamwork is quickly recognizing the
current plans of other agents and planning accordingly. In this paper, we focus
on the scenario in which teammates can communicate with one another, but only
at a cost. Thus, they must carefully balance plan recognition based on
observations vs. that based on communication. This paper proposes a new metric
for evaluating how similar are two policies that a teammate may be following -
the Expected Divergence Point (EDP). We then present a novel planning algorithm
for ad hoc teamwork, determining which query to ask and planning accordingly.
We demonstrate the effectiveness of this algorithm in a range of increasingly
general communication in ad hoc teamwork problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1402.2827v1,Harnessing the Complexity of Education with Information Technology,"Education at all levels is facing several challenges in most countries, such
as low quality, high costs, lack of educators, and unsatisfied student demand.
Traditional approaches are becoming unable to deliver the required education.
Several causes for this inefficiency can be identified. I argue that beyond
specific causes, the lack of effective education is related to complexity.
However, information technology is helping us overcome this complexity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/0906.3424v1,Decentralized Traffic Management Strategies for Sensor-Enabled Cars,"Traffic Congestions and accidents are major concerns in today's
transportation systems. This thesis investigates how to optimize traffic flow
on highways, in particular for merging situations such as intersections where a
ramp leads onto the highway. In our work, cars are equipped with sensors that
can detect distance to neighboring cars, and communicate their velocity and
acceleration readings with one another. Sensor-enabled cars can locally
exchange sensed information about the traffic and adapt their behavior much
earlier than regular cars.
  We propose proactive algorithms for merging different streams of
sensor-enabled cars into a single stream. A proactive merging algorithm
decouples the decision point from the actual merging point. Sensor-enabled cars
allow us to decide where and when a car merges before it arrives at the actual
merging point. This leads to a significant improvement in traffic flow as
velocities can be adjusted appropriately. We compare proactive merging
algorithms against the conventional priority-based merging algorithm in a
controlled simulation environment. Experiment results show that proactive
merging algorithms outperform the priority-based merging algorithm in terms of
flow and delay.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.10189v2,Primitive Recursive Ordered Fields and Some Applications,"We establish primitive recursive versions of some known facts about
computable ordered fields of reals and computable reals, and then apply them to
proving primitive recursiveness of some natural problems in linear algebra and
analysis. In particular, we find a partial primitive recursive analogue of
Ershov-Madison's theorem about real closures of computable ordered fields,
relate the corresponding fields to the primitive recursive reals, give
sufficient conditions for primitive recursive root-finding, computing normal
forms of matrices, and computing solution operators of some linear systems of
PDE.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1211.5520v1,"Accurate Demarcation of Protein Domain Linkers based on Structural
  Analysis of Linker Probable Region","In multi-domain proteins, the domains are connected by a flexible
unstructured region called as protein domain linker. The accurate demarcation
of these linkers holds a key to understanding of their biochemical and
evolutionary attributes. This knowledge helps in designing a suitable linker
for engineering stable multi-domain chimeric proteins. Here we propose a novel
method for the demarcation of the linker based on a three-dimensional protein
structure and a domain definition. The proposed method is based on biological
knowledge about structural flexibility of the linkers. We performed structural
analysis on a linker probable region (LPR) around domain boundary points of
known SCOP domains. The LPR was described using a set of overlapping peptide
fragments of fixed size. Each peptide fragment was then described by geometric
invariants (GIs) and subjected to clustering process where the fragments
corresponding to actual linker come up as outliers. We then discover the actual
linkers by finding the longest continuous stretch of outlier fragments from
LPRs. This method was evaluated on a benchmark dataset of 51 continuous
multi-domain proteins, where it achieves F1 score of 0.745 (0.83 precision and
0.66 recall). When the method was applied on 725 continuous multi-domain
proteins, it was able to identify novel linkers that were not reported
previously. This method can be used in combination with supervised / sequence
based linker prediction methods for accurate linker demarcation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.06196v1,GraPE: fast and scalable Graph Processing and Embedding,"Graph Representation Learning methods have enabled a wide range of learning
problems to be addressed for data that can be represented in graph form.
Nevertheless, several real world problems in economy, biology, medicine and
other fields raised relevant scaling problems with existing methods and their
software implementation, due to the size of real world graphs characterized by
millions of nodes and billions of edges. We present GraPE, a software resource
for graph processing and random walk based embedding, that can scale with large
and high-degree graphs and significantly speed up-computation. GraPE comprises
specialized data structures, algorithms, and a fast parallel implementation
that displays everal orders of magnitude improvement in empirical space and
time complexity compared to state of the art software resources, with a
corresponding boost in the performance of machine learning methods for edge and
node label prediction and for the unsupervised analysis of graphs.GraPE is
designed to run on laptop and desktop computers, as well as on high performance
computing clusters",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0910.4307v1,"A Geometric Approach to Solve Fuzzy Linear Systems of Differential
  Equations","In this paper, systems of linear differential equations with crisp real
coefficients and with initial condition described by a vector of fuzzy numbers
are studied. A new method based on the geometric representations of linear
transformations is proposed to find a solution. The most important difference
between this method and methods offered in previous papers is that the solution
is considered to be a fuzzy set of real vector-functions rather than a fuzzy
vector-function. Each member of the set satisfies the given system with a
certain possibility. It is shown that at any time the solution constitutes a
fuzzy region in the coordinate space, alfa-cuts of which are nested
parallelepipeds. Proposed method is illustrated on examples.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1212.5692v1,Abstract Effects and Proof-Relevant Logical Relations,"We introduce a novel variant of logical relations that maps types not merely
to partial equivalence relations on values, as is commonly done, but rather to
a proof-relevant generalisation thereof, namely setoids. The objects of a
setoid establish that values inhabit semantic types, whilst its morphisms are
understood as proofs of semantic equivalence. The transition to proof-relevance
solves two well-known problems caused by the use of existential quantification
over future worlds in traditional Kripke logical relations: failure of
admissibility, and spurious functional dependencies. We illustrate the novel
format with two applications: a direct-style validation of Pitts and Stark's
equivalences for ""new"" and a denotational semantics for a region-based effect
system that supports type abstraction in the sense that only externally visible
effects need to be tracked; non-observable internal modifications, such as the
reorganisation of a search tree or lazy initialisation, can count as `pure' or
`read only'. This `fictional purity' allows clients of a module soundly to
validate more effect-based program equivalences than would be possible with
traditional effect systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0311034v1,"Visualization of variations in human brain morphology using
  differentiating reflection functions","Conventional visualization media such as MRI prints and computer screens are
inherently two dimensional, making them incapable of displaying true 3D volume
data sets. By applying only transparency or intensity projection, and ignoring
light-matter interaction, results will likely fail to give optimal results.
Little research has been done on using reflectance functions to visually
separate the various segments of a MRI volume. We will explore if applying
specific reflectance functions to individual anatomical structures can help in
building an intuitive 2D image from a 3D dataset. We will test our hypothesis
by visualizing a statistical analysis of the genetic influences on variations
in human brain morphology because it inherently contains complex and many
different types of data making it a good candidate for our approach",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0903.4696v1,Multidimensional Online Robot Motion,"We consider three related problems of robot movement in arbitrary dimensions:
coverage, search, and navigation. For each problem, a spherical robot is asked
to accomplish a motion-related task in an unknown environment whose geometry is
learned by the robot during navigation. The robot is assumed to have tactile
and global positioning sensors. We view these problems from the perspective of
(non-linear) competitiveness as defined by Gabriely and Rimon. We first show
that in 3 dimensions and higher, there is no upper bound on competitiveness:
every online algorithm can do arbitrarily badly compared to the optimal. We
then modify the problems by assuming a fixed clearance parameter. We are able
to give optimally competitive algorithms under this assumption.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1809.07575v1,Symbolic Music Genre Transfer with CycleGAN,"Deep generative models such as Variational Autoencoders (VAEs) and Generative
Adversarial Networks (GANs) have recently been applied to style and domain
transfer for images, and in the case of VAEs, music. GAN-based models employing
several generators and some form of cycle consistency loss have been among the
most successful for image domain transfer. In this paper we apply such a model
to symbolic music and show the feasibility of our approach for music genre
transfer. Evaluations using separate genre classifiers show that the style
transfer works well. In order to improve the fidelity of the transformed music,
we add additional discriminators that cause the generators to keep the
structure of the original music mostly intact, while still achieving strong
genre transfer. Visual and audible results further show the potential of our
approach. To the best of our knowledge, this paper represents the first
application of GANs to symbolic music domain transfer.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1002.0939v1,"Virtual Machine Support for Many-Core Architectures: Decoupling Abstract
  from Concrete Concurrency Models","The upcoming many-core architectures require software developers to exploit
concurrency to utilize available computational power. Today's high-level
language virtual machines (VMs), which are a cornerstone of software
development, do not provide sufficient abstraction for concurrency concepts. We
analyze concrete and abstract concurrency models and identify the challenges
they impose for VMs. To provide sufficient concurrency support in VMs, we
propose to integrate concurrency operations into VM instruction sets.
  Since there will always be VMs optimized for special purposes, our goal is to
develop a methodology to design instruction sets with concurrency support.
Therefore, we also propose a list of trade-offs that have to be investigated to
advise the design of such instruction sets.
  As a first experiment, we implemented one instruction set extension for
shared memory and one for non-shared memory concurrency. From our experimental
results, we derived a list of requirements for a full-grown experimental
environment for further research.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1707.02617v1,Deepest Neural Networks,"This paper shows that a long chain of perceptrons (that is, a multilayer
perceptron, or MLP, with many hidden layers of width one) can be a universal
classifier. The classification procedure is not necessarily computationally
efficient, but the technique throws some light on the kind of computations
possible with narrow and deep MLPs.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1108.0486v1,"High-Performance Pseudo-Random Number Generation on Graphics Processing
  Units","This work considers the deployment of pseudo-random number generators (PRNGs)
on graphics processing units (GPUs), developing an approach based on the
xorgens generator to rapidly produce pseudo-random numbers of high statistical
quality. The chosen algorithm has configurable state size and period, making it
ideal for tuning to the GPU architecture. We present a comparison of both speed
and statistical quality with other common parallel, GPU-based PRNGs,
demonstrating favourable performance of the xorgens-based approach.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.09879v3,"AMPPERE: A Universal Abstract Machine for Privacy-Preserving Entity
  Resolution Evaluation","Entity resolution is the task of identifying records in different datasets
that refer to the same entity in the real world. In sensitive domains (e.g.
financial accounts, hospital health records), entity resolution must meet
privacy requirements to avoid revealing sensitive information such as personal
identifiable information to untrusted parties. Existing solutions are either
too algorithmically-specific or come with an implicit trade-off between
accuracy of the computation, privacy, and run-time efficiency. We propose
AMMPERE, an abstract computation model for performing universal
privacy-preserving entity resolution. AMPPERE offers abstractions that
encapsulate multiple algorithmic and platform-agnostic approaches using
variants of Jaccard similarity to perform private data matching and entity
resolution. Specifically, we show that two parties can perform entity
resolution over their data, without leaking sensitive information. We
rigorously compare and analyze the feasibility, performance overhead and
privacy-preserving properties of these approaches on the Sharemind multi-party
computation (MPC) platform as well as on PALISADE, a lattice-based homomorphic
encryption library. The AMPPERE system demonstrates the efficacy of
privacy-preserving entity resolution for real-world data while providing a
precise characterization of the induced cost of preventing information leakage.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1910.04709v1,"Modeling of negative protein-protein interactions: methods and
  experiments","Protein-protein interactions (PPIs) are of fundamental importance for the
human body, and the knowledge of their existence can facilitate very important
tasks like drug target developing and therapy design. The high-throughput
experiments for detecting new PPIs are costly and time-consuming, stressing the
need for new computational systems able to generate high-quality PPIs
predictions. These systems have to face two main problems: the high
incompleteness of the human interactome and the lack of high-quality negative
protein-protein interactions (i.e. proteins that are known to not interact).
The latter is usually overlooked by the PPIs prediction systems, causing a
significant bias in the performances and metrics. In this work, we compare
methods for simulating negative knowledge using highly reliable training and
test sets. Moreover, we measure the performances of two state-of-the-art
systems when very reliable settings are adopted.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1908.11344v1,Multivariate Spatial Data Visualization: A Survey,"Multivariate spatial data plays an important role in computational science
and engineering simulations. The potential features and hidden relationships in
multivariate data can assist scientists to gain an in-depth understanding of a
scientific process, verify a hypothesis and further discover a new physical or
chemical law. In this paper, we present a comprehensive survey of the
state-of-the-art techniques for multivariate spatial data visualization. We
first introduce the basic concept and characteristics of multivariate spatial
data, and describe three main tasks in multivariate data visualization: feature
classification, fusion visualization, and correlation analysis. Finally, we
prospect potential research topics for multivariate data visualization
according to the current research.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.10126v3,"Generative Synthetic Augmentation using Label-to-Image Translation for
  Nuclei Image Segmentation","In medical image diagnosis, pathology image analysis using semantic
segmentation becomes important for efficient screening as a field of digital
pathology. The spatial augmentation is ordinary used for semantic segmentation.
Tumor images under malignant are rare and to annotate the labels of nuclei
region takes much time-consuming. We require an effective use of dataset to
maximize the segmentation accuracy. It is expected that some augmentation to
transform generalized images influence the segmentation performance. We propose
a synthetic augmentation using label-to-image translation, mapping from a
semantic label with the edge structure to a real image. Exactly this paper deal
with stain slides of nuclei in tumor. Actually, we demonstrate several
segmentation algorithms applied to the initial dataset that contains real
images and labels using synthetic augmentation in order to add their
generalized images. We computes and reports that a proposed synthetic
augmentation procedure improve their accuracy.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1904.00619v1,Towards Ranking Geometric Automated Theorem Provers,"The field of geometric automated theorem provers has a long and rich history,
from the early AI approaches of the 1960s, synthetic provers, to today
algebraic and synthetic provers.
  The geometry automated deduction area differs from other areas by the strong
connection between the axiomatic theories and its standard models. In many
cases the geometric constructions are used to establish the theorems'
statements, geometric constructions are, in some provers, used to conduct the
proof, used as counter-examples to close some branches of the automatic proof.
Synthetic geometry proofs are done using geometric properties, proofs that can
have a visual counterpart in the supporting geometric construction.
  With the growing use of geometry automatic deduction tools as applications in
other areas, e.g. in education, the need to evaluate them, using different
criteria, is felt. Establishing a ranking among geometric automated theorem
provers will be useful for the improvement of the current
methods/implementations. Improvements could concern wider scope, better
efficiency, proof readability and proof reliability.
  To achieve the goal of being able to compare geometric automated theorem
provers a common test bench is needed: a common language to describe the
geometric problems; a comprehensive repository of geometric problems and a set
of quality measures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1805.01041v4,An Updated Experimental Evaluation of Graph Bipartization Methods,"We experimentally evaluate the practical state-of-the-art in graph
bipartization (Odd Cycle Transversal), motivated by recent advances in
near-term quantum computing hardware and the related embedding problems. We
assemble a preprocessing suite of fast input reduction routines from the Odd
Cycle Transversal (OCT) and Vertex Cover (VC) literature, and compare algorithm
implementations using Quadratic Unconstrained Binary Optimization problems from
the quantum literature. We also generate a corpus of frustrated cluster loop
graphs, which have previously been used to benchmark quantum annealing
hardware. The diversity of these graphs leads to harder OCT instances than in
existing benchmarks.
  In addition to combinatorial branching algorithms for solving OCT directly,
we study various reformulations into other NP-hard problems such as VC and
Integer Linear Programming (ILP), enabling the use of solvers such as CPLEX. We
find that for heuristic solutions with time constraints under a second,
iterative compression routines jump-started with a heuristic solution perform
best, after which point using a highly tuned solver like CPLEX is worthwhile.
Results on exact solvers are split between using ILP formulations on CPLEX and
solving VC formulations with a branch-and-reduce solver. We extend our results
with a large corpus of synthetic graphs, establishing robustness and potential
to generalize to other domain data. In total, over 8000 graph instances are
evaluated, compared to the previous canonical corpus of 100 graphs.
  Finally, we provide all code and data in an open source suite, including a
Python API for accessing reduction routines and branching algorithms, along
with scripts for fully replicating our results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.09645v1,"Urban Bike Lane Planning with Bike Trajectories: Models, Algorithms, and
  a Real-World Case Study","We study an urban bike lane planning problem based on the fine-grained bike
trajectory data, which is made available by smart city infrastructure such as
bike-sharing systems. The key decision is where to build bike lanes in the
existing road network. As bike-sharing systems become widespread in the
metropolitan areas over the world, bike lanes are being planned and constructed
by many municipal governments to promote cycling and protect cyclists.
Traditional bike lane planning approaches often rely on surveys and heuristics.
We develop a general and novel optimization framework to guide the bike lane
planning from bike trajectories. We formalize the bike lane planning problem in
view of the cyclists' utility functions and derive an integer optimization
model to maximize the utility. To capture cyclists' route choices, we develop a
bilevel program based on the Multinomial Logit model. We derive structural
properties about the base model and prove that the Lagrangian dual of the bike
lane planning model is polynomial-time solvable. Furthermore, we reformulate
the route choice based planning model as a mixed integer linear program using a
linear approximation scheme. We develop tractable formulations and efficient
algorithms to solve the large-scale optimization problem. Via a real-world case
study with a city government, we demonstrate the efficiency of the proposed
algorithms and quantify the trade-off between the coverage of bike trips and
continuity of bike lanes. We show how the network topology evolves according to
the utility functions and highlight the importance of understanding cyclists'
route choices. The proposed framework drives the data-driven urban planning
scheme in smart city operations management.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1004.1216v1,Multi-Shift de Bruijn Sequence,"A (non-circular) de Bruijn sequence w of order n is a word such that every
word of length n appears exactly once in w as a factor. In this paper, we
generalize the concept to a multi-shift setting: a multi-shift de Bruijn
sequence tau(m,n) of shift m and order n is a word such that every word of
length n appears exactly once in w as a factor that starts at index im+1 for
some integer i>=0. We show the number of the multi-shift de Bruijn sequence
tau(m,n) is (a^n)!a^{(m-n)(a^n-1)} for 1<=n<=m and is (a^m!)^{a^{n-m}} for
1<=m<=n, where a=|Sigma|. We provide two algorithms for generating a tau(m,n).
The multi-shift de Bruijn sequence is important in solving the Frobenius
problem in a free monoid.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.05378v1,Simpler Context-Dependent Logical Forms via Model Projections,"We consider the task of learning a context-dependent mapping from utterances
to denotations. With only denotations at training time, we must search over a
combinatorially large space of logical forms, which is even larger with
context-dependent utterances. To cope with this challenge, we perform
successive projections of the full model onto simpler models that operate over
equivalence classes of logical forms. Though less expressive, we find that
these simpler models are much faster and can be surprisingly effective.
Moreover, they can be used to bootstrap the full model. Finally, we collected
three new context-dependent semantic parsing datasets, and develop a new
left-to-right parser.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.13018v2,The PetscSF Scalable Communication Layer,"PetscSF, the communication component of the Portable, Extensible Toolkit for
Scientific Computation (PETSc), is designed to provide PETSc's communication
infrastructure suitable for exascale computers that utilize GPUs and other
accelerators. PetscSF provides a simple application programming interface (API)
for managing common communication patterns in scientific computations by using
a star-forest graph representation. PetscSF supports several implementations
based on MPI and NVSHMEM, whose selection is based on the characteristics of
the application or the target architecture. An efficient and portable model for
network and intra-node communication is essential for implementing large-scale
applications. The Message Passing Interface, which has been the de facto
standard for distributed memory systems, has developed into a large complex API
that does not yet provide high performance on the emerging heterogeneous
CPU-GPU-based exascale systems. In this paper, we discuss the design of
PetscSF, how it can overcome some difficulties of working directly with MPI on
GPUs, and we demonstrate its performance, scalability, and novel features.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.04937v2,Practical Budgeted Submodular Maximization,"We consider the problem of maximizing a non-negative monotone submodular
function subject to a knapsack constraint, which is also known as the Budgeted
Submodular Maximization (BSM) problem. Sviridenko (2004) showed that by
guessing 3 appropriate elements of an optimal solution, and then executing a
greedy algorithm, one can obtain the optimal approximation ratio of $\alpha
=1-1/e\approx 0.632$ for BSM. However, the need to guess (by enumeration) 3
elements makes the algorithm of Sviridenko impractical as it leads to a time
complexity of $O(n^5)$ (which can be slightly improved using the thresholding
technique of Badanidiyuru & Vondrak (2014), but only to roughly $O(n^4)$). Our
main results in this paper show that fewer guesses suffice. Specifically, by
making only 2 guesses, we get the same optimal approximation ratio of $\alpha$
with an improved time complexity of roughly $O(n^3)$. Furthermore, by making
only a single guess, we get an almost as good approximation ratio of
$0.6174>0.9767\alpha$ in roughly $O(n^2)$ time.
  Prior to our work, the only algorithms that were known to obtain an
approximation ratio close to $\alpha$ for BSM were the algorithm of Sviridenko
and an algorithm of Ene & Nguyen (2019) that achieves
$(\alpha-\epsilon)$-approximation. However, the algorithm of Ene & Nguyen
requires ${(1/\epsilon)}^{O(1/\epsilon^4)}n\log^2 n$ time, and hence, is of
theoretical interest only as ${(1/\epsilon)}^{O(1/\epsilon^4)}$ is huge even
for moderate values of $\epsilon$. In contrast, all the algorithms we analyze
are simple and parallelizable, which makes them good candidates for practical
use.
  Recently, Tang et al. (2020) studied a simple greedy algorithm that already
has a long research history, and proved that its approximation ratio is at
least 0.405. We improve over this result, and show that the approximation ratio
of this algorithm is within the range [0.427, 0.462].",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.14579v2,Intensional Kleene and Rice Theorems for Abstract Program Semantics,"Classical results in computability theory, notably Rice's theorem, focus on
the extensional content of programs, namely, on the partial recursive functions
that programs compute. Later and more recent work investigated intensional
generalisations of such results that take into account the way in which
functions are computed, thus affected by the specific programs computing them.
In this paper, we single out a novel class of program semantics based on
abstract domains of program properties that are able to capture nonextensional
aspects of program computations, such as their asymptotic complexity or logical
invariants, and allow us to generalise some foundational computability results
such as Rice's Theorem and Kleene's Second Recursion Theorem to these
semantics. In particular, it turns out that for this class of abstract program
semantics, any nontrivial abstract property is undecidable and every decidable
over-approximation necessarily includes an infinite set of false positives
which covers all the values of the semantic abstract domain.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1102.4037v1,"Computability of simple games: A complete investigation of the
  sixty-four possibilities","Classify simple games into sixteen ""types"" in terms of the four conventional
axioms: monotonicity, properness, strongness, and nonweakness. Further classify
them into sixty-four classes in terms of finiteness (existence of a finite
carrier) and algorithmic computability. For each such class, we either show
that it is empty or give an example of a game belonging to it. We observe that
if a type contains an infinite game, then it contains both computable ones and
noncomputable ones. This strongly suggests that computability is logically, as
well as conceptually, unrelated to the conventional axioms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.10107v4,Predicting Subjective Features of Questions of QA Websites using BERT,"Community Question-Answering websites, such as StackOverflow and Quora,
expect users to follow specific guidelines in order to maintain content
quality. These systems mainly rely on community reports for assessing contents,
which has serious problems such as the slow handling of violations, the loss of
normal and experienced users' time, the low quality of some reports, and
discouraging feedback to new users. Therefore, with the overall goal of
providing solutions for automating moderation actions in Q&A websites, we aim
to provide a model to predict 20 quality or subjective aspects of questions in
QA websites. To this end, we used data gathered by the CrowdSource team at
Google Research in 2019 and a fine-tuned pre-trained BERT model on our problem.
Based on the evaluation by Mean-Squared-Error (MSE), the model achieved a value
of 0.046 after 2 epochs of training, which did not improve substantially in the
next ones. Results confirm that by simple fine-tuning, we can achieve accurate
models in little time and on less amount of data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1711.09946v4,"Efficient reduction of nondeterministic automata with application to
  language inclusion testing","We present efficient algorithms to reduce the size of nondeterministic
B\""uchi word automata (NBA) and nondeterministic finite word automata (NFA),
while retaining their languages. Additionally, we describe methods to solve
PSPACE-complete automata problems like language universality, equivalence, and
inclusion for much larger instances than was previously possible ($\ge 1000$
states instead of 10-100). This can be used to scale up applications of
automata in formal verification tools and decision procedures for logical
theories. The algorithms are based on new techniques for removing transitions
(pruning) and adding transitions (saturation), as well as extensions of classic
quotienting of the state space. These techniques use criteria based on
combinations of backward and forward trace inclusions and simulation relations.
Since trace inclusion relations are themselves PSPACE-complete, we introduce
lookahead simulations as good polynomial time computable approximations
thereof. Extensive experiments show that the average-case time complexity of
our algorithms scales slightly above quadratically. (The space complexity is
worst-case quadratic.) The size reduction of the automata depends very much on
the class of instances, but our algorithm consistently reduces the size far
more than all previous techniques. We tested our algorithms on NBA derived from
LTL-formulae, NBA derived from mutual exclusion protocols and many classes of
random NBA and NFA, and compared their performance to the well-known automata
tool GOAL.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.03133v4,Group isomorphism is nearly-linear time for most orders,"We show that there is a dense set $\ourset\subseteq \mathbb{N}$ of group
orders and a constant $c$ such that for every $n\in \ourset$ we can decide in
time $O(n^2(\log n)^c)$ whether two $n\times n$ multiplication tables describe
isomorphic groups of order $n$. This improves significantly over the general
$n^{O(\log n)}$-time complexity and shows that group isomorphism can be tested
efficiently for almost all group orders $n$. We also show that in time $O(n^2
  (\log n)^c)$ it can be decided whether an $n\times n$ multiplication table
describes a group; this improves over the known $O(n^3)$ complexity. Our
complexities are calculated for a deterministic multi-tape Turing machine
model. We give the implications to a RAM model in the promise hierarchy as
well.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.10975v1,Modular Moose: A new generation software reverse engineering environment,"Advanced reverse engineering tools are required to cope with the complexity
of software systems and the specific requirements of numerous different tasks
(re-architecturing, migration, evolution). Consequently, reverse engineering
tools should adapt to a wide range of situations. Yet, because they require a
large infrastructure investment, being able to reuse these tools is key. Moose
is a reverse engineering environment answering these requirements. While Moose
started as a research project 20 years ago, it is also used in industrial
projects, exposing itself to all these difficulties. In this paper we present
ModMoose, the new version of Moose. ModMoose revolves around a new meta-model,
modular and extensible; a new toolset of generic tools (query module,
visualization engine, ...); and an open architecture supporting the
synchronization and interaction of tools per task. With ModMoose, tool
developers can develop specific meta-models by reusing existing elementary
concepts, and dedicated reverse engineering tools that can interact with the
existing ones.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1211.5256v2,Model Checking Parse Trees,"Parse trees are fundamental syntactic structures in both computational
linguistics and compilers construction. We argue in this paper that, in both
fields, there are good incentives for model-checking sets of parse trees for
some word according to a context-free grammar. We put forward the adequacy of
propositional dynamic logic (PDL) on trees in these applications, and study as
a sanity check the complexity of the corresponding model-checking problem:
although complete for exponential time in the general case, we find natural
restrictions on grammars for our applications and establish complexities
ranging from nondeterministic polynomial time to polynomial space in the
relevant cases.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1911.10341v1,A New Model in Firefighting Theory,"Continuous and discrete models for firefighting problems are well-studied in
Theoretical Computer Science. We introduce a new, discrete, and more general
framework based on a hexagonal cell graph to study firefighting problems in
varied terrains. We present three different firefighting problems in the
context of this model; for two of which, we provide efficient polynomial time
algorithms and for the third, we show NP-completeness. We also discuss possible
extensions of the model and their implications on the computational complexity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0310018v1,"The Study of the Application of a Keywords-based Chatbot System on the
  Teaching of Foreign Languages","This paper reports the findings of a study conducted on the application of an
on-line human-computer dialog system with natural language (chatbot) on the
teaching of foreign languages. A keywords-based human-computer dialog system
makes it possible that the user could chat with the computer using a natural
language, i.e. in English or in German to some extent. So an experiment has
been made using this system online to work as a chat partner with the users
learning the foreign languages. Dialogs between the users and the chatbot are
collected. Findings indicate that the dialogs between the human and the
computer are mostly very short because the user finds the responses from the
computer are mostly repeated and irrelevant with the topics and context and the
program does not understand the language at all. With analysis of the keywords
or pattern-matching mechanism used in this chatbot it can be concluded that
this kind of system can not work as a teaching assistant program in foreign
language learning.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/cs/0412088v1,"On Image Filtering, Noise and Morphological Size Intensity Diagrams","In the absence of a pure noise-free image it is hard to define what noise is,
in any original noisy image, and as a consequence also where it is, and in what
amount. In fact, the definition of noise depends largely on our own aim in the
whole image analysis process, and (perhaps more important) in our
self-perception of noise. For instance, when we perceive noise as disconnected
and small it is normal to use MM-ASF filters to treat it. There is two
evidences of this. First, in many instances there is no ideal and pure
noise-free image to compare our filtering process (nothing but our
self-perception of its pure image); second, and related with this first point,
MM transformations that we chose are only based on our self - and perhaps -
fuzzy notion. The present proposal combines the results of two MM filtering
transformations (FT1, FT2) and makes use of some measures and quantitative
relations on their Size/Intensity Diagrams to find the most appropriate noise
removal process. Results can also be used for finding the most appropriate stop
criteria, and the right sequence of MM operators combination on Alternating
Sequential Filters (ASF), if these measures are applied, for instance, on a
Genetic Algorithm's target function.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0909.1187v1,FastFlow: Efficient Parallel Streaming Applications on Multi-core,"Shared memory multiprocessors come back to popularity thanks to rapid
spreading of commodity multi-core architectures. As ever, shared memory
programs are fairly easy to write and quite hard to optimise; providing
multi-core programmers with optimising tools and programming frameworks is a
nowadays challenge. Few efforts have been done to support effective streaming
applications on these architectures. In this paper we introduce FastFlow, a
low-level programming framework based on lock-free queues explicitly designed
to support high-level languages for streaming applications. We compare FastFlow
with state-of-the-art programming frameworks such as Cilk, OpenMP, and Intel
TBB. We experimentally demonstrate that FastFlow is always more efficient than
all of them in a set of micro-benchmarks and on a real world application; the
speedup edge of FastFlow over other solutions might be bold for fine grain
tasks, as an example +35% on OpenMP, +226% on Cilk, +96% on TBB for the
alignment of protein P01111 against UniProt DB using Smith-Waterman algorithm.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0203009v1,SPINning Parallel Systems Software,"We describe our experiences in using SPIN to verify parts of the Multi
Purpose Daemon (MPD) parallel process management system. MPD is a distributed
collection of processes connected by Unix network sockets. MPD is dynamic:
processes and connections among them are created and destroyed as MPD is
initialized, runs user processes, recovers from faults, and terminates. This
dynamic nature is easily expressible in the SPIN/PROMELA framework but poses
performance and scalability challenges. We present here the results of
expressing some of the parallel algorithms of MPD and executing both simulation
and verification runs with SPIN.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.07190v3,"A Comprehensive Taxonomy for Explainable Artificial Intelligence: A
  Systematic Survey of Surveys on Methods and Concepts","In the meantime, a wide variety of terminologies, motivations, approaches,
and evaluation criteria have been developed within the research field of
explainable artificial intelligence (XAI). With the amount of XAI methods
vastly growing, a taxonomy of methods is needed by researchers as well as
practitioners: To grasp the breadth of the topic, compare methods, and to
select the right XAI method based on traits required by a specific use-case
context. Many taxonomies for XAI methods of varying level of detail and depth
can be found in the literature. While they often have a different focus, they
also exhibit many points of overlap. This paper unifies these efforts and
provides a complete taxonomy of XAI methods with respect to notions present in
the current state of research. In a structured literature analysis and
meta-study, we identified and reviewed more than 50 of the most cited and
current surveys on XAI methods, metrics, and method traits. After summarizing
them in a survey of surveys, we merge terminologies and concepts of the
articles into a unified structured taxonomy. Single concepts therein are
illustrated by more than 50 diverse selected example methods in total, which we
categorize accordingly. The taxonomy may serve both beginners, researchers, and
practitioners as a reference and wide-ranging overview of XAI method traits and
aspects. Hence, it provides foundations for targeted, use-case-oriented, and
context-sensitive future research.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1807.07814v1,"Interactive Supercomputing on 40,000 Cores for Machine Learning and Data
  Analysis","Interactive massively parallel computations are critical for machine learning
and data analysis. These computations are a staple of the MIT Lincoln
Laboratory Supercomputing Center (LLSC) and has required the LLSC to develop
unique interactive supercomputing capabilities. Scaling interactive machine
learning frameworks, such as TensorFlow, and data analysis environments, such
as MATLAB/Octave, to tens of thousands of cores presents many technical
challenges - in particular, rapidly dispatching many tasks through a scheduler,
such as Slurm, and starting many instances of applications with thousands of
dependencies. Careful tuning of launches and prepositioning of applications
overcome these challenges and allow the launching of thousands of tasks in
seconds on a 40,000-core supercomputer. Specifically, this work demonstrates
launching 32,000 TensorFlow processes in 4 seconds and launching 262,000 Octave
processes in 40 seconds. These capabilities allow researchers to rapidly
explore novel machine learning architecture and data analysis algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1503.00141v3,Multinational War is Hard,"In this paper, we show that the problem of determining whether one player can
force a win in a multiplayer version of the children's card game War is
PSPACE-hard. The same reduction shows that a related problem, asking whether a
player can survive k rounds, is PSPACE-complete when k is polynomial in the
size of the input.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1509.03915v3,An Impossibility Result for Housing Markets with Fractional Endowments,"The housing market setting constitutes a fundamental model of exchange
economies of goods. Most of the work concerning housing markets does not cater
for randomized assignments or allocation of time-shares. House allocation with
fractional endowments of houses was considered by Athanassoglou and Sethuraman
(2011) who posed the open problem whether individual rationality, weak
strategyproofness, and efficiency are compatible for the setting. We show that
the three axioms are incompatible.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0803.3976v2,On Ritt's decomposition Theorem in the case of finite fields,"A classical theorem by Ritt states that all the complete decomposition chains
of a univariate polynomial satisfying a certain tameness condition have the
same length. In this paper we present our conclusions about the generalization
of these theorem in the case of finite coefficient fields when the tameness
condition is dropped.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.09610v3,Implementing backjumping by throw/1 and catch/3 of Prolog,"We discuss how to implement backjumping (or intelligent backtracking) in
Prolog programs by means of exception handling. This seems impossible in a
general case. We provide two solutions. One works for binary programs; in a
general case it imposes a restriction on where backjumping may originate. The
other restricts a class of backjump targets. We also show how to simulate
backjumping by means of backtracking and the Prolog database.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1903.02939v1,ViTOR: Learning to Rank Webpages Based on Visual Features,"The visual appearance of a webpage carries valuable information about its
quality and can be used to improve the performance of learning to rank (LTR).
We introduce the Visual learning TO Rank (ViTOR) model that integrates
state-of-the-art visual features extraction methods by (i) transfer learning
from a pre-trained image classification model, and (ii) synthetic saliency heat
maps generated from webpage snapshots. Since there is currently no public
dataset for the task of LTR with visual features, we also introduce and release
the ViTOR dataset, containing visually rich and diverse webpages. The ViTOR
dataset consists of visual snapshots, non-visual features and relevance
judgments for ClueWeb12 webpages and TREC Web Track queries. We experiment with
the proposed ViTOR model on the ViTOR dataset and show that it significantly
improves the performance of LTR with visual features",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1909.07671v1,Spatio-Semantic ConvNet-Based Visual Place Recognition,"We present a Visual Place Recognition system that follows the two-stage
format common to image retrieval pipelines. The system encodes images of places
by employing the activations of different layers of a pre-trained,
off-the-shelf, VGG16 Convolutional Neural Network (CNN) architecture. In the
first stage of our method and given a query image of a place, a number of top
candidate images is retrieved from a previously stored database of places. In
the second stage, we propose an exhaustive comparison of the query image
against these candidates by encoding semantic and spatial information in the
form of CNN features. Results from our approach outperform by a large margin
state-of-the-art visual place recognition methods on five of the most commonly
used benchmark datasets. The performance gain is especially remarkable on the
most challenging datasets, with more than a twofold recognition improvement
with respect to the latest published work.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0512019v1,Amazing geometry of genetic space or are genetic algorithms convergent?,"There is no proof yet of convergence of Genetic Algorithms. We do not supply
it too. Instead, we present some thoughts and arguments to convince the Reader,
that Genetic Algorithms are essentially bound for success. For this purpose, we
consider only the crossover operators, single- or multiple-point, together with
selection procedure. We also give a proof that the soft selection is superior
to other selection schemes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.04318v3,Transforming task representations to perform novel tasks,"An important aspect of intelligence is the ability to adapt to a novel task
without any direct experience (zero-shot), based on its relationship to
previous tasks. Humans can exhibit this cognitive flexibility. By contrast,
models that achieve superhuman performance in specific tasks often fail to
adapt to even slight task alterations. To address this, we propose a general
computational framework for adapting to novel tasks based on their relationship
to prior tasks. We begin by learning vector representations of tasks. To adapt
to new tasks, we propose meta-mappings, higher-order tasks that transform basic
task representations. We demonstrate the effectiveness of this framework across
a wide variety of tasks and computational paradigms, ranging from regression to
image classification and reinforcement learning. We compare to both human
adaptability and language-based approaches to zero-shot learning. Across these
domains, meta-mapping is successful, often achieving 80-90% performance,
without any data, on a novel task, even when the new task directly contradicts
prior experience. We further show that meta-mapping can not only generalize to
new tasks via learned relationships, but can also generalize using novel
relationships unseen during training. Finally, using meta-mapping as a starting
point can dramatically accelerate later learning on a new task, and reduce
learning time and cumulative error substantially. Our results provide insight
into a possible computational basis of intelligent adaptability and offer a
possible framework for modeling cognitive flexibility and building more
flexible artificial intelligence systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0308027v2,A Comparison of Secret Sharing Schemes Based on Latin Squares and RSA,"In recent years there has been a great deal of work done on secret sharing
scehemes. Secret Sharing Schemes allow for the division of keys so that an
authorised set of users may access information. In this paper we wish to
present a critical comparison of two of these schemes based on Latin Squares,
[Cooper et., al.] and RSA [Shoup]. These two protocols will be examined in
terms of their positive and negative aspects of their secuirty.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1601.06580v1,Is swarm intelligence able to create mazes?,"In this paper, the idea of applying Computational Intelligence in the process
of creation board games, in particular mazes, is presented. For two different
algorithms the proposed idea has been examined. The results of the experiments
are shown and discussed to present advantages and disadvantages.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1408.1754v1,A Partial-Order Approach to Array Content Analysis,"We present a parametric abstract domain for array content analysis. The
method maintains invariants for contiguous regions of the array, similar to the
methods of Gopan, Reps and Sagiv, and of Halbwachs and Peron. However, it
introduces a novel concept of an array content graph, avoiding the need for an
up-front factorial partitioning step. The resulting analysis can be used with
arbitrary numeric relational abstract domains; we evaluate the domain on a
range of array manipulating program fragments.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.05071v1,"Conformity Assessments and Post-market Monitoring: A Guide to the Role
  of Auditing in the Proposed European AI Regulation","The proposed European Artificial Intelligence Act (AIA) is the first attempt
to elaborate a general legal framework for AI carried out by any major global
economy. As such, the AIA is likely to become a point of reference in the
larger discourse on how AI systems can (and should) be regulated. In this
article, we describe and discuss the two primary enforcement mechanisms
proposed in the AIA: the conformity assessments that providers of high-risk AI
systems are expected to conduct, and the post-market monitoring plans that
providers must establish to document the performance of high-risk AI systems
throughout their lifetimes. We argue that AIA can be interpreted as a proposal
to establish a Europe-wide ecosystem for conducting AI auditing, albeit in
other words. Our analysis offers two main contributions. First, by describing
the enforcement mechanisms included in the AIA in terminology borrowed from
existing literature on AI auditing, we help providers of AI systems understand
how they can prove adherence to the requirements set out in the AIA in
practice. Second, by examining the AIA from an auditing perspective, we seek to
provide transferable lessons from previous research about how to refine further
the regulatory approach outlined in the AIA. We conclude by highlighting seven
aspects of the AIA where amendments (or simply clarifications) would be
helpful. These include, above all, the need to translate vague concepts into
verifiable criteria and to strengthen the institutional safeguards concerning
conformity assessments based on internal checks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0
http://arxiv.org/abs/0910.3301v4,Faster Algorithms for Max-Product Message-Passing,"Maximum A Posteriori inference in graphical models is often solved via
message-passing algorithms, such as the junction-tree algorithm, or loopy
belief-propagation. The exact solution to this problem is well known to be
exponential in the size of the model's maximal cliques after it is
triangulated, while approximate inference is typically exponential in the size
of the model's factors. In this paper, we take advantage of the fact that many
models have maximal cliques that are larger than their constituent factors, and
also of the fact that many factors consist entirely of latent variables (i.e.,
they do not depend on an observation). This is a common case in a wide variety
of applications, including grids, trees, and ring-structured models. In such
cases, we are able to decrease the exponent of complexity for message-passing
by 0.5 for both exact and approximate inference.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1103.4142v1,Analysis of Randomized Work Stealing with False Sharing,"This paper analyzes the cache miss cost of algorithms when scheduled using
randomized work stealing (RWS) in a parallel environment, taking into account
the effects of false sharing.
  First, prior analyses (due to Acar et al.) are extended to incorporate false
sharing. However, to control the possible delays due to false sharing, some
restrictions on the algorithms seem necessary. Accordingly, the class of
Hierarchical Tree algorithms is introduced and their performance analyzed.
  In addition, the paper analyzes the performance of a subclass of the
Hierarchical Tree Algorithms, called HBP algorithms, when scheduled using RWS;
improved complexity bounds are obtained for this subclass. This class was
introduced in a companion paper with efficient resource oblivious computation
in mind.
  Finally, we note that in a scenario in which there is no false sharing the
results in this paper match prior bounds for cache misses but with reduced
assumptions, and in particular with no need for a bounding concave function for
the cost of cache misses as in prior work by Frigo and Strumpen. This allows
non-trivial cache miss bounds in this case to be obtained for a larger class of
algorithms.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1608.00724v1,"On the Power of Simple Reductions for the Maximum Independent Set
  Problem","Reductions---rules that reduce input size while maintaining the ability to
compute an optimal solution---are critical for developing efficient maximum
independent set algorithms in both theory and practice. While several simple
reductions have previously been shown to make small domain-specific instances
tractable in practice, it was only recently shown that advanced reductions (in
a measure-and-conquer approach) can be used to solve real-world networks on
millions of vertices [Akiba and Iwata, TCS 2016]. In this paper we compare
these state-of-the-art reductions against a small suite of simple reductions,
and come to two conclusions: just two simple reductions---vertex folding and
isolated vertex removal---are sufficient for many real-world instances, and
further, the power of the advanced rules comes largely from their initial
application (i.e., kernelization), and not their repeated application during
branch-and-bound. As a part of our comparison, we give the first experimental
evaluation of a reduction based on maximum critical independent sets, and show
it is highly effective in practice for medium-sized networks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1712.08107v1,"A Deep Learning Interpretable Classifier for Diabetic Retinopathy
  Disease Grading","Deep neural network models have been proven to be very successful in image
classification tasks, also for medical diagnosis, but their main concern is its
lack of interpretability. They use to work as intuition machines with high
statistical confidence but unable to give interpretable explanations about the
reported results. The vast amount of parameters of these models make difficult
to infer a rationale interpretation from them. In this paper we present a
diabetic retinopathy interpretable classifier able to classify retine images
into the different levels of disease severity and of explaining its results by
assigning a score for every point in the hidden and input space, evaluating its
contribution to the final classification in a linear way. The generated visual
maps can be interpreted by an expert in order to compare its own knowledge with
the interpretation given by the model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1605.01459v1,Movement Coordination in Human-Robot Teams: A Dynamical Systems Approach,"In order to be effective teammates, robots need to be able to understand
high-level human behavior to recognize, anticipate, and adapt to human motion.
We have designed a new approach to enable robots to perceive human group motion
in real-time, anticipate future actions, and synthesize their own motion
accordingly. We explore this within the context of joint action, where humans
and robots move together synchronously. In this paper, we present an
anticipation method which takes high-level group behavior into account. We
validate the method within a human-robot interaction scenario, where an
autonomous mobile robot observes a team of human dancers, and then successfully
and contingently coordinates its movements to ""join the dance"". We compared the
results of our anticipation method to move the robot with another method which
did not rely on high-level group behavior, and found our method performed
better both in terms of more closely synchronizing the robot's motion to the
team, and also exhibiting more contingent and fluent motion. These findings
suggest that the robot performs better when it has an understanding of
high-level group behavior than when it does not. This work will help enable
others in the robotics community to build more fluent and adaptable robots in
the future.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.00431v2,Contact Classification in COVID-19 Tracing,"The present paper addresses the task of reliably identifying critical
contacts by using COVID-19 tracing apps. A reliable classification is crucial
to ensure a high level of protection, and at the same time to prevent many
people from being sent to quarantine by the app. Tracing apps are based on the
capabilities of current smartphones to enable a broadest possible availability.
Existing capabilities of smartphones include the exchange of Bluetooth Low
Energy (BLE) signals and of audio signals, as well as the use of gyroscopes and
magnetic sensors. The Bluetooth power measurements, which are often used today,
may be complemented by audio ranging and attitude estimation in the future.
Smartphones are worn in different ways, often in pockets and bags, which makes
the propagation of signals and thus the classification rather unpredictable.
Relying on the cooperation of users to wear their phones hanging from their
neck would change the situation considerably. In this case the performance,
achievable with BLE and audio measurements, becomes predictable. Our analysis
identifies parameters that result in accurate warnings, at least within the
scope of validity of the models. A significant reduction of the spreading of
the disease can then be achieved by the apps, without causing many people to
unduly go to quarantine. The present paper is the first of three papers which
analyze the situation in some detail.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1806.02543v3,Grouped Gaussian Processes for Solar Power Prediction,"We consider multi-task regression models where the observations are assumed
to be a linear combination of several latent node functions and weight
functions, which are both drawn from Gaussian process priors. Driven by the
problem of developing scalable methods for forecasting distributed solar and
other renewable power generation, we propose coupled priors over groups of
(node or weight) processes to exploit spatial dependence between functions. We
estimate forecast models for solar power at multiple distributed sites and
ground wind speed at multiple proximate weather stations. Our results show that
our approach maintains or improves point-prediction accuracy relative to
competing solar benchmarks and improves over wind forecast benchmark models on
all measures. Our approach consistently dominates the equivalent model without
coupled priors, achieving faster gains in forecast accuracy. At the same time
our approach provides better quantification of predictive uncertainties.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1802.07085v1,"The isomorphism problem for finite extensions of free groups is in
  PSPACE","We present an algorithm for the following problem: given a context-free
grammar for the word problem of a virtually free group $G$, compute a finite
graph of groups $\mathcal{G}$ with finite vertex groups and fundamental group
$G$. Our algorithm is non-deterministic and runs in doubly exponential time. It
follows that the isomorphism problem of context-free groups can be solved in
doubly exponential space.
  Moreover, if, instead of a grammar, a finite extension of a free group is
given as input, the construction of the graph of groups is in NP and,
consequently, the isomorphism problem in PSPACE.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1611.00676v1,"Knowledge-infused and Consistent Complex Event Processing over Real-time
  and Persistent Streams","Emerging applications in Internet of Things (IoT) and Cyber-Physical Systems
(CPS) present novel challenges to Big Data platforms for performing online
analytics. Ubiquitous sensors from IoT deployments are able to generate data
streams at high velocity, that include information from a variety of domains,
and accumulate to large volumes on disk. Complex Event Processing (CEP) is
recognized as an important real-time computing paradigm for analyzing
continuous data streams. However, existing work on CEP is largely limited to
relational query processing, exposing two distinctive gaps for query
specification and execution: (1) infusing the relational query model with
higher level knowledge semantics, and (2) seamless query evaluation across
temporal spaces that span past, present and future events. These allow
accessible analytics over data streams having properties from different
disciplines, and help span the velocity (real-time) and volume (persistent)
dimensions. In this article, we introduce a Knowledge-infused CEP (X-CEP)
framework that provides domain-aware knowledge query constructs along with
temporal operators that allow end-to-end queries to span across real-time and
persistent streams. We translate this query model to efficient query execution
over online and offline data streams, proposing several optimizations to
mitigate the overheads introduced by evaluating semantic predicates and in
accessing high-volume historic data streams. The proposed X-CEP query model and
execution approaches are implemented in our prototype semantic CEP engine,
SCEPter. We validate our query model using domain-aware CEP queries from a
real-world Smart Power Grid application, and experimentally analyze the
benefits of our optimizations for executing these queries, using event streams
from a campus-microgrid IoT deployment.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1701.08703v4,Weighted omega-Restricted One Counter Automata,"Let $S$ be a complete star-omega semiring and $\Sigma$ be an alphabet. For a
weighted $\omega$-restricted one-counter automaton $\mathcal{C}$ with set of
states $\{1, \dots, n\}$, $n \geq 1$, we show that there exists a mixed
algebraic system over a complete semiring-semimodule pair ${((S \ll \Sigma^*
\gg)^{n\times n}, (S \ll \Sigma^{\omega}\gg)^n)}$ such that the behavior
$\Vert\mathcal{C} \Vert$ of $\mathcal{C}$ is a component of a solution of this
system. In case the basic semiring is $\mathbb{B}$ or $\mathbb{N}^{\infty}$ we
show that there exists a mixed context-free grammar that generates
$\Vert\mathcal{C} \Vert$. The construction of the mixed context-free grammar
from $\mathcal{C}$ is a generalization of the well-known triple construction in
case of restricted one-counter automata and is called now triple-pair
construction for $\omega$-restricted one-counter automata.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.12456v1,Effective Version Space Reduction for Convolutional Neural Networks,"In active learning, sampling bias could pose a serious inconsistency problem
and hinder the algorithm from finding the optimal hypothesis. However, many
methods for neural networks are hypothesis space agnostic and do not address
this problem. We examine active learning with convolutional neural networks
through the principled lens of version space reduction. We identify the
connection between two approaches---prior mass reduction and diameter
reduction---and propose a new diameter-based querying method---the minimum
Gibbs-vote disagreement. By estimating version space diameter and bias, we
illustrate how version space of neural networks evolves and examine the
realizability assumption. With experiments on MNIST, Fashion-MNIST, SVHN and
STL-10 datasets, we demonstrate that diameter reduction methods reduce the
version space more effectively and perform better than prior mass reduction and
other baselines, and that the Gibbs vote disagreement is on par with the best
query method.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.14307v1,"What Color is this? Explaining Art Restoration Research Methods using
  Interactive Museum Installations","This case study describes an approach to designing interactive museum
installations as a student project with the aim of presenting the research
results of the restoration process of paintings to a wide range of visitors.
During one and a half years, the Chair of Media Design created five interactive
media stations in two lectures to enrich the special exhibition ""Veronese: The
Cuccina Cycle. The Restored Masterpiece"". The project was realised in close
communication with the conservators of the Dresden State Art Collections and
the employees of the Science and Archaeometric Laboratory of the Dresden
University of Fine Arts. The students had to learn about the foreign content
and how to translate it into a media-related environment. With suitable
teaching methods, we pushed the students towards a deeper understanding of the
matter.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1610.00673v1,"Collective Robot Reinforcement Learning with Distributed Asynchronous
  Guided Policy Search","In principle, reinforcement learning and policy search methods can enable
robots to learn highly complex and general skills that may allow them to
function amid the complexity and diversity of the real world. However, training
a policy that generalizes well across a wide range of real-world conditions
requires far greater quantity and diversity of experience than is practical to
collect with a single robot. Fortunately, it is possible for multiple robots to
share their experience with one another, and thereby, learn a policy
collectively. In this work, we explore distributed and asynchronous policy
learning as a means to achieve generalization and improved training times on
challenging, real-world manipulation tasks. We propose a distributed and
asynchronous version of Guided Policy Search and use it to demonstrate
collective policy learning on a vision-based door opening task using four
robots. We show that it achieves better generalization, utilization, and
training times than the single robot alternative.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1902.05880v1,Robot Co-design: Beyond the Monotone Case,"Recent advances in 3D printing and manufacturing of miniaturized robotic
hardware and computing are paving the way to build inexpensive and disposable
robots. This will have a large impact on several applications including
scientific discovery (e.g., hurricane monitoring), search-and-rescue (e.g.,
operation in confined spaces), and entertainment (e.g., nano drones). The need
for inexpensive and task-specific robots clashes with the current practice,
where human experts are in charge of designing hardware and software aspects of
the robotic platform. This makes the robot design process expensive and
time-consuming, and ultimately unsuitable for small-volumes low-cost
applications. This paper considers the computational robot co-design problem,
which aims to create an automatic algorithm that selects the best robotic
modules (sensing, actuation, computing) in order to maximize the performance on
a task, while satisfying given specifications (e.g., maximum cost of the
resulting design). We propose a binary optimization formulation of the
co-design problem and show that such formulation generalizes previous work
based on strong modeling assumptions. We show that the proposed formulation can
solve relatively large co-design problems in seconds and with minimal human
intervention. We demonstrate the proposed approach in two applications: the
co-design of an autonomous drone racing platform and the co-design of a
multi-robot system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1104.4586v1,Relativistic virtual worlds: an emerging framework,"In this paper, I will attempt to establish a framework for representation in
virtual worlds that may allow for input data from many different scales and
virtual physics to be merged. For example, a typical virtual environment must
effectively handle user input, sensor data, and virtual world physics all in
real- time. Merging all of these data into a single interactive system requires
that we adapt approaches from topological methods such as n-dimensional
relativistic representation. A number of hypothetical examples will be provided
throughout the paper to clarify technical challenges that need to be overcome
to realize this vision.
  The long-term goal of this work is that truly invariant representations will
ultimately result from establishing formal, inclusive relationships between
these different domains. Using this framework, incomplete information in one or
more domains can be compensated for by parallelism and mappings within the
virtual world representation. To introduce this approach, I will review recent
developments in embodiment, virtual world technology, and neuroscience relevant
to the control of virtual worlds. The next step will be to borrow ideas from
fields such as brain science, applied mathematics, and cosmology to give proper
perspective to this approach. A simple demonstration will then be given using
an intuitive example of physical relativism. Finally, future directions for the
application of this method will be considered.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1208.2766v1,Topological properties of cellular automata on trees,"We prove that there do not exist positively expansive cellular automata
defined on the full k-ary tree shift (for k>=2). Moreover, we investigate some
topological properties of these automata and their relationships, namely
permutivity, surjectivity, preinjectivity, right-closingness and openness.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0609066v1,"Building and displaying name relations using automatic unsupervised
  analysis of newspaper articles","We present a tool that, from automatically recognised names, tries to infer
inter-person relations in order to present associated people on maps. Based on
an in-house Named Entity Recognition tool, applied on clusters of an average of
15,000 news articles per day, in 15 different languages, we build a knowledge
base that allows extracting statistical co-occurrences of persons and
visualising them on a per-person page or in various graphs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.07664v1,"Privacy-preserving feature selection: A survey and proposing a new set
  of protocols","Feature selection is the process of sieving features, in which informative
features are separated from the redundant and irrelevant ones. This process
plays an important role in machine learning, data mining and bioinformatics.
However, traditional feature selection methods are only capable of processing
centralized datasets and are not able to satisfy today's distributed data
processing needs. These needs require a new category of data processing
algorithms called privacy-preserving feature selection, which protects users'
data by not revealing any part of the data neither in the intermediate
processing nor in the final results. This is vital for the datasets which
contain individuals' data, such as medical datasets. Therefore, it is rational
to either modify the existing algorithms or propose new ones to not only
introduce the capability of being applied to distributed datasets, but also act
responsibly in handling users' data by protecting their privacy. In this paper,
we will review three privacy-preserving feature selection methods and provide
suggestions to improve their performance when any gap is identified. We will
also propose a privacy-preserving feature selection method based on the rough
set feature selection. The proposed method is capable of processing both
horizontally and vertically partitioned datasets in two- and multi-parties
scenarios.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9907029v2,A Probabilistic Analysis of the Power of Arithmetic Filters,"The assumption of real-number arithmetic, which is at the basis of
conventional geometric algorithms, has been seriously challenged in recent
years, since digital computers do not exhibit such capability.
  A geometric predicate usually consists of evaluating the sign of some
algebraic expression. In most cases, rounded computations yield a reliable
result, but sometimes rounded arithmetic introduces errors which may invalidate
the algorithms. The rounded arithmetic may produce an incorrect result only if
the exact absolute value of the algebraic expression is smaller than some
(small) varepsilon, which represents the largest error that may arise in the
evaluation of the expression. The threshold varepsilon depends on the structure
of the expression and on the adopted computer arithmetic, assuming that the
input operands are error-free.
  A pair (arithmetic engine,threshold) is an ""arithmetic filter"". In this paper
we develop a general technique for assessing the efficacy of an arithmetic
filter. The analysis consists of evaluating both the threshold and the
probability of failure of the filter.
  To exemplify the approach, under the assumption that the input points be
chosen randomly in a unit ball or unit cube with uniform density, we analyze
the two important predicates ""which-side"" and ""insphere"". We show that the
probability that the absolute values of the corresponding determinants be no
larger than some positive value V, with emphasis on small V, is Theta(V) for
the which-side predicate, while for the insphere predicate it is Theta(V^(2/3))
in dimension 1, O(sqrt(V)) in dimension 2, and O(sqrt(V) ln(1/V)) in higher
dimensions. Constants are small, and are given in the paper.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.04593v1,Strongvelope Multi-Party Encrypted Messaging Protocol design document,"In this document we describe the design of a multi-party messaging encryption
protocol ""Strongvelope"". We hope that it will prove useful to people interested
in understanding the inner workings of this protocol as well as cryptography
and security experts to review the underlying concepts and assumptions.
  In this design paper we are outlining the perspective of chat message
protection through the Strongvelope module. This is different from the product
(the Mega chat) and the transport means which it will be used with. Aspects of
the chat product and transport are only referred to where appropriate, but are
not subject to discussion in this document.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2109.09262v2,TOGA: A Neural Method for Test Oracle Generation,"Testing is widely recognized as an important stage of the software
development lifecycle. Effective software testing can provide benefits such as
bug finding, preventing regressions, and documentation. In terms of
documentation, unit tests express a unit's intended functionality, as conceived
by the developer. A test oracle, typically expressed as an condition, documents
the intended behavior of a unit under a given test prefix. Synthesizing a
functional test oracle is a challenging problem, as it must capture the
intended functionality rather than the implemented functionality.
  In this paper, we propose TOGA (a neural method for Test Oracle GenerAtion),
a unified transformer-based neural approach to infer both exceptional and
assertion test oracles based on the context of the focal method. Our approach
can handle units with ambiguous or missing documentation, and even units with a
missing implementation. We evaluate our approach on both oracle inference
accuracy and functional bug-finding. Our technique improves accuracy by 33\%
over existing oracle inference approaches, achieving 96\% overall accuracy on a
held out test dataset. Furthermore, we show that when integrated with a
automated test generation tool (EvoSuite), our approach finds 57 real world
bugs in large-scale Java programs, including 30 bugs that are not found by any
other automated testing method in our evaluation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1005.0783v2,"Software Requirements Specification of the IUfA's UUIS -- a Team 2
  COMP5541-W10 Project Approach","In the 52-page document, we describe our approach to the Software
Requirements Specification of the IUfA's UUIS prototype. This includes the
overall system description, functional requirements, non-functional
requirements, use cases, the corresponding data dictionary for all entities
involved, mock user interface (UI) design, and the overall projected cost
estimate. The design specification of UUIS can be found in arXiv:1005.0665.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2112.12313v1,Mean field game for modeling of COVID-19 spread,"The paper presents the one of possible approach to model the epidemic
propagation. The proposed model is based on the mean-field control inside
separate groups of population, namely, suspectable (S), infected (I), removed
(R) and cross-immune (C). In the paper the numerical algorithm to solve such a
problem is presented, which ensures the conservation the total mass of
population during timeline. Numerical experiments demonstrate the result of
modelling the propagation of COVID-19 virus during two 100 day periods in
Novosibirsk (Russia).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0412084v1,Map Segmentation by Colour Cube Genetic K-Mean Clustering,"Segmentation of a colour image composed of different kinds of texture regions
can be a hard problem, namely to compute for an exact texture fields and a
decision of the optimum number of segmentation areas in an image when it
contains similar and/or unstationary texture fields. In this work, a method is
described for evolving adaptive procedures for these problems. In many real
world applications data clustering constitutes a fundamental issue whenever
behavioural or feature domains can be mapped into topological domains. We
formulate the segmentation problem upon such images as an optimisation problem
and adopt evolutionary strategy of Genetic Algorithms for the clustering of
small regions in colour feature space. The present approach uses k-Means
unsupervised clustering methods into Genetic Algorithms, namely for guiding
this last Evolutionary Algorithm in his search for finding the optimal or
sub-optimal data partition, task that as we know, requires a non-trivial search
because of its NP-complete nature. To solve this task, the appropriate genetic
coding is also discussed, since this is a key aspect in the implementation. Our
purpose is to demonstrate the efficiency of Genetic Algorithms to automatic and
unsupervised texture segmentation. Some examples in Colour Maps are presented
and overall results discussed. KEYWORDS: Genetic Algorithms, Artificial
Neoteny, Dynamic Mutation Rates, Faster Convergence, Colour Image Segmentation,
Classification, Clustering.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.05861v4,"ArCOV-19: The First Arabic COVID-19 Twitter Dataset with Propagation
  Networks","In this paper, we present ArCOV-19, an Arabic COVID-19 Twitter dataset that
spans one year, covering the period from 27th of January 2020 till 31st of
January 2021. ArCOV-19 is the first publicly-available Arabic Twitter dataset
covering COVID-19 pandemic that includes about 2.7M tweets alongside the
propagation networks of the most-popular subset of them (i.e., most-retweeted
and -liked). The propagation networks include both retweets and conversational
threads (i.e., threads of replies). ArCOV-19 is designed to enable research
under several domains including natural language processing, information
retrieval, and social computing. Preliminary analysis shows that ArCOV-19
captures rising discussions associated with the first reported cases of the
disease as they appeared in the Arab world. In addition to the source tweets
and propagation networks, we also release the search queries and
language-independent crawler used to collect the tweets to encourage the
curation of similar datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1707.02344v7,Distribution Bisimilarity via the Power of Convex Algebras,"Probabilistic automata (PA), also known as probabilistic nondeterministic
labelled transition systems, combine probability and nondeterminism. They can
be given different semantics, like strong bisimilarity, convex bisimilarity, or
(more recently) distribution bisimilarity. The latter is based on the view of
PA as transformers of probability distributions, also called belief states, and
promotes distributions to first-class citizens.
  We give a coalgebraic account of distribution bisimilarity, and explain the
genesis of the belief-state transformer from a PA. To do so, we make explicit
the convex algebraic structure present in PA and identify belief-state
transformers as transition systems with state space that carries a convex
algebra. As a consequence of our abstract approach, we can give a sound proof
technique which we call bisimulation up-to convex hull.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1303.7122v2,"On the Complexity of the Decisive Problem in Simple, Regular and
  Weighted Games","We study the computational complexity of an important property of simple,
regular and weighted games, which is decisiveness. We show that this concept
can naturally be represented in the context of hypergraph theory, and that
decisiveness can be decided for simple games in quasi-polynomial time, and for
regular and weighted games in polynomial time. The strongness condition poses
the main difficulties, while properness reduces the complexity of the problem,
especially if it is amplified by regularity. On the other hand, regularity also
allows to specify the problem instances much more economically, implying a
reconsideration of the corresponding complexity measure that, as we prove, has
important structural as well as algorithmic consequences.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1310.3741v1,Evaluating parametric holonomic sequences using rectangular splitting,"We adapt the rectangular splitting technique of Paterson and Stockmeyer to
the problem of evaluating terms in holonomic sequences that depend on a
parameter. This approach allows computing the $n$-th term in a recurrent
sequence of suitable type using $O(n^{1/2})$ ""expensive"" operations at the cost
of an increased number of ""cheap"" operations.
  Rectangular splitting has little overhead and can perform better than either
naive evaluation or asymptotically faster algorithms for ranges of $n$
encountered in applications. As an example, fast numerical evaluation of the
gamma function is investigated. Our work generalizes two previous algorithms of
Smith.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1401.1347v1,MANIAC Challenge: The Wolf-pack strategy,"MANIAC Challenge raises a problem of game theory, different players
strategies intertwine and the success of any player is dependent on the actions
of all players in the system. A truly fair scenario is when all the strategies
are identical, all the nodes co-operate and they all equally share the rewards
and risks that come with every transfer. A successful strategy is one that
tries to diverge from the equilibrium to maximize its own gains and it manages
to do so. We propose the wolf-pack strategy. Unlike standard game-theory based
strategies, our strategy does not penalize the nodes that diverge from fairness
or from equilibrium, as we believe most nodes will do so in an attempt to get
an advantage over the other nodes. The wolf-pack strategy will try to always
find the most successful node or nodes and penalize them. We believe that just
like in nature, a number of small predators can take down the bigger, more
profitable ones. Furthermore during the Challenge we test two different
strategies that provide completely opposite results. These offer a clear
picture of what the best strategy is and the problems of the current system.",0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1503.04911v1,Typing Classes and Mixins with Intersection Types,"We study an assignment system of intersection types for a lambda-calculus
with records and a record-merge operator, where types are preserved both under
subject reduction and expansion. The calculus is expressive enough to naturally
represent mixins as functions over recursively defined classes, whose fixed
points, the objects, are recursive records. In spite of the double recursion
that is involved in their definition, classes and mixins can be meaningfully
typed without resorting to neither recursive nor F-bounded polymorphic types.
  We then adapt mixin construct and composition to Java and C#, relying solely
on existing features in such a way that the resulting code remains typable in
the respective type systems. We exhibit some example code, and study its
typings in the intersection type system via interpretation into the
lambda-calculus with records we have proposed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1710.09934v1,"Data-driven Feature Sampling for Deep Hyperspectral Classification and
  Segmentation","The high dimensionality of hyperspectral imaging forces unique challenges in
scope, size and processing requirements. Motivated by the potential for an
in-the-field cell sorting detector, we examine a $\textit{Synechocystis sp.}$
PCC 6803 dataset wherein cells are grown alternatively in nitrogen rich or
deplete cultures. We use deep learning techniques to both successfully classify
cells and generate a mask segmenting the cells/condition from the background.
Further, we use the classification accuracy to guide a data-driven, iterative
feature selection method, allowing the design neural networks requiring 90%
fewer input features with little accuracy degradation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1904.08233v1,"Performance Analysis of Linear Algebraic Functions using Reconfigurable
  Computing","This paper introduces a new mapping of geometrical transformation on the
MorphoSys (M1) reconfigurable computing (RC) system. New mapping techniques for
some linear algebraic functions are recalled. A new mapping for geometrical
transformation operations is introduced and their performance on the M1 system
is evaluated. The translation and scaling transformation addressed in this
mapping employ some vector-vector and vector-scalar operations [6-7]. A
performance analysis study of the M1 RC system is also presented to evaluate
the efficiency of the algorithm execution. Numerical examples were simulated to
validate our results, using the MorphoSys mULATE program, which emulates M1
operations.",0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.08777v3,"An asymptotic analysis of probabilistic logic programming, with
  implications for expressing projective families of distributions","Probabilistic logic programming is a major part of statistical relational
artificial intelligence, where approaches from logic and probability are
brought together to reason about and learn from relational domains in a setting
of uncertainty. However, the behaviour of statistical relational
representations across variable domain sizes is complex, and scaling inference
and learning to large domains remains a significant challenge. In recent years,
connections have emerged between domain size dependence, lifted inference and
learning from sampled subpopulations. The asymptotic behaviour of statistical
relational representations has come under scrutiny, and projectivity was
investigated as the strongest form of domain-size dependence, in which query
marginals are completely independent of the domain size.
  In this contribution we show that every probabilistic logic program under the
distribution semantics is asymptotically equivalent to an acyclic probabilistic
logic program consisting only of determinate clauses over probabilistic facts.
We conclude that every probabilistic logic program inducing a projective family
of distributions is in fact everywhere equivalent to a program from this
fragment, and we investigate the consequences for the projective families of
distributions expressible by probabilistic logic programs.
  To facilitate the application of classical results from finite model theory,
we introduce the abstract distribution semantics, defined as an arbitrary
logical theory over probabilistic facts. This bridges the gap to the
distribution semantics underlying probabilistic logic programming. In this
representation, determinate logic programs correspond to quantifier-free
theories, making asymptotic quantifier elimination results available for the
setting of probabilistic logic programming.
  This paper is under consideration for acceptance in TPLP.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.09870v1,"Visualizing Uncertainty in Probabilistic Graphs with Network
  Hypothetical Outcome Plots (NetHOPs)","Probabilistic graphs are challenging to visualize using the traditional
node-link diagram. Encoding edge probability using visual variables like width
or fuzziness makes it difficult for users of static network visualizations to
estimate network statistics like densities, isolates, path lengths, or
clustering under uncertainty. We introduce Network Hypothetical Outcome Plots
(NetHOPs), a visualization technique that animates a sequence of network
realizations sampled from a network distribution defined by probabilistic
edges. NetHOPs employ an aggregation and anchoring algorithm used in dynamic
and longitudinal graph drawing to parameterize layout stability for uncertainty
estimation. We present a community matching algorithm to enable visualizing the
uncertainty of cluster membership and community occurrence. We describe the
results of a study in which 51 network experts used NetHOPs to complete a set
of common visual analysis tasks and reported how they perceived network
structures and properties subject to uncertainty. Participants' estimates fell,
on average, within 11% of the ground truth statistics, suggesting NetHOPs can
be a reasonable approach for enabling network analysts to reason about multiple
properties under uncertainty. Participants appeared to articulate the
distribution of network statistics slightly more accurately when they could
manipulate the layout anchoring and the animation speed. Based on these
findings, we synthesize design recommendations for developing and using
animated visualizations for probabilistic networks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1310.3195v1,Ehrenfeucht-Fraisse Games on Omega-Terms,"Fragments of first-order logic over words can often be characterized in terms
of finite monoids or finite semigroups. Usually these algebraic descriptions
yield decidability of the question whether a given regular language is
definable in a particular fragment. An effective algebraic characterization can
be obtained from identities of so-called omega-terms. In order to show that a
given fragment satisfies some identity of omega-terms, one can use
Ehrenfeucht-Fraisse games on word instances of the omega-terms. The resulting
proofs often require a significant amount of book-keeping with respect to the
constants involved. In this paper we introduce Ehrenfeucht-Fraisse games on
omega-terms. To this end we assign a labeled linear order to every omega-term.
Our main theorem shows that a given fragment satisfies some identity of
omega-terms if and only if Duplicator has a winning strategy for the game on
the resulting linear orders. This allows to avoid the book-keeping. As an
application of our main result, we show that one can decide in exponential time
whether all aperiodic monoids satisfy some given identity of omega-terms,
thereby improving a result of McCammond (Int. J. Algebra Comput., 2001).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.00463v3,Single-shot Hyperspectral-Depth Imaging with Learned Diffractive Optics,"Imaging depth and spectrum have been extensively studied in isolation from
each other for decades. Recently, hyperspectral-depth (HS-D) imaging emerges to
capture both information simultaneously by combining two different imaging
systems; one for depth, the other for spectrum. While being accurate, this
combinational approach induces increased form factor, cost, capture time, and
alignment/registration problems. In this work, departing from the combinational
principle, we propose a compact single-shot monocular HS-D imaging method. Our
method uses a diffractive optical element (DOE), the point spread function of
which changes with respect to both depth and spectrum. This enables us to
reconstruct spectrum and depth from a single captured image. To this end, we
develop a differentiable simulator and a neural-network-based reconstruction
that are jointly optimized via automatic differentiation. To facilitate
learning the DOE, we present a first HS-D dataset by building a benchtop HS-D
imager that acquires high-quality ground truth. We evaluate our method with
synthetic and real experiments by building an experimental prototype and
achieve state-of-the-art HS-D imaging results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.03876v1,Constant Space and Non-Constant Time in Distributed Computing,"While the relationship of time and space is an established topic in
traditional centralised complexity theory, this is not the case in distributed
computing. We aim to remedy this by studying the time and space complexity of
algorithms in a weak message-passing model of distributed computing. While a
constant number of communication rounds implies a constant number of states
visited during the execution, the other direction is not clear at all. We
consider several graph families and show that indeed, there exist non-trivial
graph problems that are solvable by constant-space algorithms but that require
a non-constant running time. This provides us with a new complexity class for
distributed computing and raises interesting questions about the existence of
further combinations of time and space complexity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1511.04919v2,Tales told by coloured tangles,"Tangle machines are a topologically inspired diagrammatic formalism to
describe information flow in networks. This paper begins with an expository
account of tangle machines motivated by the problem of describing `covariance
intersection' fusion of Gaussian estimators in networks. It then gives two
examples in which tangle machines tell stories of adiabatic quantum
computations, and discusses learning tangle machines from data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0306020v2,"On the Verge of One Petabyte - the Story Behind the BaBar Database
  System","The BaBar database has pioneered the use of a commercial ODBMS within the HEP
community. The unique object-oriented architecture of Objectivity/DB has made
it possible to manage over 700 terabytes of production data generated since
May'99, making the BaBar database the world's largest known database. The
ongoing development includes new features, addressing the ever-increasing
luminosity of the detector as well as other changing physics requirements.
Significant efforts are focused on reducing space requirements and operational
costs. The paper discusses our experience with developing a large scale
database system, emphasizing universal aspects which may be applied to any
large scale system, independently of underlying technology used.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.06503v2,Packaging research artefacts with RO-Crate,"An increasing number of researchers support reproducibility by including
pointers to and descriptions of datasets, software and methods in their
publications. However, scientific articles may be ambiguous, incomplete and
difficult to process by automated systems. In this paper we introduce RO-Crate,
an open, community-driven, and lightweight approach to packaging research
artefacts along with their metadata in a machine readable manner. RO-Crate is
based on Schema$.$org annotations in JSON-LD, aiming to establish best
practices to formally describe metadata in an accessible and practical way for
their use in a wide variety of situations.
  An RO-Crate is a structured archive of all the items that contributed to a
research outcome, including their identifiers, provenance, relations and
annotations. As a general purpose packaging approach for data and their
metadata, RO-Crate is used across multiple areas, including bioinformatics,
digital humanities and regulatory sciences. By applying ""just enough"" Linked
Data standards, RO-Crate simplifies the process of making research outputs FAIR
while also enhancing research reproducibility.
  An RO-Crate for this article is available at
https://w3id.org/ro/doi/10.5281/zenodo.5146227",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0506023v1,Sparse Covariance Selection via Robust Maximum Likelihood Estimation,"We address a problem of covariance selection, where we seek a trade-off
between a high likelihood against the number of non-zero elements in the
inverse covariance matrix. We solve a maximum likelihood problem with a penalty
term given by the sum of absolute values of the elements of the inverse
covariance matrix, and allow for imposing bounds on the condition number of the
solution. The problem is directly amenable to now standard interior-point
algorithms for convex optimization, but remains challenging due to its size. We
first give some results on the theoretical computational complexity of the
problem, by showing that a recent methodology for non-smooth convex
optimization due to Nesterov can be applied to this problem, to greatly improve
on the complexity estimate given by interior-point algorithms. We then examine
two practical algorithms aimed at solving large-scale, noisy (hence dense)
instances: one is based on a block-coordinate descent approach, where columns
and rows are updated sequentially, another applies a dual version of Nesterov's
method.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1607.03443v1,"A Survey about Prediction-Based Data Reduction in Wireless Sensor
  Networks","One of the main characteristics of Wireless Sensor Networks (WSNs) is the
constrained energy resources of their wireless sensor nodes. Although this
issue has been addressed in several works and got a lot of attention within the
years, the most recent advances pointed out that the energy harvesting and
wireless charging techniques may offer means to overcome such a limitation.
Consequently, an issue that had been put in second place, now emerges: the low
availability of spectrum resources. Because of it, the incorporation of the
WSNs into the Internet of Things and the exponential growth of the latter may
be hindered if no control over the data generation is taken. Alternatively,
part of the sensed data can be predicted without triggering transmissions and
congesting the wireless medium. In this work, we analyze and categorize
existing prediction-based data reduction mechanisms that have been designed for
WSNs. Our main contribution is a systematic procedure for selecting a scheme to
make predictions in WSNs, based on WSNs' constraints, characteristics of
prediction methods and monitored data. Finally, we conclude the paper with a
discussion about future challenges and open research directions in the use of
prediction methods to support the WSNs' growth.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.06499v1,LSCP: Enhanced Large Scale Colloquial Persian Language Understanding,"Language recognition has been significantly advanced in recent years by means
of modern machine learning methods such as deep learning and benchmarks with
rich annotations. However, research is still limited in low-resource formal
languages. This consists of a significant gap in describing the colloquial
language especially for low-resourced ones such as Persian. In order to target
this gap for low resource languages, we propose a ""Large Scale Colloquial
Persian Dataset"" (LSCP). LSCP is hierarchically organized in a semantic
taxonomy that focuses on multi-task informal Persian language understanding as
a comprehensive problem. This encompasses the recognition of multiple semantic
aspects in the human-level sentences, which naturally captures from the
real-world sentences. We believe that further investigations and processing, as
well as the application of novel algorithms and methods, can strengthen
enriching computerized understanding and processing of low resource languages.
The proposed corpus consists of 120M sentences resulted from 27M tweets
annotated with parsing tree, part-of-speech tags, sentiment polarity and
translation in five different languages.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.00936v2,"MAUIL: Multi-level Attribute Embedding for Semi-supervised User Identity
  Linkage","User identity linkage (UIL) across social networks has recently attracted an
increasing amount of attention due to its significant research challenges and
practical value. Most of the existing methods use a single method to express
different types of attribute features. However, the simplex pattern can neither
cover the entire set of different attribute features nor capture the
higher-level semantic features in the attribute text. This paper establishes a
novel semisupervised model, namely the multilevel attribute embedding for
semisupervised user identity linkage (MAUIL), to seek the common user identity
across social networks. MAUIL includes two components: multilevel attribute
embedding and regularized canonical correlation analysis (RCCA)-based linear
projection. Specifically, the text attributes for each network are first
divided into three types: character-level, word-level, and topic-level
attributes. Second, unsupervised approaches are employed to extract the
corresponding three types of text attribute features, and user relationships
are embedded as a complimentary feature. All the resultant features are
combined to form the final representation of each user. Finally, target social
networks are projected into a common correlated space by RCCA with the help of
a small number of prematched user pairs. We demonstrate the superiority of the
proposed method over state-of-the-art methods through extensive experiments on
two real-world datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0007014v1,The Sound Manifesto,"Computing practice today depends on visual output to drive almost all user
interaction. Other senses, such as audition, may be totally neglected, or used
tangentially, or used in highly restricted specialized ways. We have excellent
audio rendering through D-A conversion, but we lack rich general facilities for
modeling and manipulating sound comparable in quality and flexibility to
graphics. We need co-ordinated research in several disciplines to improve the
use of sound as an interactive information channel.
  Incremental and separate improvements in synthesis, analysis, speech
processing, audiology, acoustics, music, etc. will not alone produce the
radical progress that we seek in sonic practice. We also need to create a new
central topic of study in digital audio research. The new topic will assimilate
the contributions of different disciplines on a common foundation. The key
central concept that we lack is sound as a general-purpose information channel.
We must investigate the structure of this information channel, which is driven
by the co-operative development of auditory perception and physical sound
production. Particular audible encodings, such as speech and music, illuminate
sonic information by example, but they are no more sufficient for a
characterization than typography is sufficient for a characterization of visual
information.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1011.1783v3,OCamlJIT 2.0 - Faster Objective Caml,"This paper presents the current state of an ongoing research project to
improve the performance of the OCaml byte-code interpreter using Just-In-Time
native code generation. Our JIT engine OCamlJIT2 currently runs on x86-64
processors, mimicing precisely the behavior of the OCaml virtual machine. Its
design and implementation is described, and performance measures are given.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0008009v1,Data Mining to Measure and Improve the Success of Web Sites,"For many companies, competitiveness in e-commerce requires a successful
presence on the web. Web sites are used to establish the company's image, to
promote and sell goods and to provide customer support. The success of a web
site affects and reflects directly the success of the company in the electronic
market. In this study, we propose a methodology to improve the ``success'' of
web sites, based on the exploitation of navigation pattern discovery. In
particular, we present a theory, in which success is modelled on the basis of
the navigation behaviour of the site's users. We then exploit WUM, a navigation
pattern discovery miner, to study how the success of a site is reflected in the
users' behaviour. With WUM we measure the success of a site's components and
obtain concrete indications of how the site should be improved. We report on
our first experiments with an online catalog, the success of which we have
studied. Our mining analysis has shown very promising results, on the basis of
which the site is currently undergoing concrete improvements.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0205050v1,"A Network-Flow Technique for Finding Low-Weight Bounded-Degree Spanning
  Trees","The problem considered is the following. Given a graph with edge weights
satisfying the triangle inequality, and a degree bound for each vertex, compute
a low-weight spanning tree such that the degree of each vertex is at most its
specified bound. The problem is NP-hard (it generalizes Traveling Salesman
(TSP)). This paper describes a network-flow heuristic for modifying a given
tree T to meet the constraints. Choosing T to be a minimum spanning tree (MST)
yields approximation algorithms with performance guarantee less than 2 for the
problem on geometric graphs with L_p-norms. The paper also describes a
Euclidean graph whose minimum TSP costs twice the MST, disproving a conjecture
made in ``Low-Degree Spanning Trees of Small Weight'' (1996).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1206.0377v1,Automated Word Puzzle Generation via Topic Dictionaries,"We propose a general method for automated word puzzle generation. Contrary to
previous approaches in this novel field, the presented method does not rely on
highly structured datasets obtained with serious human annotation effort: it
only needs an unstructured and unannotated corpus (i.e., document collection)
as input. The method builds upon two additional pillars: (i) a topic model,
which induces a topic dictionary from the input corpus (examples include e.g.,
latent semantic analysis, group-structured dictionaries or latent Dirichlet
allocation), and (ii) a semantic similarity measure of word pairs. Our method
can (i) generate automatically a large number of proper word puzzles of
different types, including the odd one out, choose the related word and
separate the topics puzzle. (ii) It can easily create domain-specific puzzles
by replacing the corpus component. (iii) It is also capable of automatically
generating puzzles with parameterizable levels of difficulty suitable for,
e.g., beginners or intermediate learners.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.08180v1,Interpretable Local Tree Surrogate Policies,"High-dimensional policies, such as those represented by neural networks,
cannot be reasonably interpreted by humans. This lack of interpretability
reduces the trust users have in policy behavior, limiting their use to
low-impact tasks such as video games. Unfortunately, many methods rely on
neural network representations for effective learning. In this work, we propose
a method to build predictable policy trees as surrogates for policies such as
neural networks. The policy trees are easily human interpretable and provide
quantitative predictions of future behavior. We demonstrate the performance of
this approach on several simulated tasks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1703.09620v1,"Universal Reasoning, Rational Argumentation and Human-Machine
  Interaction","Classical higher-order logic, when utilized as a meta-logic in which various
other (classical and non-classical) logics can be shallowly embedded, is well
suited for realising a universal logic reasoning approach. Universal logic
reasoning in turn, as envisioned already by Leibniz, may support the rigorous
formalisation and deep logical analysis of rational arguments within machines.
A respective universal logic reasoning framework is described and a range of
exemplary applications are discussed. In the future, universal logic reasoning
in combination with appropriate, controlled forms of rational argumentation may
serve as a communication layer between humans and intelligent machines.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.06513v2,Model Checking Spatial Logics for Closure Spaces,"Spatial aspects of computation are becoming increasingly relevant in Computer
Science, especially in the field of collective adaptive systems and when
dealing with systems distributed in physical space. Traditional formal
verification techniques are well suited to analyse the temporal evolution of
programs; however, properties of space are typically not taken into account
explicitly. We present a topology-based approach to formal verification of
spatial properties depending upon physical space. We define an appropriate
logic, stemming from the tradition of topological interpretations of modal
logics, dating back to earlier logicians such as Tarski, where modalities
describe neighbourhood. We lift the topological definitions to the more general
setting of closure spaces, also encompassing discrete, graph-based structures.
We extend the framework with a spatial surrounded operator, a propagation
operator and with some collective operators. The latter are interpreted over
arbitrary sets of points instead of individual points in space. We define
efficient model checking procedures, both for the individual and the collective
spatial fragments of the logic and provide a proof-of-concept tool.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0309003v1,Model Checking Linear Logic Specifications,"The overall goal of this paper is to investigate the theoretical foundations
of algorithmic verification techniques for first order linear logic
specifications. The fragment of linear logic we consider in this paper is based
on the linear logic programming language called LO enriched with universally
quantified goal formulas. Although LO was originally introduced as a
theoretical foundation for extensions of logic programming languages, it can
also be viewed as a very general language to specify a wide range of
infinite-state concurrent systems.
  Our approach is based on the relation between backward reachability and
provability highlighted in our previous work on propositional LO programs.
Following this line of research, we define here a general framework for the
bottom-up evaluation of first order linear logic specifications. The evaluation
procedure is based on an effective fixpoint operator working on a symbolic
representation of infinite collections of first order linear logic formulas.
The theory of well quasi-orderings can be used to provide sufficient conditions
for the termination of the evaluation of non trivial fragments of first order
linear logic.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0405081v1,An Analysis of Lambek's Production Machines,"Lambek's production machines may be used to generate and recognize sentences
in a subset of the language described by a production grammar. We determine in
this paper the subset of the language of a grammar generated and recognized by
such machines.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0909.4692v1,Planar Subgraph Isomorphism Revisited,"The problem of Subgraph Isomorphism is defined as follows: Given a pattern H
and a host graph G on n vertices, does G contain a subgraph that is isomorphic
to H? Eppstein [SODA 95, J'GAA 99] gives the first linear time algorithm for
subgraph isomorphism for a fixed-size pattern, say of order k, and arbitrary
planar host graph, improving upon the O(n^\sqrt{k})-time algorithm when using
the ``Color-coding'' technique of Alon et al [J'ACM 95]. Eppstein's algorithm
runs in time k^O(k) n, that is, the dependency on k is superexponential. We
solve an open problem posed in Eppstein's paper and improve the running time to
2^O(k) n, that is, single exponential in k while keeping the term in n linear.
Next to deciding subgraph isomorphism, we can construct a solution and
enumerate all solutions in the same asymptotic running time. We may list w
subgraphs with an additive term O(w k) in the running time of our algorithm. We
introduce the technique of ""embedded dynamic programming"" on a suitably
structured graph decomposition, which exploits the topology of the underlying
embeddings of the subgraph pattern (rather than of the host graph). To achieve
our results, we give an upper bound on the number of partial solutions in each
dynamic programming step as a function of pattern size--as it turns out, for
the planar subgraph isomorphism problem, that function is single exponential in
the number of vertices in the pattern.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.13512v1,Add a SideNet to your MainNet,"As the performance and popularity of deep neural networks has increased, so
too has their computational cost. There are many effective techniques for
reducing a network's computational footprint (quantisation, pruning, knowledge
distillation), but these lead to models whose computational cost is the same
regardless of their input. Our human reaction times vary with the complexity of
the tasks we perform: easier tasks (e.g. telling apart dogs from boat) are
executed much faster than harder ones (e.g. telling apart two similar looking
breeds of dogs). Driven by this observation, we develop a method for adaptive
network complexity by attaching a small classification layer, which we call
SideNet, to a large pretrained network, which we call MainNet. Given an input,
the SideNet returns a classification if its confidence level, obtained via
softmax, surpasses a user determined threshold, and only passes it along to the
large MainNet for further processing if its confidence is too low. This allows
us to flexibly trade off the network's performance with its computational cost.
Experimental results show that simple single hidden layer perceptron SideNets
added onto pretrained ResNet and BERT MainNets allow for substantial decreases
in compute with minimal drops in performance on image and text classification
tasks. We also highlight three other desirable properties of our method, namely
that the classifications obtained by SideNets are calibrated, complementary to
other compute reduction techniques, and that they enable the easy exploration
of compute accuracy space.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1402.2664v3,Network-Based Vertex Dissolution,"We introduce a graph-theoretic vertex dissolution model that applies to a
number of redistribution scenarios such as gerrymandering in political
districting or work balancing in an online situation. The central aspect of our
model is the deletion of certain vertices and the redistribution of their load
to neighboring vertices in a completely balanced way.
  We investigate how the underlying graph structure, the knowledge of which
vertices should be deleted, and the relation between old and new vertex loads
influence the computational complexity of the underlying graph problems. Our
results establish a clear borderline between tractable and intractable cases.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1805.00982v2,k-SVRG: Variance Reduction for Large Scale Optimization,"Variance reduced stochastic gradient (SGD) methods converge significantly
faster than the vanilla SGD counterpart. However, these methods are not very
practical on large scale problems, as they either i) require frequent passes
over the full data to recompute gradients---without making any progress during
this time (like for SVRG), or ii)~they require additional memory that can
surpass the size of the input problem (like for SAGA). In this work, we propose
$k$-SVRG that addresses these issues by making best use of the \emph{available}
memory and minimizes the stalling phases without progress. We prove linear
convergence of $k$-SVRG on strongly convex problems and convergence to
stationary points on non-convex problems. Numerical experiments show the
effectiveness of our method.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.02002v1,"The Key Elements of Viral Advertising. From Motivation to Emotion in the
  Most Shared Videos","From its origins in the mid 90s, the application of the concept of virality
to commercial communication has represented an opportunity for brands to cross
the traditional barriers of the audience concerning advertising and turn it
into active communicator of brand messages. Viral marketing is based, since
then, on two basic principles: offer free and engaging content that mask its
commercial purpose to the individual and using a peer-to-peer dissemination
system. The transformation of the passive spectator into an active user who
broadcasts advertising messages promoted by sponsors, responds to needs and
motivations of individuals and content features which have been described by
previous research in this field, mainly through quantitative methods based on
user perceptions. This paper focusses on those elements detected in its
previous research as promoters of the sharing action in the 25 most shared
viral video ads between 2006 and 2013 using content analysis. The results
obtained show the most common features in these videos and the prominent
presence of surprise and joy as dominant emotions in the most successful viral
videos.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1707.07857v1,"Motion-Appearance Interactive Encoding for Object Segmentation in
  Unconstrained Videos","We present a novel method of integrating motion and appearance cues for
foreground object segmentation in unconstrained videos. Unlike conventional
methods encoding motion and appearance patterns individually, our method puts
particular emphasis on their mutual assistance. Specifically, we propose using
an interactively constrained encoding (ICE) scheme to incorporate motion and
appearance patterns into a graph that leads to a spatiotemporal energy
optimization. The reason of utilizing ICE is that both motion and appearance
cues for the same target share underlying correlative structure, thus can be
exploited in a deeply collaborative manner. We perform ICE not only in the
initialization but also in the refinement stage of a two-layer framework for
object segmentation. This scheme allows our method to consistently capture
structural patterns about object perceptions throughout the whole framework.
Our method can be operated on superpixels instead of raw pixels to reduce the
number of graph nodes by two orders of magnitude. Moreover, we propose to
partially explore the multi-object localization problem with inter-occlusion by
weighted bipartite graph matching. Comprehensive experiments on three benchmark
datasets (i.e., SegTrack, MOViCS, and GaTech) demonstrate the effectiveness of
our approach compared with extensive state-of-the-art methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0503078v1,"Obtaining Membership Functions from a Neuron Fuzzy System extended by
  Kohonen Network","This article presents the Neo-Fuzzy-Neuron Modified by Kohonen Network
(NFN-MK), an hybrid computational model that combines fuzzy system technique
and artificial neural networks. Its main task consists in the automatic
generation of membership functions, in particular, triangle forms, aiming a
dynamic modeling of a system. The model is tested by simulating real systems,
here represented by a nonlinear mathematical function. Comparison with the
results obtained by traditional neural networks, and correlated studies of
neurofuzzy systems applied in system identification area, shows that the NFN-MK
model has a similar performance, despite its greater simplicity.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1311.4201v1,"Sound and Precise Malware Analysis for Android via Pushdown Reachability
  and Entry-Point Saturation","We present Anadroid, a static malware analysis framework for Android apps.
Anadroid exploits two techniques to soundly raise precision: (1) it uses a
pushdown system to precisely model dynamically dispatched interprocedural and
exception-driven control-flow; (2) it uses Entry-Point Saturation (EPS) to
soundly approximate all possible interleavings of asynchronous entry points in
Android applications. (It also integrates static taint-flow analysis and least
permissions analysis to expand the class of malicious behaviors which it can
catch.) Anadroid provides rich user interface support for human analysts which
must ultimately rule on the ""maliciousness"" of a behavior.
  To demonstrate the effectiveness of Anadroid's malware analysis, we had teams
of analysts analyze a challenge suite of 52 Android applications released as
part of the Auto- mated Program Analysis for Cybersecurity (APAC) DARPA
program. The first team analyzed the apps using a ver- sion of Anadroid that
uses traditional (finite-state-machine-based) control-flow-analysis found in
existing malware analysis tools; the second team analyzed the apps using a
version of Anadroid that uses our enhanced pushdown-based
control-flow-analysis. We measured machine analysis time, human analyst time,
and their accuracy in flagging malicious applications. With pushdown analysis,
we found statistically significant (p < 0.05) decreases in time: from 85
minutes per app to 35 minutes per app in human plus machine analysis time; and
statistically significant (p < 0.05) increases in accuracy with the
pushdown-driven analyzer: from 71% correct identification to 95% correct
identification.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0905.2449v1,"The Role of Self-Forensics in Vehicle Crash Investigations and Event
  Reconstruction","This paper further introduces and formalizes a novel concept of
self-forensics for automotive vehicles, specified in the Forensic Lucid
language. We argue that self-forensics, with the forensics taken out of the
cybercrime domain, is applicable to ""self-dissection"" of intelligent vehicles
and hardware systems for automated incident and anomaly analysis and event
reconstruction by the software with or without the aid of the engineering teams
in a variety of forensic scenarios. We propose a formal design, requirements,
and specification of the self-forensic enabled units (similar to blackboxes) in
vehicles that will help investigation of incidents and also automated reasoning
and verification of theories along with the events reconstruction in a formal
model. We argue such an analysis is beneficial to improve the safety of the
passengers and their vehicles, like the airline industry does for planes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.14127v2,"On the molecular mechanism behind the bubble rise velocity jump
  discontinuity in viscoelastic liquids","Bubbles rising in viscoelastic liquids may exhibit a jump discontinuity of
the rise velocity as a critical bubble volume is exceeded. The phenomenon has
been extensively investigated in the literature, both by means of experiments
as well as via numerical simulations. The occurrence of the velocity jump has
been associated with a change of the bubble shape under the formation of a
pointed tip at the rear end and to the appearance of a so-called negative wake
with the liquid velocity behind the bubble, pointing in the opposite direction
to that in viscous Newtonian fluids. We revisit this topic, starting with a
review of the state of knowledge on the interrelations between the mentioned
characteristic features. In search for a convincing explanation of the jump
phenomenon, we performed detailed numerical simulations of the transient rise
of single bubbles in 3D, allowing for a local polymer molecular conformation
tensor analysis. The latter shows that polymer molecules traveling along the
upper bubble hemisphere are stretched in the circumferential direction, due to
the flow kinematics. Then, depending on the relaxation time scale of the
polymer, the stored elastic energy is either unloaded essentially above or
below the bubble's equator. In the former case, this slows down the bubble,
while the bubble gets accelerated otherwise. In this latter case, the relative
velocity of the polymer molecules against the bubble is increased, giving rise
to a self-amplification of the effect and thus causing the bubble rise velocity
to jump to a higher level. Detailed experimental velocity measurements in the
liquid field around the bubble confirmed the conclusion that the ratio of the
time scale of the Lagrangian transport of polymer molecules along the bubble
contour to the relaxation time scale of the polymer molecules determines the
sub- or supercritical state of the bubble motion.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0803.2675v4,Digital Ecosystems: Self-Organisation of Evolving Agent Populations,"A primary motivation for our research in Digital Ecosystems is the desire to
exploit the self-organising properties of biological ecosystems. Ecosystems are
thought to be robust, scalable architectures that can automatically solve
complex, dynamic problems. Self-organisation is perhaps one of the most
desirable features in the systems that we engineer, and it is important for us
to be able to measure self-organising behaviour. We investigate the
self-organising aspects of Digital Ecosystems, created through the application
of evolutionary computing to Multi-Agent Systems (MASs), aiming to determine a
macroscopic variable to characterise the self-organisation of the evolving
agent populations within. We study a measure for the self-organisation called
Physical Complexity; based on statistical physics, automata theory, and
information theory, providing a measure of information relative to the
randomness in an organism's genome, by calculating the entropy in a population.
We investigate an extension to include populations of variable length, and then
built upon this to construct an efficiency measure to investigate clustering
within evolving agent populations. Overall an insight has been achieved into
where and how self-organisation occurs in our Digital Ecosystem, and how it can
be quantified.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1501.06137v1,"Building Bridges into the Unknown: Personalizing Connections to
  Little-known Countries","How are you related to Malawi? Do recent events on the Comoros effect you in
any subtle way? Who in your extended social network is in Croatia? We seldom
ask ourselves these questions, yet a ""long tail"" of content beyond our everyday
knowledge is waiting to be explored. In this work we propose a recommendation
task of creating interest in little-known content by building personalized
""bridges"" to users. We consider an example task of interesting users in
little-known countries, and propose a system which aggregates a user's Twitter
profile, network, and tweets to create an interest model, which is then matched
to a library of knowledge about the countries. We perform a user study of 69
participants and conduct 11 in-depth interviews in order to evaluate the
efficacy of the proposed approach and gather qualitative insight into the
effect of multi-faceted use of Twitter on the perception of the bridges. We
find the increase in interest concerning little-known content to greatly depend
on the pre-existing disposition to it. Additionally, we discover a set of vital
properties good bridges must possess, including recency, novelty, emotiveness,
and a proper selection of language. Using the proposed approach we aim to
harvest the ""invisible connections"" to make explicit the idea of a ""small
world"" where even a faraway country is more closely connected to you than you
might have imagined.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0308022v1,"Extending Dublin Core Metadata to Support the Description and Discovery
  of Language Resources","As language data and associated technologies proliferate and as the language
resources community expands, it is becoming increasingly difficult to locate
and reuse existing resources. Are there any lexical resources for such-and-such
a language? What tool works with transcripts in this particular format? What is
a good format to use for linguistic data of this type? Questions like these
dominate many mailing lists, since web search engines are an unreliable way to
find language resources. This paper reports on a new digital infrastructure for
discovering language resources being developed by the Open Language Archives
Community (OLAC). At the core of OLAC is its metadata format, which is designed
to facilitate description and discovery of all kinds of language resources,
including data, tools, or advice. The paper describes OLAC metadata, its
relationship to Dublin Core metadata, and its dissemination using the metadata
harvesting protocol of the Open Archives Initiative.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.07129v2,Autoencoders as Tools for Program Synthesis,"Recently there have been many advances in research on language modeling of
source code. Applications range from code suggestion and completion to code
summarization. However, complete program synthesis of industry-grade
programming languages remains an open problem. In this work, we introduce and
experimentally validate a variational autoencoder model for program synthesis
of industry-grade programming languages. This model makes use of the inherent
tree structure of code and can be used in conjunction with gradient free
optimization techniques like evolutionary methods to generate programs that
maximize a given fitness function, for instance, passing a set of test cases. A
demonstration is avaliable at https://tree2tree.app",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1806.01524v1,Construction of all-in-focus images assisted by depth sensing,"Multi-focus image fusion is a technique for obtaining an all-in-focus image
in which all objects are in focus to extend the limited depth of field (DoF) of
an imaging system. Different from traditional RGB-based methods, this paper
presents a new multi-focus image fusion method assisted by depth sensing. In
this work, a depth sensor is used together with a color camera to capture
images of a scene. A graph-based segmentation algorithm is used to segment the
depth map from the depth sensor, and the segmented regions are used to guide a
focus algorithm to locate in-focus image blocks from among multi-focus source
images to construct the reference all-in-focus image. Five test scenes and six
evaluation metrics were used to compare the proposed method and representative
state-of-the-art algorithms. Experimental results quantitatively demonstrate
that this method outperforms existing methods in both speed and quality (in
terms of comprehensive fusion metrics). The generated images can potentially be
used as reference all-in-focus images.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.08852v7,On 1-factorizations of Bipartite Kneser Graphs,"It is a challenging open problem to construct an explicit 1-factorization of
the bipartite Kneser graph $H(v,t)$, which contains as vertices all $t$-element
and $(v-t)$-element subsets of $[v]:=\{1,\ldots,v\}$ and an edge between any
two vertices when one is a subset of the other. In this paper, we propose a new
framework for designing such 1-factorizations, by which we solve a nontrivial
case where $t=2$ and $v$ is an odd prime power. We also revisit two classic
constructions for the case $v=2t+1$ --- the \emph{lexical factorization} and
\emph{modular factorization}. We provide their simplified definitions and study
their inner structures. As a result, an optimal algorithm is designed for
computing the lexical factorizations. (An analogous algorithm for the modular
factorization is trivial.)",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0802.3940v1,Spreadsheet Structure Discovery with Logic Programming,"Our term ""structure discovery"" denotes the recovery of structure, such as the
grouping of cells, that was intended by a spreadsheet's author but is not
explicit in the spreadsheet. We are implementing structure discovery tools in
the logic-programming language Prolog for our spreadsheet analysis program
Model Master, by writing grammars for spreadsheet structures. The objective is
an ""intelligent structure monitor"" to run beside Excel, allowing users to
reconfigure spreadsheets to the representational needs of the task at hand.
This could revolutionise spreadsheet ""best practice"". We also describe a
formulation of spreadsheet reverse-engineering based on ""arrows"".",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1
http://arxiv.org/abs/cs/0005026v1,"A One-Time Pad based Cipher for Data Protection in Distributed
  Environments","A one-time pad (OTP) based cipher to insure both data protection and
integrity when mobile code arrives to a remote host is presented. Data
protection is required when a mobile agent could retrieve confidential
information that would be encrypted in untrusted nodes of the network; in this
case, information management could not rely on carrying an encryption key. Data
integrity is a prerequisite because mobile code must be protected against
malicious hosts that, by counterfeiting or removing collected data, could cover
information to the server that has sent the agent. The algorithm described in
this article seems to be simple enough, so as to be easily implemented. This
scheme is based on a non-interactive protocol and allows a remote host to
change its own data on-the-fly and, at the same time, protecting information
against handling by other hosts.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.04578v2,"Optimal-size problem kernels for $d$-Hitting Set in linear time and
  space","The known linear-time kernelizations for $d$-Hitting Set guarantee linear
worst-case running times using a quadratic-size data structure (that is not
fully initialized). Getting rid of this data structure, we show that problem
kernels of asymptotically optimal size $O(k^d)$ for $d$-Hitting Set are
computable in linear time and space. Additionally, we experimentally compare
the linear-time kernelizations for $d$-Hitting Set to each other and to a
classical data reduction algorithm due to Weihe.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1004.3165v5,"The space complexity of recognizing well-parenthesized expressions in
  the streaming model: the Index function revisited","We show an Omega(sqrt{n}/T) lower bound for the space required by any
unidirectional constant-error randomized T-pass streaming algorithm that
recognizes whether an expression over two types of parenthesis is
well-parenthesized. This proves a conjecture due to Magniez, Mathieu, and Nayak
(2009) and rigorously establishes that bidirectional streams are exponentially
more efficient in space usage as compared with unidirectional ones. We obtain
the lower bound by establishing the minimum amount of information that is
necessarily revealed by the players about their respective inputs in a
two-party communication protocol for a variant of the Index function, namely
Augmented Index. The information cost trade-off is obtained by a novel
application of the conceptually simple and familiar ideas such as average
encoding and the cut-and-paste property of randomized protocols.
  Motivated by recent examples of exponential savings in space by streaming
quantum algorithms, we also study quantum protocols for Augmented Index.
Defining an appropriate notion of information cost for quantum protocols
involves a delicate balancing act between its applicability and the ease with
which we can analyze it. We define a notion of quantum information cost which
reflects some of the non-intuitive properties of quantum information and give a
trade-off for this notion. While this trade-off demonstrates the strength of
our proof techniques, it does not lead to a space lower bound for checking
parentheses. We leave such an implication for quantum streaming algorithms as
an intriguing open question.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.4925v1,Cylindrical Algebraic Decomposition Using Local Projections,"We present an algorithm which computes a cylindrical algebraic decomposition
of a semialgebraic set using projection sets computed for each cell separately.
Such local projection sets can be significantly smaller than the global
projection set used by the Cylindrical Algebraic Decomposition (CAD) algorithm.
This leads to reduction in the number of cells the algorithm needs to
construct. We give an empirical comparison of our algorithm and the classical
CAD algorithm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1808.05379v1,Computer Modeling of Personal Autonomy and Legal Equilibrium,"Empirical studies of personal autonomy as state and status of individual
freedom, security, and capacity to control own life, particularly by
independent legal reasoning, are need dependable models and methods of precise
computation. Three simple models of personal autonomy are proposed. The linear
model of personal autonomy displays a relation between freedom as an amount of
agent's action and responsibility as an amount of legal reaction and shows
legal equilibrium, the balance of rights and duties needed for sustainable
development of any community. The model algorithm of judge personal autonomy
shows that judicial decision making can be partly automated, like other human
jobs. Model machine learning of autonomous lawyer robot under operating system
constitution illustrates the idea of robot rights. Robots, i.e. material and
virtual mechanisms serving the people, deserve some legal guarantees of their
rights such as robot rights to exist, proper function and be protected by the
law. Robots, actually, are protected as any human property by the wide scope of
laws, starting with Article 17 of Universal Declaration of Human Rights, but
the current level of human trust in autonomous devices and their role in
contemporary society needs stronger legislation to guarantee the robot rights.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.06840v1,"The Conference Paper Assignment Problem: Using Order Weighted Averages
  to Assign Indivisible Goods","Motivated by the common academic problem of allocating papers to referees for
conference reviewing we propose a novel mechanism for solving the assignment
problem when we have a two sided matching problem with preferences from one
side (the agents/reviewers) over the other side (the objects/papers) and both
sides have capacity constraints. The assignment problem is a fundamental
problem in both computer science and economics with application in many areas
including task and resource allocation. We draw inspiration from multi-criteria
decision making and voting and use order weighted averages (OWAs) to propose a
novel and flexible class of algorithms for the assignment problem. We show an
algorithm for finding a $\Sigma$-OWA assignment in polynomial time, in contrast
to the NP-hardness of finding an egalitarian assignment. Inspired by this
setting we observe an interesting connection between our model and the classic
proportional multi-winner election problem in social choice.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.01315v1,"FC-based shock-dynamics solver with neural-network localized
  artificial-viscosity assignment","This paper presents a spectral scheme for the numerical solution of nonlinear
conservation laws in non-periodic domains under arbitrary boundary conditions.
The approach relies on the use of the Fourier Continuation (FC) method for
spectral representation of non-periodic functions in conjunction with smooth
localized artificial viscosity assignments produced by means of a
Shock-Detecting Neural Network (SDNN). Like previous shock capturing schemes
and artificial viscosity techniques, the combined FC-SDNN strategy effectively
controls spurious oscillations in the proximity of discontinuities. Thanks to
its use of a localized but smooth artificial viscosity term, whose support is
restricted to a vicinity of flow-discontinuity points, the algorithm enjoys
spectral accuracy and low dissipation away from flow discontinuities, and, in
such regions, it produces smooth numerical solutions -- as evidenced by an
essential absence of spurious oscillations in level set lines. The FC-SDNN
viscosity assignment, which does not require use of problem-dependent
algorithmic parameters, induces a significantly lower overall dissipation than
other methods, including the Fourier-spectral versions of the previous entropy
viscosity method. The character of the proposed algorithm is illustrated with a
variety of numerical results for the linear advection, Burgers and Euler
equations in one and two-dimensional non-periodic spatial domains.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1002.0561v1,Individual focus and knowledge contribution,"Before contributing new knowledge, individuals must attain requisite
background knowledge or skills through schooling, training, practice, and
experience. Given limited time, individuals often choose either to focus on few
areas, where they build deep expertise, or to delve less deeply and distribute
their attention and efforts across several areas. In this paper we measure the
relationship between the narrowness of focus and the quality of contribution
across a range of both traditional and recent knowledge sharing media,
including scholarly articles, patents, Wikipedia, and online question and
answer forums. Across all systems, we observe a small but significant positive
correlation between focus and quality.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2005.12150v2,"Design of a dynamic and self adapting system, supported with artificial
  intelligence, machine learning and real time intelligence for predictive
  cyber risk analytics in extreme environments, cyber risk in the colonisation
  of Mars","Multiple governmental agencies and private organisations have made
commitments for the colonisation of Mars. Such colonisation requires complex
systems and infrastructure that could be very costly to repair or replace in
cases of cyber attacks. This paper surveys deep learning algorithms, IoT cyber
security and risk models, and established mathematical formulas to identify the
best approach for developing a dynamic and self adapting system for predictive
cyber risk analytics supported with Artificial Intelligence and Machine
Learning and real time intelligence in edge computing. The paper presents a new
mathematical approach for integrating concepts for cognition engine design,
edge computing and Artificial Intelligence and Machine Learning to automate
anomaly detection. This engine instigates a step change by applying Artificial
Intelligence and Machine Learning embedded at the edge of IoT networks, to
deliver safe and functional real time intelligence for predictive cyber risk
analytics. This will enhance capacities for risk analytics and assists in the
creation of a comprehensive and systematic understanding of the opportunities
and threats that arise when edge computing nodes are deployed, and when
Artificial Intelligence and Machine Learning technologies are migrated to the
periphery of the internet and into local IoT networks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1105.2397v1,"Incremental Cycle Detection, Topological Ordering, and Strong Component
  Maintenance","We present two on-line algorithms for maintaining a topological order of a
directed $n$-vertex acyclic graph as arcs are added, and detecting a cycle when
one is created. Our first algorithm handles $m$ arc additions in $O(m^{3/2})$
time. For sparse graphs ($m/n = O(1)$), this bound improves the best previous
bound by a logarithmic factor, and is tight to within a constant factor among
algorithms satisfying a natural {\em locality} property. Our second algorithm
handles an arbitrary sequence of arc additions in $O(n^{5/2})$ time. For
sufficiently dense graphs, this bound improves the best previous bound by a
polynomial factor. Our bound may be far from tight: we show that the algorithm
can take $\Omega(n^2 2^{\sqrt{2\lg n}})$ time by relating its performance to a
generalization of the $k$-levels problem of combinatorial geometry. A
completely different algorithm running in $\Theta(n^2 \log n)$ time was given
recently by Bender, Fineman, and Gilbert. We extend both of our algorithms to
the maintenance of strong components, without affecting the asymptotic time
bounds.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1701.06507v2,Plausible Shading Decomposition For Layered Photo Retouching,"Photographers routinely compose multiple manipulated photos of the same scene
(layers) into a single image, which is better than any individual photo could
be alone. Similarly, 3D artists set up rendering systems to produce layered
images to contain only individual aspects of the light transport, which are
composed into the final result in post-production. Regrettably, both approaches
either take considerable time to capture, or remain limited to synthetic
scenes. In this paper, we suggest a system to allow decomposing a single image
into a plausible shading decomposition (PSD) that approximates effects such as
shadow, diffuse illumination, albedo, and specular shading. This decomposition
can then be manipulated in any off-the-shelf image manipulation software and
recomposited back. We perform such a decomposition by learning a convolutional
neural network trained using synthetic data. We demonstrate the effectiveness
of our decomposition on synthetic (i.e., rendered) and real data (i.e.,
photographs), and use them for common photo manipulation, which are nearly
impossible to perform otherwise from single images.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1902.04809v1,Rewriting Abstract Structures: Materialization Explained Categorically,"The paper develops an abstract (over-approximating) semantics for
double-pushout rewriting of graphs and graph-like objects. The focus is on the
so-called materialization of left-hand sides from abstract graphs, a central
concept in previous work. The first contribution is an accessible, general
explanation of how materializations arise from universal properties and
categorical constructions, in particular partial map classifiers, in a topos.
Second, we introduce an extension by enriching objects with annotations and
give a precise characterization of strongest post-conditions, which are
effectively computable under certain assumptions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1912.07618v3,"Deep Learning for Cardiologist-level Myocardial Infarction Detection in
  Electrocardiograms","Myocardial infarction is the leading cause of death worldwide. In this paper,
we design domain-inspired neural network models to detect myocardial
infarction. First, we study the contribution of various leads. This systematic
analysis, first of its kind in the literature, indicates that out of 15 ECG
leads, data from the v6, vz, and ii leads are critical to correctly identify
myocardial infarction. Second, we use this finding and adapt the ConvNetQuake
neural network model--originally designed to identify earthquakes--to attain
state-of-the-art classification results for myocardial infarction, achieving
$99.43\%$ classification accuracy on a record-wise split, and $97.83\%$
classification accuracy on a patient-wise split. These two results represent
cardiologist-level performance level for myocardial infarction detection after
feeding only 10 seconds of raw ECG data into our model. Third, we show that our
multi-ECG-channel neural network achieves cardiologist-level performance
without the need of any kind of manual feature extraction or data
pre-processing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1504.07149v2,Trip-Based Public Transit Routing,"We study the problem of computing all Pareto-optimal journeys in a public
transit network regarding the two criteria of arrival time and number of
transfers taken. We take a novel approach, focusing on trips and transfers
between them, allowing fine-grained modeling. Our experiments on the
metropolitan network of London show that the algorithm computes full 24-hour
profiles in 70 ms after a preprocessing phase of 30 s, allowing fast queries in
dynamic scenarios.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1503.02551v2,"Kernel-Based Just-In-Time Learning for Passing Expectation Propagation
  Messages","We propose an efficient nonparametric strategy for learning a message
operator in expectation propagation (EP), which takes as input the set of
incoming messages to a factor node, and produces an outgoing message as output.
This learned operator replaces the multivariate integral required in classical
EP, which may not have an analytic expression. We use kernel-based regression,
which is trained on a set of probability distributions representing the
incoming messages, and the associated outgoing messages. The kernel approach
has two main advantages: first, it is fast, as it is implemented using a novel
two-layer random feature representation of the input message distributions;
second, it has principled uncertainty estimates, and can be cheaply updated
online, meaning it can request and incorporate new training data when it
encounters inputs on which it is uncertain. In experiments, our approach is
able to solve learning problems where a single message operator is required for
multiple, substantially different data sets (logistic regression for a variety
of classification problems), where it is essential to accurately assess
uncertainty and to efficiently and robustly update the message operator.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0808.2662v3,Multitask Efficiencies in the Decision Tree Model,"In Direct Sum problems [KRW], one tries to show that for a given
computational model, the complexity of computing a collection of finite
functions on independent inputs is approximately the sum of their individual
complexities. In this paper, by contrast, we study the diversity of ways in
which the joint computational complexity can behave when all the functions are
evaluated on a common input. We focus on the deterministic decision tree model,
with depth as the complexity measure; in this model we prove a result to the
effect that the 'obvious' constraints on joint computational complexity are
essentially the only ones.
  The proof uses an intriguing new type of cryptographic data structure called
a `mystery bin' which we construct using a small polynomial separation between
deterministic and unambiguous query complexity shown by Savicky. We also pose a
variant of the Direct Sum Conjecture of [KRW] which, if proved for a single
family of functions, could yield an analogous result for models such as the
communication model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1712.09302v1,On the Semantics of Intensionality and Intensional Recursion,"Intensionality is a phenomenon that occurs in logic and computation. In the
most general sense, a function is intensional if it operates at a level finer
than (extensional) equality. This is a familiar setting for computer
scientists, who often study different programs or processes that are
interchangeable, i.e. extensionally equal, even though they are not implemented
in the same way, so intensionally distinct. Concomitant with intensionality is
the phenomenon of intensional recursion, which refers to the ability of a
program to have access to its own code. In computability theory, intensional
recursion is enabled by Kleene's Second Recursion Theorem. This thesis is
concerned with the crafting of a logical toolkit through which these phenomena
can be studied. Our main contribution is a framework in which mathematical and
computational constructions can be considered either extensionally, i.e. as
abstract values, or intensionally, i.e. as fine-grained descriptions of their
construction. Once this is achieved, it may be used to analyse intensional
recursion.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0910.2066v2,A Lossless Fuzzy Binary AND/OR Compressor,"In this report, a new fuzzy 2bit-AND parallel-to-OR, or simply, a fuzzy
binary AND/OR (FBAR) text data compression model as an algorithm is suggested
for bettering spatial locality limits on nodes during database transactions.
The current model incorporates a four-layer application technique:
string-to-AND/OR pairwise binary bit + fuzzy quantum with noise conversions.
This technique promotes a lossless data compression ratio of 2:1 up to values
approximately = 3:1, generating a spatially-efficient compressed data file
compared to nowadays data compressors. Data decompression/specific data
reconstruction initiates an AND/OR pattern match technique in respect of fuzzy
quantum indicators in the binary function field. The reconstruction of data
occurs in the 4th layer using encryption methods. It is hypothesized that
significant data compression ratio of 2n:1 for n>3:1 ratios, e.g., 32~64:1 are
achievable via fuzzy qubit indexing over classical byte blocks for every bit
position fragmented into a (1/2 upper +1/2 lower)-bit noise frequency parallel
to its counterpart signal comprised of AND/ORed-bit polarity orientation, ready
for an identical data decompression.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0207055v1,The Rise and Fall of the Church-Turing Thesis,"The essay consists of three parts. In the first part, it is explained how
theory of algorithms and computations evaluates the contemporary situation with
computers and global networks. In the second part, it is demonstrated what new
perspectives this theory opens through its new direction that is called theory
of super-recursive algorithms. These algorithms have much higher computing
power than conventional algorithmic schemes. In the third part, we explicate
how realization of what this theory suggests might influence life of people in
future. It is demonstrated that now the theory is far ahead computing practice
and practice has to catch up with the theory. We conclude with a comparison of
different approaches to the development of information technology.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.06671v1,"Optimally Fast Soft Shadows on Curved Terrain with Dynamic Programming
  and Maximum Mipmaps","We present a simple, novel method of efficiently rendering ray cast soft
shadows on curved terrain by using dynamic programming and maximum mipmaps to
rapidly find a global minimum shadow cost in constant runtime complexity.
Additionally, we apply a new method of reducing view ray computation times that
pre-displaces the terrain mesh to bootstrap ray starting positions. Combining
these two methods, our ray casting engine runs in real-time with more than 200%
speed up over uniform ray stepping with comparable image quality and without
hardware ray tracing acceleration. To add support for accurate planetary
ephemerides and interactive features, we integrated the engine into
celestia.Sci, a general space simulation software. We demonstrate the ability
of our engine to accurately handle a large range of distance scales by using it
to generate videos of lunar landing trajectories. The numerical error when
compared with real lunar mission imagery is small, demonstrating the accuracy
and efficiency of our approach.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.2437v4,"A New Exact Algorithm for Traveling Salesman Problem with Time
  Complexity Interval (O(n^4), O(n^3*2^n))","Traveling salesman problem is a NP-hard problem. Until now, researchers have
not found a polynomial time algorithm for traveling salesman problem. Among the
existing algorithms, dynamic programming algorithm can solve the problem in
time O(n^2*2^n) where n is the number of nodes in the graph. The branch-and-cut
algorithm has been applied to solve the problem with a large number of nodes.
However, branch-and-cut algorithm also has an exponential worst-case running
time.
  In this paper, a new exact algorithm for traveling salesman problem is
proposed. The algorithm can be used to solve an arbitrary instance of traveling
salesman problem in real life and the time complexity interval of the algorithm
is (O(n^4), O(n^3*2^n)). It means that for some instances, the algorithm can
find the optimal solution in polynomial time although the algorithm also has an
exponential worst-case running time. In other words, the algorithm tells us
that not all the instances of traveling salesman problem need exponential time
to compute the optimal solution. The algorithm of this paper can not only
assist us to solve traveling salesman problem better, but also can assist us to
deepen the comprehension of the relationship between NP-complete and P.
Therefore, it is considerable in the further research on traveling salesman
problem and NP-hard problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1802.03726v2,Event Structures for Petri nets with Persistence,"Event structures are a well-accepted model of concurrency. In a seminal paper
by Nielsen, Plotkin and Winskel, they are used to establish a bridge between
the theory of domains and the approach to concurrency proposed by Petri. A
basic role is played by an unfolding construction that maps (safe) Petri nets
into a subclass of event structures, called prime event structures, where each
event has a uniquely determined set of causes. Prime event structures, in turn,
can be identified with their domain of configurations. At a categorical level,
this is nicely formalised by Winskel as a chain of coreflections.
  Contrary to prime event structures, general event structures allow for the
presence of disjunctive causes, i.e., events can be enabled by distinct minimal
sets of events. In this paper, we extend the connection between Petri nets and
event structures in order to include disjunctive causes. In particular, we show
that, at the level of nets, disjunctive causes are well accounted for by
persistent places. These are places where tokens, once generated, can be used
several times without being consumed and where multiple tokens are interpreted
collectively, i.e., their histories are inessential. Generalising the work on
ordinary nets, Petri nets with persistence are related to a new subclass of
general event structures, called locally connected, by means of a chain of
coreflections relying on an unfolding construction.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1703.04912v2,Syntax-Preserving Belief Change Operators for Logic Programs,"Recent methods have adapted the well-established AGM and belief base
frameworks for belief change to cover belief revision in logic programs. In
this study here, we present two new sets of belief change operators for logic
programs. They focus on preserving the explicit relationships expressed in the
rules of a program, a feature that is missing in purely semantic approaches
that consider programs only in their entirety. In particular, operators of the
latter class fail to satisfy preservation and support, two important properties
for belief change in logic programs required to ensure intuitive results.
  We address this shortcoming of existing approaches by introducing partial
meet and ensconcement constructions for logic program belief change, which
allow us to define syntax-preserving operators that satisfy preservation and
support. Our work is novel in that our constructions not only preserve more
information from a logic program during a change operation than existing ones,
but they also facilitate natural definitions of contraction operators, the
first in the field to the best of our knowledge.
  In order to evaluate the rationality of our operators, we translate the
revision and contraction postulates from the AGM and belief base frameworks to
the logic programming setting. We show that our operators fully comply with the
belief base framework and formally state the interdefinability between our
operators. We further propose an algorithm that is based on modularising a
logic program to reduce partial meet and ensconcement revisions or contractions
to performing the operation only on the relevant modules of that program.
Finally, we compare our approach to two state-of-the-art logic program revision
methods and demonstrate that our operators address the shortcomings of one and
generalise the other method.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1010.2396v1,N^N^N does not satisfy Normann's condition,"We prove that the Kleene-Kreisel space $N^N^N$ does not satisfy Normann's
condition. A topological space $X$ is said to fulfil Normann's condition, if
every functionally closed subset of $X$ is an intersection of clopen sets. The
investigation of this property is motivated by its strong relationship to a
problem in Computable Analysis. D. Normann has proved that in order to
establish non-coincidence of the extensional hierarchy and the intensional
hierarchy of functionals over the reals it is enough to show that $N^N^N$ fails
the above condition.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1507.07064v2,Strategyproof Quota Mechanisms for Multiple Assignment Problems,"We study the problem of allocating multiple objects to agents without
transferable utilities, where each agent may receive more than one object
according to a quota. Under lexicographic preferences, we characterize the set
of strategyproof, non-bossy, and neutral quota mechanisms and show that under a
mild Pareto efficiency condition, serial dictatorship quota mechanisms are the
only mechanisms satisfying these properties. Dropping the neutrality
requirement, this class of quota mechanisms further expands to sequential
dictatorship quota mechanisms. We then extend quota mechanisms to randomized
settings, and show that the random serial dictatorship quota mechanisms (RSDQ)
are envyfree, strategyproof, and ex post efficient for any number of agents and
objects and any quota system, proving that the well-studied Random Serial
Dictatorship (RSD) satisfies envyfreeness when preferences are lexicographic.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1712.09686v1,Deviator Detection under Imperfect Monitoring,"Grim-trigger strategies are a fundamental mechanism for sustaining equilibria
in iterated games: the players cooperate along an agreed path, and as soon as
one player deviates, the others form a coalition to play him down to his minmax
level. A precondition to triggering such a strategy is that the identity of the
deviating player becomes common knowledge among the other players. This can be
difficult or impossible to attain in games where the information structure
allows only imperfect monitoring of the played actions or of the global state.
  We study the problem of synthesising finite-state strategies for detecting
the deviator from an agreed strategy profile in games played on finite graphs
with different information structures. We show that the problem is undecidable
in the general case where the global state cannot be monitored. On the other
hand, we prove that under perfect monitoring of the global state and imperfect
monitoring of actions, the problem becomes decidable, and we present an
effective synthesis procedure that covers infinitely repeated games with
private monitoring.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1108.2865v1,Conscious Machines and Consciousness Oriented Programming,"In this paper, we investigate the following question: how could you write
such computer programs that can work like conscious beings? The motivation
behind this question is that we want to create such applications that can see
the future. The aim of this paper is to provide an overall conceptual framework
for this new approach to machine consciousness. So we introduce a new
programming paradigm called Consciousness Oriented Programming (COP).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0510065v1,A new authentication protocol for revocable anonymity in ad-hoc networks,"This paper describes a new protocol for authentication in ad-hoc networks.
The protocol has been designed to meet specialized requirements of ad-hoc
networks, such as lack of direct communication between nodes or requirements
for revocable anonymity. At the same time, a ad-hoc authentication protocol
must be resistant to spoofing, eavesdropping and playback, and
man-in-the-middle attacks. The article analyzes existing authentication methods
based on the Public Key Infrastructure, and finds that they have several
drawbacks in ad-hoc networks. Therefore, a new authentication protocol, basing
on established cryptographic primitives (Merkle's puzzles and zero-knowledge
proofs) is proposed. The protocol is studied for a model ad-hoc chat
application that provides private conversations.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1203.0518v1,Overview of EIREX 2011: Crowdsourcing,"The second Information Retrieval Education through EXperimentation track
(EIREX 2011) was run at the University Carlos III of Madrid, during the 2011
spring semester. EIREX 2011 is the second in a series of experiments designed
to foster new Information Retrieval (IR) education methodologies and resources,
with the specific goal of teaching undergraduate IR courses from an
experimental perspective. For an introduction to the motivation behind the
EIREX experiments, see the first sections of [Urbano et al., 2011a]. For
information on other editions of EIREX and related data, see the website at
http://ir.kr.inf.uc3m.es/eirex/. The EIREX series have the following goals: a)
to help students get a view of the Information Retrieval process as they would
find it in a real-world scenario, either industrial or academic; b) to make
students realize the importance of laboratory experiments in Computer Science
and have them initiated in their execution and analysis; c) to create a public
repository of resources to teach Information Retrieval courses; d) to seek the
collaboration and active participation of other Universities in this endeavor.
This overview paper summarizes the results of the EIREX 2011 track, focusing on
the creation of the test collection and the analysis to assess its reliability.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/cs/0508053v1,Measuring Semantic Similarity by Latent Relational Analysis,"This paper introduces Latent Relational Analysis (LRA), a method for
measuring semantic similarity. LRA measures similarity in the semantic
relations between two pairs of words. When two pairs have a high degree of
relational similarity, they are analogous. For example, the pair cat:meow is
analogous to the pair dog:bark. There is evidence from cognitive science that
relational similarity is fundamental to many cognitive and linguistic tasks
(e.g., analogical reasoning). In the Vector Space Model (VSM) approach to
measuring relational similarity, the similarity between two pairs is calculated
by the cosine of the angle between the vectors that represent the two pairs.
The elements in the vectors are based on the frequencies of manually
constructed patterns in a large corpus. LRA extends the VSM approach in three
ways: (1) patterns are derived automatically from the corpus, (2) Singular
Value Decomposition is used to smooth the frequency data, and (3) synonyms are
used to reformulate word pairs. This paper describes the LRA algorithm and
experimentally compares LRA to VSM on two tasks, answering college-level
multiple-choice word analogy questions and classifying semantic relations in
noun-modifier expressions. LRA achieves state-of-the-art results, reaching
human-level performance on the analogy questions and significantly exceeding
VSM performance on both tasks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.04990v1,"On Probabilistic Termination of Functional Programs with Continuous
  Distributions","We study termination of higher-order probabilistic functional programs with
recursion, stochastic conditioning and sampling from continuous distributions.
  Reasoning about the termination probability of programs with continuous
distributions is hard, because the enumeration of terminating executions cannot
provide any non-trivial bounds. We present a new operational semantics based on
traces of intervals, which is sound and complete with respect to the standard
sampling-based semantics, in which (countable) enumeration can provide
arbitrarily tight lower bounds. Consequently we obtain the first proof that
deciding almost-sure termination (AST) for programs with continuous
distributions is $\Pi^0_2$-complete. We also provide a compositional
representation of our semantics in terms of an intersection type system.
  In the second part, we present a method of proving AST for non-affine
programs, i.e., recursive programs that can, during the evaluation of the
recursive body, make multiple recursive calls (of a first-order function) from
distinct call sites. Unlike in a deterministic language, the number of
recursion call sites has direct consequences on the termination probability.
Our framework supports a proof system that can verify AST for programs that are
well beyond the scope of existing methods.
  We have constructed prototype implementations of our method of computing
lower bounds of termination probability, and AST verification.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.04698v2,Improving Place Recognition Using Dynamic Object Detection,"We present a novel approach to place recognition well-suited to environments
with many dynamic objects--objects that may or may not be present in an agent's
subsequent visits. By incorporating an object-detecting preprocessing step, our
approach yields high-quality place representations that incorporate object
information. Not only does this result in significantly improved place
recognition in dynamic environments, it also significantly reduces
memory/storage requirements, which may increase the effectiveness of mobile
agents with limited resources.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
http://arxiv.org/abs/1007.0880v1,On the independence polynomial of an antiregular graph,"A graph with at most two vertices of the same degree is called antiregular
(Merris 2003), maximally nonregular (Zykov 1990) or quasiperfect (Behzad,
Chartrand 1967). If s_{k} is the number of independent sets of cardinality k in
a graph G, then I(G;x) = s_{0} + s_{1}x + ... + s_{alpha}x^{alpha} is the
independence polynomial of G (Gutman, Harary 1983), where alpha = alpha(G) is
the size of a maximum independent set. In this paper we derive closed formulae
for the independence polynomials of antiregular graphs. In particular, we
deduce that every antiregular graph A is uniquely defined by its independence
polynomial I(A;x), within the family of threshold graphs. Moreover, I(A;x) is
logconcave with at most two real roots, and I(A;-1) belongs to {-1,0}.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.15697v1,Logic of fusion,"The starting point of this work is the observation that the Curry-Howard
isomorphism, relating types and propositions, programs and proofs, composition
and cut, extends to the correspondence of program fusion and cut elimination.
This simple idea suggests logical interpretations of some of the basic methods
of generic and transformational programming. In the present paper, we provide a
logical analysis of the general form of build fusion, also known as
deforestation, over the inductive and the coinductive datatypes, regular or
nested. The analysis is based on a novel logical interpretation of
parametricity in terms of the paranatural transformations, introduced in the
paper.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1403.7308v2,Data Generators for Learning Systems Based on RBF Networks,"There are plenty of problems where the data available is scarce and
expensive. We propose a generator of semi-artificial data with similar
properties to the original data which enables development and testing of
different data mining algorithms and optimization of their parameters. The
generated data allow a large scale experimentation and simulations without
danger of overfitting. The proposed generator is based on RBF networks, which
learn sets of Gaussian kernels. These Gaussian kernels can be used in a
generative mode to generate new data from the same distributions. To assess
quality of the generated data we evaluated the statistical properties of the
generated data, structural similarity and predictive similarity using
supervised and unsupervised learning techniques. To determine usability of the
proposed generator we conducted a large scale evaluation using 51 UCI data
sets. The results show a considerable similarity between the original and
generated data and indicate that the method can be useful in several
development and simulation scenarios. We analyze possible improvements in
classification performance by adding different amounts of generated data to the
training set, performance on high dimensional data sets, and conditions when
the proposed approach is successful.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.04419v2,Asymmetrical Bi-RNN for pedestrian trajectory encoding,"Pedestrian motion behavior involves a combination of individual goals and
social interactions with other agents. In this article, we present an
asymmetrical bidirectional recurrent neural network architecture called U-RNN
to encode pedestrian trajectories and evaluate its relevance to replace LSTMs
for various forecasting models. Experimental results on the Trajnet++ benchmark
show that the U-LSTM variant yields better results regarding every available
metrics (ADE, FDE, Collision rate) than common trajectory encoders for a
variety of approaches and interaction modules, suggesting that the proposed
approach is a viable alternative to the de facto sequence encoding RNNs.
  Our implementation of the asymmetrical Bi-RNNs for the Trajnet++ benchmark is
available at:
github.com/JosephGesnouin/Asymmetrical-Bi-RNNs-to-encode-pedestrian-trajectories",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1305.7429v3,A Distributed SDN Control Plane for Consistent Policy Updates,"Software-defined networking (SDN) is a novel paradigm that out-sources the
control of packet-forwarding switches to a set of software controllers. The
most fundamental task of these controllers is the correct implementation of the
\emph{network policy}, i.e., the intended network behavior. In essence, such a
policy specifies the rules by which packets must be forwarded across the
network.
  This paper studies a distributed SDN control plane that enables
\emph{concurrent} and \emph{robust} policy implementation. We introduce a
formal model describing the interaction between the data plane and a
distributed control plane (consisting of a collection of fault-prone
controllers). Then we formulate the problem of \emph{consistent} composition of
concurrent network policy updates (short: the \emph{CPC Problem}). To
anticipate scenarios in which some conflicting policy updates must be rejected,
we enable the composition via a natural \emph{transactional} interface with
all-or-nothing semantics.
  We show that the ability of an $f$-resilient distributed control plane to
process concurrent policy updates depends on the tag complexity, i. e., the
number of policy labels (a.k.a. \emph{tags}) available to the controllers, and
describe a CPC protocol with optimal tag complexity $f+2$.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.05160v1,Vulnerability Detection is Just the Beginning,"Vulnerability detection plays a key role in secure software development.
There are many different vulnerability detection tools and techniques to choose
from, and insufficient information on which vulnerability detection techniques
to use and when. The goal of this research is to assist managers and other
decision-makers on software projects in making informed choices about the use
of different software vulnerability detection techniques through empirical
analysis of the efficiency and effectiveness of each technique. We will examine
the relationships between the vulnerability detection technique used to find a
vulnerability, the type of vulnerability found, the exploitability of the
vulnerability, and the effort needed to fix a vulnerability on two projects
where we ensure all vulnerabilities found have been fixed. We will then examine
how these relationships are seen in Open Source Software more broadly where
practitioners may use different vulnerability detection techniques, or may not
fix all vulnerabilities found due to resource constraints.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0801.3111v1,"Analysis of Estimation of Distribution Algorithms and Genetic Algorithms
  on NK Landscapes","This study analyzes performance of several genetic and evolutionary
algorithms on randomly generated NK fitness landscapes with various values of n
and k. A large number of NK problem instances are first generated for each n
and k, and the global optimum of each instance is obtained using the
branch-and-bound algorithm. Next, the hierarchical Bayesian optimization
algorithm (hBOA), the univariate marginal distribution algorithm (UMDA), and
the simple genetic algorithm (GA) with uniform and two-point crossover
operators are applied to all generated instances. Performance of all algorithms
is then analyzed and compared, and the results are discussed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.06893v1,"BARR-C:2018 and MISRA C:2012: Synergy Between the Two Most Widely Used C
  Coding Standards","The Barr Group's Embedded C Coding Standard (BARR-C:2018, which originates
from the 2009 Netrino's Embedded C Coding Standard) is, for coding standards
used by the embedded system industry, second only in popularity to MISRA C.
However, the choice between MISRA C:2012 and BARR-C:2018 needs not be a hard
decision since they are complementary in two quite different ways. On the one
hand, BARR-C:2018 has removed all the incompatibilities with respect to MISRA
C:2012 that were present in the previous edition (BARR-C:2013). As a result,
disregarding programming style, BARR-C:2018 defines a subset of C that, while
preventing a significant number of programming errors, is larger than the one
defined by MISRA C:2012. On the other hand, concerning programming style,
whereas MISRA C leaves this to individual organizations, BARR-C:2018 defines a
programming style aimed primarily at minimizing programming errors. As a
result, BARR-C:2018 can be seen as a first, dramatically useful step to C
language subsetting that is suitable for all kinds of projects; critical
projects can then evolve toward MISRA C:2012 compliance smoothly while
maintaining the BARR-C programming style. In this paper, we introduce
BARR-C:2018, we describe its relationship with MISRA C:2012, and we discuss the
parallel and serial adoption of the two coding standards.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.06489v1,"Shallow Optical Flow Three-Stream CNN for Macro- and Micro-Expression
  Spotting from Long Videos","Facial expressions vary from the visible to the subtle. In recent years, the
analysis of micro-expressions $-$ a natural occurrence resulting from the
suppression of one's true emotions, has drawn the attention of researchers with
a broad range of potential applications. However, spotting microexpressions in
long videos becomes increasingly challenging when intertwined with normal or
macro-expressions. In this paper, we propose a shallow optical flow
three-stream CNN (SOFTNet) model to predict a score that captures the
likelihood of a frame being in an expression interval. By fashioning the
spotting task as a regression problem, we introduce pseudo-labeling to
facilitate the learning process. We demonstrate the efficacy and efficiency of
the proposed approach on the recent MEGC 2020 benchmark, where state-of-the-art
performance is achieved on CAS(ME)$^{2}$ with equally promising results on SAMM
Long Videos.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0104003v1,Chain Programs for Writing Deterministic Metainterpreters,"Many metainterpreters found in the logic programming literature are
nondeterministic in the sense that the selection of program clauses is not
determined. Examples are the familiar ""demo"" and ""vanilla"" metainterpreters.
For some applications this nondeterminism is convenient. In some cases,
however, a deterministic metainterpreter, having an explicit selection of
clauses, is needed. Such cases include (1) conversion of OR parallelism into
AND parallelism for ""committed-choice"" processors, (2) logic-based,
imperative-language implementation of search strategies, and (3) simulation of
bounded-resource reasoning.
  Deterministic metainterpreters are difficult to write because the programmer
must be concerned about the set of unifiers of the children of a node in the
derivation tree. We argue that it is both possible and advantageous to write
these metainterpreters by reasoning in terms of object programs converted into
a syntactically restricted form that we call ""chain"" form, where we can forget
about unification, except for unit clauses. We give two transformations
converting logic programs into chain form, one for ""moded"" programs (implicit
in two existing exhaustive-traversal methods for committed-choice execution),
and one for arbitrary definite programs. As illustrations of our approach we
show examples of the three applications mentioned above.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0202019v2,Hypernets -- Good (G)news for Gnutella,"Criticism of Gnutella network scalability has rested on the bandwidth
attributes of the original interconnection topology: a Cayley tree. Trees, in
general, are known to have lower aggregate bandwidth than higher dimensional
topologies e.g., hypercubes, meshes and tori. Gnutella was intended to support
thousands to millions of peers. Studies of interconnection topologies in the
literature, however, have focused on hardware implementations which are limited
by cost to a few thousand nodes. Since the Gnutella network is virtual,
hyper-topologies are relatively unfettered by such constraints. We present
performance models for several plausible hyper-topologies and compare their
query throughput up to millions of peers. The virtual hypercube and the virtual
hypertorus are shown to offer near linear scalability subject to the number of
peer TCP/IP connections that can be simultaneously kept open.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.11593v1,"Automatic Detection of Injection and Press Mold Parts on 2D Drawing
  Using Deep Neural Network","This paper proposes a method to automatically detect the key feature parts in
a CAD of commercial TV and monitor using a deep neural network. We developed a
deep learning pipeline that can detect the injection parts such as hook, boss,
undercut and press parts such as DPS, Embo-Screwless, Embo-Burring, and EMBO in
the 2D CAD drawing images. We first cropped the drawing to a specific size for
the training efficiency of a deep neural network. Then, we use Cascade R-CNN to
find the position of injection and press parts and use Resnet-50 to predict the
orientation of the parts. Finally, we convert the position of the parts found
through the cropped image to the position of the original image. As a result,
we obtained detection accuracy of injection and press parts with 84.1% in AP
(Average Precision), 91.2% in AR(Average Recall), 72.0% in AP, 87.0% in AR, and
orientation accuracy of injection and press parts with 94.4% and 92.0%, which
can facilitate the faster design in industrial product design.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1012.3828v3,"The model checking problem for intuitionistic propositional logic with
  one variable is AC1-complete","We show that the model checking problem for intuitionistic propositional
logic with one variable is complete for logspace-uniform AC1. As basic tool we
use the connection between intuitionistic logic and Heyting algebra, and
investigate its complexity theoretical aspects. For superintuitionistic logics
with one variable, we obtain NC1-completeness for the model checking problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.04319v2,Characterizing the Root Landscape of Certificate Transparency Logs,"Internet security and privacy stand on the trustworthiness of public
certificates signed by Certificate Authorities (CAs). However, software
products do not trust the same CAs and therefore maintain different root
stores, each typically containing hundreds of trusted roots capable of issuing
""trusted"" certificates for any domain. Incidents with misissued certificates
motivated Google to implement and enforce Certificate Transparency (CT). CT
logs archive certificates in a public, auditable and append-only manner. The
adoption of CT changed the trust landscape. As a part of this change, CT logs
started to maintain their own root lists and log certificates that chain back
to one of the trusted roots. In this paper, we present the first
characterization of this emerging CT root store landscape, as well as the tool
that we developed for data collection, visualization, and analysis of the root
stores. We compare the logs' root stores and quantify their changes with
respect to both each other and the root stores of major software vendors, look
at evolving vendor CT policies, and show that root store mismanagement may be
linked to log misbehavior. Finally, we present and discuss the results of a
survey that we have sent to the log operators participating in Apple's and
Google's CT log programs.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0401026v1,EcoLab: Agent Based Modeling for C++ programmers,"\EcoLab{} is an agent based modeling system for C++ programmers, strongly
influenced by the design of Swarm. This paper is just a brief outline of
\EcoLab's features, more details can be found in other published articles,
documentation and source code from the \EcoLab{} website.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0809.1366v1,Network Coding Security: Attacks and Countermeasures,"By allowing intermediate nodes to perform non-trivial operations on packets,
such as mixing data from multiple streams, network coding breaks with the
ruling store and forward networking paradigm and opens a myriad of challenging
security questions. Following a brief overview of emerging network coding
protocols, we provide a taxonomy of their security vulnerabilities, which
highlights the differences between attack scenarios in which network coding is
particularly vulnerable and other relevant cases in which the intrinsic
properties of network coding allow for stronger and more efficient security
solutions than classical routing. Furthermore, we give practical examples where
network coding can be combined with classical cryptography both for secure
communication and secret key distribution. Throughout the paper we identify a
number of research challenges deemed relevant towards the applicability of
secure network coding in practical networks.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.16057v1,DAEMA: Denoising Autoencoder with Mask Attention,"Missing data is a recurrent and challenging problem, especially when using
machine learning algorithms for real-world applications. For this reason,
missing data imputation has become an active research area, in which recent
deep learning approaches have achieved state-of-the-art results. We propose
DAEMA (Denoising Autoencoder with Mask Attention), an algorithm based on a
denoising autoencoder architecture with an attention mechanism. While most
imputation algorithms use incomplete inputs as they would use complete data -
up to basic preprocessing (e.g. mean imputation) - DAEMA leverages a mask-based
attention mechanism to focus on the observed values of its inputs. We evaluate
DAEMA both in terms of reconstruction capabilities and downstream prediction
and show that it achieves superior performance to state-of-the-art algorithms
on several publicly available real-world datasets under various missingness
settings.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1502.03556v1,"An Efficient Metric of Automatic Weight Generation for Properties in
  Instance Matching Technique","The proliferation of heterogeneous data sources of semantic knowledge base
intensifies the need of an automatic instance matching technique. However, the
efficiency of instance matching is often influenced by the weight of a property
associated to instances. Automatic weight generation is a non-trivial, however
an important task in instance matching technique. Therefore, identifying an
appropriate metric for generating weight for a property automatically is
nevertheless a formidable task. In this paper, we investigate an approach of
generating weights automatically by considering hypotheses: (1) the weight of a
property is directly proportional to the ratio of the number of its distinct
values to the number of instances contain the property, and (2) the weight is
also proportional to the ratio of the number of distinct values of a property
to the number of instances in a training dataset. The basic intuition behind
the use of our approach is the classical theory of information content that
infrequent words are more informative than frequent ones. Our mathematical
model derives a metric for generating property weights automatically, which is
applied in instance matching system to produce re-conciliated instances
efficiently. Our experiments and evaluations show the effectiveness of our
proposed metric of automatic weight generation for properties in an instance
matching technique.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1510.01113v2,RAID: A Relation-Augmented Image Descriptor,"As humans, we regularly interpret images based on the relations between image
regions. For example, a person riding object X, or a plank bridging two
objects. Current methods provide limited support to search for images based on
such relations. We present RAID, a relation-augmented image descriptor that
supports queries based on inter-region relations. The key idea of our
descriptor is to capture the spatial distribution of simple point-to-region
relationships to describe more complex relationships between two image regions.
We evaluate the proposed descriptor by querying into a large subset of the
Microsoft COCO database and successfully extract nontrivial images
demonstrating complex inter-region relations, which are easily missed or
erroneously classified by existing methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1809.04468v5,"An Integrated First-Order Theory of Points and Intervals over Linear
  Orders (Part II)","There are two natural and well-studied approaches to temporal ontology and
reasoning: point-based and interval-based. Usually, interval-based temporal
reasoning deals with points as a particular case of duration-less intervals. A
recent result by Balbiani, Goranko, and Sciavicco presented an explicit
two-sorted point-interval temporal framework in which time instants (points)
and time periods (intervals) are considered on a par, allowing the perspective
to shift between these within the formal discourse. We consider here two-sorted
first-order languages based on the same principle, and therefore including
relations, as first studied by Reich, among others, between points, between
intervals, and inter-sort. We give complete classifications of its
sub-languages in terms of relative expressive power, thus determining how many,
and which, are the intrinsically different extensions of two-sorted first-order
logic with one or more such relations. This approach roots out the classical
problem of whether or not points should be included in a interval-based
semantics. In this Part II, we deal with the cases of all dense and the case of
all unbounded linearly ordered sets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9911012v2,Cox's Theorem Revisited,"The assumptions needed to prove Cox's Theorem are discussed and examined.
Various sets of assumptions under which a Cox-style theorem can be proved are
provided, although all are rather strong and, arguably, not natural.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.05561v1,"Did Chatbots Miss Their 'Apollo Moment'? A Survey of the Potential, Gaps
  and Lessons from Using Collaboration Assistants During COVID-19","Artificial Intelligence (AI) technologies have long been positioned as a tool
to provide crucial data-driven decision support to people. In this survey
paper, we look at how AI in general, and collaboration assistants (CAs or
chatbots for short) in particular, have been used during a true global exigency
- the COVID-19 pandemic. The key observation is that chatbots missed their
""Apollo moment"" when they could have really provided contextual, personalized,
reliable decision support at scale that the state-of-the-art makes possible. We
review the existing capabilities that are feasible and methods, identify the
potential that chatbots could have met, the use-cases they were deployed on,
the challenges they faced and gaps that persisted, and draw lessons that, if
implemented, would make them more relevant in future health emergencies.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.07701v1,Computing the exact number of periodic orbits for planar flows,"In this paper, we consider the problem of determining the \emph{exact} number
of periodic orbits for polynomial planar flows. This problem is a variant of
Hilbert's 16th problem. Using a natural definition of computability, we show
that the problem is noncomputable on the one hand and, on the other hand,
computable uniformly on the set of all structurally stable systems defined on
the unit disk. We also prove that there is a family of polynomial planar
systems which does not have a computable sharp upper bound on the number of its
periodic orbits.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.00254v1,The Complexity of Learning Approval-Based Multiwinner Voting Rules,"We study the PAC learnability of multiwinner voting, focusing on the class of
approval-based committee scoring (ABCS) rules. These are voting rules applied
on profiles with approval ballots, where each voter approves some of the
candidates. ABCS rules adapt positional scoring rules in single-winner voting
by assuming that each committee of $k$ candidates collects from each voter a
score, that depends on the size of the voter's ballot and on the size of its
intersection with the committee. Then, committees of maximum score are the
winning ones. Our goal is to learn a target rule (i.e., to learn the
corresponding scoring function) using information about the winning committees
of a small number of sampled profiles. Despite the existence of exponentially
many outcomes compared to single-winner elections, we show that the sample
complexity is still low: a polynomial number of samples carries enough
information for learning the target committee with high confidence and
accuracy. Unfortunately, even simple tasks that need to be solved for learning
from these samples are intractable. We prove that deciding whether there exists
some ABCS rule that makes a given committee winning in a given profile is a
computationally hard problem. Our results extend to the class of sequential
Thiele rules, which have received attention due to their simplicity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1207.5293v2,"Probability Bracket Notation, Multivariable Systems and Static Bayesian
  Networks","Probability Bracket Notation (PBN) is applied to systems of multiple random
variables for preliminary study of static Bayesian Networks (BN) and
Probabilistic Graphic Models (PGM). The famous Student BN Example is explored
to show the local independences and reasoning power of a BN. Software package
Elvira is used to graphically display the student BN. Our investigation shows
that PBN provides a consistent and convenient alternative to manipulate many
expressions related to joint, marginal and conditional probability
distributions in static BN.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1210.2752v1,"Statistical Properties of Inter-arrival Times Distribution in Social
  Tagging Systems","Folksonomies provide a rich source of data to study social patterns taking
place on the World Wide Web. Here we study the temporal patterns of users'
tagging activity. We show that the statistical properties of inter-arrival
times between subsequent tagging events cannot be explained without taking into
account correlation in users' behaviors. This shows that social interaction in
collaborative tagging communities shapes the evolution of folksonomies. A
consensus formation process involving the usage of a small number of tags for a
given resources is observed through a numerical and analytical analysis of some
well-known folksonomy datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.00612v1,On the Threshold of Intractability,"We study the computational complexity of the graph modification problems
Threshold Editing and Chain Editing, adding and deleting as few edges as
possible to transform the input into a threshold (or chain) graph. In this
article, we show that both problems are NP-complete, resolving a conjecture by
Natanzon, Shamir, and Sharan (Discrete Applied Mathematics, 113(1):109--128,
2001). On the positive side, we show the problem admits a quadratic vertex
kernel. Furthermore, we give a subexponential time parameterized algorithm
solving Threshold Editing in $2^{O(\surd k \log k)} + \text{poly}(n)$ time,
making it one of relatively few natural problems in this complexity class on
general graphs. These results are of broader interest to the field of social
network analysis, where recent work of Brandes (ISAAC, 2014) posits that the
minimum edit distance to a threshold graph gives a good measure of consistency
for node centralities. Finally, we show that all our positive results extend to
the related problem of Chain Editing, as well as the completion and deletion
variants of both problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9810008v1,Axiomatizing Flat Iteration,"Flat iteration is a variation on the original binary version of the Kleene
star operation P*Q, obtained by restricting the first argument to be a sum of
atomic actions. It generalizes prefix iteration, in which the first argument is
a single action. Complete finite equational axiomatizations are given for five
notions of bisimulation congruence over basic CCS with flat iteration, viz.
strong congruence, branching congruence, eta-congruence, delay congruence and
weak congruence. Such axiomatizations were already known for prefix iteration
and are known not to exist for general iteration. The use of flat iteration has
two main advantages over prefix iteration: 1.The current axiomatizations
generalize to full CCS, whereas the prefix iteration approach does not allow an
elimination theorem for an asynchronous parallel composition operator. 2.The
greater expressiveness of flat iteration allows for much shorter completeness
proofs.
  In the setting of prefix iteration, the most convenient way to obtain the
completeness theorems for eta-, delay, and weak congruence was by reduction to
the completeness theorem for branching congruence. In the case of weak
congruence this turned out to be much simpler than the only direct proof found.
In the setting of flat iteration on the other hand, the completeness theorems
for delay and weak (but not eta-) congruence can equally well be obtained by
reduction to the one for strong congruence, without using branching congruence
as an intermediate step. Moreover, the completeness results for prefix
iteration can be retrieved from those for flat iteration, thus obtaining a
second indirect approach for proving completeness for delay and weak congruence
in the setting of prefix iteration.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.02550v2,"VBridge: Connecting the Dots Between Features and Data to Explain
  Healthcare Models","Machine learning (ML) is increasingly applied to Electronic Health Records
(EHRs) to solve clinical prediction tasks. Although many ML models perform
promisingly, issues with model transparency and interpretability limit their
adoption in clinical practice. Directly using existing explainable ML
techniques in clinical settings can be challenging. Through literature surveys
and collaborations with six clinicians with an average of 17 years of clinical
experience, we identified three key challenges, including clinicians'
unfamiliarity with ML features, lack of contextual information, and the need
for cohort-level evidence. Following an iterative design process, we further
designed and developed VBridge, a visual analytics tool that seamlessly
incorporates ML explanations into clinicians' decision-making workflow. The
system includes a novel hierarchical display of contribution-based feature
explanations and enriched interactions that connect the dots between ML
features, explanations, and data. We demonstrated the effectiveness of VBridge
through two case studies and expert interviews with four clinicians, showing
that visually associating model explanations with patients' situational records
can help clinicians better interpret and use model predictions when making
clinician decisions. We further derived a list of design implications for
developing future explainable ML tools to support clinical decision-making.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0902.1665v1,"Novel anisotropic continuum-discrete damage model capable of
  representing localized failure of massive structures. Part II: identification
  from tests under heterogeneous stress field","In Part I of this paper we have presented a simple model capable of
describing the localized failure of a massive structure. In this part, we
discuss the identification of the model parameters from two kinds of
experiments: a uniaxial tensile test and a three-point bending test. The former
is used only for illustration of material parameter response dependence, and we
focus mostly upon the latter, discussing the inverse optimization problem for
which the specimen is subjected to a heterogeneous stress field.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0707.4166v1,Parsimony Principles for Software Components and Metalanguages,"Software is a communication system. The usual topic of communication is
program behavior, as encoded by programs. Domain-specific libraries are
codebooks, domain-specific languages are coding schemes, and so forth. To turn
metaphor into method, we adapt toolsfrom information theory--the study of
efficient communication--to probe the efficiency with which languages and
libraries let us communicate programs. In previous work we developed an
information-theoretic analysis of software reuse in problem domains. This new
paper uses information theory to analyze tradeoffs in the design of components,
generators, and metalanguages. We seek answers to two questions: (1) How can we
judge whether a component is over- or under-generalized? Drawing on minimum
description length principles, we propose that the best component yields the
most succinct representation of the use cases. (2) If we view a programming
language as an assemblage of metalanguages, each providing a complementary
style of abstraction, how can these metalanguages aid or hinder us in
efficiently describing software? We describe a complex triangle of interactions
between the power of an abstraction mechanism, the amount of reuse it enables,
and the cognitive difficulty of its use.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1708.01335v1,"Compact, Provably-Good LPs for Orienteering and Regret-Bounded Vehicle
  Routing","We develop polynomial-size LP-relaxations for {\em orienteering} and the {\em
regret-bounded vehicle routing problem} (\rvrp) and devise suitable LP-rounding
algorithms that lead to various new insights and approximation results for
these problems. In orienteering, the goal is to find a maximum-reward
$r$-rooted path, possibly ending at a specified node, of length at most some
given budget $B$. In \rvrp, the goal is to find the minimum number of
$r$-rooted paths of {\em regret} at most a given bound $R$ that cover all
nodes, where the regret of an $r$-$v$ path is its length $-$ $c_{rv}$.
  For {\em rooted orienteering}, we introduce a natural bidirected
LP-relaxation and obtain a simple $3$-approximation algorithm via LP-rounding.
This is the {\em first LP-based} guarantee for this problem. We also show that
{\em point-to-point} (\ptp) {\em orienteering} can be reduced to a
regret-version of rooted orienteering at the expense of a factor-2 loss in
approximation. For \rvrp, we propose two compact LPs that lead to significant
improvements, in both approximation ratio and running time, over the approach
in~\cite{FriggstadS14}. One of these is a natural modification of the LP for
rooted orienteering; the other is an unconventional formulation that is
motivated by certain structural properties of an \rvrp-solution, which leads to
a $15$-approximation algorithm for \rvrp.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1602.01198v2,k-variates++: more pluses in the k-means++,"k-means++ seeding has become a de facto standard for hard clustering
algorithms. In this paper, our first contribution is a two-way generalisation
of this seeding, k-variates++, that includes the sampling of general densities
rather than just a discrete set of Dirac densities anchored at the point
locations, and a generalisation of the well known Arthur-Vassilvitskii (AV)
approximation guarantee, in the form of a bias+variance approximation bound of
the global optimum. This approximation exhibits a reduced dependency on the
""noise"" component with respect to the optimal potential --- actually
approaching the statistical lower bound. We show that k-variates++ reduces to
efficient (biased seeding) clustering algorithms tailored to specific
frameworks; these include distributed, streaming and on-line clustering, with
direct approximation results for these algorithms. Finally, we present a novel
application of k-variates++ to differential privacy. For either the specific
frameworks considered here, or for the differential privacy setting, there is
little to no prior results on the direct application of k-means++ and its
approximation bounds --- state of the art contenders appear to be significantly
more complex and / or display less favorable (approximation) properties. We
stress that our algorithms can still be run in cases where there is \textit{no}
closed form solution for the population minimizer. We demonstrate the
applicability of our analysis via experimental evaluation on several domains
and settings, displaying competitive performances vs state of the art.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.01377v1,Normalisers of primitive permutation groups in quasipolynomial time,"We show that given generators for subgroups $G$ and $H$ of $\mathrm{S}_n$, if
$G$ is primitive then generators for $\mathrm{N}_H(G)$ may be computed in
quasipolynomial time, namely $2^{O(\log^3 n)}$. The previous best known bound
was simply exponential.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.14554v1,Indirect Identification of Psychosocial Risks from Natural Language,"During the perinatal period, psychosocial health risks, including depression
and intimate partner violence, are associated with serious adverse health
outcomes for parents and children. To appropriately intervene, healthcare
professionals must first identify those at risk, yet stigma often prevents
people from directly disclosing the information needed to prompt an assessment.
We examine indirect methods of eliciting and analyzing information that could
indicate psychosocial risks. Short diary entries by peripartum women exhibit
thematic patterns, extracted by topic modeling, and emotional perspective,
drawn from dictionary-informed sentiment features. Using these features, we use
regularized regression to predict screening measures of depression and
psychological aggression by an intimate partner. Journal text entries
quantified through topic models and sentiment features show promise for
depression prediction, with performance almost as good as closed-form
questions. Text-based features were less useful for prediction of intimate
partner violence, but moderately indirect multiple-choice questioning allowed
for detection without explicit disclosure. Both methods may serve as an initial
or complementary screening approach to detecting stigmatized risks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1411.5289v2,Generalizing the Liveness Based Points-to Analysis,"The original liveness based flow and context sensitive points-to analysis
(LFCPA) is restricted to scalar pointer variables and scalar pointees on stack
and static memory. In this paper, we extend it to support heap memory and
pointer expressions involving structures, unions, arrays, and pointer
arithmetic. The key idea behind these extensions involves constructing bounded
names for locations in terms of compile time constants (names and fixed
offsets), and introducing sound approximations when it is not possible to do
so. We achieve this by defining a grammar for pointer expressions, suitable
memory models and location naming conventions, and some key evaluations of
pointer expressions that compute the named locations. These extensions preserve
the spirit of the original LFCPA which is evidenced by the fact that although
the lattices and flow functions change, the overall data flow equations remain
unchanged.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.12992v2,"Index handling and assign optimization for Algorithmic Differentiation
  reuse index managers","For operator overloading Algorithmic Differentiation tools, the
identification of primal variables and adjoint variables is usually done via
indices. Two common schemes exist for their management and distribution. The
linear approach is easy to implement and supports memory optimization with
respect to copy statements. On the other hand, the reuse approach requires more
implementation effort but results in much smaller adjoint vectors, which are
more suitable for the vector mode of Algorithmic Differentiation. In this
paper, we present both approaches, how to implement them, and discuss their
advantages, disadvantages and properties of the resulting Algorithmic
Differentiation type. In addition, a new management scheme is presented which
supports copy optimizations and the reuse of indices, thus combining the
advantages of the other two. The implementations of all three schemes are
compared on a simple synthetic example and on a real world example using the
computational fluid dynamics solver in SU2.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0605007v2,Systematic Topology Analysis and Generation Using Degree Correlations,"We present a new, systematic approach for analyzing network topologies. We
first introduce the dK-series of probability distributions specifying all
degree correlations within d-sized subgraphs of a given graph G. Increasing
values of d capture progressively more properties of G at the cost of more
complex representation of the probability distribution. Using this series, we
can quantitatively measure the distance between two graphs and construct random
graphs that accurately reproduce virtually all metrics proposed in the
literature. The nature of the dK-series implies that it will also capture any
future metrics that may be proposed. Using our approach, we construct graphs
for d=0,1,2,3 and demonstrate that these graphs reproduce, with increasing
accuracy, important properties of measured and modeled Internet topologies. We
find that the d=2 case is sufficient for most practical purposes, while d=3
essentially reconstructs the Internet AS- and router-level topologies exactly.
We hope that a systematic method to analyze and synthesize topologies offers a
significant improvement to the set of tools available to network topology and
protocol researchers.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.11504v1,A First Step Towards Content Protecting Plagiarism Detection,"Plagiarism detection systems are essential tools for safeguarding academic
and educational integrity. However, today's systems require disclosing the full
content of the input documents and the document collection to which the input
documents are compared. Moreover, the systems are centralized and under the
control of individual, typically commercial providers. This situation raises
procedural and legal concerns regarding the confidentiality of sensitive data,
which can limit or prohibit the use of plagiarism detection services. To
eliminate these weaknesses of current systems, we seek to devise a plagiarism
detection approach that does not require a centralized provider nor exposing
any content as cleartext. This paper presents the initial results of our
research. Specifically, we employ Private Set Intersection to devise a
content-protecting variant of the citation-based similarity measure
Bibliographic Coupling implemented in our plagiarism detection system HyPlag.
Our evaluation shows that the content-protecting method achieves the same
detection effectiveness as the original method while making common attacks to
disclose the protected content practically infeasible. Our future work will
extend this successful proof-of-concept by devising plagiarism detection
methods that can analyze the entire content of documents without disclosing it
as cleartext.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.15116v1,Towards Flying through Modular Forms,"Modular forms are highly self-symmetric functions studied in number theory,
with connections to several areas of mathematics. But they are rarely
visualized. We discuss ongoing work to compute and visualize modular forms as
3D surfaces and to use these techniques to make videos flying around the peaks
and canyons of these ""modular terrains."" Our goal is to make beautiful
visualizations exposing the symmetries of these functions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0006047v1,Geometric Morphology of Granular Materials,"We present a new method to transform the spectral pixel information of a
micrograph into an affine geometric description, which allows us to analyze the
morphology of granular materials. We use spectral and pulse-coupled neural
network based segmentation techniques to generate blobs, and a newly developed
algorithm to extract dilated contours. A constrained Delaunay tesselation of
the contour points results in a triangular mesh. This mesh is the basic
ingredient of the Chodal Axis Transform, which provides a morphological
decomposition of shapes. Such decomposition allows for grain separation and the
efficient computation of the statistical features of granular materials.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.14527v1,"Human-centric Data Dissemination in the IoP: Large-scale Modeling and
  Evaluation","Data management using Device-to-Device (D2D) communications and opportunistic
networks (ONs) is one of the main focuses of human-centric pervasive Internet
services. In the recently proposed ""Internet of People"" paradigm, accessing
relevant data dynamically generated in the environment nearby is one of the key
services. Moreover, personal mobile devices become proxies of their human users
while exchanging data in the cyber world and, thus, largely use ONs and D2D
communications for exchanging data directly. Recently, researchers have
successfully demonstrated the viability of embedding human cognitive schemes in
data dissemination algorithms for ONs. In this paper, we consider one such
scheme based on the recognition heuristic, a human decision-making scheme used
to efficiently assess the relevance of data. While initial evidence about its
effectiveness is available, the evaluation of its behaviour in large-scale
settings is still unsatisfactory. To overcome these limitations, we have
developed a novel hybrid modelling methodology, which combines an analytical
model of data dissemination within small-scale communities of mobile users,
with detailed simulations of interactions between different communities. This
methodology allows us to evaluate the algorithm in large-scale city- and
country-wide scenarios. Results confirm the effectiveness of cognitive data
dissemination schemes, even when content popularity is very heterogenous.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1902.10928v4,Interaction-aware Kalman Neural Networks for Trajectory Prediction,"Forecasting the motion of surrounding obstacles (vehicles, bicycles,
pedestrians and etc.) benefits the on-road motion planning for intelligent and
autonomous vehicles. Complex scenes always yield great challenges in modeling
the patterns of surrounding traffic. For example, one main challenge comes from
the intractable interaction effects in a complex traffic system. In this paper,
we propose a multi-layer architecture Interaction-aware Kalman Neural Networks
(IaKNN) which involves an interaction layer for resolving high-dimensional
traffic environmental observations as interaction-aware accelerations, a motion
layer for transforming the accelerations to interaction aware trajectories, and
a filter layer for estimating future trajectories with a Kalman filter network.
Attributed to the multiple traffic data sources, our end-to-end trainable
approach technically fuses dynamic and interaction-aware trajectories boosting
the prediction performance. Experiments on the NGSIM dataset demonstrate that
IaKNN outperforms the state-of-the-art methods in terms of effectiveness for
traffic trajectory prediction.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0405111v2,Attrition Defenses for a Peer-to-Peer Digital Preservation System,"In peer-to-peer systems, attrition attacks include both traditional,
network-level denial of service attacks as well as application-level attacks in
which malign peers conspire to waste loyal peers' resources. We describe
several defenses for LOCKSS, a peer-to-peer digital preservation system, that
help ensure that application-level attacks even from powerful adversaries are
less effective than simple network-level attacks, and that network-level
attacks must be intense, wide-spread, and prolonged to impair the system.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0412034v1,"The informatization of design works at industry firm during its
  renovation","The characteristic of design works on firm at its renovation and of the
common directions of their informatization is given. The implantation of a CAD
is selected as the key direction, and the requirements to a complex CAD-system
are stated. The methods of such a CAD-system development are featured, and the
connectedness of this development with the process of integration of
information space of design department of the firm is characterized. The
experience of development and implantation of a complex CAD of renovation of
firms TechnoCAD GlassX lies in a basis of this reviewing",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0706.3132v1,EasyVoice: Integrating voice synthesis with Skype,"This paper presents EasyVoice, a system that integrates voice synthesis with
Skype. EasyVoice allows a person with voice disabilities to talk with another
person located anywhere in the world, removing an important obstacle that
affect these people during a phone or VoIP-based conversation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/0709.0624v1,On Faster Integer Calculations using Non-Arithmetic Primitives,"The unit cost model is both convenient and largely realistic for describing
integer decision algorithms over (+,*). Additional operations like division
with remainder or bitwise conjunction, although equally supported by computing
hardware, may lead to a considerable drop in complexity. We show a variety of
concrete problems to benefit from such NON-arithmetic primitives by presenting
and analyzing corresponding fast algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1710.04640v4,Hard and Easy Instances of L-Tromino Tilings,"We study tilings of regions in the square lattice with L-shaped trominoes.
Deciding the existence of a tiling with L-trominoes for an arbitrary region in
general is NP-complete, nonetheless, we identify restrictions to the problem
where it either remains NP-complete or has a polynomial time algorithm. First,
we characterize the possibility of when an Aztec rectangle and an Aztec diamond
has an L-tromino tiling. Then, we study tilings of arbitrary regions where only
$180^\circ$ rotations of L-trominoes are available. For this particular case we
show that deciding the existence of a tiling remains NP-complete; yet, if a
region does not contains certain so-called ""forbidden polyominoes"" as
sub-regions, then there exists a polynomial time algorithm for deciding a
tiling.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0207094v1,Answer Sets for Consistent Query Answering in Inconsistent Databases,"A relational database is inconsistent if it does not satisfy a given set of
integrity constraints. Nevertheless, it is likely that most of the data in it
is consistent with the constraints. In this paper we apply logic programming
based on answer sets to the problem of retrieving consistent information from a
possibly inconsistent database. Since consistent information persists from the
original database to every of its minimal repairs, the approach is based on a
specification of database repairs using disjunctive logic programs with
exceptions, whose answer set semantics can be represented and computed by
systems that implement stable model semantics. These programs allow us to
declare persistence by defaults and repairing changes by exceptions. We
concentrate mainly on logic programs for binary integrity constraints, among
which we find most of the integrity constraints found in practice.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.13159v1,"Weierstrass semigroup at $m+1$ rational points in maximal curves which
  cannot be covered by the Hermitian curve","We determine the Weierstrass semigroup $H(P_\infty,P_1,\ldots,P_m)$ at
several rational points on the maximal curves which cannot be covered by the
Hermitian curve introduced by Tafazolian, Teher\'an-Herrera, and Torres.
Furthermore, we present some conditions to find pure gaps. We use this
semigroup to obtain AG codes with better relative parameters than comparable
one-point AG codes arising from these curves.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1102.2134v3,"Well-Quasi-Ordering of Matrices under Schur Complement and Applications
  to Directed Graphs","In [Rank-Width and Well-Quasi-Ordering of Skew-Symmetric or Symmetric
Matrices, arXiv:1007.3807v1] Oum proved that, for a fixed finite field
$\mathbf{F}$, any infinite sequence $M_1,M_2,...$ of (skew) symmetric matrices
over $\mathbf{F}$ of bounded $\mathbf{F}$-rank-width has a pair $i< j$, such
that $M_i$ is isomorphic to a principal submatrix of a principal pivot
transform of $M_j$. We generalise this result to $\sigma$-symmetric matrices
introduced by Rao and myself in [The Rank-Width of Edge-Coloured Graphs,
arXiv:0709.1433v4]. (Skew) symmetric matrices are special cases of
$\sigma$-symmetric matrices. As a by-product, we obtain that for every infinite
sequence $G_1,G_2,...$ of directed graphs of bounded rank-width there exist a
pair $i<j$ such that $G_i$ is a pivot-minor of $G_j$. Another consequence is
that non-singular principal submatrices of a $\sigma$-symmetric matrix form a
delta-matroid. We extend in this way the notion of representability of
delta-matroids by Bouchet.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.07888v1,"Metatheory.jl: Fast and Elegant Algebraic Computation in Julia with
  Extensible Equality Saturation","We introduce Metatheory.jl: a lightweight and performant general purpose
symbolics and metaprogramming framework meant to simplify the act of writing
complex Julia metaprograms and to significantly enhance Julia with a native
term rewriting system, based on state-of-the-art equality saturation
techniques, and a dynamic first class Abstract Syntax Tree (AST) pattern
matching system that is dynamically composable in an algebraic fashion, taking
full advantage of the language's powerful reflection capabilities. Our
contribution allows to perform general purpose symbolic mathematics,
manipulation, optimization, synthesis or analysis of syntactically valid Julia
expressions with a clean and concise programming interface, both during
compilation or execution of programs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0106011v1,Computational properties of environment-based disambiguation,"The standard pipeline approach to semantic processing, in which sentences are
morphologically and syntactically resolved to a single tree before they are
interpreted, is a poor fit for applications such as natural language
interfaces. This is because the environment information, in the form of the
objects and events in the application's run-time environment, cannot be used to
inform parsing decisions unless the input sentence is semantically analyzed,
but this does not occur until after parsing in the single-tree semantic
architecture. This paper describes the computational properties of an
alternative architecture, in which semantic analysis is performed on all
possible interpretations during parsing, in polynomial time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.13475v1,Efficient GPU Thread Mapping on Embedded 2D Fractals,"This work proposes a new approach for mapping GPU threads onto a family of
discrete embedded 2D fractals. A block-space map $\lambda:
\mathbb{Z}_{\mathbb{E}}^{2} \mapsto \mathbb{Z}_{\mathbb{F}}^{2}$ is proposed,
from Euclidean parallel space $\mathbb{E}$ to embedded fractal space
$\mathbb{F}$, that maps in $\mathcal{O}(\log_2 \log_2(n))$ time and uses no
more than $\mathcal{O}(n^\mathbb{H})$ threads with $\mathbb{H}$ being the
Hausdorff dimension of the fractal, making it parallel space efficient. When
compared to a bounding-box (BB) approach, $\lambda(\omega)$ offers a
sub-exponential improvement in parallel space and a monotonically increasing
speedup $n \ge n_0$. The Sierpinski gasket fractal is used as a particular case
study and the experimental performance results show that $\lambda(\omega)$
reaches up to $9\times$ of speedup over the bounding-box approach. A
tensor-core based implementation of $\lambda(\omega)$ is also proposed for
modern GPUs, providing up to $\sim40\%$ of extra performance. The results
obtained in this work show that doing efficient GPU thread mapping on fractal
domains can significantly improve the performance of several applications that
work with this type of geometry.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.05800v2,On Testability of First-Order Properties in Bounded-Degree Graphs,"We study property testing of properties that are definable in first-order
logic (FO) in the bounded-degree graph and relational structure models. We show
that any FO property that is defined by a formula with quantifier prefix
$\exists^*\forall^*$ is testable (i.e., testable with constant query
complexity), while there exists an FO property that is expressible by a formula
with quantifier prefix $\forall^*\exists^*$ that is not testable. In the dense
graph model, a similar picture is long known (Alon, Fischer, Krivelevich,
Szegedy, Combinatorica 2000), despite the very different nature of the two
models. In particular, we obtain our lower bound by a first-order formula that
defines a class of bounded-degree expanders, based on zig-zag products of
graphs. We expect this to be of independent interest. We then prove testability
of some first-order properties that speak about isomorphism types of
neighbourhoods, including testability of $1$-neighbourhood-freeness, and
$r$-neighbourhood-freeness under a mild assumption on the degrees.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1808.04211v1,The possibility of use of QR-codes in teaching physics,"In the article discusses the possibility of using of QR-codes in teaching
physics. It is noted that the technology of recognition of QR-codes can be
attributed to elements of mobile information and education environment. On the
basis of summarizing existing research discusses the advantages and
disadvantages of using QR-codes, and the application of codes in the learning
process. Examples of the use of QR-codes in teaching physics (of physical
quests and web quests, of games, quizzes, polls, creating a virtual exhibition,
creating applications to educational facilities, the creation and study of
computer models of physical phenomena and processes, organization Self-Test)
are described. Found that the mobile learning available to pupils (students),
and elements of the mobile information-educational environment (including
technology development and recognition of QR-codes) have sufficient capacity in
teaching physics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/2112.15198v2,Massively Parallelized Interpolated Factored Green Function Method,"This paper presents the first parallel implementation of the novel
""Interpolated Factored Green Function"" (IFGF) method introduced recently for
the accelerated evaluation of discrete integral operators arising in wave
scattering and other areas (Bauinger and Bruno, Jour. Computat. Phys., 2021).
On the basis of the hierarchical IFGF interpolation strategy, the proposed
(hybrid MPI-OpenMP) parallel implementation results in highly efficient data
communication, and it exhibits in practice excellent parallel scaling up to
large numbers of cores - without any hard limitations on the number of cores
concurrently employed with high efficiency. Moreover, on any given number of
cores, the proposed parallel approach preserves the O(N log N) computing cost
inherent in the sequential version of the IFGF algorithm. Unlike other
approaches, the IFGF method does not utilize the Fast Fourier Transform (FFT),
and it is thus better suited than other methods for efficient parallelization
in distributed-memory computer systems. In particular, the IFGF method relies
on a ""peer-to-peer"" strategy wherein, at every level, field propagation is
directly enacted via ""exchanges"" between ""peer"" polynomials of low and constant
degree, without data accumulation in large-scale ""telephone-central""
mathematical constructs such as those in the Fast Multipole Method (FMM) or
pure FFT-based approaches. A variety of numerical results presented in this
paper illustrate the character of the proposed parallel algorithm, including
excellent weak and strong parallel scaling properties in all cases considered -
for problems of up to 4,096 wavelengths in acoustic size, and scaling tests
spanning from 1 compute core to all 1,680 cores available in the High
Performance Computing cluster used.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.13117v1,ESBMC-Solidity: An SMT-Based Model Checker for Solidity Smart Contracts,"Smart contracts written in Solidity are programs used in blockchain networks,
such as Etherium, for performing transactions. However, as with any piece of
software, they are prone to errors and may present vulnerabilities, which
malicious attackers could then use. This paper proposes a solidity frontend for
the efficient SMT-based context-bounded model checker (ESBMC), named
ESBMC-Solidity, which provides a way of verifying such contracts with its
framework. A benchmark suite with vulnerable smart contracts was also developed
for evaluation and comparison with other verification tools. The experiments
performed here showed that ESBMC-Solidity detected all vulnerabilities, was the
fastest tool, and provided a counterexample for each benchmark. A demonstration
is available at https://youtu.be/3UH8_1QAVN0.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.11651v1,CosmoVAE: Variational Autoencoder for CMB Image Inpainting,"Cosmic microwave background radiation (CMB) is critical to the understanding
of the early universe and precise estimation of cosmological constants. Due to
the contamination of thermal dust noise in the galaxy, the CMB map that is an
image on the two-dimensional sphere has missing observations, mainly
concentrated on the equatorial region. The noise of the CMB map has a
significant impact on the estimation precision for cosmological parameters.
Inpainting the CMB map can effectively reduce the uncertainty of parametric
estimation. In this paper, we propose a deep learning-based variational
autoencoder --- CosmoVAE, to restoring the missing observations of the CMB map.
The input and output of CosmoVAE are square images. To generate training,
validation, and test data sets, we segment the full-sky CMB map into many small
images by Cartesian projection. CosmoVAE assigns physical quantities to the
parameters of the VAE network by using the angular power spectrum of the
Gaussian random field as latent variables. CosmoVAE adopts a new loss function
to improve the learning performance of the model, which consists of $\ell_1$
reconstruction loss, Kullback-Leibler divergence between the posterior
distribution of encoder network and the prior distribution of latent variables,
perceptual loss, and total-variation regularizer. The proposed model achieves
state of the art performance for Planck \texttt{Commander} 2018 CMB map
inpainting.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0104016v4,The Gibbs Representation of 3D Rotations,"This paper revisits the little-known Gibbs-Rodrigues representation of
rotations in a three-dimensional space and demonstrates a set of algorithms for
handling it. In this representation the rotation is itself represented as a
three-dimensional vector. The vector is parallel to the axis of rotation and
its three components transform covariantly on change of coordinates. The
mapping from rotations to vectors is 1:1 apart from computation error. The
discontinuities of the representation require special handling but are not
problematic. The rotation matrix can be generated efficiently from the vector
without the use of transcendental functions, and vice-versa. The representation
is more efficient than Euler angles, has affinities with Hassenpflug's Argyris
angles and is very closely related to the quaternion representation. While the
quaternion representation avoids the discontinuities inherent in any
3-component representation, this problem is readily overcome. The present paper
gives efficient algorithms for computing the set of rotations which map a given
vector to another of the same length and the rotation which maps a given pair
of vectors to another pair of the same length and subtended angle.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.12659v1,"It is rotating leaders who build the swarm: social network determinants
  of growth for healthcare virtual communities of practice","Purpose: The purpose of this paper is to identify the factors influencing the
growth of healthcare virtual communities of practice (VCoPs) through a
seven-year longitudinal study conducted using metrics from social-network and
semantic analysis. By studying online communication along the three dimensions
of social interactions (connectivity, interactivity and language use), the
authors aim to provide VCoP managers with valuable insights to improve the
success of their communities. Design/methodology/approach: Communications over
a period of seven years (April 2008 to April 2015) and between 14,000 members
of 16 different healthcare VCoPs coexisting on the same web platform were
analysed. Multilevel regression models were used to reveal the main
determinants of community growth over time. Independent variables were derived
from social network and semantic analysis measures. Findings: Results show that
structural and content-based variables predict the growth of the community.
Progressively, more people will join a community if its structure is more
centralised, leaders are more dynamic (they rotate more) and the language used
in the posts is less complex. Research limitations/implications: The available
data set included one Web platform and a limited number of control variables.
To consolidate the findings of the present study, the experiment should be
replicated on other healthcare VCoPs. Originality/value: The study provides
useful recommendations for setting up and nurturing the growth of professional
communities, considering, at the same time, the interaction patterns among the
community members, the dynamic evolution of these interactions and the use of
language. New analytical tools are presented, together with the use of
innovative interaction metrics, that can significantly influence community
growth, such as rotating leadership.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.09861v1,"Convergent Finite Difference Methods for Fully Nonlinear Elliptic
  Equations in Three Dimensions","We introduce a generalized finite difference method for solving a large range
of fully nonlinear elliptic partial differential equations in three dimensions.
Methods are based on Cartesian grids, augmented by additional points carefully
placed along the boundary at high resolution. We introduce and analyze a
least-squares approach to building consistent, monotone approximations of
second directional derivatives on these grids. We then show how to efficiently
approximate functions of the eigenvalues of the Hessian through a multi-level
discretization of orthogonal coordinate frames in $\mathbb{R}^3$. The resulting
schemes are monotone and fit within many recently developed convergence
frameworks for fully nonlinear elliptic equations including non-classical
Dirichlet problems that admit discontinuous solutions, Monge-Amp\`ere type
equations in optimal transport, and eigenvalue problems involving nonlinear
elliptic operators. Computational examples demonstrate the success of this
method on a wide range of challenging examples.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0306042v1,"IGUANA Architecture, Framework and Toolkit for Interactive Graphics","IGUANA is a generic interactive visualisation framework based on a C++
component model. It provides powerful user interface and visualisation
primitives in a way that is not tied to any particular physics experiment or
detector design. The article describes interactive visualisation tools built
using IGUANA for the CMS and D0 experiments, as well as generic GEANT4 and
GEANT3 applications. It covers features of the graphical user interfaces, 3D
and 2D graphics, high-quality vector graphics output for print media, various
textual, tabular and hierarchical data views, and integration with the
application through control panels, a command line and different
multi-threading models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0612003v2,Predicate Abstraction via Symbolic Decision Procedures,"We present a new approach for performing predicate abstraction based on
symbolic decision procedures. Intuitively, a symbolic decision procedure for a
theory takes a set of predicates in the theory and symbolically executes a
decision procedure on all the subsets over the set of predicates. The result of
the symbolic decision procedure is a shared expression (represented by a
directed acyclic graph) that implicitly represents the answer to a predicate
abstraction query.
  We present symbolic decision procedures for the logic of Equality and
Uninterpreted Functions (EUF) and Difference logic (DIFF) and show that these
procedures run in pseudo-polynomial (rather than exponential) time. We then
provide a method to construct symbolic decision procedures for simple mixed
theories (including the two theories mentioned above) using an extension of the
Nelson-Oppen combination method. We present preliminary evaluation of our
Procedure on predicate abstraction benchmarks from device driver verification
in SLAM.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.7990v1,Predicting User Engagement in Twitter with Collaborative Ranking,"Collaborative Filtering (CF) is a core component of popular web-based
services such as Amazon, YouTube, Netflix, and Twitter. Most applications use
CF to recommend a small set of items to the user. For instance, YouTube
presents to a user a list of top-n videos she would likely watch next based on
her rating and viewing history. Current methods of CF evaluation have been
focused on assessing the quality of a predicted rating or the ranking
performance for top-n recommended items. However, restricting the recommender
system evaluation to these two aspects is rather limiting and neglects other
dimensions that could better characterize a well-perceived recommendation. In
this paper, instead of optimizing rating or top-n recommendation, we focus on
the task of predicting which items generate the highest user engagement. In
particular, we use Twitter as our testbed and cast the problem as a
Collaborative Ranking task where the rich features extracted from the metadata
of the tweets help to complement the transaction information limited to user
ids, item ids, ratings and timestamps. We learn a scoring function that
directly optimizes the user engagement in terms of nDCG@10 on the predicted
ranking. Experiments conducted on an extended version of the MovieTweetings
dataset, released as part of the RecSys Challenge 2014, show the effectiveness
of our approach.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.7143v3,"Millions of Little Minions: Using Packets for Low Latency Network
  Programming and Visibility (Extended Version)","This paper presents a practical approach to rapidly introduce new dataplane
functionality into networks: End-hosts embed tiny programs into packets to
actively query and manipulate a network's internal state. We show how this
""tiny packet program"" (TPP) interface gives end-hosts unprecedented visibility
into network behavior, enabling them to work with the network to achieve a
common goal. Our design leverages what each component does best: (a) switches
forward and execute tiny packet programs (at most 5 instructions) at line rate,
and (b) end-hosts perform arbitrary computation on network state, which are
easy to evolve. Using a hardware prototype on a NetFPGA, we show our design is
feasible, at a reasonable cost. By implementing three different research
proposals, we show that TPPs are also useful. And finally, we present an
architecture in which they can be made secure.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.00058v1,"Generalized Rational Variable Projection With Application in ECG
  Compression","In this paper we develop an adaptive transform-domain technique based on
rational function systems. It is of general importance in several areas of
signal theory, including filter design, transfer function approximation, system
identification, control theory etc. The construction of the proposed method is
discussed in the framework of a general mathematical model called variable
projection. First we generalize this method by adding dimension type free
parameters. Then we deal with the optimization problem of the free parameters.
To this order, based on the well-known particle swarm optimization (PSO)
algorithm, we develop the multi-dimensional hyperbolic PSO algorithm. It is
designed especially for the rational transforms in question. As a result, the
system along with its dimension is dynamically optimized during the process.
The main motivation was to increase the adaptivity while keeping the
computational complexity manageable. We note that the proposed method is of
general nature. As a case study the problem of electrocardiogram (ECG) signal
compression is discussed. By means of comparison tests performed on the
PhysioNet MIT-BIH Arrhythmia database we demonstrate that our method
outperforms other transformation techniques.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1506.03662v4,Variance Reduced Stochastic Gradient Descent with Neighbors,"Stochastic Gradient Descent (SGD) is a workhorse in machine learning, yet its
slow convergence can be a computational bottleneck. Variance reduction
techniques such as SAG, SVRG and SAGA have been proposed to overcome this
weakness, achieving linear convergence. However, these methods are either based
on computations of full gradients at pivot points, or on keeping per data point
corrections in memory. Therefore speed-ups relative to SGD may need a minimal
number of epochs in order to materialize. This paper investigates algorithms
that can exploit neighborhood structure in the training data to share and
re-use information about past stochastic gradients across data points, which
offers advantages in the transient optimization phase. As a side-product we
provide a unified convergence analysis for a family of variance reduction
algorithms, which we call memorization algorithms. We provide experimental
results supporting our theory.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1707.05390v1,TensorLog: Deep Learning Meets Probabilistic DBs,"We present an implementation of a probabilistic first-order logic called
TensorLog, in which classes of logical queries are compiled into differentiable
functions in a neural-network infrastructure such as Tensorflow or Theano. This
leads to a close integration of probabilistic logical reasoning with
deep-learning infrastructure: in particular, it enables high-performance deep
learning frameworks to be used for tuning the parameters of a probabilistic
logic. Experimental results show that TensorLog scales to problems involving
hundreds of thousands of knowledge-base triples and tens of thousands of
examples.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1404.5432v3,Win-Win Kernelization for Degree Sequence Completion Problems,"We study provably effective and efficient data reduction for a class of
NP-hard graph modification problems based on vertex degree properties. We show
fixed-parameter tractability for NP-hard graph completion (that is, edge
addition) cases while we show that there is no hope to achieve analogous
results for the corresponding vertex or edge deletion versions. Our algorithms
are based on transforming graph completion problems into efficiently solvable
number problems and exploiting f-factor computations for translating the
results back into the graph setting. Our core observation is that we encounter
a win-win situation: either the number of edge additions is small or the
problem is polynomial-time solvable. This approach helps in answering an open
question by Mathieson and Szeider [JCSS 2012] concerning the polynomial
kernelizability of Degree Constraint Edge Addition and leads to a general
method of approaching polynomial-time preprocessing for a wider class of degree
sequence completion problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1210.6379v2,Adaptable processes,"We propose the concept of adaptable processes as a way of overcoming the
limitations that process calculi have for describing patterns of dynamic
process evolution. Such patterns rely on direct ways of controlling the
behavior and location of running processes, and so they are at the heart of the
adaptation capabilities present in many modern concurrent systems. Adaptable
processes have a location and are sensible to actions of dynamic update at
runtime; this allows to express a wide range of evolvability patterns for
concurrent processes. We introduce a core calculus of adaptable processes and
propose two verification problems for them: bounded and eventual adaptation.
While the former ensures that the number of consecutive erroneous states that
can be traversed during a computation is bound by some given number k, the
latter ensures that if the system enters into a state with errors then a state
without errors will be eventually reached. We study the (un)decidability of
these two problems in several variants of the calculus, which result from
considering dynamic and static topologies of adaptable processes as well as
different evolvability patterns. Rather than a specification language, our
calculus intends to be a basis for investigating the fundamental properties of
evolvable processes and for developing richer languages with evolvability
capabilities.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1408.7114v1,Faster Computation of Expected Hypervolume Improvement,"The expected improvement algorithm (or efficient global optimization) aims
for global continuous optimization with a limited budget of black-box function
evaluations. It is based on a statistical model of the function learned from
previous evaluations and an infill criterion - the expected improvement - used
to find a promising point for a new evaluation. The `expected improvement'
infill criterion takes into account the mean and variance of a predictive
multivariate Gaussian distribution.
  The expected improvement algorithm has recently been generalized to
multiobjective optimization. In order to measure the improvement of a Pareto
front quantitatively the gain in dominated (hyper-)volume is used. The
computation of the expected hypervolume improvement (EHVI) is a
multidimensional integration of a step-wise defined non-linear function related
to the Gaussian probability density function over an intersection of boxes.
This paper provides a new algorithm for the exact computation of the expected
improvement to more than two objective functions. For the bicriteria case it
has a time complexity in $O(n^2)$ with $n$ denoting the number of points in the
current best Pareto front approximation. It improves previously known
algorithms with time complexity $O(n^3 \log n)$. For tricriteria optimization
we devise an algorithm with time complexity of $O(n^3)$. Besides discussing the
new time complexity bounds the speed of the new algorithm is also tested
empirically on test data. It is shown that further improvements in speed can be
achieved by reusing data structures built up in previous iterations. The
resulting numerical algorithms can be readily used in existing implementations
of hypervolume-based expected improvement algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.03578v2,A Survey on Homomorphic Encryption Schemes: Theory and Implementation,"Legacy encryption systems depend on sharing a key (public or private) among
the peers involved in exchanging an encrypted message. However, this approach
poses privacy concerns. Especially with popular cloud services, the control
over the privacy of the sensitive data is lost. Even when the keys are not
shared, the encrypted material is shared with a third party that does not
necessarily need to access the content. Moreover, untrusted servers, providers,
and cloud operators can keep identifying elements of users long after users end
the relationship with the services. Indeed, Homomorphic Encryption (HE), a
special kind of encryption scheme, can address these concerns as it allows any
third party to operate on the encrypted data without decrypting it in advance.
Although this extremely useful feature of the HE scheme has been known for over
30 years, the first plausible and achievable Fully Homomorphic Encryption (FHE)
scheme, which allows any computable function to perform on the encrypted data,
was introduced by Craig Gentry in 2009. Even though this was a major
achievement, different implementations so far demonstrated that FHE still needs
to be improved significantly to be practical on every platform. First, we
present the basics of HE and the details of the well-known Partially
Homomorphic Encryption (PHE) and Somewhat Homomorphic Encryption (SWHE), which
are important pillars of achieving FHE. Then, the main FHE families, which have
become the base for the other follow-up FHE schemes are presented. Furthermore,
the implementations and recent improvements in Gentry-type FHE schemes are also
surveyed. Finally, further research directions are discussed. This survey is
intended to give a clear knowledge and foundation to researchers and
practitioners interested in knowing, applying, as well as extending the state
of the art HE, PHE, SWHE, and FHE systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0
http://arxiv.org/abs/0805.1386v3,A language for mathematical knowledge management,"We argue that the language of Zermelo Fraenkel set theory with definitions
and partial functions provides the most promising bedrock semantics for
communicating and sharing mathematical knowledge. We then describe a syntactic
sugaring of that language that provides a way of writing remarkably readable
assertions without straying far from the set-theoretic semantics. We illustrate
with some examples of formalized textbook definitions from elementary set
theory and point-set topology. We also present statistics concerning the
complexity of these definitions, under various complexity measures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1006.5922v3,"Using Repeating Decimals As An Alternative To Prime Numbers In
  Encryption","This article is meant to provide an additional point of view, applying known
knowledge, to supply keys that have a series of non-repeating digits, in a
manner that is not usually thought of. Traditionally, prime numbers are used in
encryption as keys that have non-repeating sequences. Non-repetition of digits
in a key is very sought after in encryption. Uniqueness in a digit sequence
defeats decryption by method. In searching for methods of non-decryptable
encryption as well as ways to provide unique sequences, other than using prime
numbers, the idea of using repeating decimals came to me. Applied correctly, a
repeating decimal series of sufficient length will stand in as well for a prime
number. This is so, because only numbers prime to each other will produce
repeating decimals and; within the repeating sequence there is uniqueness of
digit sequence.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1212.1984v3,"Geo-Indistinguishability: Differential Privacy for Location-Based
  Systems","The growing popularity of location-based systems, allowing unknown/untrusted
servers to easily collect huge amounts of information regarding users'
location, has recently started raising serious privacy concerns. In this paper
we study geo-indistinguishability, a formal notion of privacy for
location-based systems that protects the user's exact location, while allowing
approximate information - typically needed to obtain a certain desired service
- to be released. Our privacy definition formalizes the intuitive notion of
protecting the user's location within a radius r with a level of privacy that
depends on r, and corresponds to a generalized version of the well-known
concept of differential privacy. Furthermore, we present a perturbation
technique for achieving geo-indistinguishability by adding controlled random
noise to the user's location. We demonstrate the applicability of our technique
on a LBS application. Finally, we compare our mechanism with other ones in the
literature. It turns our that our mechanism offers the best privacy guarantees,
for the same utility, among all those which do not depend on the prior.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1804.09411v2,"Stable-Matching Voronoi Diagrams: Combinatorial Complexity and
  Algorithms","We study algorithms and combinatorial complexity bounds for
\emph{stable-matching Voronoi diagrams}, where a set, $S$, of $n$ point sites
in the plane determines a stable matching between the points in $\mathbb{R}^2$
and the sites in $S$ such that (i) the points prefer sites closer to them and
sites prefer points closer to them, and (ii) each site has a quota or
""appetite"" indicating the area of the set of points that can be matched to it.
Thus, a stable-matching Voronoi diagram is a solution to the well-known post
office problem with the added (realistic) constraint that each post office has
a limit on the size of its jurisdiction. Previous work on the stable-matching
Voronoi diagram provided existence and uniqueness proofs, but did not analyze
its combinatorial or algorithmic complexity. In this paper, we show that a
stable-matching Voronoi diagram of $n$ point sites has $O(n^{2+\varepsilon})$
faces and edges, for any $\varepsilon>0$, and show that this bound is almost
tight by giving a family of diagrams with $\Theta(n^2)$ faces and edges. We
also provide a discrete algorithm for constructing it in $O(n^3\log n+n^2f(n))$
time in the real-RAM model of computation, where $f(n)$ is the runtime of a
geometric primitive (which we define) that can be approximated numerically, but
cannot, in general, be performed exactly in an algebraic model of computation.
We show, however, how to compute the geometric primitive exactly for polygonal
convex distance functions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0003013v1,A flexible framework for defeasible logics,"Logics for knowledge representation suffer from over-specialization: while
each logic may provide an ideal representation formalism for some problems, it
is less than optimal for others. A solution to this problem is to choose from
several logics and, when necessary, combine the representations. In general,
such an approach results in a very difficult problem of combination. However,
if we can choose the logics from a uniform framework then the problem of
combining them is greatly simplified. In this paper, we develop such a
framework for defeasible logics. It supports all defeasible logics that satisfy
a strong negation principle. We use logic meta-programs as the basis for the
framework.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0511098v1,Information and Stock Prices: A Simple Introduction,"This article summarizes recent research in financial economics about why
information, such as earnings announcements, moves stock prices. The article
does not presume any prior exposure to finance beyond what you might read in
newspapers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
http://arxiv.org/abs/1505.03014v1,"Frappe: Understanding the Usage and Perception of Mobile App
  Recommendations In-The-Wild","This paper describes a real world deployment of a context-aware mobile app
recommender system (RS) called Frappe. Utilizing a hybrid-approach, we
conducted a large-scale app market deployment with 1000 Android users combined
with a small-scale local user study involving 33 users. The resulting usage
logs and subjective feedback enabled us to gather key insights into (1)
context-dependent app usage and (2) the perceptions and experiences of
end-users while interacting with context-aware mobile app recommendations.
While Frappe performs very well based on usage-centric evaluation metrics
insights from the small-scale study reveal some negative user experiences. Our
results point to a number of actionable lessons learned specifically related to
designing, deploying and evaluating mobile context-aware RS in-the-wild with
real users.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.06256v2,A $(1.4 + )$-approximation algorithm for the $2$-Max-Duo problem,"The maximum duo-preservation string mapping (Max-Duo) problem is the
complement of the well studied minimum common string partition (MCSP) problem,
both of which have applications in many fields including text compression and
bioinformatics. $k$-Max-Duo is the restricted version of Max-Duo, where every
letter of the alphabet occurs at most $k$ times in each of the strings, which
is readily reduced into the well known maximum independent set (MIS) problem on
a graph of maximum degree $\Delta \le 6(k-1)$. In particular, $2$-Max-Duo can
then be approximated arbitrarily close to $1.8$ using the state-of-the-art
approximation algorithm for the MIS problem. $2$-Max-Duo was proved APX-hard
and very recently a $(1.6 + \epsilon)$-approximation was claimed, for any
$\epsilon > 0$. In this paper, we present a vertex-degree reduction technique,
based on which, we show that $2$-Max-Duo can be approximated arbitrarily close
to $1.4$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.06503v1,When can Graph Hyperbolicity be computed in Linear Time?,"Hyperbolicity measures, in terms of (distance) metrics, how close a given
graph is to being a tree. Due to its relevance in modeling real-world networks,
hyperbolicity has seen intensive research over the last years. Unfortunately,
the best known algorithms for computing the hyperbolicity number of a graph
(the smaller, the more tree-like) have running time $O(n^4)$, where $n$ is the
number of graph vertices. Exploiting the framework of parameterized complexity
analysis, we explore possibilities for ""linear-time FPT"" algorithms to compute
hyperbolicity. For instance, we show that hyperbolicity can be computed in time
$O(2^{O(k)} + n +m)$ ($m$ being the number of graph edges) while at the same
time, unless the SETH fails, there is no $2^{o(k)}n^2$-time algorithm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1509.07404v1,"Parameterized Algorithms for Min-Max Multiway Cut and List Digraph
  Homomorphism","In this paper we design {\sf FPT}-algorithms for two parameterized problems.
The first is \textsc{List Digraph Homomorphism}: given two digraphs $G$ and $H$
and a list of allowed vertices of $H$ for every vertex of $G$, the question is
whether there exists a homomorphism from $G$ to $H$ respecting the list
constraints. The second problem is a variant of \textsc{Multiway Cut}, namely
\textsc{Min-Max Multiway Cut}: given a graph $G$, a non-negative integer
$\ell$, and a set $T$ of $r$ terminals, the question is whether we can
partition the vertices of $G$ into $r$ parts such that (a) each part contains
one terminal and (b) there are at most $\ell$ edges with only one endpoint in
this part. We parameterize \textsc{List Digraph Homomorphism} by the number $w$
of edges of $G$ that are mapped to non-loop edges of $H$ and we give a time
$2^{O(\ell\cdot\log h+\ell^2\cdot \log \ell)}\cdot n^{4}\cdot \log n$
algorithm, where $h$ is the order of the host graph $H$. We also prove that
\textsc{Min-Max Multiway Cut} can be solved in time $2^{O((\ell r)^2\log \ell
r)}\cdot n^{4}\cdot \log n$. Our approach introduces a general problem, called
{\sc List Allocation}, whose expressive power permits the design of
parameterized reductions of both aforementioned problems to it. Then our
results are based on an {\sf FPT}-algorithm for the {\sc List Allocation}
problem that is designed using a suitable adaptation of the {\em randomized
contractions} technique (introduced by [Chitnis, Cygan, Hajiaghayi, Pilipczuk,
and Pilipczuk, FOCS 2012]).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1703.10415v1,A Polynomial-time Algorithm to Achieve Extended Justified Representation,"We consider a committee voting setting in which each voter approves of a
subset of candidates and based on the approvals, a target number of candidates
are to be selected. In particular we focus on the axiomatic property called
extended justified representation (EJR). Although a committee satisfying EJR is
guaranteed to exist, the computational complexity of finding such a committee
has been an open problem and explicitly mentioned in multiple recent papers. We
settle the complexity of finding a committee satisfying EJR by presenting a
polynomial-time algorithm for the problem. Our algorithmic approach may be
useful for constructing other voting rules in multi-winner voting.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1902.10655v1,"Linear Time Visualization and Search in Big Data using Pixellated Factor
  Space Mapping","It is demonstrated how linear computational time and storage efficient
approaches can be adopted when analyzing very large data sets. More
importantly, interpretation is aided and furthermore, basic processing is
easily supported. Such basic processing can be the use of supplementary, i.e.
contextual, elements, or particular associations. Furthermore pixellated grid
cell contents can be utilized as a basic form of imposed clustering. For a
given resolution level, here related to an associated m-adic ($m$ here is a
non-prime integer) or p-adic ($p$ is prime) number system encoding, such
pixellated mapping results in partitioning. The association of a range of
m-adic and p-adic representations leads naturally to an imposed hierarchical
clustering, with partition levels corresponding to the m-adic-based and
p-adic-based representations and displays. In these clustering embedding and
imposed cluster structures, some analytical visualization and search
applications are described",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1310.1368v1,A note on random greedy coloring of uniform hypergraphs,"The smallest number of edges forming an n-uniform hypergraph which is not
r-colorable is denoted by m(n,r). Erd\H{o}s and Lov\'{a}sz conjectured that
m(n,2)=\theta(n 2^n)$. The best known lower bound m(n,2)=\Omega(sqrt(n/log(n))
2^n) was obtained by Radhakrishnan and Srinivasan in 2000. We present a simple
proof of their result. The proof is based on analysis of random greedy coloring
algorithm investigated by Pluh\'ar in 2009. The proof method extends to the
case of r-coloring, and we show that for any fixed r we have
m(n,r)=\Omega((n/log(n))^(1-1/r) r^n) improving the bound of Kostochka from
2004. We also derive analogous bounds on minimum edge degree of an n-uniform
hypergraph that is not r-colorable.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0504030v2,Sufficient conditions for convergence of the Sum-Product Algorithm,"We derive novel conditions that guarantee convergence of the Sum-Product
algorithm (also known as Loopy Belief Propagation or simply Belief Propagation)
to a unique fixed point, irrespective of the initial messages. The
computational complexity of the conditions is polynomial in the number of
variables. In contrast with previously existing conditions, our results are
directly applicable to arbitrary factor graphs (with discrete variables) and
are shown to be valid also in the case of factors containing zeros, under some
additional conditions. We compare our bounds with existing ones, numerically
and, if possible, analytically. For binary variables with pairwise
interactions, we derive sufficient conditions that take into account local
evidence (i.e., single variable factors) and the type of pair interactions
(attractive or repulsive). It is shown empirically that this bound outperforms
existing bounds.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.02116v2,Ask the GRU: Multi-Task Learning for Deep Text Recommendations,"In a variety of application domains the content to be recommended to users is
associated with text. This includes research papers, movies with associated
plot summaries, news articles, blog posts, etc. Recommendation approaches based
on latent factor models can be extended naturally to leverage text by employing
an explicit mapping from text to factors. This enables recommendations for new,
unseen content, and may generalize better, since the factors for all items are
produced by a compactly-parametrized model. Previous work has used topic models
or averages of word embeddings for this mapping. In this paper we present a
method leveraging deep recurrent neural networks to encode the text sequence
into a latent vector, specifically gated recurrent units (GRUs) trained
end-to-end on the collaborative filtering task. For the task of scientific
paper recommendation, this yields models with significantly higher accuracy. In
cold-start scenarios, we beat the previous state-of-the-art, all of which
ignore word order. Performance is further improved by multi-task learning,
where the text encoder network is trained for a combination of content
recommendation and item metadata prediction. This regularizes the collaborative
filtering model, ameliorating the problem of sparsity of the observed rating
matrix.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1312.5150v1,Semantic Jira - Semantic Expert Finder in the Bug Tracking Tool Jira,"The semantic expert recommender extension for the Jira bug tracking system
semantically searches for similar tickets in Jira and recommends experts and
links to existing organizational (Wiki) knowledge for each ticket. This helps
to avoid redundant work and supports the search and collaboration with experts
in the project management and maintenance phase based on semantically enriched
tickets in Jira.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/cs/0405058v1,Neighborhood-Based Topology Recognition in Sensor Networks,"We consider a crucial aspect of self-organization of a sensor network
consisting of a large set of simple sensor nodes with no location hardware and
only very limited communication range. After having been distributed randomly
in a given two-dimensional region, the nodes are required to develop a sense
for the environment, based on a limited amount of local communication. We
describe algorithmic approaches for determining the structure of boundary nodes
of the region, and the topology of the region. We also develop methods for
determining the outside boundary, the distance to the closest boundary for each
point, the Voronoi diagram of the different boundaries, and the geometric
thickness of the network. Our methods rely on a number of natural assumptions
that are present in densely distributed sets of nodes, and make use of a
combination of stochastics, topology, and geometry. Evaluation requires only a
limited number of simple local computations.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.01251v1,"Health Detection on Cattle Compressed Images in Precision Livestock
  Farming","The constant population growth brings the needing to make up for food also
grows at the same rate. The livestock provides one-third of humans protein base
as meat and milk. To improve cattles health and welfare the pastoral farming
employs Precision Livestock farming (PLF). This technique implementation brings
a challenge to minimize energy consumption due to farmers not having enough
energy or devices to transmit large volumes of information at the size are
received from their farms monitors. Therefore, in this project, we will design
an algorithm to compress and decompress images reducing energy consumption with
the less information lost. Initially, the related problems have been read and
analyzed to learn about the techniques used in the past and to be updated with
the current works. We implemented Seam Carving and LZW algorithms. The
compression of all images, around 1000 takes a time of 5 hours 10 min. We got a
compression rate of 1.82:1 with 13.75s average time for each file and a
decompression rate of 1.64:1 and 7.5 s average time for each file. The memory
consumption we obtained was between 146MB and 504 MB and time consumption was
between 30,5s for 90MB to 12192s for 24410 MB, it was all files.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1804.01614v3,Pigeonring: A Principle for Faster Thresholded Similarity Search,"The pigeonhole principle states that if $n$ items are contained in $m$ boxes,
then at least one box has no more than $n / m$ items. It is utilized to solve
many data management problems, especially for thresholded similarity searches.
Despite many pigeonhole principle-based solutions proposed in the last few
decades, the condition stated by the principle is weak. It only constrains the
number of items in a single box. By organizing the boxes in a ring, we propose
a new principle, called the pigeonring principle, which constrains the number
of items in multiple boxes and yields stronger conditions. To utilize the new
principle, we focus on problems defined in the form of identifying data objects
whose similarities or distances to the query is constrained by a threshold.
Many solutions to these problems utilize the pigeonhole principle to find
candidates that satisfy a filtering condition. By the new principle, stronger
filtering conditions can be established. We show that the pigeonhole principle
is a special case of the new principle. This suggests that all the pigeonhole
principle-based solutions are possible to be accelerated by the new principle.
A universal filtering framework is introduced to encompass the solutions to
these problems based on the new principle. Besides, we discuss how to quickly
find candidates specified by the new principle. The implementation requires
only minor modifications on top of existing pigeonhole principle-based
algorithms. Experimental results on real datasets demonstrate the applicability
of the new principle as well as the superior performance of the algorithms
based on the new principle.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1504.06744v2,Quantitative Games under Failures,"We study a generalisation of sabotage games, a model of dynamic network games
introduced by van Benthem. The original definition of the game is inherently
finite and therefore does not allow one to model infinite processes. We propose
an extension of the sabotage games in which the first player (Runner) traverses
an arena with dynamic weights determined by the second player (Saboteur). In
our model of quantitative sabotage games, Saboteur is now given a budget that
he can distribute amongst the edges of the graph, whilst Runner attempts to
minimise the quantity of budget witnessed while completing his task. We show
that, on the one hand, for most of the classical cost functions considered in
the literature, the problem of determining if Runner has a strategy to ensure a
cost below some threshold is EXPTIME-complete. On the other hand, if the budget
of Saboteur is fixed a priori, then the problem is in PTIME for most cost
functions. Finally, we show that restricting the dynamics of the game also
leads to better complexity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0904.4836v1,"FaceBots: Steps Towards Enhanced Long-Term Human-Robot Interaction by
  Utilizing and Publishing Online Social Information","Our project aims at supporting the creation of sustainable and meaningful
longer-term human-robot relationships through the creation of embodied robots
with face recognition and natural language dialogue capabilities, which exploit
and publish social information available on the web (Facebook). Our main
underlying experimental hypothesis is that such relationships can be
significantly enhanced if the human and the robot are gradually creating a pool
of shared episodic memories that they can co-refer to (shared memories), and if
they are both embedded in a social web of other humans and robots they both
know and encounter (shared friends). In this paper, we are presenting such a
robot, which as we will see achieves two significant novelties.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.06756v1,Super-Resolution of Positive Sources on an Arbitrarily Fine Grid,"In super-resolution it is necessary to locate with high precision point
sources from noisy observations of the spectrum of the signal at low
frequencies capped by f_c. In the case when the point sources are positive and
are located on a grid, it has been recently established that the
super-resolution problem can be solved via linear programming in a stable
manner and that the method is nearly optimal in the minimax sense. The quality
of the reconstruction critically depends on the Rayleigh regularity of the
support of the signal; that is, on the maximum number of sources that can occur
within an interval of side length about 1/f_c. This work extends the earlier
result and shows that the conclusion continues to hold when the locations of
the point sources are arbitrary, i.e., the grid is arbitrarily fine. The proof
relies on new interpolation constructions in Fourier analysis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1710.06501v1,Do Convolutional Neural Networks Learn Class Hierarchy?,"Convolutional Neural Networks (CNNs) currently achieve state-of-the-art
accuracy in image classification. With a growing number of classes, the
accuracy usually drops as the possibilities of confusion increase.
Interestingly, the class confusion patterns follow a hierarchical structure
over the classes. We present visual-analytics methods to reveal and analyze
this hierarchy of similar classes in relation with CNN-internal data. We found
that this hierarchy not only dictates the confusion patterns between the
classes, it furthermore dictates the learning behavior of CNNs. In particular,
the early layers in these networks develop feature detectors that can separate
high-level groups of classes quite well, even after a few training epochs. In
contrast, the latter layers require substantially more epochs to develop
specialized feature detectors that can separate individual classes. We
demonstrate how these insights are key to significant improvement in accuracy
by designing hierarchy-aware CNNs that accelerate model convergence and
alleviate overfitting. We further demonstrate how our methods help in
identifying various quality issues in the training data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1402.3144v2,"A Robust Ensemble Approach to Learn From Positive and Unlabeled Data
  Using SVM Base Models","We present a novel approach to learn binary classifiers when only positive
and unlabeled instances are available (PU learning). This problem is routinely
cast as a supervised task with label noise in the negative set. We use an
ensemble of SVM models trained on bootstrap resamples of the training data for
increased robustness against label noise. The approach can be considered in a
bagging framework which provides an intuitive explanation for its mechanics in
a semi-supervised setting. We compared our method to state-of-the-art
approaches in simulations using multiple public benchmark data sets. The
included benchmark comprises three settings with increasing label noise: (i)
fully supervised, (ii) PU learning and (iii) PU learning with false positives.
Our approach shows a marginal improvement over existing methods in the second
setting and a significant improvement in the third.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.09872v2,"LT4REC:A Lottery Ticket Hypothesis Based Multi-task Practice for Video
  Recommendation System","Click-through rate prediction (CTR) and post-click conversion rate prediction
(CVR) play key roles across all industrial ranking systems, such as
recommendation systems, online advertising, and search engines. Different from
the extensive research on CTR, there is much less research on CVR estimation,
whose main challenge is extreme data sparsity with one or two orders of
magnitude reduction in the number of samples than CTR. People try to solve this
problem with the paradigm of multi-task learning with the sufficient samples of
CTR, but the typical hard sharing method can't effectively solve this problem,
because it is difficult to analyze which parts of network components can be
shared and which parts are in conflict, i.e., there is a large inaccuracy with
artificially designed neurons sharing. In this paper, we model CVR in a
brand-new method by adopting the lottery-ticket-hypothesis-based sparse sharing
multi-task learning, which can automatically and flexibly learn which neuron
weights to be shared without artificial experience. Experiments on the dataset
gathered from traffic logs of Tencent video's recommendation system demonstrate
that sparse sharing in the CVR model significantly outperforms competitive
methods. Due to the nature of weight sparsity in sparse sharing, it can also
significantly reduce computational complexity and memory usage which are very
important in the industrial recommendation system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.06054v5,"Fair Preprocessing: Towards Understanding Compositional Fairness of Data
  Transformers in Machine Learning Pipeline","In recent years, many incidents have been reported where machine learning
models exhibited discrimination among people based on race, sex, age, etc.
Research has been conducted to measure and mitigate unfairness in machine
learning models. For a machine learning task, it is a common practice to build
a pipeline that includes an ordered set of data preprocessing stages followed
by a classifier. However, most of the research on fairness has considered a
single classifier based prediction task. What are the fairness impacts of the
preprocessing stages in machine learning pipeline? Furthermore, studies showed
that often the root cause of unfairness is ingrained in the data itself, rather
than the model. But no research has been conducted to measure the unfairness
caused by a specific transformation made in the data preprocessing stage. In
this paper, we introduced the causal method of fairness to reason about the
fairness impact of data preprocessing stages in ML pipeline. We leveraged
existing metrics to define the fairness measures of the stages. Then we
conducted a detailed fairness evaluation of the preprocessing stages in 37
pipelines collected from three different sources. Our results show that certain
data transformers are causing the model to exhibit unfairness. We identified a
number of fairness patterns in several categories of data transformers.
Finally, we showed how the local fairness of a preprocessing stage composes in
the global fairness of the pipeline. We used the fairness composition to choose
appropriate downstream transformer that mitigates unfairness in the machine
learning pipeline.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.14339v3,Behavioural Preorders via Graded Monads,"Like notions of process equivalence, behavioural preorders on processes come
in many flavours, ranging from fine-grained comparisons such as ready
simulation to coarse-grained ones such as trace inclusion. Often, such
behavioural preorders are characterized in terms of theory inclusion in
dedicated characteristic logics; e.g. simulation is characterized by theory
inclusion in the positive fragment of Hennessy-Milner logic. We introduce a
unified semantic framework for behavioural preorders and their characteristic
logics in which we parametrize the system type as a functor on the category
$\mathsf{Pos}$ of partially ordered sets following the paradigm of universal
coalgebra, while behavioural preorders are captured as graded monads on
$\mathsf{Pos}$, in generalization of a previous approach to notions of process
equivalence. We show that graded monads on $\mathsf{Pos}$ are induced by a form
of graded inequational theories that we introduce here. Moreover, we provide a
general notion of modal logic compatible with a given graded behavioural
preorder, along with a criterion for expressiveness, in the indicated sense of
characterization of the behavioural preorder by theory inclusion. We illustrate
our main result on various behavioural preorders on labelled transition systems
and probabilistic transition systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1004.2159v2,Algebraic Proofs over Noncommutative Formulas,"We study possible formulations of algebraic propositional proof systems
operating with noncommutative formulas. We observe that a simple formulation
gives rise to systems at least as strong as Frege---yielding a semantic way to
define a Cook-Reckhow (i.e., polynomially verifiable) algebraic analog of Frege
proofs, different from that given in [BIKPRS96,GH03]. We then turn to an
apparently weaker system, namely, polynomial calculus (PC) where polynomials
are written as ordered formulas (PC over ordered formulas, for short): an
ordered polynomial is a noncommutative polynomial in which the order of
products in every monomial respects a fixed linear order on variables; an
algebraic formula is ordered if the polynomial computed by each of its
subformulas is ordered. We show that PC over ordered formulas is strictly
stronger than resolution, polynomial calculus and polynomial calculus with
resolution (PCR) and admits polynomial-size refutations for the pigeonhole
principle and the Tseitin's formulas. We conclude by proposing an approach for
establishing lower bounds on PC over ordered formulas proofs, and related
systems, based on properties of lower bounds on noncommutative formulas.
  The motivation behind this work is developing techniques incorporating rank
arguments (similar to those used in algebraic circuit complexity) for
establishing lower bounds on propositional proofs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1302.2529v2,"VM-MAD: a cloud/cluster software for service-oriented academic
  environments","The availability of powerful computing hardware in IaaS clouds makes cloud
computing attractive also for computational workloads that were up to now
almost exclusively run on HPC clusters.
  In this paper we present the VM-MAD Orchestrator software: an open source
framework for cloudbursting Linux-based HPC clusters into IaaS clouds but also
computational grids. The Orchestrator is completely modular, allowing flexible
configurations of cloudbursting policies. It can be used with any batch system
or cloud infrastructure, dynamically extending the cluster when needed. A
distinctive feature of our framework is that the policies can be tested and
tuned in a simulation mode based on historical or synthetic cluster accounting
data.
  In the paper we also describe how the VM-MAD Orchestrator was used in a
production environment at the FGCZ to speed up the analysis of mass
spectrometry-based protein data by cloudbursting to the Amazon EC2. The
advantages of this hybrid system are shown with a large evaluation run using
about hundred large EC2 nodes.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.09918v1,"Kontrol Edilebilir ptSTL Formulu Sentezi -- Synthesis of Controllable
  ptSTL Formulas","In this work, we develop an approach to anomaly detection and prevention
problem using Signal Temporal Logic (STL). This approach consists of two steps:
detection of the causes of the anomalities as STL formulas and prevention of
the satisfaction of the formula via controller synthesis. This work focuses on
the first step and proposes a formula template such that any controllable cause
can be represented in this template. An efficient algorithm to synthesize
formulas in this template is presented. Finally, the results are shown on an
example.
  -----
  Bu bildiride anomali tespiti ve onlenmesi problemine, Sinyal Zamansal Mantigi
(Signal Temporal Logic) tabanli iki asamali bir cozum sunulmaktadir. Ilk asama
nedenlerin tespiti, ikinci asama ise bir kontrol stratejisi ile nedenlerin
sistem uzerinde engellenmesidir. Iki asama birbirine bagimlidir. Bu bildiride,
ilk asama olan istenmeyen olaylarin nedenlerinin tespitinde kullanilan neden
formulu sablonu gelistirilmektedir. Bildiride kullanilan sablon ile butun
kontrol edilebilir formuller tanimlanabilmektedir. Bu sablon icin verimli bir
formul sentezleme algoritmasi sunulmus, ve sonuclar ornek bir sistem uzerinde
gosterilmistir.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.05084v1,"A Survey on Echo Chambers on Social Media: Description, Detection and
  Mitigation","Echo chambers on social media are a significant problem that can elicit a
number of negative consequences, most recently affecting the response to
COVID-19. Echo chambers promote conspiracy theories about the virus and are
found to be linked to vaccine hesitancy, less compliance with mask mandates,
and the practice of social distancing. Moreover, the problem of echo chambers
is connected to other pertinent issues like political polarization and the
spread of misinformation. An echo chamber is defined as a network of users in
which users only interact with opinions that support their pre-existing beliefs
and opinions, and they exclude and discredit other viewpoints. This survey aims
to examine the echo chamber phenomenon on social media from a social computing
perspective and provide a blueprint for possible solutions. We survey the
related literature to understand the attributes of echo chambers and how they
affect the individual and society at large. Additionally, we show the
mechanisms, both algorithmic and psychological, that lead to the formation of
echo chambers. These mechanisms could be manifested in two forms: (1) the bias
of social media's recommender systems and (2) internal biases such as
confirmation bias and homophily. While it is immensely challenging to mitigate
internal biases, there has been great efforts seeking to mitigate the bias of
recommender systems. These recommender systems take advantage of our own biases
to personalize content recommendations to keep us engaged in order to watch
more ads. Therefore, we further investigate different computational approaches
for echo chamber detection and prevention, mainly based around recommender
systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1506.07615v2,"Completing Low-Rank Matrices with Corrupted Samples from Few
  Coefficients in General Basis","Subspace recovery from corrupted and missing data is crucial for various
applications in signal processing and information theory. To complete missing
values and detect column corruptions, existing robust Matrix Completion (MC)
methods mostly concentrate on recovering a low-rank matrix from few corrupted
coefficients w.r.t. standard basis, which, however, does not apply to more
general basis, e.g., Fourier basis. In this paper, we prove that the range
space of an $m\times n$ matrix with rank $r$ can be exactly recovered from few
coefficients w.r.t. general basis, though $r$ and the number of corrupted
samples are both as high as $O(\min\{m,n\}/\log^3 (m+n))$. Our model covers
previous ones as special cases, and robust MC can recover the intrinsic matrix
with a higher rank. Moreover, we suggest a universal choice of the
regularization parameter, which is $\lambda=1/\sqrt{\log n}$. By our
$\ell_{2,1}$ filtering algorithm, which has theoretical guarantees, we can
further reduce the computational cost of our model. As an application, we also
find that the solutions to extended robust Low-Rank Representation and to our
extended robust MC are mutually expressible, so both our theory and algorithm
can be applied to the subspace clustering problem with missing values under
certain conditions. Experiments verify our theories.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/cs/0210022v3,An Elementary Fragment of Second-Order Lambda Calculus,"A fragment of second-order lambda calculus (System F) is defined that
characterizes the elementary recursive functions. Type quantification is
restricted to be non-interleaved and stratified, i.e., the types are assigned
levels, and a quantified variable can only be instantiated by a type of smaller
level, with a slightly liberalized treatment of the level zero.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.1630v2,"Abstractions and sensor design in partial-information, reactive
  controller synthesis","Automated synthesis of reactive control protocols from temporal logic
specifications has recently attracted considerable attention in various
applications in, for example, robotic motion planning, network management, and
hardware design. An implicit and often unrealistic assumption in this past work
is the availability of complete and precise sensing information during the
execution of the controllers. In this paper, we use an abstraction procedure
for systems with partial observation and propose a formalism to investigate
effects of limitations in sensing. The abstraction procedure enables the
existing synthesis methods with partial observation to be applicable and
efficient for systems with infinite (or finite but large number of) states.
This formalism enables us to systematically discover sensing modalities
necessary in order to render the underlying synthesis problems feasible. We use
counterexamples, which witness unrealizability potentially due to the
limitations in sensing and the coarseness in the abstract system, and
interpolation-based techniques to refine the model and the sensing modalities,
i.e., to identify new sensors to be included, in such synthesis problems. We
demonstrate the method on examples from robotic motion planning.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.12178v1,Learning to Generate Wasserstein Barycenters,"Optimal transport is a notoriously difficult problem to solve numerically,
with current approaches often remaining intractable for very large scale
applications such as those encountered in machine learning. Wasserstein
barycenters -- the problem of finding measures in-between given input measures
in the optimal transport sense -- is even more computationally demanding as it
requires to solve an optimization problem involving optimal transport
distances. By training a deep convolutional neural network, we improve by a
factor of 60 the computational speed of Wasserstein barycenters over the
fastest state-of-the-art approach on the GPU, resulting in milliseconds
computational times on $512\times512$ regular grids. We show that our network,
trained on Wasserstein barycenters of pairs of measures, generalizes well to
the problem of finding Wasserstein barycenters of more than two measures. We
demonstrate the efficiency of our approach for computing barycenters of
sketches and transferring colors between multiple images.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1605.07003v2,Image Restoration with Locally Selected Class-Adapted Models,"State-of-the-art algorithms for imaging inverse problems (namely deblurring
and reconstruction) are typically iterative, involving a denoising operation as
one of its steps. Using a state-of-the-art denoising method in this context is
not trivial, and is the focus of current work. Recently, we have proposed to
use a class-adapted denoiser (patch-based using Gaussian mixture models) in a
so-called plug-and-play scheme, wherein a state-of-the-art denoiser is plugged
into an iterative algorithm, leading to results that outperform the best
general-purpose algorithms, when applied to an image of a known class (e.g.
faces, text, brain MRI). In this paper, we extend that approach to handle
situations where the image being processed is from one of a collection of
possible classes or, more importantly, contains regions of different classes.
More specifically, we propose a method to locally select one of a set of
class-adapted Gaussian mixture patch priors, previously estimated from clean
images of those classes. Our approach may be seen as simultaneously performing
segmentation and restoration, thus contributing to bridging the gap between
image restoration/reconstruction and analysis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1612.02652v1,Operations in the era of large distributed telescopes,"The previous generation of astronomical instruments tended to consist of
single receivers in the focal point of one or more physical reflectors. Because
of this, most astronomical data sets were small enough that the raw data could
easily be downloaded and processed on a single machine.
  In the last decade, several large, complex Radio Astronomy instruments have
been built and the SKA is currently being designed. Many of these instruments
have been designed by international teams, and, in the case of LOFAR span an
area larger than a single country. Such systems are ICT telescopes and consist
mainly of complex software. This causes the main operational issues to be
related to the ICT systems and not the telescope hardware. However, it is
important that the operations of the ICT systems are coordinated with the
traditional operational work. Managing the operations of such telescopes
therefore requires an approach that significantly differs from classical
telescope operations.
  The goal of this session is to bring together members of operational teams
responsible for such large-scale ICT telescopes. This gathering will be used to
exchange experiences and knowledge between those teams. Also, we consider such
a meeting as very valuable input for future instrumentation, especially the SKA
and its regional centres.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1906.01455v1,"Algorithmically generating new algebraic features of polynomial systems
  for machine learning","There are a variety of choices to be made in both computer algebra systems
(CASs) and satisfiability modulo theory (SMT) solvers which can impact
performance without affecting mathematical correctness. Such choices are
candidates for machine learning (ML) approaches, however, there are
difficulties in applying standard ML techniques, such as the efficient
identification of ML features from input data which is typically a polynomial
system. Our focus is selecting the variable ordering for cylindrical algebraic
decomposition (CAD), an important algorithm implemented in several CASs, and
now also SMT-solvers. We created a framework to describe all the previously
identified ML features for the problem and then enumerated all options in this
framework to automatically generation many more features. We validate the
usefulness of these with an experiment which shows that an ML choice for CAD
variable ordering is superior to those made by human created heuristics, and
further improved with these additional features. We expect that this technique
of feature generation could be useful for other choices related to CAD, or even
choices for other algorithms with polynomial systems for input.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0306004v2,Managing Dynamic User Communities in a Grid of Autonomous Resources,"One of the fundamental concepts in Grid computing is the creation of Virtual
Organizations (VO's): a set of resource consumers and providers that join
forces to solve a common problem. Typical examples of Virtual Organizations
include collaborations formed around the Large Hadron Collider (LHC)
experiments. To date, Grid computing has been applied on a relatively small
scale, linking dozens of users to a dozen resources, and management of these
VO's was a largely manual operation. With the advance of large collaboration,
linking more than 10000 users with a 1000 sites in 150 counties, a
comprehensive, automated management system is required. It should be simple
enough not to deter users, while at the same time ensuring local site autonomy.
The VO Management Service (VOMS), developed by the EU DataGrid and DataTAG
projects[1, 2], is a secured system for managing authorization for users and
resources in virtual organizations. It extends the existing Grid Security
Infrastructure[3] architecture with embedded VO affiliation assertions that can
be independently verified by all VO members and resource providers. Within the
EU DataGrid project, Grid services for job submission, file- and database
access are being equipped with fine- grained authorization systems that take VO
membership into account. These also give resource owners the ability to ensure
site security and enforce local access policies. This paper will describe the
EU DataGrid security architecture, the VO membership service and the local site
enforcement mechanisms Local Centre Authorization Service (LCAS), Local
Credential Mapping Service(LCMAPS) and the Java Trust and Authorization
Manager.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1709.02208v1,"Simulating Cellular Communications in Vehicular Networks: Making SimuLTE
  Interoperable with Veins","The evolution of cellular technologies toward 5G progressively enables
efficient and ubiquitous communications in an increasing number of fields.
Among these, vehicular networks are being considered as one of the most
promising and challenging applications, requiring support for communications in
high-speed mobility and delay-constrained information exchange in proximity. In
this context, simulation frameworks under the OMNeT++ umbrella are already
available: SimuLTE and Veins for cellular and vehicular systems, respectively.
In this paper, we describe the modifications that make SimuLTE interoperable
with Veins and INET, which leverage the OMNeT++ paradigm, and allow us to
achieve our goal without any modification to either of the latter two. We
discuss the limitations of the previous solution, namely VeinsLTE, which
integrates all three in a single framework, thus preventing independent
evolution and upgrades of each building block.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.09342v1,Linear Time Construction of Indexable Founder Block Graphs,"We introduce a compact pangenome representation based on an optimal
segmentation concept that aims to reconstruct founder sequences from a multiple
sequence alignment (MSA). Such founder sequences have the feature that each row
of the MSA is a recombination of the founders. Several linear time dynamic
programming algorithms have been previously devised to optimize segmentations
that induce founder blocks that then can be concatenated into a set of founder
sequences. All possible concatenation orders can be expressed as a founder
block graph. We observe a key property of such graphs: if the node labels
(founder segments) do not repeat in the paths of the graph, such graphs can be
indexed for efficient string matching. We call such graphs segment repeat-free
founder block graphs.
  We give a linear time algorithm to construct a segment repeat-free founder
block graph given an MSA. The algorithm combines techniques from the founder
segmentation algorithms (Cazaux et al. SPIRE 2019) and fully-functional
bidirectional Burrows-Wheeler index (Belazzougui and Cunial, CPM 2019). We
derive a succinct index structure to support queries of arbitrary length in the
paths of the graph.
  Experiments on an MSA of SAR-CoV-2 strains are reported. An MSA of size
$410\times 29811$ is compacted in one minute into a segment repeat-free founder
block graph of 3900 nodes and 4440 edges. The maximum length and total length
of node labels is 12 and 34968, respectively. The index on the graph takes only
$3\%$ of the size of the MSA.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0911.1174v1,Sharp Dichotomies for Regret Minimization in Metric Spaces,"The Lipschitz multi-armed bandit (MAB) problem generalizes the classical
multi-armed bandit problem by assuming one is given side information consisting
of a priori upper bounds on the difference in expected payoff between certain
pairs of strategies. Classical results of (Lai and Robbins 1985) and (Auer et
al. 2002) imply a logarithmic regret bound for the Lipschitz MAB problem on
finite metric spaces. Recent results on continuum-armed bandit problems and
their generalizations imply lower bounds of $\sqrt{t}$, or stronger, for many
infinite metric spaces such as the unit interval. Is this dichotomy universal?
We prove that the answer is yes: for every metric space, the optimal regret of
a Lipschitz MAB algorithm is either bounded above by any $f\in \omega(\log t)$,
or bounded below by any $g\in o(\sqrt{t})$. Perhaps surprisingly, this
dichotomy does not coincide with the distinction between finite and infinite
metric spaces; instead it depends on whether the completion of the metric space
is compact and countable. Our proof connects upper and lower bound techniques
in online learning with classical topological notions such as perfect sets and
the Cantor-Bendixson theorem. Among many other results, we show a similar
dichotomy for the ""full-feedback"" (a.k.a., ""best-expert"") version.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.01168v1,Kleene Theorems for Free Choice Nets Labelled with Distributed Alphabets,"We provided (PNSE'2014) expressions for free choice nets having ""distributed
choice property"" which makes the nets ""direct product"" representable.
  In a recent work (PNSE'2016), we gave equivalent syntax for a larger class of
free choice nets obtained by dropping distributed choice property.
  In both these works, the classes of free choice nets were restricted by a
""product condition"" on the set of final markings. In this paper we do away with
this restriction and give expressions for the resultant classes of nets which
correspond to ""free choice synchronous products and Zielonka automata"". For
free choice nets with distributed choice property, we give an alternative
characterization using properties checkable in polynomial time.
  Free choice nets we consider are 1-bounded, S-coverable, and are labelled
with distributed alphabets, where S-components of the associated S-cover
respect the given alphabet distribution.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1003.4629v2,A Review of Error Estimation in Adaptive Quadrature,"The most critical component of any adaptive numerical quadrature routine is
the estimation of the integration error. Since the publication of the first
algorithms in the 1960s, many error estimation schemes have been presented,
evaluated and discussed. This paper presents a review of existing error
estimation techniques and discusses their differences and their common
features. Some common shortcomings of these algorithms are discussed and a new
general error estimation technique is presented.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.03425v1,An Algorithmic Meta-Theorem for Graph Modification to Planarity and FOL,"In general, a graph modification problem is defined by a graph modification
operation $\boxtimes$ and a target graph property ${\cal P}$. Typically, the
modification operation $\boxtimes$ may be vertex removal}, edge removal}, edge
contraction}, or edge addition and the question is, given a graph $G$ and an
integer $k$, whether it is possible to transform $G$ to a graph in ${\cal P}$
after applying $k$ times the operation $\boxtimes$ on $G$. This problem has
been extensively studied for particilar instantiations of $\boxtimes$ and
${\cal P}$. In this paper we consider the general property ${\cal P}_{{\phi}}$
of being planar and, moreover, being a model of some First-Order Logic sentence
${\phi}$ (an FOL-sentence). We call the corresponding meta-problem Graph
$\boxtimes$-Modification to Planarity and ${\phi}$ and prove the following
algorithmic meta-theorem: there exists a function $f:\Bbb{N}^{2}\to\Bbb{N}$
such that, for every $\boxtimes$ and every FOL sentence ${\phi}$, the Graph
$\boxtimes$-Modification to Planarity and ${\phi}$ is solvable in
$f(k,|{\phi}|)\cdot n^2$ time. The proof constitutes a hybrid of two different
classic techniques in graph algorithms. The first is the irrelevant vertex
technique that is typically used in the context of Graph Minors and deals with
properties such as planarity or surface-embeddability (that are not
FOL-expressible)
  and the second is the use of Gaifman's Locality Theorem that is the
theoretical base for the meta-algorithmic study of FOL-expressible problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0508084v5,Source Coding for Quasiarithmetic Penalties,"Huffman coding finds a prefix code that minimizes mean codeword length for a
given probability distribution over a finite number of items. Campbell
generalized the Huffman problem to a family of problems in which the goal is to
minimize not mean codeword length but rather a generalized mean known as a
quasiarithmetic or quasilinear mean. Such generalized means have a number of
diverse applications, including applications in queueing. Several
quasiarithmetic-mean problems have novel simple redundancy bounds in terms of a
generalized entropy. A related property involves the existence of optimal
codes: For ``well-behaved'' cost functions, optimal codes always exist for
(possibly infinite-alphabet) sources having finite generalized entropy. Solving
finite instances of such problems is done by generalizing an algorithm for
finding length-limited binary codes to a new algorithm for finding optimal
binary codes for any quasiarithmetic mean with a convex cost function. This
algorithm can be performed using quadratic time and linear space, and can be
extended to other penalty functions, some of which are solvable with similar
space and time complexity, and others of which are solvable with slightly
greater complexity. This reduces the computational complexity of a problem
involving minimum delay in a queue, allows combinations of previously
considered problems to be optimized, and greatly expands the space of problems
solvable in quadratic time and linear space. The algorithm can be extended for
purposes such as breaking ties among possibly different optimal codes, as with
bottom-merge Huffman coding.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0707.0743v1,DIANA Scheduling Hierarchies for Optimizing Bulk Job Scheduling,"The use of meta-schedulers for resource management in large-scale distributed
systems often leads to a hierarchy of schedulers. In this paper, we discuss why
existing meta-scheduling hierarchies are sometimes not sufficient for Grid
systems due to their inability to re-organise jobs already scheduled locally.
Such a job re-organisation is required to adapt to evolving loads which are
common in heavily used Grid infrastructures. We propose a peer-to-peer
scheduling model and evaluate it using case studies and mathematical modelling.
We detail the DIANA (Data Intensive and Network Aware) scheduling algorithm and
its queue management system for coping with the load distribution and for
supporting bulk job scheduling. We demonstrate that such a system is beneficial
for dynamic, distributed and self-organizing resource management and can assist
in optimizing load or job distribution in complex Grid infrastructures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1906.03365v4,Global Semantic Description of Objects based on Prototype Theory,"In this paper, we introduce a novel semantic description approach inspired on
Prototype Theory foundations. We propose a Computational Prototype Model (CPM)
that encodes and stores the central semantic meaning of objects category: the
semantic prototype. Also, we introduce a Prototype-based Description Model that
encodes the semantic meaning of an object while describing its features using
our CPM model. Our description method uses semantic prototypes computed by
CNN-classifications models to create discriminative signatures that describe an
object highlighting its most distinctive features within the category. Our
experiments show that: i) our CPM model (semantic prototype + distance metric)
is able to describe the internal semantic structure of objects categories; ii)
our semantic distance metric can be understood as the object visual typicality
score within a category; iii) our descriptor encoding is semantically
interpretable and significantly outperforms other image global encodings in
clustering and classification tasks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1711.08006v1,Relating Input Concepts to Convolutional Neural Network Decisions,"Many current methods to interpret convolutional neural networks (CNNs) use
visualization techniques and words to highlight concepts of the input seemingly
relevant to a CNN's decision. The methods hypothesize that the recognition of
these concepts are instrumental in the decision a CNN reaches, but the nature
of this relationship has not been well explored. To address this gap, this
paper examines the quality of a concept's recognition by a CNN and the degree
to which the recognitions are associated with CNN decisions. The study
considers a CNN trained for scene recognition over the ADE20k dataset. It uses
a novel approach to find and score the strength of minimally distributed
representations of input concepts (defined by objects in scene images) across
late stage feature maps. Subsequent analysis finds evidence that concept
recognition impacts decision making. Strong recognition of concepts
frequently-occurring in few scenes are indicative of correct decisions, but
recognizing concepts common to many scenes may mislead the network.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.09392v1,Distributed Learning via Filtered Hyperinterpolation on Manifolds,"Learning mappings of data on manifolds is an important topic in contemporary
machine learning, with applications in astrophysics, geophysics, statistical
physics, medical diagnosis, biochemistry, 3D object analysis. This paper
studies the problem of learning real-valued functions on manifolds through
filtered hyperinterpolation of input-output data pairs where the inputs may be
sampled deterministically or at random and the outputs may be clean or noisy.
Motivated by the problem of handling large data sets, it presents a parallel
data processing approach which distributes the data-fitting task among multiple
servers and synthesizes the fitted sub-models into a global estimator. We prove
quantitative relations between the approximation quality of the learned
function over the entire manifold, the type of target function, the number of
servers, and the number and type of available samples. We obtain the
approximation rates of convergence for distributed and non-distributed
approaches. For the non-distributed case, the approximation order is optimal.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.13849v2,Research and Education Towards Smart and Sustainable World,"We propose a vision for directing research and education in the ICT field.
Our Smart and Sustainable World vision targets at prosperity for the people and
the planet through better awareness and control of both human-made and natural
environment. The needs of the society, individuals, and industries are
fulfilled with intelligent systems that sense their environment, make proactive
decisions on actions advancing their goals, and perform the actions on the
environment. We emphasize artificial intelligence, feedback loops, human
acceptance and control, intelligent use of basic resources, performance
parameters, mission-oriented interdisciplinary research, and a holistic systems
view complementing the conventional analytical reductive view as a research
paradigm especially for complex problems. To serve a broad audience, we explain
these concepts and list the essential literature. We suggest planning research
and education by specifying, in a step-wise manner, scenarios, performance
criteria, system models, research problems and education content, resulting in
common goals and a coherent project portfolio as well as education curricula.
Research and education produce feedback to support evolutionary development and
encourage creativity in research. Finally, we propose concrete actions for
realizing this approach.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1205.4489v1,ISWAR: An Imaging System with Watermarking and Attack Resilience,"With the explosive growth of internet technology, easy transfer of digital
multimedia is feasible. However, this kind of convenience with which authorized
users can access information, turns out to be a mixed blessing due to
information piracy. The emerging field of Digital Rights Management (DRM)
systems addresses issues related to the intellectual property rights of digital
content. In this paper, an object-oriented (OO) DRM system, called ""Imaging
System with Watermarking and Attack Resilience"" (ISWAR), is presented that
generates and authenticates color images with embedded mechanisms for
protection against infringement of ownership rights as well as security
attacks. In addition to the methods, in the object-oriented sense, for
performing traditional encryption and decryption, the system implements methods
for visible and invisible watermarking. This paper presents one visible and one
invisible watermarking algorithm that have been integrated in the system. The
qualitative and quantitative results obtained for these two watermarking
algorithms with several benchmark images indicate that high-quality watermarked
images are produced by the algorithms. With the help of experimental results it
is demonstrated that the presented invisible watermarking techniques are
resilient to the well known benchmark attacks and hence a fail-safe method for
providing constant protection to ownership rights.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0
http://arxiv.org/abs/cs/0104001v1,Mantaining Dynamic Matrices for Fully Dynamic Transitive Closure,"In this paper we introduce a general framework for casting fully dynamic
transitive closure into the problem of reevaluating polynomials over matrices.
With this technique, we improve the best known bounds for fully dynamic
transitive closure. In particular, we devise a deterministic algorithm for
general directed graphs that achieves $O(n^2)$ amortized time for updates,
while preserving unit worst-case cost for queries. In case of deletions only,
our algorithm performs updates faster in O(n) amortized time.
  Our matrix-based approach yields an algorithm for directed acyclic graphs
that breaks through the $O(n^2)$ barrier on the single-operation complexity of
fully dynamic transitive closure. We can answer queries in $O(n^\epsilon)$ time
and perform updates in $O(n^{\omega(1,\epsilon,1)-\epsilon}+n^{1+\epsilon})$
time, for any $\epsilon\in[0,1]$, where $\omega(1,\epsilon,1)$ is the exponent
of the multiplication of an $n\times n^{\epsilon}$ matrix by an
$n^{\epsilon}\times n$ matrix. The current best bounds on
$\omega(1,\epsilon,1)$ imply an $O(n^{0.58})$ query time and an $O(n^{1.58})$
update time. Our subquadratic algorithm is randomized, and has one-side error.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0410031v2,Similarity-Based Supervisory Control of Discrete Event Systems,"Due to the appearance of uncontrollable events in discrete event systems, one
may wish to replace the behavior leading to the uncontrollability of
pre-specified language by some quite similar one. To capture this similarity,
we introduce metric to traditional supervisory control theory and generalize
the concept of original controllability to $\ld$-controllability, where $\ld$
indicates the similarity degree of two languages. A necessary and sufficient
condition for a language to be $\ld$-controllable is provided. We then examine
some properties of $\ld$-controllable languages and present an approach to
optimizing a realization.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0801.3119v1,"Categorisation of Spreadsheet Use within Organisations, Incorporating
  Risk: A Progress Report","There has been a significant amount of research into spreadsheets over the
last two decades. Errors in spreadsheets are well documented. Once used mainly
for simple functions such as logging, tracking and totalling information,
spreadsheets with enhanced formulas are being used for complex calculative
models. There are many software packages and tools which assist in detecting
errors within spreadsheets. There has been very little evidence of
investigation into the spreadsheet risks associated with the main stream
operations within an organisation. This study is a part of the investigation
into the means of mitigating risks associated with spreadsheet use within
organisations. In this paper the authors present and analyse three proposed
models for categorisation of spreadsheet use and the level of risks involved.
The models are analysed in the light of current knowledge and the general risks
associated with organisations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1
http://arxiv.org/abs/cs/9905009v1,Inside-Outside Estimation of a Lexicalized PCFG for German,"The paper describes an extensive experiment in inside-outside estimation of a
lexicalized probabilistic context free grammar for German verb-final clauses.
Grammar and formalism features which make the experiment feasible are
described. Successive models are evaluated on precision and recall of phrase
markup.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.06963v2,"Needle in a Haystack: Label-Efficient Evaluation under Extreme Class
  Imbalance","Important tasks like record linkage and extreme classification demonstrate
extreme class imbalance, with 1 minority instance to every 1 million or more
majority instances. Obtaining a sufficient sample of all classes, even just to
achieve statistically-significant evaluation, is so challenging that most
current approaches yield poor estimates or incur impractical cost. Where
importance sampling has been levied against this challenge, restrictive
constraints are placed on performance metrics, estimates do not come with
appropriate guarantees, or evaluations cannot adapt to incoming labels. This
paper develops a framework for online evaluation based on adaptive importance
sampling. Given a target performance metric and model for $p(y|x)$, the
framework adapts a distribution over items to label in order to maximize
statistical precision. We establish strong consistency and a central limit
theorem for the resulting performance estimates, and instantiate our framework
with worked examples that leverage Dirichlet-tree models. Experiments
demonstrate an average MSE superior to state-of-the-art on fixed label budgets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0806.1802v1,"Une nouvelle rgle de combinaison rpartissant le conflit -
  Applications en imagerie Sonar et classification de cibles Radar","These last years, there were many studies on the problem of the conflict
coming from information combination, especially in evidence theory. We can
summarise the solutions for manage the conflict into three different
approaches: first, we can try to suppress or reduce the conflict before the
combination step, secondly, we can manage the conflict in order to give no
influence of the conflict in the combination step, and then take into account
the conflict in the decision step, thirdly, we can take into account the
conflict in the combination step. The first approach is certainly the better,
but not always feasible. It is difficult to say which approach is the best
between the second and the third. However, the most important is the produced
results in applications. We propose here a new combination rule that
distributes the conflict proportionally on the element given this conflict. We
compare these different combination rules on real data in Sonar imagery and
Radar target classification.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1909.12536v1,"Entropy Stable p-Nonconforming Discretizations with the
  Summation-by-Parts Property for the Compressible Euler equations","The entropy conservative/stable algorithm of Friedrich~\etal (2018) for
hyperbolic conservation laws on nonconforming p-refined/coarsened Cartesian
grids, is extended to curvilinear grids for the compressible Euler equations.
The primary focus is on constructing appropriate coupling procedures across the
curvilinear nonconforming interfaces. A simple and flexible approach is
proposed that uses interpolation operators from one element to the other. On
the element faces, the analytic metrics are used to construct coupling terms,
while metric terms in the volume are approximated to satisfy a discretization
of the geometric conservation laws. The resulting scheme is entropy
conservative/stable, elementwise conservative, and freestream preserving. The
accuracy and stability properties of the resulting numerical algorithm are
shown to be comparable to those of the original conforming scheme (~p+1
convergence) in the context of the isentropic Euler vortex and the inviscid
Taylor-Green vortex problems on manufactured high order grids.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1209.1327v1,The frog and the octopus: a conceptual model of software development,"We propose a conceptual model of software development that encompasses all
approaches: traditional or agile, light and heavy, for large and small
development efforts. The model identifies both the common aspects in all
software development, i.e., elements found in some form or another in each and
every software development project (Intent, Product, People, Work, Time,
Quality, Risk, Cost, Value), as well as the variable part, i.e., the main
factors that cause the very wide variations we can find in the software
development world (Size, Age, Criticality, Architecture stability, Business
model, Governance, Rate of change, Geographic distribution). We show how the
model can be used as an explanatory theory of software development, as a tool
for analysis of practices, techniques, processes, as the basis for curriculum
design or for software process adoption and improvement, and to support
empirical research on software development methods. This model is also proposed
as a way to depolarize the debate on agile methods versus the
rest-of-the-world: a unified model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2006.07163v2,Nefele: Process Orchestration for the Cloud,"Virtualization, either at OS- or hardware level, plays an important role in
cloud computing. It enables easier automation and faster deployment in
distributed environments. While virtualized infrastructures provide a level of
management flexibility, they lack practical abstraction of the distributed
resources. A developer in such an environment still needs to deal with all the
complications of building a distributed software system. Different
orchestration systems are built to provide that abstraction; however, they do
not solve the inherent challenges of distributed systems, such as
synchronization issues or resilience to failures. This paper introduces Nefele,
a decentralized process orchestration system that automatically deploys and
manages individual processes, rather than containers/VMs, within a cluster.
Nefele is inspired by the Single System Image (SSI) vision of mitigating the
intricacies of remote execution, yet it maintains the flexibility and
performance of virtualized infrastructures. Nefele offers a set of APIs for
building cloud-native applications that lets the developer easily build,
deploy, and scale applications in a cloud environment. We have implemented and
deployed Nefele on a cluster in our datacenter and evaluated its performance.
Our evaluations show that Nefele can effectively deploy, scale, and monitor
processes across a distributed environment, while it incorporates essential
primitives to build a distributed software system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1211.3063v1,"From Angular Manifolds to the Integer Lattice: Guaranteed Orientation
  Estimation with Application to Pose Graph Optimization","Estimating the orientations of nodes in a pose graph from relative angular
measurements is challenging because the variables live on a manifold product
with nontrivial topology and the maximum-likelihood objective function is
non-convex and has multiple local minima; these issues prevent iterative
solvers to be robust for large amounts of noise. This paper presents an
approach that allows working around the problem of multiple minima, and is
based on the insight that the original estimation problem on orientations is
equivalent to an unconstrained quadratic optimization problem on integer
vectors. This equivalence provides a viable way to compute the maximum
likelihood estimate and allows guaranteeing that such estimate is almost surely
unique. A deeper consequence of the derivation is that the maximum likelihood
solution does not necessarily lead to an estimate that is ""close"" to the actual
nodes orientations, hence it is not necessarily the best choice for the problem
at hand. To alleviate this issue, our algorithm computes a set of estimates,
for which we can derive precise probabilistic guarantees. Experiments show that
the method is able to tolerate extreme amounts of noise (e.g., {\sigma} =
30{\deg} on each measurement) that are above all noise levels of sensors
commonly used in mapping. For most range-finder-based scenarios, the
multi-hypothesis estimator returns only a single hypothesis, because the
problem is very well constrained. Finally, using the orientations estimate
provided by our method to bootstrap the initial guess of pose graph
optimization methods improves their robustness and makes them avoid local
minima even for high levels of noise.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0212034v1,Types of Cost in Inductive Concept Learning,"Inductive concept learning is the task of learning to assign cases to a
discrete set of classes. In real-world applications of concept learning, there
are many different types of cost involved. The majority of the machine learning
literature ignores all types of cost (unless accuracy is interpreted as a type
of cost measure). A few papers have investigated the cost of misclassification
errors. Very few papers have examined the many other types of cost. In this
paper, we attempt to create a taxonomy of the different types of cost that are
involved in inductive concept learning. This taxonomy may help to organize the
literature on cost-sensitive learning. We hope that it will inspire researchers
to investigate all types of cost in inductive concept learning in more depth.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0711.0607v1,Exploring the Composition of Unit Test Suites,"In agile software development, test code can considerably contribute to the
overall source code size. Being a valuable asset both in terms of verification
and documentation, the composition of a test suite needs to be well understood
in order to identify opportunities as well as weaknesses for further evolution.
In this paper, we argue that the visualization of structural characteristics is
a viable means to support the exploration of test suites. Thanks to general
agreement on a limited set of key test design principles, such visualizations
are relatively easy to interpret. In particular, we present visualizations that
support testers in (i) locating test cases; (ii) examining the relation between
test code and production code; and (iii) studying the composition of and
dependencies within test cases. By means of two case studies, we demonstrate
how visual patterns help to identify key test suite characteristics. This
approach forms the first step in assisting a developer to build up
understanding about test suites beyond code reading.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9906032v1,Formal Modeling in a Commercial Setting: A Case Study,"This paper describes a case study conducted in collaboration with Nortel to
demonstrate the feasibility of applying formal modeling techniques to
telecommunication systems. A formal description language, SDL, was chosen by
our qualitative CASE tool evaluation to model a multimedia-messaging system
described by an 80-page natural language specification. Our model was used to
identify errors in the software requirements document and to derive test
suites, shadowing the existing development process and keeping track of a
variety of productivity data.",0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/0706.0103v4,Many concepts and two logics of algorithmic reduction,"Within the program of finding axiomatizations for various parts of
computability logic, it was proved earlier that the logic of interactive Turing
reduction is exactly the implicative fragment of Heyting's intuitionistic
calculus. That sort of reduction permits unlimited reusage of the computational
resource represented by the antecedent. An at least equally basic and natural
sort of algorithmic reduction, however, is the one that does not allow such
reusage. The present article shows that turning the logic of the first sort of
reduction into the logic of the second sort of reduction takes nothing more
than just deleting the contraction rule from its Gentzen-style axiomatization.
The first (Turing) sort of interactive reduction is also shown to come in three
natural versions. While those three versions are very different from each
other, their logical behaviors (in isolation) turn out to be indistinguishable,
with that common behavior being precisely captured by implicative
intuitionistic logic. Among the other contributions of the present article is
an informal introduction of a series of new -- finite and bounded -- versions
of recurrence operations and the associated reduction operations. An online
source on computability logic can be found at
http://www.cis.upenn.edu/~giorgi/cl.html",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1906.04003v3,"Data-driven quasi-interpolant spline surfaces for point cloud
  approximation","In this paper we investigate a local surface approximation, the Weighted
Quasi Interpolant Spline Approximation (wQISA), specifically designed for large
and noisy point clouds. We briefly describe the properties of the wQISA
representation and introduce a novel data-driven implementation, which combines
prediction capability and complexity efficiency. We provide an extended
comparative analysis with other continuous approximations on real data,
including different types of surfaces and levels of noise, such as 3D models,
terrain data and digital environmental data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1910.09118v1,Automated Reasoning with Restricted Intensional Sets,"Intensional sets, i.e., sets given by a property rather than by enumerating
elements, are widely recognized as a key feature to describe complex problems
(see, e.g., specification languages such as B and Z). Notwithstanding, very few
tools exist supporting high-level automated reasoning on general formulas
involving intensional sets. In this paper we present a decision procedure for a
first-order logic language offering both extensional and (a restricted form of)
intensional sets (RIS). RIS are introduced as first-class citizens of the
language and set-theoretical operators on RIS are dealt with as constraints.
Syntactic restrictions on RIS guarantee that the denoted sets are finite,
though unbounded. The language of RIS, called L_RIS , is parametric with
respect to any first-order theory X providing at least equality and a decision
procedure for X-formulas. In particular, we consider the instance of L_RIS when
X is the theory of hereditarily finite sets and binary relations. We also
present a working implementation of this instance as part of the {log} tool and
we show through a number of examples and two case studies that, although RIS
are a subclass of general intensional sets, they are still sufficiently
expressive as to encode and solve many interesting problems. Finally, an
extensive empirical evaluation provides evidence that the tool can be used in
practice.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1911.07707v1,Building Fast Fuzzers,"Fuzzing is one of the key techniques for evaluating the robustness of
programs against attacks. Fuzzing has to be effective in producing inputs that
cover functionality and find vulnerabilities. But it also has to be efficient
in producing such inputs quickly. Random fuzzers are very efficient, as they
can quickly generate random inputs; but they are not very effective, as the
large majority of inputs generated is syntactically invalid. Grammar-based
fuzzers make use of a grammar (or another model for the input language) to
produce syntactically correct inputs, and thus can quickly cover input space
and associated functionality. Existing grammar-based fuzzers are surprisingly
inefficient, though: Even the fastest grammar fuzzer Dharma still produces
inputs about a thousand times slower than the fastest random fuzzer. So far,
one can have an effective or an efficient fuzzer, but not both.
  In this paper, we describe how to build fast grammar fuzzers from the ground
up, treating the problem of fuzzing from a programming language implementation
perspective. Starting with a Python textbook approach, we adopt and adapt
optimization techniques from functional programming and virtual machine
implementation techniques together with other novel domain-specific
optimizations in a step-by-step fashion. In our F1 prototype fuzzer, these
improve production speed by a factor of 100--300 over the fastest grammar
fuzzer Dharma. As F1 is even 5--8 times faster than a lexical random fuzzer, we
can find bugs faster and test with much larger valid inputs than previously
possible.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1101.0115v1,First-order Fragments with Successor over Infinite Words,"We consider fragments of first-order logic and as models we allow finite and
infinite words simultaneously. The only binary relations apart from equality
are order comparison < and the successor predicate +1. We give
characterizations of the fragments Sigma2 = Sigma2[<,+1] and FO2 = FO2[<,+1] in
terms of algebraic and topological properties. To this end we introduce the
factor topology over infinite words. It turns out that a language L is in the
intersection of FO2 and Sigma2 if and only if L is the interior of an FO2
language. Symmetrically, a language is in the intersection of FO2 and Pi2 if
and only if it is the topological closure of an FO2 language. The fragment
Delta2, which by definition is the intersection of Sigma2 and Pi2 contains
exactly the clopen languages in FO2. In particular, over infinite words Delta2
is a strict subclass of FO2. Our characterizations yield decidability of the
membership problem for all these fragments over finite and infinite words; and
as a corollary we also obtain decidability for infinite words. Moreover, we
give a new decidable algebraic characterization of dot-depth 3/2 over finite
words. Decidability of dot-depth 3/2 over finite words was first shown by
Gla{\ss}er and Schmitz in STACS 2000, and decidability of the membership
problem for FO2 over infinite words was shown 1998 by Wilke in his habilitation
thesis whereas decidability of Sigma2 over infinite words was not known before.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.02881v1,"Graph Infomax Adversarial Learning for Treatment Effect Estimation with
  Networked Observational Data","Treatment effect estimation from observational data is a critical research
topic across many domains. The foremost challenge in treatment effect
estimation is how to capture hidden confounders. Recently, the growing
availability of networked observational data offers a new opportunity to deal
with the issue of hidden confounders. Unlike networked data in traditional
graph learning tasks, such as node classification and link detection, the
networked data under the causal inference problem has its particularity, i.e.,
imbalanced network structure. In this paper, we propose a Graph Infomax
Adversarial Learning (GIAL) model for treatment effect estimation, which makes
full use of the network structure to capture more information by recognizing
the imbalance in network structure. We evaluate the performance of our GIAL
model on two benchmark datasets, and the results demonstrate superiority over
the state-of-the-art methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0909.4575v2,Randomness Efficient Steganography,"Steganographic protocols enable one to embed covert messages into
inconspicuous data over a public communication channel in such a way that no
one, aside from the sender and the intended receiver, can even detect the
presence of the secret message. In this paper, we provide a new
provably-secure, private-key steganographic encryption protocol secure in the
framework of Hopper et al. We first present a ""one-time stegosystem"" that
allows two parties to transmit messages of length at most that of the shared
key with information-theoretic security guarantees. The employment of a
pseudorandom generator (PRG) permits secure transmission of longer messages in
the same way that such a generator allows the use of one-time pad encryption
for messages longer than the key in symmetric encryption. The advantage of our
construction, compared to all previous work is randomness efficiency: in the
information theoretic setting our protocol embeds a message of length n bits
using a shared secret key of length (1+o(1))n bits while achieving security
2^{-n/log^{O(1)}n}; simply put this gives a rate of key over message that is 1
as n tends to infinity (the previous best result achieved a constant rate
greater than 1 regardless of the security offered). In this sense, our protocol
is the first truly randomness efficient steganographic system. Furthermore, in
our protocol, we can permit a portion of the shared secret key to be public
while retaining precisely n private key bits. In this setting, by separating
the public and the private randomness of the shared key, we achieve security of
2^{-n}. Our result comes as an effect of the application of randomness
extractors to stegosystem design. To the best of our knowledge this is the
first time extractors have been applied in steganography.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.06120v2,A fast time domain solver for the equilibrium Dyson equation,"We consider the numerical solution of the real time equilibrium Dyson
equation, which is used in calculations of the dynamical properties of quantum
many-body systems. We show that this equation can be written as a system of
coupled, nonlinear, convolutional Volterra integro-differential equations, for
which the kernel depends self-consistently on the solution. As is typical in
the numerical solution of Volterra-type equations, the computational bottleneck
is the quadratic-scaling cost of history integration. However, the structure of
the nonlinear Volterra integral operator precludes the use of standard fast
algorithms. We propose a quasilinear-scaling FFT-based algorithm which respects
the structure of the nonlinear integral operator. The resulting method can
reach large propagation times, and is thus well-suited to explore quantum
many-body phenomena at low energy scales. We demonstrate the solver with two
standard model systems: the Bethe graph, and the Sachdev-Ye-Kitaev model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1201.0349v1,Why We Shouldn't Forget Multicast in Name-oriented Publish/Subscribe,"Name-oriented networks introduce the vision of an information-centric,
secure, globally available publish-subscribe infrastructure. Current approaches
concentrate on unicast-based pull mechanisms and thereby fall short in
automatically updating content at receivers. In this paper, we argue that an
inclusion of multicast will grant additional benefits to the network layer,
namely efficient distribution of real-time data, a many-to-many communication
model, and simplified rendezvous processes. These aspects are comprehensively
reflected by a group-oriented naming concept that integrates the various
available group schemes and introduces new use cases. A first draft of this
name-oriented multicast access has been implemented in the HAMcast middleware.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0203013v1,Representing and Aggregating Conflicting Beliefs,"We consider the two-fold problem of representing collective beliefs and
aggregating these beliefs. We propose modular, transitive relations for
collective beliefs. They allow us to represent conflicting opinions and they
have a clear semantics. We compare them with the quasi-transitive relations
often used in Social Choice. Then, we describe a way to construct the belief
state of an agent informed by a set of sources of varying degrees of
reliability. This construction circumvents Arrow's Impossibility Theorem in a
satisfactory manner. Finally, we give a simple set-theory-based operator for
combining the information of multiple agents. We show that this operator
satisfies the desirable invariants of idempotence, commutativity, and
associativity, and, thus, is well-behaved when iterated, and we describe a
computationally effective way of computing the resulting belief state.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.08906v1,"Characterizing and Improving the Resilience of Accelerators in
  Autonomous Robots","Motion planning is a computationally intensive and well-studied problem in
autonomous robots. However, motion planning hardware accelerators (MPA) must be
soft-error resilient for deployment in safety-critical applications, and
blanket application of traditional mitigation techniques is ill-suited due to
cost, power, and performance overheads. We propose Collision Exposure Factor
(CEF), a novel metric to assess the failure vulnerability of circuits
processing spatial relationships, including motion planning. CEF is based on
the insight that the safety violation probability increases with the surface
area of the physical space exposed by a bit-flip. We evaluate CEF on four MPAs.
We demonstrate empirically that CEF is correlated with safety violation
probability, and that CEF-aware selective error mitigation provides 12.3x,
9.6x, and 4.2x lower Failures-In-Time (FIT) rate on average for the same amount
of protected memory compared to uniform, bit-position, and
access-frequency-aware selection of critical data. Furthermore, we show how to
employ CEF to enable fault characterization using 23,000x fewer fault injection
(FI) experiments than exhaustive FI, and evaluate our FI approach on different
robots and MPAs. We demonstrate that CEF-aware FI can provide insights on
vulnerable bits in an MPA while taking the same amount of time as uniform
statistical FI. Finally, we use the CEF to formulate guidelines for designing
soft-error resilient MPAs.",0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1301.6118v2,X THEN X: Manipulation of Same-System Runoff Elections,"Do runoff elections, using the same voting rule as the initial election but
just on the winning candidates, increase or decrease the complexity of
manipulation? Does allowing revoting in the runoff increase or decrease the
complexity relative to just having a runoff without revoting? For both weighted
and unweighted voting, we show that even for election systems with simple
winner problems the complexity of manipulation, manipulation with runoffs, and
manipulation with revoting runoffs are independent, in the abstract. On the
other hand, for some important, well-known election systems we determine what
holds for each of these cases. For no such systems do we find runoffs lowering
complexity, and for some we find that runoffs raise complexity. Ours is the
first paper to show that for natural, unweighted election systems, runoffs can
increase the manipulation complexity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.02046v1,Older Adults and Brain-Computer Interface: An Exploratory Study,"In this exploratory study, we examine the possibilities of non-invasive
Brain-Computer Interface (BCI) in the context of Smart Home Technology (SHT)
targeted at older adults. During two workshops, one stationary, and one online
via Zoom, we researched the insights of the end users concerning the potential
of the BCI in the SHT setting. We explored its advantages and drawbacks, and
the features older adults see as vital as well as the ones that they would
benefit from. Apart from evaluating the participants' perception of such
devices during the two workshops we also analyzed some key considerations
resulting from the insights gathered during the workshops, such as potential
barriers, ways to mitigate them, strengths and opportunities connected to BCI.
These may be useful for designing BCI interaction paradigms and pinpointing
areas of interest to pursue in further studies.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.13324v1,"On Generalized Metric Spaces for the Simply Typed Lambda-Calculus
  (Extended Version)","Generalized metrics, arising from Lawvere's view of metric spaces as enriched
categories, have been widely applied in denotational semantics as a way to
measure to which extent two programs behave in a similar, although non
equivalent, way. However, the application of generalized metrics to
higher-order languages like the simply typed lambda calculus has so far proved
unsatisfactory. In this paper we investigate a new approach to the construction
of cartesian closed categories of generalized metric spaces. Our starting point
is a quantitative semantics based on a generalization of usual logical
relations. Within this setting, we show that several families of generalized
metrics provide ways to extend the Euclidean metric to all higher-order types.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.06066v2,The linear arboricity conjecture for graphs of low degeneracy,"A $k$-linear coloring of a graph $G$ is an edge coloring of $G$ with $k$
colors so that each color class forms a linear forest -- a forest whose each
connected component is a path. The linear arboricity $\chi_l'(G)$ of $G$ is the
minimum integer $k$ such that there exists a $k$-linear coloring of $G$.
Akiyama, Exoo and Harary conjectured in 1980 that for every graph $G$,
$\chi_l'(G)\leq \left \lceil \frac{\Delta(G)+1}{2}\right\rceil$ where
$\Delta(G)$ is the maximum degree of $G$. First, we prove the conjecture for
3-degenerate graphs. This establishes the conjecture for graphs of treewidth at
most 3 and provides an alternative proof for the conjecture in some classes of
graphs like cubic graphs and triangle-free planar graphs for which the
conjecture was already known to be true. Next, for every 2-degenerate graph
$G$, we show that $\chi'_l(G)=\left\lceil\frac{\Delta(G)}{2}\right\rceil$ if
$\Delta(G)\geq 5$. We conjecture that this equality holds also when
$\Delta(G)\in\{3,4\}$ and show that this is the case for some well-known
subclasses of 2-degenerate graphs. All our proofs can be converted into linear
time algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.07531v2,Popular Matchings with Multiple Partners,"Our input is a bipartite graph $G = (A \cup B,E)$ where each vertex in $A
\cup B$ has a preference list strictly ranking its neighbors. The vertices in
$A$ and in $B$ are called students and courses, respectively. Each student $a$
seeks to be matched to $\mathsf{cap}(a) \ge 1$ courses while each course $b$
seeks $\mathsf{cap}(b) \ge 1$ many students to be matched to it. The
Gale-Shapley algorithm computes a pairwise-stable matching (one with no
blocking edge) in $G$ in linear time. We consider the problem of computing a
popular matching in $G$ -- a matching $M$ is popular if $M$ cannot lose an
election to any matching where vertices cast votes for one matching versus
another. Our main contribution is to show that a max-size popular matching in
$G$ can be computed by the 2-level Gale-Shapley algorithm in linear time. This
is an extension of the classical Gale-Shapley algorithm and we prove its
correctness via linear programming.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1209.4927v2,"History-Preserving Bisimilarity for Higher-Dimensional Automata via Open
  Maps","We show that history-preserving bisimilarity for higher-dimensional automata
has a simple characterization directly in terms of higher-dimensional
transitions. This implies that it is decidable for finite higher-dimensional
automata. To arrive at our characterization, we apply the open-maps framework
of Joyal, Nielsen and Winskel in the category of unfoldings of precubical sets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.08011v1,One-way resynchronizability of word transducers,"The origin semantics for transducers was proposed in 2014, and led to various
characterizations and decidability results that are in contrast with the
classical semantics. In this paper we add a further decidability result for
characterizing transducers that are close to one-way transducers in the origin
semantics. We show that it is decidable whether a non-deterministic two-way
word transducer can be resynchronized by a bounded, regular resynchronizer into
an origin-equivalent one-way transducer. The result is in contrast with the
usual semantics, where it is undecidable to know if a non-deterministic two-way
transducer is equivalent to some one-way transducer.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1912.10631v1,A Component-Based Formal Language Workbench,"The CBS framework supports component-based specification of programming
languages. It aims to significantly reduce the effort of formal language
specification, and thereby encourage language developers to exploit formal
semantics more widely. CBS provides an extensive library of reusable language
specification components, facilitating co-evolution of languages and their
specifications.
  After introducing CBS and its formal definition, this short paper reports
work in progress on generating an IDE for CBS from the definition. It also
considers the possibility of supporting component-based language specification
in other formal language workbenches.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.09288v1,"On combinatorial optimization for dominating sets (literature survey,
  new models)","The paper focuses on some versions of connected dominating set problems:
basic problems and multicriteria problems. A literature survey on basic problem
formulations and solving approaches is presented. The basic connected
dominating set problems are illustrated by simplifyed numerical examples. New
integer programming formulations of dominating set problems (with multiset
estimates) are suggested.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0
http://arxiv.org/abs/cs/0001003v1,Why C++ is not very fit for GUI programming,"With no intent of starting a holy war, this paper lists several annoying C++
birthmarks that the author has come across developing GUI class libraries.
C++'s view of classes, instances and hierarchies appears tantalizingly close to
GUI concepts of controls, widgets, window classes and subwindows. OO models of
C++ and of a window system are however different. C++ was designed to be a
""static"" language with a lexical name scoping, static type checking and
hierarchies defined at compile time. Screen objects on the other hand are
inherently dynamic; they usually live well beyond the procedure/block that
created them; the hierarchy of widgets is defined to a large extent by layout,
visibility and event flow. Many GUI fundamentals such as dynamic and geometric
hierarchies of windows and controls, broadcasting and percolation of events are
not supported directly by C++ syntax or execution semantics (or supported as
""exceptions"" -- pun intended). Therefore these features have to be emulated in
C++ GUI code. This leads to duplication of a graphical toolkit or a window
manager functionality, code bloat, engaging in unsafe practices and forgoing of
many strong C++ features (like scoping rules and compile-time type checking).
This paper enumerates a few major C++/GUI sores and illustrates them on simple
examples.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1101.2985v1,"Resequencing: A Method for Conforming to Conventions for Sharing Credits
  Among Multiple Authors","Devising an appropriate scheme that assigns the weights to share credits
among multiple authors of a paper is a challenging task. This challenge comes
from the fact that different types of conventions might be followed among
different research discipline or research groups. In this paper, we discuss
that for the purpose of evaluating the quality of research produced by authors,
one can resequence either authors or weights and can apply a weight assignment
policy which the evaluator deems fit for the particular research discipline or
research group.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1906.01082v4,"Representation Theoretic Patterns in Multi-Frequency Class Averaging for
  Three-Dimensional Cryo-Electron Microscopy","We develop in this paper a novel intrinsic classification algorithm --
multi-frequency class averaging (MFCA) -- for classifying noisy projection
images obtained from three-dimensional cryo-electron microscopy (cryo-EM) by
the similarity among their viewing directions. This new algorithm leverages
multiple irreducible representations of the unitary group to introduce
additional redundancy into the representation of the optimal in-plane
rotational alignment, extending and outperforming the existing class averaging
algorithm that uses only a single representation. The formal algebraic model
and representation theoretic patterns of the proposed MFCA algorithm extend the
framework of Hadani and Singer to arbitrary irreducible representations of the
unitary group. We conceptually establish the consistency and stability of MFCA
by inspecting the spectral properties of a generalized local parallel transport
operator through the lens of Wigner $D$-matrices. We demonstrate the efficacy
of the proposed algorithm with numerical experiments.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0906.1845v1,"Towards Improving Validation, Verification, Crash Investigations, and
  Event Reconstruction of Flight-Critical Systems with Self-Forensics","This paper introduces a novel concept of self-forensics to complement the
standard autonomic self-CHOP properties of the self-managed systems, to be
specified in the Forensic Lucid language. We argue that self-forensics, with
the forensics taken out of the cybercrime domain, is applicable to
""self-dissection"" for the purpose of verification of autonomous software and
hardware systems of flight-critical systems for automated incident and anomaly
analysis and event reconstruction by the engineering teams in a variety of
incident scenarios during design and testing as well as actual flight data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1509.03604v2,"Fundamental concepts in the Cyclus nuclear fuel cycle simulation
  framework","As nuclear power expands, technical, economic, political, and environmental
analyses of nuclear fuel cycles by simulators increase in importance. To date,
however, current tools are often fleet-based rather than discrete and
restrictively licensed rather than open source. Each of these choices presents
a challenge to modeling fidelity, generality, efficiency, robustness, and
scientific transparency. The Cyclus nuclear fuel cycle simulator framework and
its modeling ecosystem incorporate modern insights from simulation science and
software architecture to solve these problems so that challenges in nuclear
fuel cycle analysis can be better addressed. A summary of the Cyclus fuel cycle
simulator framework and its modeling ecosystem are presented. Additionally, the
implementation of each is discussed in the context of motivating challenges in
nuclear fuel cycle simulation. Finally, the current capabilities of Cyclus are
demonstrated for both open and closed fuel cycles.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.13420v1,A Study on Lip Localization Techniques used for Lip reading from a Video,"In this paper some of the different techniques used to localize the lips from
the face are discussed and compared along with its processing steps. Lip
localization is the basic step needed to read the lips for extracting visual
information from the video input. The techniques could be applied on asymmetric
lips and also on the mouth with visible teeth, tongue & mouth with moustache.
In the process of Lip reading the following steps are generally used. They are,
initially locating lips in the first frame of the video input, then tracking
the lips in the following frames using the resulting pixel points of initial
step and at last converting the tracked lip model to its corresponding matched
letter to give the visual information. A new proposal is also initiated from
the discussed techniques. The lip reading is useful in Automatic Speech
Recognition when the audio is absent or present low with or without noise in
the communication systems. Human Computer communication also will require
speech recognition.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.13740v3,"Recent Advances in Selective Image Encryption and its Indispensability
  due to COVID-19","The COVID-19 pandemic serves as a grim reminder of the unexpected nature of
these outbreaks and gives rise to a unique set of research challenges in a
variety of fields. As people all over the world adjust to this new 'normal',
with most workplaces, from companies to educational institutions shifting
online, enormous surges in the transmission of images and videos have been
observed, creating record-breaking stresses on the internet backbone. At the
same time, maintaining the privacy and security of the users' data is of
immense importance, this is where fast and efficient image encryption
algorithms play a vital role. This paper discusses the calamitous effects of
the pandemic on the world population and how their changes in multimedia
consumption have led to an urgent need for the development and deployment of
secure and fast image encryption, especially selective image encryption
techniques. It carefully surveys the most recent advances in this field,
discusses their real-world effects and finally explores some future research
avenues, to provide swift relief and recover from the disastrous effects of the
pandemic.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1702.08144v2,Synchronization Problems in Automata without Non-trivial Cycles,"We study the computational complexity of various problems related to
synchronization of weakly acyclic automata, a subclass of widely studied
aperiodic automata. We provide upper and lower bounds on the length of a
shortest word synchronizing a weakly acyclic automaton or, more generally, a
subset of its states, and show that the problem of approximating this length is
hard. We investigate the complexity of finding a synchronizing set of states of
maximum size. We also show inapproximability of the problem of computing the
rank of a subset of states in a binary weakly acyclic automaton and prove that
several problems related to recognizing a synchronizing subset of states in
such automata are NP-complete.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.08927v1,"The Impact of Content Commenting on User Continuance in Online Q&A
  Communities: An Affordance Perspective","Online question-and-answer (Q&A) communities provide convenient and
innovative ways for participants to share information and collaboratively solve
problems with others. A growing challenge for those Q&A communities is to
encourage and maintain ongoing user participation. From the perspective of
motivational affordances, this study proposes a research framework to explain
the effect of content commenting on user continuance behavior in online Q&A
communities. The moderating role of participant's tenure in the relationship
between content commenting and user continuance is also explored. Using a
longitudinal panel dataset collected from a large online Q&A community, this
research empirically tests the effect of content commenting on continued user
participation in the Q&A community. The results show that both comment receipt
and comment provisioning are important motivating factors for user continuance
in the community. Specifically, received comments on questions submitted by a
participant have a positive effect on the participant's continuance of posting
questions, while answer comments both received and posted by a participant have
positive impact on user continuance of posting answers in the community. In
addition, tenure in the community is indeed found to have a significant
negative moderating effect on the relationship between content commenting and
user continuance. This research not only offers a more nuanced theoretical
understanding of how content commenting affects continued user involvement and
how participants' tenure in the community moderates the impact of content
commenting, but also provides implications for improving user continuance in
online Q&A communities.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.04448v1,Toward Semi-Automatic Misconception Discovery Using Code Embeddings,"Understanding students' misconceptions is important for effective teaching
and assessment. However, discovering such misconceptions manually can be
time-consuming and laborious. Automated misconception discovery can address
these challenges by highlighting patterns in student data, which domain experts
can then inspect to identify misconceptions. In this work, we present a novel
method for the semi-automated discovery of problem-specific misconceptions from
students' program code in computing courses, using a state-of-the-art code
classification model. We trained the model on a block-based programming dataset
and used the learned embedding to cluster incorrect student submissions. We
found these clusters correspond to specific misconceptions about the problem
and would not have been easily discovered with existing approaches. We also
discuss potential applications of our approach and how these misconceptions
inform domain-specific insights into students' learning processes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1210.7942v4,Algebraic properties of generalized Rijndael-like ciphers,"We provide conditions under which the set of Rijndael functions considered as
permutations of the state space and based on operations of the finite field
$\GF (p^k)$ ($p\geq 2$ a prime number) is not closed under functional
composition. These conditions justify using a sequential multiple encryption to
strengthen the AES (Rijndael block cipher with specific block sizes) in case
AES became practically insecure. In Sparr and Wernsdorf (2008), R. Sparr and R.
Wernsdorf provided conditions under which the group generated by the
Rijndael-like round functions based on operations of the finite field $\GF
(2^k)$ is equal to the alternating group on the state space. In this paper we
provide conditions under which the group generated by the Rijndael-like round
functions based on operations of the finite field $\GF (p^k)$ ($p\geq 2$) is
equal to the symmetric group or the alternating group on the state space.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.08652v1,"Prediction of Cancer Microarray and DNA Methylation Data using
  Non-negative Matrix Factorization","Over the past few years, there has been a considerable spread of microarray
technology in many biological patterns, particularly in those pertaining to
cancer diseases like leukemia, prostate, colon cancer, etc. The primary
bottleneck that one experiences in the proper understanding of such datasets
lies in their dimensionality, and thus for an efficient and effective means of
studying the same, a reduction in their dimension to a large extent is deemed
necessary. This study is a bid to suggesting different algorithms and
approaches for the reduction of dimensionality of such microarray datasets.
This study exploits the matrix-like structure of such microarray data and uses
a popular technique called Non-Negative Matrix Factorization (NMF) to reduce
the dimensionality, primarily in the field of biological data. Classification
accuracies are then compared for these algorithms. This technique gives an
accuracy of 98%.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.14092v1,"Counting Short Vector Pairs by Inner Product and Relations to the
  Permanent","Given as input two $n$-element sets $\mathcal A,\mathcal B\subseteq\{0,1\}^d$
with $d=c\log n\leq(\log n)^2/(\log\log n)^4$ and a target $t\in
\{0,1,\ldots,d\}$, we show how to count the number of pairs $(x,y)\in \mathcal
A\times \mathcal B$ with integer inner product $\langle x,y \rangle=t$
deterministically, in $n^2/2^{\Omega\bigl(\!\sqrt{\log n\log \log n/(c\log^2
c)}\bigr)}$ time. This demonstrates that one can solve this problem in
deterministic subquadratic time almost up to $\log^2 n$ dimensions, nearly
matching the dimension bound of a subquadratic randomized detection algorithm
of Alman and Williams [FOCS 2015]. We also show how to modify their randomized
algorithm to count the pairs w.h.p., to obtain a fast randomized algorithm. Our
deterministic algorithm builds on a novel technique of reconstructing a
function from sum-aggregates by prime residues, which can be seen as an {\em
additive} analog of the Chinese Remainder Theorem. As our second contribution,
we relate the fine-grained complexity of the task of counting of vector pairs
by inner product to the task of computing a zero-one matrix permanent over the
integers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1810.11711v2,"Regularization Effect of Fast Gradient Sign Method and its
  Generalization","Fast Gradient Sign Method (FGSM) is a popular method to generate adversarial
examples that make neural network models robust against perturbations. Despite
its empirical success, its theoretical property is not well understood. This
paper develops theory to explain the regularization effect of Generalized FGSM,
a class of methods to generate adversarial examples. Motivated from the
relationship between FGSM and LASSO penalty, the asymptotic properties of
Generalized FGSM are derived in the Generalized Linear Model setting, which is
essentially the 1-layer neural network setting with certain activation
functions. In such simple neural network models, I prove that Generalized FGSM
estimation is root n-consistent and weakly oracle under proper conditions. The
asymptotic results are also highly similar to penalized likelihood estimation.
Nevertheless, Generalized FGSM introduces additional bias when data sampling is
not sign neutral, a concept I introduce to describe the balance-ness of the
noise signs. Although the theory in this paper is developed under simple neural
network settings, I argue that it may give insights and justification for FGSM
in deep neural network settings as well.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0606128v1,"Automatic forming lists of semantically related terms based on texts
  rating in the corpus with hyperlinks and categories (In Russian)","HITS adapted algorithm for synonym search, the program architecture, and the
program work evaluation with test examples are presented in the paper.
Synarcher program for synonym (and related terms) search in the text corpus of
special structure (Wikipedia) was developed. The results of search are
presented in the form of a graph. It is possible to explore the graph and
search graph elements interactively. The proposed algorithm could be applied to
the search request extending and for synonym dictionary forming.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1601.08051v1,Minimal Suffix and Rotation of a Substring in Optimal Time,"For a text given in advance, the substring minimal suffix queries ask to
determine the lexicographically minimal non-empty suffix of a substring
specified by the location of its occurrence in the text. We develop a data
structure answering such queries optimally: in constant time after linear-time
preprocessing. This improves upon the results of Babenko et al. (CPM 2014),
whose trade-off solution is characterized by $\Theta(n\log n)$ product of these
time complexities. Next, we extend our queries to support concatenations of
$O(1)$ substrings, for which the construction and query time is preserved. We
apply these generalized queries to compute lexicographically minimal and
maximal rotations of a given substring in constant time after linear-time
preprocessing.
  Our data structures mainly rely on properties of Lyndon words and Lyndon
factorizations. We combine them with further algorithmic and combinatorial
tools, such as fusion trees and the notion of order isomorphism of strings.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.04219v1,Structural Decompositions of Epistemic Logic Programs,"Epistemic logic programs (ELPs) are a popular generalization of standard
Answer Set Programming (ASP) providing means for reasoning over answer sets
within the language. This richer formalism comes at the price of higher
computational complexity reaching up to the fourth level of the polynomial
hierarchy. However, in contrast to standard ASP, dedicated investigations
towards tractability have not been undertaken yet. In this paper, we give first
results in this direction and show that central ELP problems can be solved in
linear time for ELPs exhibiting structural properties in terms of bounded
treewidth. We also provide a full dynamic programming algorithm that adheres to
these bounds. Finally, we show that applying treewidth to a novel dependency
structure---given in terms of epistemic literals---allows to bound the number
of ASP solver calls in typical ELP solving procedures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.05944v2,"fenicsR13: A Tensorial Mixed Finite Element Solver for the Linear R13
  Equations Using the FEniCS Computing Platform","We present a mixed finite element solver for the linearized R13 equations of
non-equilibrium gas dynamics. The Python implementation builds upon the
software tools provided by the FEniCS computing platform. We describe a new
tensorial approach utilizing the extension capabilities of FEniCS's Unified
Form Language (UFL) to define required differential operators for tensors above
second degree. The presented solver serves as an example for implementing
tensorial variational formulations in FEniCS, for which the documentation and
literature seem to be very sparse. Using the software abstraction levels
provided by the UFL allows an almost one-to-one correspondence between the
underlying mathematics and the resulting source code. Test cases support the
correctness of the proposed method using validation with exact solutions. To
justify the usage of extended gas flow models, we discuss typical application
cases involving rarefaction effects. We provide the documented and validated
solver publicly.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0403033v1,"Integrating design synthesis and assembly of structured objects in a
  visual design language","Computer Aided Design systems provide tools for building and manipulating
models of solid objects. Some also provide access to programming languages so
that parametrised designs can be expressed. There is a sharp distinction,
therefore, between building models, a concrete graphical editing activity, and
programming, an abstract, textual, algorithm-construction activity. The
recently proposed Language for Structured Design (LSD) was motivated by a
desire to combine the design and programming activities in one language. LSD
achieves this by extending a visual logic programming language to incorporate
the notions of solids and operations on solids. Here we investigate another
aspect of the LSD approach; namely, that by using visual logic programming as
the engine to drive the parametrised assembly of objects, we also gain the
powerful symbolic problem-solving capability that is the forte of logic
programming languages. This allows the designer/programmer to work at a higher
level, giving declarative specifications of a design in order to obtain the
design descriptions. Hence LSD integrates problem solving, design synthesis,
and prototype assembly in a single homogeneous programming/design environment.
We demonstrate this specification-to-final-assembly capability using the
masterkeying problem for designing systems of locks and keys.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0904.3612v1,Variations of the Turing Test in the Age of Internet and Virtual Reality,"Inspired by Hofstadter's Coffee-House Conversation (1982) and by the science
fiction short story SAM by Schattschneider (1988), we propose and discuss
criteria for non-mechanical intelligence. Firstly, we emphasize the practical
need for such tests in view of massively multiuser online role-playing games
(MMORPGs) and virtual reality systems like Second Life. Secondly, we
demonstrate Second Life as a useful framework for implementing (some iterations
of) that test.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.09557v1,"A framework to semantify BPMN models using DEMO business transaction
  pattern","BPMN is a specification language widely used by industry and researchers for
business process modeling and execution. It defines clearly how to articulate
its concepts, but do not provide mechanism to represent the semantics of the
produced models. This paper addresses the problem of how to improve the
expressiveness of BPMN models, proposing a definition for the semantics of a
business process within a BPMN model, and improving the completeness of the
models in a systematic manner, so that models can describe far more situations
with few extra managed complexity. We conceive a framework based on the
business transaction patterns available in the enterprise ontology body of
knowledge to prescribe the foundations of semantic BPMN models. A tool has been
developed to automate the framework. Then, two industrial proof-of-concepts are
used to measure its coverage, both positive and negative, and to argue about
our proposal's usefulness. After that, the proposal is compared with others
using a systematic literature review. A full BPMN pattern is proposed
encompassing the happy flow, the declinations, the rejections and the
revocations, without adding any new element to the BPMN specification. A
software tool has been developed, and made publicly available, to support the
automatic generation of the BPMN models from the proposed patterns. Our
semantified BPMN pattern allowed the identification of a large amount of
implicit, and other not implemented, situations in both proof-of-concepts. It
is concluded that the usage of a semantic solution, grounded on a sound
pattern, allows the systematic enrichment of the BPMN models with a bounded
effort. Moreover, to simplify the BPMN executable models' implementation, its
elements could be classified as implicit, explicit, or not implemented yet.
Finally, related work indicates that this work is demanded, but no full
solutions are available.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1504.05932v1,Allocating Indivisible Items in Categorized Domains,"We formulate a general class of allocation problems called categorized domain
allocation problems (CDAPs), where indivisible items from multiple categories
are allocated to agents without monetary transfer and each agent gets at least
one item per category.
  We focus on basic CDAPs, where the number of items in each category is equal
to the number of agents. We characterize serial dictatorships for basic CDAPs
by a minimal set of three axiomatic properties: strategy-proofness,
non-bossiness, and category-wise neutrality. Then, we propose a natural
extension of serial dictatorships called categorial sequential allocation
mechanisms (CSAMs), which allocate the items in multiple rounds: in each round,
the active agent chooses an item from a designated category. We fully
characterize the worst-case rank efficiency of CSAMs for optimistic and
pessimistic agents, and provide a bound for strategic agents. We also conduct
experiments to compare expected rank efficiency of various CSAMs w.r.t. random
generated data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1604.00162v1,"Relations between assumption-based approaches in nonmonotonic logic and
  formal argumentation","In this paper we make a contribution to the unification of formal models of
defeasible reasoning. We present several translations between formal
argumentation frameworks and nonmonotonic logics for reasoning with plausible
assumptions. More specifically, we translate adaptive logics into
assumption-based argumentation and ASPIC+, ASPIC+ into assumption-based
argumentation and a fragment of assumption-based argumentation into adaptive
logics. Adaptive logics are closely related to Makinson's default assumptions
and to a significant class of systems within the tradition of preferential
semantics in the vein of KLM and Shoham. Thus, our results also provide close
links between formal argumentation and the latter approaches.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0705.1986v1,On the Hopcroft's minimization algorithm,"We show that the absolute worst case time complexity for Hopcroft's
minimization algorithm applied to unary languages is reached only for de Bruijn
words. A previous paper by Berstel and Carton gave the example of de Bruijn
words as a language that requires O(n log n) steps by carefully choosing the
splitting sets and processing these sets in a FIFO mode. We refine the previous
result by showing that the Berstel/Carton example is actually the absolute
worst case time complexity in the case of unary languages. We also show that a
LIFO implementation will not achieve the same worst time complexity for the
case of unary languages. Lastly, we show that the same result is valid also for
the cover automata and a modification of the Hopcroft's algorithm, modification
used in minimization of cover automata.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1105.3853v5,"The taming of recurrences in computability logic through cirquent
  calculus, Part I","This paper constructs a cirquent calculus system and proves its soundness and
completeness with respect to the semantics of computability logic (see
http://www.cis.upenn.edu/~giorgi/cl.html). The logical vocabulary of the system
consists of negation, parallel conjunction, parallel disjunction, branching
recurrence, and branching corecurrence. The article is published in two parts,
with (the present) Part I containing preliminaries and a soundness proof, and
(the forthcoming) Part II containing a completeness proof.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0404029v1,The Effect of Faults on Network Expansion,"In this paper we study the problem of how resilient networks are to node
faults. Specifically, we investigate the question of how many faults a network
can sustain so that it still contains a large (i.e. linear-sized) connected
component that still has approximately the same expansion as the original
fault-free network. For this we apply a pruning technique which culls away
parts of the faulty network which have poor expansion. This technique can be
applied to both adversarial faults and to random faults. For adversarial faults
we prove that for every network with expansion alpha, a large connected
component with basically the same expansion as the original network exists for
up to a constant times alpha n faults. This result is tight in the sense that
every graph G of size n and uniform expansion alpha(.), i.e. G has an expansion
of alpha(n) and every subgraph G' of size m of G has an expansion of
O(alpha(m)), can be broken into sublinear components with omega(alpha(n) n)
faults.
  For random faults we observe that the situation is significantly different,
because in this case the expansion of a graph only gives a very weak bound on
its resilience to random faults. More specifically, there are networks of
uniform expansion O(sqrt{n}) that are resilient against a constant fault
probability but there are also networks of uniform expansion Omega(1/log n)
that are not resilient against a O(1/log n) fault probability. Thus, a
different parameter is needed. For this we introduce the span of a graph which
allows us to determine the maximum fault probability in a much better way than
the expansion can. We use the span to show the first known results for the
effect of random faults on the expansion of d-dimensional meshes.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0612138v1,"Accommodating Sample Size Effect on Similarity Measures in Speaker
  Clustering","We investigate the symmetric Kullback-Leibler (KL2) distance in speaker
clustering and its unreported effects for differently-sized feature matrices.
Speaker data is represented as Mel Frequency Cepstral Coefficient (MFCC)
vectors, and features are compared using the KL2 metric to form clusters of
speech segments for each speaker. We make two observations with respect to
clustering based on KL2: 1.) The accuracy of clustering is strongly dependent
on the absolute lengths of the speech segments and their extracted feature
vectors. 2.) The accuracy of the similarity measure strongly degrades with the
length of the shorter of the two speech segments. These effects of length can
be attributed to the measure of covariance used in KL2. We demonstrate an
empirical correction of this sample-size effect that increases clustering
accuracy. We draw parallels to two Vector Quantization-based (VQ) similarity
measures, one which exhibits an equivalent effect of sample size, and the
second being less influenced by it.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0412080v1,"The Biological Concept of Neoteny in Evolutionary Colour Image
  Segmentation - Simple Experiments in Simple Non-Memetic Genetic Algorithms","Neoteny, also spelled Paedomorphosis, can be defined in biological terms as
the retention by an organism of juvenile or even larval traits into later life.
In some species, all morphological development is retarded; the organism is
juvenilized but sexually mature. Such shifts of reproductive capability would
appear to have adaptive significance to organisms that exhibit it. In terms of
evolutionary theory, the process of paedomorphosis suggests that larval stages
and developmental phases of existing organisms may give rise, under certain
circumstances, to wholly new organisms. Although the present work does not
pretend to model or simulate the biological details of such a concept in any
way, these ideas were incorporated by a rather simple abstract computational
strategy, in order to allow (if possible) for faster convergence into simple
non-memetic Genetic Algorithms, i.e. without using local improvement procedures
(e.g. via Baldwin or Lamarckian learning). As a case-study, the Genetic
Algorithm was used for colour image segmentation purposes by using K-mean
unsupervised clustering methods, namely for guiding the evolutionary algorithm
in his search for finding the optimal or sub-optimal data partition. Average
results suggest that the use of neotonic strategies by employing juvenile
genotypes into the later generations and the use of linear-dynamic mutation
rates instead of constant, can increase fitness values by 58% comparing to
classical Genetic Algorithms, independently from the starting population
characteristics on the search space. KEYWORDS: Genetic Algorithms, Artificial
Neoteny, Dynamic Mutation Rates, Faster Convergence, Colour Image Segmentation,
Classification, Clustering.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1108.4942v1,"Making Use of Advances in Answer-Set Programming for Abstract
  Argumentation Systems","Dung's famous abstract argumentation frameworks represent the core formalism
for many problems and applications in the field of argumentation which
significantly evolved within the last decade. Recent work in the field has thus
focused on implementations for these frameworks, whereby one of the main
approaches is to use Answer-Set Programming (ASP). While some of the
argumentation semantics can be nicely expressed within the ASP language, others
required rather cumbersome encoding techniques. Recent advances in ASP systems,
in particular, the metasp optimization frontend for the ASP-package
gringo/claspD provides direct commands to filter answer sets satisfying certain
subset-minimality (or -maximality) constraints. This allows for much simpler
encodings compared to the ones in standard ASP language. In this paper, we
experimentally compare the original encodings (for the argumentation semantics
based on preferred, semi-stable, and respectively, stage extensions) with new
metasp encodings. Moreover, we provide novel encodings for the recently
introduced resolution-based grounded semantics. Our experimental results
indicate that the metasp approach works well in those cases where the
complexity of the encoded problem is adequately mirrored within the metasp
approach.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0901.0290v1,"Offline Algorithmic Techniques for Several Content Delivery Problems in
  Some Restricted Types of Distributed Systems","In this paper we consider several content delivery problems (broadcast and
multicast, in particular) in some restricted types of distributed systems (e.g.
optical Grids and wireless sensor networks with tree-like topologies). For each
problem we provide efficient algorithmic techniques for computing optimal
content delivery strategies. The techniques we present are offline, which means
that they can be used only when full information is available and the problem
parameters do not fluctuate too much.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1504.02796v1,Model Counting Modulo Theories,"This thesis is concerned with the quantitative assessment of security in
software. More specifically, it tackles the problem of efficient computation of
channel capacity, the maximum amount of confidential information leaked by
software, measured in Shannon entropy or R\'{e}nyi's min-entropy.
  Most approaches to computing channel capacity are either efficient and return
only (possibly very loose) upper bounds, or alternatively are inefficient but
precise; few target realistic programs. In this thesis, we present a novel
approach to the problem by reducing it to a model counting problem on
first-order logic, which we name Model Counting Modulo Theories or #SMT for
brevity.
  For quantitative security, our contribution is twofold. First, on the
theoretical side we establish the connections between measuring confidentiality
leaks and fundamental verification algorithms like Symbolic Execution, SMT
solvers and DPLL. Second, exploiting these connections, we develop novel
#SMT-based techniques to compute channel capacity, which achieve both accuracy
and efficiency. These techniques are scalable to real-world programs, and
illustrative case studies include C programs from Linux kernel, a Java program
from a European project and anonymity protocols.
  For formal verification, our contribution is also twofold. First, we
introduce and study a new research problem, namely #SMT, which has other
potential applications beyond computing channel capacity, such as returning
multiple-counterexamples for Bounded Model Checking or automated test
generation. Second, we propose an alternative approach for Bounded Model
Checking using classical Symbolic Execution, which can be parallelised to
leverage modern multi-core and distributed architecture.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1809.04770v1,"Bounded Symbolic Execution for Runtime Error Detection of Erlang
  Programs","Dynamically typed languages, like Erlang, allow developers to quickly write
programs without explicitly providing any type information on expressions or
function definitions. However, this feature makes those languages less reliable
than statically typed languages, where many runtime errors can be detected at
compile time. In this paper, we present a preliminary work on a tool that, by
using the well-known techniques of metaprogramming and symbolic execution, can
be used to perform bounded verification of Erlang programs. In particular, by
using Constraint Logic Programming, we develop an interpreter that, given an
Erlang program and a symbolic input for that program, returns answer
constraints that represent sets of concrete data for which the Erlang program
generates a runtime error.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1509.03021v1,Reasoning about modular datatypes with Mendler induction,"In functional programming, datatypes a la carte provide a convenient modular
representation of recursive datatypes, based on their initial algebra
semantics. Unfortunately it is highly challenging to implement this technique
in proof assistants that are based on type theory, like Coq. The reason is that
it involves type definitions, such as those of type-level fixpoint operators,
that are not strictly positive. The known work-around of impredicative
encodings is problematic, insofar as it impedes conventional inductive
reasoning. Weak induction principles can be used instead, but they considerably
complicate proofs.
  This paper proposes a novel and simpler technique to reason inductively about
impredicative encodings, based on Mendler-style induction. This technique
involves dispensing with dependent induction, ensuring that datatypes can be
lifted to predicates and relying on relational formulations. A case study on
proving subject reduction for structural operational semantics illustrates that
the approach enables modular proofs, and that these proofs are essentially
similar to conventional ones.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1904.05926v1,"Method of Self-Similar Load Balancing in Network Intrusion Detection
  System","In this paper, the problem of load balancing in network intrusion detection
system is considered. Load balancing method based on work of several components
of network intrusion detection system and on the analysis of multifractal
properties of incoming traffic is proposed. The proposed method takes into
account a degree of multifractality for calculation of deep packet inspection
time, on the basis of which the time necessary for comparing the packet with
the signatures is calculated. Load balancing rules are generated using the
estimated average deep packet inspection time and the multifractality
parameters of incoming load. Comparative analysis of the proposed load
balancing method with the standard one showed that the proposed method improves
the quality of service parameters and the percentage of packets that are not
analyzed.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.00946v1,"A Self-Supervised Learning Approach to Rapid Path Planning for Car-Like
  Vehicles Maneuvering in Urban Environment","An efficient path planner for autonomous car-like vehicles should handle the
strong kinematic constraints, particularly in confined spaces commonly
encountered while maneuvering in city traffic, and should enable rapid
planning, as the city traffic scenarios are highly dynamic. State-of-the-art
planning algorithms handle such difficult cases at high computational cost,
often yielding non-deterministic results. However, feasible local paths can be
quickly generated leveraging the past planning experience gained in the same or
similar environment. While learning through supervised training is problematic
for real traffic scenarios, we introduce in this paper a novel neural
network-based method for path planning, which employs a gradient-based
self-supervised learning algorithm to predict feasible paths. This approach
strongly exploits the experience gained in the past and rapidly yields feasible
maneuver plans for car-like vehicles with limited steering-angle. The
effectiveness of such an approach has been confirmed by computational
experiments.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1503.02353v3,Fast Distributed Algorithms for Connectivity and MST in Large Graphs,"Motivated by the increasing need to understand the algorithmic foundations of
distributed large-scale graph computations, we study a number of fundamental
graph problems in a message-passing model for distributed computing where $k
\geq 2$ machines jointly perform computations on graphs with $n$ nodes
(typically, $n \gg k$). The input graph is assumed to be initially randomly
partitioned among the $k$ machines, a common implementation in many real-world
systems. Communication is point-to-point, and the goal is to minimize the
number of communication rounds of the computation.
  Our main result is an (almost) optimal distributed randomized algorithm for
graph connectivity. Our algorithm runs in $\tilde{O}(n/k^2)$ rounds
($\tilde{O}$ notation hides a $\poly\log(n)$ factor and an additive
$\poly\log(n)$ term). This improves over the best previously known bound of
$\tilde{O}(n/k)$ [Klauck et al., SODA 2015], and is optimal (up to a
polylogarithmic factor) in view of an existing lower bound of
$\tilde{\Omega}(n/k^2)$. Our improved algorithm uses a bunch of techniques,
including linear graph sketching, that prove useful in the design of efficient
distributed graph algorithms. Using the connectivity algorithm as a building
block, we then present fast randomized algorithms for computing minimum
spanning trees, (approximate) min-cuts, and for many graph verification
problems. All these algorithms take $\tilde{O}(n/k^2)$ rounds, and are optimal
up to polylogarithmic factors. We also show an almost matching lower bound of
$\tilde{\Omega}(n/k^2)$ rounds for many graph verification problems by
leveraging lower bounds in random-partition communication complexity.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0705.3949v1,Translating a first-order modal language to relational algebra,"This paper is about Kripke structures that are inside a relational database
and queried with a modal language. At first the modal language that is used is
introduced, followed by a definition of the database and relational algebra.
Based on these definitions two things are presented: a mapping from components
of the modal structure to a relational database schema and instance, and a
translation from queries in the modal language to relational algebra queries.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.10977v2,"Time is What Prevents Everything from Happening at Once: Propagation
  Time-conscious Influence Maximization","The influence maximization (IM) problem as defined in the seminal paper by
Kempe et al. has received widespread attention from various research
communities, leading to the design of a wide variety of solutions.
Unfortunately, this classical IM problem ignores the fact that time taken for
influence propagation to reach the largest scope can be significant in
realworld social networks, during which the underlying network itself may have
evolved. This phenomenon may have considerable adverse impact on the quality of
selected seeds and as a result all existing techniques that use this classical
definition as their building block generate seeds with suboptimal influence
spread. In this paper, we revisit the classical IM problem and propose a more
realistic version called PROTEUS-IM (Propagation Time conscious Influence
Maximization) to replace it by addressing the aforementioned limitation.
Specifically, as influence propagation may take time, we assume that the
underlying social network may evolve during influence propagation.
Consequently, PROTEUSIM aims to select seeds in the current network to maximize
influence spread in the future instance of the network at the end of influence
propagation process without assuming complete topological knowledge of the
future network. We propose a greedy and a Reverse Reachable (RR) set-based
algorithms called PROTEUS-GENIE and PROTEUS-SEER, respectively, to address this
problem. Our algorithms utilize the state-of-the-art Forest Fire Model for
modeling network evolution during influence propagation to find superior
quality seeds. Experimental study on real and synthetic social networks shows
that our proposed algorithms consistently outperform state-of-the-art classical
IM algorithms with respect to seed set quality.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1101.3682v3,Diversification improves interpolation,"We consider the problem of interpolating an unknown multivariate polynomial
with coefficients taken from a finite field or as numerical approximations of
complex numbers. Building on the recent work of Garg and Schost, we improve on
the best-known algorithm for interpolation over large finite fields by
presenting a Las Vegas randomized algorithm that uses fewer black box
evaluations. Using related techniques, we also address numerical interpolation
of sparse polynomials with complex coefficients, and provide the first provably
stable algorithm (in the sense of relative error) for this problem, at the cost
of modestly more evaluations. A key new technique is a randomization which
makes all coefficients of the unknown polynomial distinguishable, producing
what we call a diverse polynomial. Another departure from most previous
approaches is that our algorithms do not rely on root finding as a subroutine.
We show how these improvements affect the practical performance with trial
implementations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.00972v1,MIS: Multimodal Interaction Services in a cloud perspective,"The Web is becoming more and more a wide software framework on which each one
can compose and use contents, software applications and services. It can offer
adequate computational resources to manage the complexity implied by the use of
the five senses when involved in human machine interaction. The core of the
paper describes how SOA (Service Oriented Architecture) can support multimodal
interaction by pushing the I/O processing and reasoning to the cloud, improving
naturalness. The benefits of cloud computing for multimodal interaction have
been identified by emphasizing the flexibility and scalability of a SOA, and
its characteristics to provide a more holistic view of interaction according to
the variety of situations and users.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.00304v2,"Interval Type-2 Enhanced Possibilistic Fuzzy C-Means Clustering for Gene
  Expression Data Analysis","Both FCM and PCM clustering methods have been widely applied to pattern
recognition and data clustering. Nevertheless, FCM is sensitive to noise and
PCM occasionally generates coincident clusters. PFCM is an extension of the PCM
model by combining FCM and PCM, but this method still suffers from the
weaknesses of PCM and FCM. In the current paper, the weaknesses of the PFCM
algorithm are corrected and the enhanced possibilistic fuzzy c-means (EPFCM)
clustering algorithm is presented. EPFCM can still be sensitive to noise.
Therefore, we propose an interval type-2 enhanced possibilistic fuzzy c-means
(IT2EPFCM) clustering method by utilizing two fuzzifiers $(m_1, m_2)$ for fuzzy
memberships and two fuzzifiers $({\theta}_1, {\theta}_2)$ for possibilistic
typicalities. Our computational results show the superiority of the proposed
approaches compared with several state-of-the-art techniques in the literature.
Finally, the proposed methods are implemented for analyzing microarray gene
expression data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.02722v2,Mutual Visibility in Graphs,"Let $G=(V,E)$ be a graph and $P\subseteq V$ a set of points. Two points are
mutually visible if there is a shortest path between them without further
points. $P$ is a mutual-visibility set if its points are pairwise mutually
visible. The mutual-visibility number of $G$ is the size of any largest
mutual-visibility set. In this paper we start the study about this new
invariant and the mutual-visibility sets in undirected graphs. We introduce the
mutual-visibility problem which asks to find a mutual-visibility set with a
size larger than a given number. We show that this problem is NP-complete,
whereas, to check whether a given set of points is a mutual-visibility set is
solvable in polynomial time. Then we study mutual-visibility sets and
mutual-visibility numbers on special classes of graphs, such as block graphs,
trees, grids, tori, complete bipartite graphs, cographs. We also provide some
relations of the mutual-visibility number of a graph with other invariants.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.03916v2,"Balanced Allocations with Incomplete Information: The Power of Two
  Queries","We consider the allocation of $m$ balls into $n$ bins with incomplete
information. In the classical Two-Choice process a ball first queries the load
of two randomly chosen bins and is then placed in the least loaded bin. In our
setting, each ball also samples two random bins but can only estimate a bin's
load by sending binary queries of the form ""Is the load at least the median?""
or ""Is the load at least 100?"".
  For the lightly loaded case $m=O(n)$, Feldheim and Gurel-Gurevich (2021)
showed that with one query it is possible to achieve a maximum load of
$O(\sqrt{\log n/\log \log n})$, and posed the question whether a maximum load
of $m/n+O(\sqrt{\log n/\log \log n})$ is possible for any $m = \Omega(n)$. In
this work, we resolve this open problem by proving a lower bound of
$m/n+\Omega( \sqrt{\log n})$ for a fixed $m=\Theta(n \sqrt{\log n})$, and a
lower bound of $m/n+\Omega(\log n/\log \log n)$ for some $m$ depending on the
used strategy.
  We complement this negative result by proving a positive result for multiple
queries. In particular, we show that with only two binary queries per chosen
bin, there is an oblivious strategy which ensures a maximum load of
$m/n+O(\sqrt{\log n})$ for any $m \geq 1$. Further, for any number of $k =
O(\log \log n)$ binary queries, the upper bound on the maximum load improves to
$m/n + O(k(\log n)^{1/k})$ for any $m \geq 1$.
  Further, this result for $k$ queries implies (i) new bounds for the
$(1+\beta)$-process introduced by Peres et al (2015), (ii) new bounds for the
graphical balanced allocation process on dense expander graphs, and (iii) the
bound of $m/n+O(\log \log n)$ on the maximum load achieved by the Two-Choice
process, including the heavily loaded case $m=\Omega(n)$ derived by Berenbrink
et al. (2006). One novel aspect of our proofs is the use of multiple
super-exponential potential functions, which might be of use in future work.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.00118v1,ISHNE: Influence Self-attention for Heterogeneous Network Embedding,"In recent years, Graph Neural Networks has received enormous attention from
academia for its huge potential of modeling the network traits such as
macrostructure and single node attributes. However, prior mainstream works
mainly focus on homogeneous network and lack the capacity to characterize the
network heterogeneous property. Besides, most previous literature cannot model
the influence under microscope vision, making it infeasible to model the joint
relation between the heterogeneity and mutual interaction within multiple
relation type. In this paper, we propose an Influence Self-attention network to
address the difficulties mentioned above. To model heterogeneity and mutual
interaction, we redesign attention mechanism with influence factor on the
single-type relation level, which learns the importance coefficient from its
adjacent neighbors under the same meta-path based patterns. To incorporate the
heterogeneous meta-path in a unified dimension, we developed a self-attention
based framework for meta-path relation fusion according to the learned
meta-path coefficient. Our experimental results demonstrate that our framework
not only achieve higher results than current state-of-the-art baselines, but
also show promising vision on depicting heterogeneous interactive relations
under complicated network structure.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.10570v3,The complexity of recognizing minimally tough graphs,"A graph is called $t$-tough if the removal of any vertex set $S$ that
disconnects the graph leaves at most $|S|/t$ components. The toughness of a
graph is the largest $t$ for which the graph is $t$-tough. A graph is minimally
$t$-tough if the toughness of the graph is $t$ and the deletion of any edge
from the graph decreases the toughness. The complexity class DP is the set of
all languages that can be expressed as the intersection of a language in NP and
a language in coNP. In this paper, we prove that recognizing minimally
$t$-tough graphs is DP-complete for any positive rational number $t$. We
introduce a new notion called weighted toughness, which has a key role in our
proof.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.09809v1,"Curiosity Notebook: The Design of a Research Platform for Learning by
  Teaching","While learning by teaching is a popular pedagogical technique, it is a
learning phenomenon that is difficult to study due to variability in the
tutor-tutee pairings and learning environments. In this paper, we introduce the
Curiosity Notebook, a web-based research infrastructure for studying learning
by teaching via the use of a teachable agent. We describe and provide rationale
for the set of features that are essential for such a research infrastructure,
outline how these features have evolved over two design iterations of the
Curiosity Notebook and through two studies -- a 4-week field study with 12
elementary school students interacting with a NAO robot and an hour-long online
observational study with 41 university students interacting with an agent --
demonstrate the utility of our platform for making observations of
learning-by-teaching phenomena in diverse learning environments. Based on these
findings, we conclude the paper by reflecting on our design evolution and
envisioning future iterations of the Curiosity Notebook.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1806.06347v2,Cover Song Synthesis by Analogy,"In this work, we pose and address the following ""cover song analogies""
problem: given a song A by artist 1 and a cover song A' of this song by artist
2, and given a different song B by artist 1, synthesize a song B' which is a
cover of B in the style of artist 2. Normally, such a polyphonic style transfer
problem would be quite challenging, but we show how the cover songs example
constrains the problem, making it easier to solve. First, we extract the
longest common beat-synchronous subsequence between A and A', and we time
stretch the corresponding beat intervals in A' so that they align with A. We
then derive a version of joint 2D convolutional NMF, which we apply to the
constant-Q spectrograms of the synchronized segments to learn a translation
dictionary of sound templates from A to A'. Finally, we apply the learned
templates as filters to the song B, and we mash up the translated filtered
components into the synthesized song B' using audio mosaicing. We showcase our
algorithm on several examples, including a synthesized cover version of Michael
Jackson's ""Bad"" by Alien Ant Farm, learned from the latter's ""Smooth Criminal""
cover.'",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.00326v1,"Unsafe At Any Level: NHTSA's levels of automation are a liability for
  autonomous vehicle design and regulation","Walter Huang, a 38-year-old Apple Inc. engineer, died on March 23, 2018,
after his Tesla Model X crashed into a highway barrier in Mountain View,
California. Tesla immediately disavowed responsibility for the accident. ""The
fundamental premise of both moral and legal liability is a broken promise, and
there was none here: [Mr. Huang] was well aware that the Autopilot was not
perfect [and the] only way for this accident to have occurred is if Mr. Huang
was not paying attention to the road, despite the car providing multiple
warnings to do so."" This is the standard response from Tesla and Uber, the
manufacturers of the automated vehicles involved in the six fatal accidents to
date: the automated vehicle isn't perfect, the driver knew it wasn't perfect,
and if only the driver had been paying attention and heeded the vehicle's
warnings, the accident would never have occurred.
  However, as researchers focused on human-automation interaction in aviation
and military operations, we cannot help but wonder if there really are no
broken promises and no legal liabilities. Science has a critical role in
determining legal liability, and courts appropriately rely on scientists and
engineers to determine whether an accident, or harm, was foreseeable.
Specifically, a designer could be found liable if, at the time of the accident,
scientists knew there was a systematic relationship between the accident and
the designer's untaken precaution.
  Nearly 70 years of research provides an undeniable answer: It is
insufficient, inappropriate, and dangerous to automate everything you can and
leave the rest to the human. There is a systematic relationship between the
design of automated vehicles and the types of accidents that are occurring now
and will inevitably continue to occur in the future. These accidents were not
unforeseeable and the drivers were not exclusively to blame.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0
http://arxiv.org/abs/cs/0505085v1,Improving PARMA Trailing,"Taylor introduced a variable binding scheme for logic variables in his PARMA
system, that uses cycles of bindings rather than the linear chains of bindings
used in the standard WAM representation. Both the HAL and dProlog languages
make use of the PARMA representation in their Herbrand constraint solvers.
Unfortunately, PARMA's trailing scheme is considerably more expensive in both
time and space consumption. The aim of this paper is to present several
techniques that lower the cost.
  First, we introduce a trailing analysis for HAL using the classic PARMA
trailing scheme that detects and eliminates unnecessary trailings. The
analysis, whose accuracy comes from HAL's determinism and mode declarations,
has been integrated in the HAL compiler and is shown to produce space
improvements as well as speed improvements. Second, we explain how to modify
the classic PARMA trailing scheme to halve its trailing cost. This technique is
illustrated and evaluated both in the context of dProlog and HAL. Finally, we
explain the modifications needed by the trailing analysis in order to be
combined with our modified PARMA trailing scheme. Empirical evidence shows that
the combination is more effective than any of the techniques when used in
isolation.
  To appear in Theory and Practice of Logic Programming.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1709.02210v1,Opportunistic Networking Protocol Simulator for OMNeT++,"The number of computing devices of the Internet of Things (IoT) is expected
to grow by billions. New networking architectures are being considered to
handle communications in the IoT. One of these architectures is Opportunistic
Networking (OppNets). To evaluate the performance of OppNets, an OMNeT++ based
modular simulator is built with models that handle the operations of the
different protocol layers of an OppNets based node. The work presented here
provides the details of this simulator, called the Opportunistic Protocol
Simulator (OPS).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0008007v1,Tagger Evaluation Given Hierarchical Tag Sets,"We present methods for evaluating human and automatic taggers that extend
current practice in three ways. First, we show how to evaluate taggers that
assign multiple tags to each test instance, even if they do not assign
probabilities. Second, we show how to accommodate a common property of manually
constructed ``gold standards'' that are typically used for objective
evaluation, namely that there is often more than one correct answer. Third, we
show how to measure performance when the set of possible tags is
tree-structured in an IS-A hierarchy. To illustrate how our methods can be used
to measure inter-annotator agreement, we show how to compute the kappa
coefficient over hierarchical tag sets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0404010v2,On the universality of rank distributions of website popularity,"We present an extensive analysis of long-term statistics of the queries to
websites using logs collected on several web caches in Russian academic
networks and on US IRCache caches. We check the sensitivity of the statistics
to several parameters: (1) duration of data collection, (2) geographical
location of the cache server collecting data, and (3) the year of data
collection. We propose a two-parameter modification of the Zipf law and
interpret the parameters. We find that the rank distribution of websites is
stable when approximated by the modified Zipf law. We suggest that website
popularity may be a universal property of Internet.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1704.05132v1,"A hybrid CPU-GPU parallelization scheme of variable neighborhood search
  for inventory optimization problems","In this paper, we study various parallelization schemes for the Variable
Neighborhood Search (VNS) metaheuristic on a CPU-GPU system via OpenMP and
OpenACC. A hybrid parallel VNS method is applied to recent benchmark problem
instances for the multi-product dynamic lot sizing problem with product returns
and recovery, which appears in reverse logistics and is known to be NP-hard. We
report our findings regarding these parallelization approaches and present
promising computational results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0610104v1,ARQ Diversity in Fading Random Access Channels,"A cross-layer optimization approach is adopted for the design of symmetric
random access wireless systems. Instead of the traditional collision model, a
more realistic physical layer model is considered. Based on this model, an
Incremental Redundancy Automatic Repeat reQuest (IR-ARQ) scheme, tailored to
jointly combat the effects of collisions, multi-path fading, and additive
noise, is developed. The Diversity-Multiplexing-Delay tradeoff (DMDT) of the
proposed scheme is analyzed for fully-loaded queues, and compared with that of
Gallager tree algorithm for collision resolution and the network-assisted
diversity multiple access (NDMA) protocol of Tsatsanis et al.. The fully-loaded
queue model is then replaced by one with random arrivals, under which these
protocols are compared in terms of the stability region, average delay and
diversity gain. Overall, our analytical and numerical results establish the
superiority of the proposed IR-ARQ scheme and reveal some important insights.
For example, it turns out that the performance is optimized, for a given total
throughput, by maximizing the probability that a certain user sends a new
packet and minimizing the transmission rate employed by each user.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1802.08454v2,Faithful Semantical Embedding of a Dyadic Deontic Logic in HOL,"A shallow semantical embedding of a dyadic deontic logic by Carmo and Jones
in classical higher-order logic is presented. This embedding is proven sound
and complete, that is, faithful.
  The work presented here provides the theoretical foundation for the
implementation and automation of dyadic deontic logic within off-the-shelf
higher-order theorem provers and proof assistants.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1801.02080v1,An Intelligent Future Mobile Terminal Architecture,"In this paper, a novel Extended Cognitive Mobile Terminal (ExCogNet-MT)
scheme is presented. In this scheme, a ""test bench"" at receiver's Mobile
Terminal (MT) can estimate the channel Signal to Noise Ratio (SNR) and can
detect the jamming signal. The estimation scheme compares the Standard
Deviation (SD) of received signal and processed signal, and on the bases of
this comparison the ""test bench"" can determine the BER and corresponding SNR
value of the channel. Simulation results demonstrated that under certain
scenarios estimated SNR value can be helpful for tuning the parameters of
protocol stack of 802.11a and WiMaxm.",0,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
http://arxiv.org/abs/1507.00990v1,Using the Johnson-Lindenstrauss lemma in linear and integer programming,"The Johnson-Lindenstrauss lemma allows dimension reduction on real vectors
with low distortion on their pairwise Euclidean distances. This result is often
used in algorithms such as $k$-means or $k$ nearest neighbours since they only
use Euclidean distances, and has sometimes been used in optimization algorithms
involving the minimization of Euclidean distances. In this paper we introduce a
first attempt at using this lemma in the context of feasibility problems in
linear and integer programming, which cannot be expressed only in function of
Euclidean distances.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.04075v1,"Defender Policy Evaluation and Resource Allocation Using MITRE ATT&CK
  Evaluations Data","Protecting against multi-step attacks of uncertain duration and timing forces
defenders into an indefinite, always ongoing, resource-intensive response. To
effectively allocate resources, a defender must be able to analyze multi-step
attacks under assumption of constantly allocating resources against an
uncertain stream of potentially undetected attacks. To achieve this goal, we
present a novel methodology that applies a game-theoretic approach to the
attack, attacker, and defender data derived from MITRE's ATT&CK Framework. Time
to complete attack steps is drawn from a probability distribution determined by
attacker and defender strategies and capabilities. This constraints attack
success parameters and enables comparing different defender resource allocation
strategies. By approximating attacker-defender games as Markov processes, we
represent the attacker-defender interaction, estimate the attack success
parameters, determine the effects of attacker and defender strategies, and
maximize opportunities for defender strategy improvements against an uncertain
stream of attacks. This novel representation and analysis of multi-step attacks
enables defender policy optimization and resource allocation, which we
illustrate using the data from MITRE's APT3 ATT&CK Evaluations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0
http://arxiv.org/abs/1303.3077v2,Using Mathematica & Matlab for CAGD/CAD research and education,"In CAGD/CAD research and education, users are involved with development of
mathematical algorithms and followed by the analysis of the resultant
algorithm. This process involves geometric display which can only be carried
out with high end graphics display. There are many approaches practiced and one
of the so-called easiest approaches is by using C/C++ programming language and
OpenGL application program interface, API. There are practitioners uses C/C++
programming language to develop the algorithms and finally utilize AutoCAD for
graphics display. On the other hand, high end CAD users manage to use Auto Lisp
as their programming language in AutoCAD. Nevertheless, these traditional ways
are definitely time consuming. This paper introduces an alternative method
whereby the practitioners may maximize scientific computation programs, SCPs:
Mathematica and MATLAB in the context of CAGD/CAD for research and education.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0212043v1,Computing Conformal Structure of Surfaces,"This paper solves the problem of computing conformal structures of general
2-manifolds represented as triangle meshes. We compute conformal structures in
the following way: first compute homology bases from simplicial complex
structures, then construct dual cohomology bases and diffuse them to harmonic
1-forms. Next, we construct bases of holomorphic differentials. We then obtain
period matrices by integrating holomorphic differentials along homology bases.
We also study the global conformal mapping between genus zero surfaces and
spheres, and between general meshes and planes. Our method of computing
conformal structures can be applied to tackle fundamental problems in computer
aid design and computer graphics, such as geometry classification and
identification, and surface global parametrization.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0305025v1,"Simultaneous Dempster-Shafer clustering and gradual determination of
  number of clusters using a neural network structure","In this paper we extend an earlier result within Dempster-Shafer theory
[""Fast Dempster-Shafer Clustering Using a Neural Network Structure,"" in Proc.
Seventh Int. Conf. Information Processing and Management of Uncertainty in
Knowledge-Based Systems (IPMU'98)] where several pieces of evidence were
clustered into a fixed number of clusters using a neural structure. This was
done by minimizing a metaconflict function. We now develop a method for
simultaneous clustering and determination of number of clusters during
iteration in the neural structure. We let the output signals of neurons
represent the degree to which a pieces of evidence belong to a corresponding
cluster. From these we derive a probability distribution regarding the number
of clusters, which gradually during the iteration is transformed into a
determination of number of clusters. This gradual determination is fed back
into the neural structure at each iteration to influence the clustering
process.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.02447v1,Learning Language Games through Interaction,"We introduce a new language learning setting relevant to building adaptive
natural language interfaces. It is inspired by Wittgenstein's language games: a
human wishes to accomplish some task (e.g., achieving a certain configuration
of blocks), but can only communicate with a computer, who performs the actual
actions (e.g., removing all red blocks). The computer initially knows nothing
about language and therefore must learn it from scratch through interaction,
while the human adapts to the computer's capabilities. We created a game in a
blocks world and collected interactions from 100 people playing it. First, we
analyze the humans' strategies, showing that using compositionality and
avoiding synonyms correlates positively with task performance. Second, we
compare computer strategies, showing how to quickly learn a semantic parsing
model from scratch, and that modeling pragmatics further accelerates learning
for successful players.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1812.06120v2,"Simulation to Scaled City: Zero-Shot Policy Transfer for Traffic Control
  via Autonomous Vehicles","Using deep reinforcement learning, we train control policies for autonomous
vehicles leading a platoon of vehicles onto a roundabout. Using Flow, a library
for deep reinforcement learning in micro-simulators, we train two policies, one
policy with noise injected into the state and action space and one without any
injected noise. In simulation, the autonomous vehicle learns an emergent
metering behavior for both policies in which it slows to allow for smoother
merging. We then directly transfer this policy without any tuning to the
University of Delaware Scaled Smart City (UDSSC), a 1:25 scale testbed for
connected and automated vehicles. We characterize the performance of both
policies on the scaled city. We show that the noise-free policy winds up
crashing and only occasionally metering. However, the noise-injected policy
consistently performs the metering behavior and remains collision-free,
suggesting that the noise helps with the zero-shot policy transfer.
Additionally, the transferred, noise-injected policy leads to a 5% reduction of
average travel time and a reduction of 22% in maximum travel time in the UDSSC.
Videos of the controllers can be found at
https://sites.google.com/view/iccps-policy-transfer.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0304029v1,An XML based Document Suite,"We report about the current state of development of a document suite and its
applications. This collection of tools for the flexible and robust processing
of documents in German is based on the use of XML as unifying formalism for
encoding input and output data as well as process information. It is organized
in modules with limited responsibilities that can easily be combined into
pipelines to solve complex tasks. Strong emphasis is laid on a number of
techniques to deal with lexical and conceptual gaps that are typical when
starting a new application.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0806.2448v2,Logical Reasoning for Higher-Order Functions with Local State,"We introduce an extension of Hoare logic for call-by-value higher-order
functions with ML-like local reference generation. Local references may be
generated dynamically and exported outside their scope, may store higher-order
functions and may be used to construct complex mutable data structures. This
primitive is captured logically using a predicate asserting reachability of a
reference name from a possibly higher-order datum and quantifiers over hidden
references. We explore the logic's descriptive and reasoning power with
non-trivial programming examples combining higher-order procedures and
dynamically generated local state. Axioms for reachability and local invariant
play a central role for reasoning about the examples.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1311.5018v1,"On the impact of explicit or semi-implicit integration methods over the
  stability of real-time numerical simulations","Physics-based animation of soft or rigid bodies for real-time applications
often suffers from numerical instabilities. We analyse one of the most common
sources of unwanted behaviour: the numerical integration strategy. To assess
the impact of popular integration methods, we consider a scenario where soft
and hard constraints are added to a custom designed deformable linear object.
Since the goal for this class of simulation methods is to attain interactive
frame-rates, we present the drawbacks of using explicit integration methods
over inherently stable, implicit integrators. To help numerical solver
designers better understand the impact of an integrator on a certain simulated
world, we have conceived a method of benchmarking the efficiency of an
integrator with respect to its speed, stability and symplecticity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.06975v1,Bias Correction in Saupe Tensor Estimation,"Estimation of the Saupe tensor is central to the determination of molecular
structures from residual dipolar couplings (RDC) or chemical shift
anisotropies. Assuming a given template structure, the singular value
decomposition (SVD) method proposed in Losonczi et al. 1999 has been used
traditionally to estimate the Saupe tensor. Despite its simplicity, whenever
the template structure has large structural noise, the eigenvalues of the
estimated tensor have a magnitude systematically smaller than their actual
values. This leads to systematic error when calculating the eigenvalue
dependent parameters, magnitude and rhombicity. We propose here a Monte Carlo
simulation method to remove such bias. We further demonstrate the effectiveness
of our method in the setting when the eigenvalue estimates from multiple
template protein fragments are available and their average is used as an
improved eigenvalue estimator. For both synthetic and experimental RDC datasets
of ubiquitin, when using template fragments corrupted by large noise, the
magnitude of our proposed bias-reduced estimator generally reaches at least 90%
of the actual value, whereas the magnitude of SVD estimator can be shrunk below
80% of the true value.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.13021v1,"Convex Online Video Frame Subset Selection using Multiple Criteria for
  Data Efficient Autonomous Driving","Training vision-based Urban Autonomous driving models is a challenging
problem, which is highly researched in recent times. Training such models is a
data-intensive task requiring the storage and processing of vast volumes of
(possibly redundant) driving video data. In this paper, we study the problem of
developing data-efficient autonomous driving systems. In this context, we study
the problem of multi-criteria online video frame subset selection. We study
convex optimization-based solutions and show that they are unable to provide
solutions with high weightage to the loss of selected video frames. We design a
novel convex optimization-based multi-criteria online subset selection
algorithm that uses a thresholded concave function of selection variables. We
also propose and study a submodular optimization-based algorithm. Extensive
experiments using the driving simulator CARLA show that we are able to drop 80%
of the frames while succeeding to complete 100% of the episodes w.r.t. the
model trained on 100% data, in the most difficult task of taking turns. This
results in a training time of less than 30% compared to training on the whole
dataset. We also perform detailed experiments on prediction performances of
various affordances used by the Conditional Affordance Learning (CAL) model and
show that our subset selection improves performance on the crucial affordance
""Relative Angle"" during turns.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.13090v1,"SciClops: Detecting and Contextualizing Scientific Claims for Assisting
  Manual Fact-Checking","This paper describes SciClops, a method to help combat online scientific
misinformation. Although automated fact-checking methods have gained
significant attention recently, they require pre-existing ground-truth
evidence, which, in the scientific context, is sparse and scattered across a
constantly-evolving scientific literature. Existing methods do not exploit this
literature, which can effectively contextualize and combat science-related
fallacies. Furthermore, these methods rarely require human intervention, which
is essential for the convoluted and critical domain of scientific
misinformation. SciClops involves three main steps to process scientific claims
found in online news articles and social media postings: extraction,
clustering, and contextualization. First, the extraction of scientific claims
takes place using a domain-specific, fine-tuned transformer model. Second,
similar claims extracted from heterogeneous sources are clustered together with
related scientific literature using a method that exploits their content and
the connections among them. Third, check-worthy claims, broadcasted by popular
yet unreliable sources, are highlighted together with an enhanced fact-checking
context that includes related verified claims, news articles, and scientific
papers. Extensive experiments show that SciClops tackles sufficiently these
three steps, and effectively assists non-expert fact-checkers in the
verification of complex scientific claims, outperforming commercial
fact-checking systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1404.0842v1,Efficient Decomposition of Bimatrix Games (Extended Abstract),"Exploiting the algebraic structure of the set of bimatrix games, a
divide-and-conquer algorithm for finding Nash equilibria is proposed. The
algorithm is fixed-parameter tractable with the size of the largest irreducible
component of a game as parameter. An implementation of the algorithm is shown
to yield a significant performance increase on inputs with small parameters.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1102.3610v2,Heterogeneous download times in a homogeneous BitTorrent swarm,"Modeling and understanding BitTorrent (BT) dynamics is a recurrent research
topic mainly due to its high complexity and tremendous practical efficiency.
Over the years, different models have uncovered various phenomena exhibited by
the system, many of which have direct impact on its performance. In this paper
we identify and characterize a phenomenon that has not been previously
observed: homogeneous peers (with respect to their upload capacities)
experience heterogeneous download rates. The consequences of this phenomenon
have direct impact on peer and system performance, such as high variability of
download times, unfairness with respect to peer arrival order, bursty
departures and content synchronization. Detailed packet-level simulations and
prototype-based experiments on the Internet were performed to characterize this
phenomenon. We also develop a mathematical model that accurately predicts the
heterogeneous download rates of the homogeneous peers as a function of their
content. Although this phenomenon is more prevalent in unpopular swarms (very
few peers), these by far represent the most common type of swarm in BT.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1102.5425v1,Tight Bounds for Parallel Randomized Load Balancing,"We explore the fundamental limits of distributed balls-into-bins algorithms.
We present an adaptive symmetric algorithm that achieves a bin load of two in
log* n+O(1) communication rounds using O(n) messages in total. Larger bin loads
can be traded in for smaller time complexities. We prove a matching lower bound
of (1-o(1))log* n on the time complexity of symmetric algorithms that guarantee
small bin loads at an asymptotically optimal message complexity of O(n). For
each assumption of the lower bound, we provide an algorithm violating it, in
turn achieving a constant maximum bin load in constant time.
  As an application, we consider the following problem. Given a fully connected
graph of n nodes, where each node needs to send and receive up to n messages,
and in each round each node may send one message over each link, deliver all
messages as quickly as possible to their destinations. We give a simple and
robust algorithm of time complexity O(log* n) for this task and provide a
generalization to the case where all nodes initially hold arbitrary sets of
messages. A less practical algorithm terminates within asymptotically optimal
O(1) rounds. All these bounds hold with high probability.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1210.6284v1,Reify Your Collection Queries for Modularity and Speed!,"Modularity and efficiency are often contradicting requirements, such that
programers have to trade one for the other. We analyze this dilemma in the
context of programs operating on collections. Performance-critical code using
collections need often to be hand-optimized, leading to non-modular, brittle,
and redundant code. In principle, this dilemma could be avoided by automatic
collection-specific optimizations, such as fusion of collection traversals,
usage of indexing, or reordering of filters. Unfortunately, it is not obvious
how to encode such optimizations in terms of ordinary collection APIs, because
the program operating on the collections is not reified and hence cannot be
analyzed.
  We propose SQuOpt, the Scala Query Optimizer--a deep embedding of the Scala
collections API that allows such analyses and optimizations to be defined and
executed within Scala, without relying on external tools or compiler
extensions. SQuOpt provides the same ""look and feel"" (syntax and static typing
guarantees) as the standard collections API. We evaluate SQuOpt by
re-implementing several code analyses of the Findbugs tool using SQuOpt, show
average speedups of 12x with a maximum of 12800x and hence demonstrate that
SQuOpt can reconcile modularity and efficiency in real-world applications.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0909.1198v2,A rich hierarchy of functionals of finite types,"We are considering typed hierarchies of total, continuous functionals using
complete, separable metric spaces at the base types. We pay special attention
to the so called Urysohn space constructed by P. Urysohn. One of the properties
of the Urysohn space is that every other separable metric space can be
isometrically embedded into it. We discuss why the Urysohn space may be
considered as the universal model of possibly infinitary outputs of algorithms.
The main result is that all our typed hierarchies may be topologically
embedded, type by type, into the corresponding hierarchy over the Urysohn
space. As a preparation for this, we prove an effective density theorem that is
also of independent interest.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0102004v1,Computational Geometry Column 41,"The recent result that n congruent balls in R^d have at most 4 distinct
geometric permutations is described.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1610.01185v6,Recursion-Theoretic Ranking and Compression,"For which sets A does there exist a mapping, computed by a total or partial
recursive function, such that the mapping, when its domain is restricted to A,
is a 1-to-1, onto mapping to $\Sigma^*$? And for which sets A does there exist
such a mapping that respects the lexicographical ordering within A? Both cases
are types of perfect, minimal hash functions. The complexity-theoretic versions
of these notions are known as compression functions and ranking functions. The
present paper defines and studies the recursion-theoretic versions of
compression and ranking functions, and in particular studies the question of
which sets have, or lack, such functions. Thus, this is a case where, in
contrast to the usual direction of notion transferal, notions from complexity
theory are inspiring notions, and an investigation, in computability theory.
  We show that the rankable and compressible sets broadly populate the
1-truth-table degrees, and we prove that every nonempty coRE cylinder is
recursively compressible.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.06748v1,"From Matching with Diversity Constraints to Matching with Regional
  Quotas","In the past few years, several new matching models have been proposed and
studied that take into account complex distributional constraints. Relevant
lines of work include (1) school choice with diversity constraints where
students have (possibly overlapping) types and (2) hospital-doctor matching
where various regional quotas are imposed. In this paper, we present a
polynomial-time reduction to transform an instance of (1) to an instance of (2)
and we show how the feasibility and stability of corresponding matchings are
preserved under the reduction. Our reduction provides a formal connection
between two important strands of work on matching with distributional
constraints. We then apply the reduction in two ways. Firstly, we show that it
is NP-complete to check whether a feasible and stable outcome for (1) exists.
Due to our reduction, these NP-completeness results carry over to setting (2).
In view of this, we help unify some of the results that have been presented in
the literature. Secondly, if we have positive results for (2), then we have
corresponding results for (1). One key conclusion of our results is that
further developments on axiomatic and algorithmic aspects of hospital-doctor
matching with regional quotas will result in corresponding results for school
choice with diversity constraints.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.10812v1,"Quantum Optical Convolutional Neural Network: A Novel Image Recognition
  Framework for Quantum Computing","Large machine learning models based on Convolutional Neural Networks (CNNs)
with rapidly increasing number of parameters, trained with massive amounts of
data, are being deployed in a wide array of computer vision tasks from
self-driving cars to medical imaging. The insatiable demand for computing
resources required to train these models is fast outpacing the advancement of
classical computing hardware, and new frameworks including Optical Neural
Networks (ONNs) and quantum computing are being explored as future
alternatives.
  In this work, we report a novel quantum computing based deep learning model,
the Quantum Optical Convolutional Neural Network (QOCNN), to alleviate the
computational bottleneck in future computer vision applications. Using the
popular MNIST dataset, we have benchmarked this new architecture against a
traditional CNN based on the seminal LeNet model. We have also compared the
performance with previously reported ONNs, namely the GridNet and ComplexNet,
as well as a Quantum Optical Neural Network (QONN) that we built by combining
the ComplexNet with quantum based sinusoidal nonlinearities. In essence, our
work extends the prior research on QONN by adding quantum convolution and
pooling layers preceding it.
  We have evaluated all the models by determining their accuracies, confusion
matrices, Receiver Operating Characteristic (ROC) curves, and Matthews
Correlation Coefficients. The performance of the models were similar overall,
and the ROC curves indicated that the new QOCNN model is robust. Finally, we
estimated the gains in computational efficiencies from executing this novel
framework on a quantum computer. We conclude that switching to a quantum
computing based approach to deep learning may result in comparable accuracies
to classical models, while achieving unprecedented boosts in computational
performances and drastic reduction in power consumption.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0411004v1,Computational Aspects of a Numerical Model for Combustion Flow,"A computational method for numeric resolution of a PDEs system, based on a
Finite Differences schema integrated by interpolations of partial results, and
an estimate of the error of its solution respect to the normal FD solution.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.01277v1,"Data augmentation and pre-trained networks for extremely low data
  regimes unsupervised visual inspection","The use of deep features coming from pre-trained neural networks for
unsupervised anomaly detection purposes has recently gathered momentum in the
computer vision field. In particular, industrial inspection applications can
take advantage of such features, as demonstrated by the multiple successes of
related methods on the MVTec Anomaly Detection (MVTec AD) dataset. These
methods make use of neural networks pre-trained on auxiliary classification
tasks such as ImageNet. However, to our knowledge, no comparative study of
robustness to the low data regimes between these approaches has been conducted
yet. For quality inspection applications, the handling of limited sample sizes
may be crucial as large quantities of images are not available for small
series. In this work, we aim to compare three approaches based on deep
pre-trained features when varying the quantity of available data in MVTec AD:
KNN, Mahalanobis, and PaDiM. We show that although these methods are mostly
robust to small sample sizes, they still can benefit greatly from using data
augmentation in the original image space, which allows to deal with very small
production runs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.5400v4,Buffer Overflow Analysis for C,"Buffer overflow detection and mitigation for C programs has been an important
concern for a long time. This paper defines a string buffer overflow analysis
for C programs. The key ideas of our formulation are (a) separating buffers
from the pointers that point to them, (b) modelling buffers in terms of sizes
and sets of positions of null characters, and (c) defining stateless functions
to compute the sets of null positions and mappings between buffers and
pointers.
  This exercise has been carried out to test the feasibility of describing such
an analysis in terms of lattice valued functions and relations to facilitate
automatic construction of an analyser without the user having to write
C/C++/Java code. This is facilitated by devising stateless formulations because
stateful formulations combine features through side effects in states raising a
natural requirement of C/C++/Java code to be written to describe them. Given
the above motivation, the focus of this paper is not to build good static
approximations for buffer overflow analysis but to show how given static
approximations could be formalized in terms of stateless formulations so that
they become amenable to automatic construction of analysers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1712.03512v1,"Comparative analysis of criteria for filtering time series of word usage
  frequencies","This paper describes a method of nonlinear wavelet thresholding of time
series. The Ramachandran-Ranganathan runs test is used to assess the quality of
approximation. To minimize the objective function, it is proposed to use
genetic algorithms - one of the stochastic optimization methods. The suggested
method is tested both on the model series and on the word frequency series
using the Google Books Ngram data. It is shown that method of filtering which
uses the runs criterion shows significantly better results compared with the
standard wavelet thresholding. The method can be used when quality of filtering
is of primary importance but not the speed of calculations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0407054v2,From truth to computability I,"The recently initiated approach called computability logic is a formal theory
of interactive computation. See a comprehensive online source on the subject at
http://www.cis.upenn.edu/~giorgi/cl.html . The present paper contains a
soundness and completeness proof for the deductive system CL3 which axiomatizes
the most basic first-order fragment of computability logic called the
finite-depth, elementary-base fragment. Among the potential application areas
for this result are the theory of interactive computation, constructive applied
theories, knowledgebase systems, systems for resource-bound planning and
action. This paper is self-contained as it reintroduces all relevant
definitions as well as main motivations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1711.06241v2,Deceptiveness of internet data for disease surveillance,"Quantifying how many people are or will be sick, and where, is a critical
ingredient in reducing the burden of disease because it helps the public health
system plan and implement effective outbreak response. This process of disease
surveillance is currently based on data gathering using clinical and laboratory
methods; this distributed human contact and resulting bureaucratic data
aggregation yield expensive procedures that lag real time by weeks or months.
The promise of new surveillance approaches using internet data, such as web
event logs or social media messages, is to achieve the same goal but faster and
cheaper. However, prior work in this area lacks a rigorous model of information
flow, making it difficult to assess the reliability of both specific approaches
and the body of work as a whole.
  We model disease surveillance as a Shannon communication. This new framework
lets any two disease surveillance approaches be compared using a unified
vocabulary and conceptual model. Using it, we describe and compare the
deficiencies suffered by traditional and internet-based surveillance, introduce
a new risk metric called deceptiveness, and offer mitigations for some of these
deficiencies. This framework also makes the rich tools of information theory
applicable to disease surveillance. This better understanding will improve the
decision-making of public health practitioners by helping to leverage
internet-based surveillance in a way complementary to the strengths of
traditional surveillance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1403.2765v2,Generalizers: New Metaobjects for Generalized Dispatch,"This paper introduces a new metaobject, the generalizer, which complements
the existing specializer metaobject. With the help of examples, we show that
this metaobject allows for the efficient implementation of complex
non-class-based dispatch within the framework of existing metaobject protocols.
We present our modifications to the generic function invocation protocol from
the Art of the Metaobject Protocol; in combination with previous work, this
produces a fully-functional extension of the existing mechanism for method
selection and combination, including support for method combination completely
independent from method selection. We discuss our implementation, within the
SBCL implementation of Common Lisp, and in that context compare the performance
of the new protocol with the standard one, demonstrating that the new protocol
can be tolerably efficient.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.03289v2,"QarSUMO: A Parallel, Congestion-optimized Traffic Simulator","Traffic simulators are important tools for tasks such as urban planning and
transportation management. Microscopic simulators allow per-vehicle movement
simulation, but require longer simulation time. The simulation overhead is
exacerbated when there is traffic congestion and most vehicles move slowly.
This in particular hurts the productivity of emerging urban computing studies
based on reinforcement learning, where traffic simulations are heavily and
repeatedly used for designing policies to optimize traffic related tasks.
  In this paper, we develop QarSUMO, a parallel, congestion-optimized version
of the popular SUMO open-source traffic simulator. QarSUMO performs high-level
parallelization on top of SUMO, to utilize powerful multi-core servers and
enables future extension to multi-node parallel simulation if necessary. The
proposed design, while partly sacrificing speedup, makes QarSUMO compatible
with future SUMO improvements. We further contribute such an improvement by
modifying the SUMO simulation engine for congestion scenarios where the update
computation of consecutive and slow-moving vehicles can be simplified.
  We evaluate QarSUMO with both real-world and synthetic road network and
traffic data, and examine its execution time as well as simulation accuracy
relative to the original, sequential SUMO.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0311018v1,"Ackermann Encoding, Bisimulations, and OBDDs","We propose an alternative way to represent graphs via OBDDs based on the
observation that a partition of the graph nodes allows sharing among the
employed OBDDs. In the second part of the paper we present a method to compute
at the same time the quotient w.r.t. the maximum bisimulation and the OBDD
representation of a given graph. The proposed computation is based on an
OBDD-rewriting of the notion of Ackermann encoding of hereditarily finite sets
into natural numbers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1403.5805v1,Parallel Implementations of the Jacobi Linear Algebraic Systems Solve,"The objective of this research is to construct parallel implementations of
the Jacobi algorithm used for the solution of linear algebraic systems, to
measure their speedup with respect to the serial case and to compare each
other, regarding their efficiency. The programming paradigm used in this
implementation is the message passing model, while, the used MPI implementation
is the MPICH implementation of the Argonne National Laboratory.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1712.09855v1,Beyond-Planarity: Density Results for Bipartite Graphs,"Beyond-planarity focuses on the study of geometric and topological graphs
that are in some sense nearly-planar. Here, planarity is relaxed by allowing
edge crossings, but only with respect to some local forbidden crossing
configurations. Early research dates back to the 1960s (e.g., Avital and Hanani
1966) for extremal problems on geometric graphs, but is also related to graph
drawing problems where visual clutter by edge crossings should be minimized
(e.g., Huang et al. 2008) that could negatively affect the readability of the
drawing. Different types of forbidden crossing configurations give rise to
different families of nearly-planar graphs.
  Most of the literature focuses on Tur\'an-type problems, which ask for the
maximum number of edges a nearly-planar graph can have. Here, we study this
problem for bipartite topological graphs, considering several types of
nearly-planar graphs, i.e., 1-planar, 2-planar, fan-planar, and RAC graphs. We
prove bounds on the number of edges that are tight up to small additive
constants; some of them are surprising and not along the lines of the known
results for non-bipartite graphs. Our findings lead to an improvement of the
leading constant of the well-known Crossing Lemma for bipartite graphs, as well
as to a number of interesting research questions on topological graphs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1509.08778v3,"The Impact of Dual Prediction Schemes on the Reduction of the Number of
  Transmissions in Sensor Networks","Future Internet of Things (IoT) applications will require that billions of
wireless devices transmit data to the cloud frequently. However, the wireless
medium access is pointed as a problem for the next generations of wireless
networks; hence, the number of data transmissions in Wireless Sensor Networks
(WSNs) can quickly become a bottleneck, disrupting the exponential growth in
the number of interconnected devices, sensors, and amount of produced data.
Therefore, keeping a low number of data transmissions is critical to
incorporate new sensor nodes and measure a great variety of parameters in
future generations of WSNs. Thanks to the high accuracy and low complexity of
state-of-the-art forecasting algorithms, Dual Prediction Schemes (DPSs) are
potential candidates to optimize the data transmissions in WSNs at the finest
level because they facilitate for sensor nodes to avoid unnecessary
transmissions without affecting the quality of their measurements. In this
work, we present a sensor network model that uses statistical theorems to
describe the expected impact of DPSs and data aggregation in WSNs. We aim to
provide a foundation for future works by characterizing the theoretical gains
of processing data in sensors and conditioning its transmission to the
predictions' accuracy. Our simulation results show that the number of
transmissions can be reduced by almost 98% in the sensor nodes with the highest
workload. We also detail the impact of predicting and aggregating transmissions
according to the parameters that can be observed in common scenarios, such as
sensor nodes' transmission ranges, the correlation between measurements of
different sensors, and the period between two consecutive measurements in a
sensor.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0305040v1,Bounded LTL Model Checking with Stable Models,"In this paper bounded model checking of asynchronous concurrent systems is
introduced as a promising application area for answer set programming. As the
model of asynchronous systems a generalisation of communicating automata,
1-safe Petri nets, are used. It is shown how a 1-safe Petri net and a
requirement on the behaviour of the net can be translated into a logic program
such that the bounded model checking problem for the net can be solved by
computing stable models of the corresponding program. The use of the stable
model semantics leads to compact encodings of bounded reachability and deadlock
detection tasks as well as the more general problem of bounded model checking
of linear temporal logic. Correctness proofs of the devised translations are
given, and some experimental results using the translation and the Smodels
system are presented.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0111024v1,Building Multi-Platform User Interfaces with UIML,"There has been a widespread emergence of computing devices in the past few
years that go beyond the capabilities of traditional desktop computers.
However, users want to use the same kinds of applications and access the same
data and information on these appliances that they can access on their desktop
computers. The user interfaces for these platforms go beyond the traditional
interaction metaphors. It is a challenge to build User Interfaces (UIs) for
these devices of differing capabilities that allow the end users to perform the
same kinds of tasks. The User Interface Markup Language (UIML) is an XML-based
language that allows the canonical description of UIs for different platforms.
We describe the language features of UIML that facilitate the development of
multi-platform UIs. We also describe the key aspects of our approach that makes
UIML succeed where previous approaches failed, namely the division in the
representation of a UI, the use of a generic vocabulary, and an integrated
development environment specifically designed for transformation-based UI
development. Finally we describe the initial details of a multi-step usability
engineering process for building multi-platform UI using UIML.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1712.08482v1,"Encoding Watermark Numbers as Reducible Permutation Graphs using
  Self-inverting Permutations","Several graph theoretic watermark methods have been proposed to encode
numbers as graph structures in software watermarking environments. In this
paper, we propose an efficient and easily implementable codec system for
encoding watermark numbers as reducible permutation flow-graphs and, thus, we
extend the class of graphs used in such a watermarking environment. More
precisely, we present an algorithm for encoding a watermark number $w$ as a
self-inverting permutation $\pi^*$, an algorithm for encoding the
self-inverting permutation $\pi^*$ into a reducible permutation graph
$F[\pi^*]$ whose structure resembles the structure of real program graphs, as
well as decoding algorithms which extract the permutation $\pi^*$ from the
reducible permutation graph $F[\pi^*]$ and the number $w$ from $\pi^*$. Both
the encoding and the decoding process takes time and space linear in the length
of the binary representation of $w$. The two main components of our proposed
codec system, i.e., the self-inverting permutation $\pi^*$ and the reducible
permutation graph $F[\pi^*]$, incorporate the binary representation of the
watermark~$w$ in their structure and possess important structural properties,
which make our system resilient to attacks; to this end, we experimentally
evaluated our system under edge modification attacks on the graph $F[\pi^*]$
and the results show that we can detect such attacks with high probability.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.07052v1,"Effects of Hints on Debugging Scratch Programs: An Empirical Study with
  Primary School Teachers in Training","Bugs in learners' programs are often the result of fundamental
misconceptions. Teachers frequently face the challenge of first having to
understand such bugs, and then suggest ways to fix them. In order to enable
teachers to do so effectively and efficiently, it is desirable to support them
in recognising and fixing bugs. Misconceptions often lead to recurring patterns
of similar bugs, enabling automated tools to provide this support in terms of
hints on occurrences of common bug patterns. In this paper, we investigate to
what extent the hints improve the effectiveness and efficiency of teachers in
debugging learners' programs using a cohort of 163 primary school teachers in
training, tasked to correct buggy Scratch programs, with and without hints on
bug patterns. Our experiment suggests that automatically generated hints can
reduce the effort of finding and fixing bugs from 8.66 to 5.24 minutes, while
increasing the effectiveness by 34% more correct solutions. While this
improvement is convincing, arguably teachers in training might first need to
learn debugging ""the hard way"" to not miss the opportunity to learn by relying
on tools. We therefore investigate whether the use of hints during training
affects their ability to recognise and fix bugs without hints. Our experiment
provides no significant evidence that either learning to debug with hints or
learning to debug ""the hard way"" leads to better learning effects. Overall,
this suggests that bug patterns might be a useful concept to include in the
curriculum for teachers in training, while tool-support to recognise these
patterns is desirable for teachers in practice.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/2101.04109v2,"Explain and Predict, and then Predict Again","A desirable property of learning systems is to be both effective and
interpretable. Towards this goal, recent models have been proposed that first
generate an extractive explanation from the input text and then generate a
prediction on just the explanation called explain-then-predict models. These
models primarily consider the task input as a supervision signal in learning an
extractive explanation and do not effectively integrate rationales data as an
additional inductive bias to improve task performance. We propose a novel yet
simple approach ExPred, that uses multi-task learning in the explanation
generation phase effectively trading-off explanation and prediction losses. And
then we use another prediction network on just the extracted explanations for
optimizing the task performance. We conduct an extensive evaluation of our
approach on three diverse language datasets -- fact verification, sentiment
classification, and QA -- and find that we substantially outperform existing
approaches.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.09294v1,DynIMS: A Dynamic Memory Controller for In-memory Storage on HPC Systems,"In order to boost the performance of data-intensive computing on HPC systems,
in-memory computing frameworks, such as Apache Spark and Flink, use local DRAM
for data storage. Optimizing the memory allocation to data storage is critical
to delivering performance to traditional HPC compute jobs and throughput to
data-intensive applications sharing the HPC resources. Current practices that
statically configure in-memory storage may leave inadequate space for compute
jobs or lose the opportunity to utilize more available space for data-intensive
applications. In this paper, we explore techniques to dynamically adjust
in-memory storage and make the right amount of space for compute jobs. We have
developed a dynamic memory controller, DynIMS, which infers memory demands of
compute tasks online and employs a feedback-based control model to adapt the
capacity of in-memory storage. We test DynIMS using mixed HPCC and Spark
workloads on a HPC cluster. Experimental results show that DynIMS can achieve
up to 5X performance improvement compared to systems with static memory
allocations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.0744v2,"Extraction of Pharmacokinetic Evidence of Drug-drug Interactions from
  the Literature","Drug-drug interaction (DDI) is a major cause of morbidity and mortality and a
subject of intense scientific interest. Biomedical literature mining can aid
DDI research by extracting evidence for large numbers of potential interactions
from published literature and clinical databases. Though DDI is investigated in
domains ranging in scale from intracellular biochemistry to human populations,
literature mining has not been used to extract specific types of experimental
evidence, which are reported differently for distinct experimental goals. We
focus on pharmacokinetic evidence for DDI, essential for identifying causal
mechanisms of putative interactions and as input for further pharmacological
and pharmaco-epidemiology investigations. We used manually curated corpora of
PubMed abstracts and annotated sentences to evaluate the efficacy of literature
mining on two tasks: first, identifying PubMed abstracts containing
pharmacokinetic evidence of DDIs; second, extracting sentences containing such
evidence from abstracts. We implemented a text mining pipeline and evaluated it
using several linear classifiers and a variety of feature transforms. The most
important textual features in the abstract and sentence classification tasks
were analyzed. We also investigated the performance benefits of using features
derived from PubMed metadata fields, various publicly available named entity
recognizers, and pharmacokinetic dictionaries. Several classifiers performed
very well in distinguishing relevant and irrelevant abstracts (reaching
F1~=0.93, MCC~=0.74, iAUC~=0.99) and sentences (F1~=0.76, MCC~=0.65,
iAUC~=0.83). We found that word bigram features were important for achieving
optimal classifier performance and that features derived from Medical Subject
Headings (MeSH) terms significantly improved abstract classification. ...",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1709.02094v1,"On the Complexity of Model Checking for Syntactically Maximal Fragments
  of the Interval Temporal Logic HS with Regular Expressions","In this paper, we investigate the model checking (MC) problem for Halpern and
Shoham's interval temporal logic HS. In the last years, interval temporal logic
MC has received an increasing attention as a viable alternative to the
traditional (point-based) temporal logic MC, which can be recovered as a
special case. Most results have been obtained under the homogeneity assumption,
that constrains a proposition letter to hold over an interval if and only if it
holds over each component state. Recently, Lomuscio and Michaliszyn proposed a
way to relax such an assumption by exploiting regular expressions to define the
behaviour of proposition letters over intervals in terms of their component
states. When homogeneity is assumed, the exact complexity of MC is a difficult
open question for full HS and for its two syntactically maximal fragments
AA'BB'E' and AA'EB'E'. In this paper, we provide an asymptotically optimal
bound to the complexity of these two fragments under the more expressive
semantic variant based on regular expressions by showing that their MC problem
is AEXP_pol-complete, where AEXP_pol denotes the complexity class of problems
decided by exponential-time bounded alternating Turing Machines making a
polynomially bounded number of alternations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0307001v1,"Serving Database Information Using a Flexible Server in a Three Tier
  Architecture","The D0 experiment at Fermilab relies on a central Oracle database for storing
all detector calibration information. Access to this data is needed by hundreds
of physics applications distributed worldwide. In order to meet the demands of
these applications from scarce resources, we have created a distributed system
that isolates the user applications from the database facilities. This system,
known as the Database Application Network (DAN) operates as the middle tier in
a three tier architecture. A DAN server employs a hierarchical caching scheme
and database connection management facility that limits access to the database
resource. The modular design allows for caching strategies and database access
components to be determined by runtime configuration. To solve scalability
problems, a proxy database component allows for DAN servers to be arranged in a
hierarchy. Also included is an event based monitoring system that is currently
being used to collect statistics for performance analysis and problem
diagnosis. DAN servers are currently implemented as a Python multithreaded
program using CORBA for network communications and interface specification. The
requirement details, design, and implementation of DAN are discussed along with
operational experience and future plans.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1808.09853v1,Counting Independent Sets in Cocomparability Graphs,"We show that the number of independent sets in cocomparability graphs can be
counted in linear time, as can counting cliques in comparability graphs. By
contrast, counting cliques in cocomparabilty graphs and counting independent
sets in comparability graphs are #P-complete. We extend these results to
counting maximal cliques and independent sets. We also consider the
fixed-parameter versions of counting cliques and independent sets of given size
$k$. Finally, we combine the results to show that both counting cliques and
independent sets in permutation graphs are in linear time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.05426v4,Semantic Relations and Deep Learning,"The second edition of ""Semantic Relations Between Nominals"" by Vivi Nastase,
Stan Szpakowicz, Preslav Nakov and Diarmuid \'O S\'eaghdha has been published
in April 2021 by Morgan & Claypool
(www.morganclaypoolpublishers.com/catalog_Orig/product_info.php?products_id=1627).
A new Chapter 5 of the book, by Vivi Nastase and Stan Szpakowicz, discusses
relation classification/extraction in the deep-learning paradigm which arose
after the first edition appeared. This is Chapter 5, made public by the kind
permission of Morgan & Claypool.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0210016v2,Compact Floor-Planning via Orderly Spanning Trees,"Floor-planning is a fundamental step in VLSI chip design. Based upon the
concept of orderly spanning trees, we present a simple O(n)-time algorithm to
construct a floor-plan for any n-node plane triangulation. In comparison with
previous floor-planning algorithms in the literature, our solution is not only
simpler in the algorithm itself, but also produces floor-plans which require
fewer module types. An equally important aspect of our new algorithm lies in
its ability to fit the floor-plan area in a rectangle of size (n-1)x(2n+1)/3.
Lower bounds on the worst-case area for floor-planning any plane triangulation
are also provided in the paper.",0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.04052v1,Using Image Processing Techniques to Increase Safety in Shooting Ranges,"Accidents are a leading cause of deaths in armed forces. The Aim of this
paper is to minimize the accidents caused using weapons in the armed forces.
Developing artificial intelligence technologies aim to increase efficiency more
and more wherever people exist. Giving guns to inexperienced, untrained, or
unpredictable mentally unhealthy people in shooting ranges used for gun
training can be risky and fatal. With the use of image processing technologies
in these shooting ranges, it is aimed to minimize the risk of life-threatening
accidents that may be caused by this people. Artificial intelligence is trained
for the targets to be used in shooting ranges. When the camera of weapon sees
these targets, it switches from safe mode to firing mode. When a risky
situation occurs in shooting range, the gun turns itself into safe mode with
various additional security measures.",0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0607109v2,Complexity and Applications of Edge-Induced Vertex-Cuts,"Motivated by hypergraph decomposition algorithms, we introduce the notion of
edge-induced vertex-cuts and compare it with the well-known notions of
edge-cuts and vertex-cuts. We investigate the complexity of computing minimum
edge-induced vertex-cuts and demonstrate the usefulness of our notion by
applications in network reliability and constraint satisfaction.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.02594v1,Horizontal Product Differentiation in Varian's Model of Sales,"We consider the explicit introduction of firms' choice of location to
Varian's model of sales for a two-stage spatial competition model based on a
standard Hotelling's linear city model. This model is the formalization of
Varian's model of sales in the context of Hotelling's spatial competition. We
obtain three main results. First, we show that there exists a subgame perfect
equilibrium in which each firm chooses a symmetric mixed strategy equilibrium
profile. This equilibrium includes symmetric location pairs and asymmetric
location pairs. Second, the equilibrium behaviors in our model are randomized
at both location and price stages. Third, we show that expected profits in a
subgame perfect equilibrium are equal to the maximum monopoly profit from an
uninformed market. Thus, even when product differentiation is explicitly
introduced into a Varian-type model, Varian's implication can be retained; the
opportunity for profit in an informed market is lost with competition.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1507.08717v1,Systematic Verification of the Modal Logic Cube in Isabelle/HOL,"We present an automated verification of the well-known modal logic cube in
Isabelle/HOL, in which we prove the inclusion relations between the cube's
logics using automated reasoning tools. Prior work addresses this problem but
without restriction to the modal logic cube, and using encodings in first-order
logic in combination with first-order automated theorem provers. In contrast,
our solution is more elegant, transparent and effective. It employs an
embedding of quantified modal logic in classical higher-order logic. Automated
reasoning tools, such as Sledgehammer with LEO-II, Satallax and CVC4, Metis and
Nitpick, are employed to achieve full automation. Though successful, the
experiments also motivate some technical improvements in the Isabelle/HOL tool.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1604.08936v1,Complexity Hierarchies and Higher-Order Cons-Free Rewriting,"Constructor rewriting systems are said to be cons-free if, roughly,
constructor terms in the right-hand sides of rules are subterms of constructor
terms in the left-hand side; the computational intuition is that rules cannot
build new data structures. It is well-known that cons-free programming
languages can be used to characterize computational complexity classes, and
that cons-free first-order term rewriting can be used to characterize the set
of polynomial-time decidable sets.
  We investigate cons-free higher-order term rewriting systems, the complexity
classes they characterize, and how these depend on the order of the types used
in the systems. We prove that, for every k $\geq$ 1, left-linear cons-free
systems with type order k characterize E$^k$TIME if arbitrary evaluation is
used (i.e., the system does not have a fixed reduction strategy).
  The main difference with prior work in implicit complexity is that (i) our
results hold for non-orthogonal term rewriting systems with possible rule
overlaps with no assumptions about reduction strategy, (ii) results for such
term rewriting systems have previously only been obtained for k = 1, and with
additional syntactic restrictions on top of cons-freeness and left-linearity.
  Our results are apparently among the first implicit characterizations of the
hierarchy E = E$^1$TIME $\subseteq$ E$^2$TIME $\subseteq$ .... Our work
confirms prior results that having full non-determinism (via overlaps of rules)
does not directly allow characterization of non-deterministic complexity
classes like NE. We also show that non-determinism makes the classes
characterized highly sensitive to minor syntactic changes such as admitting
product types or non-left-linear rules.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1510.06788v2,Prodirect Manipulation: Bidirectional Programming for the Masses,"Software interfaces today generally fall at either end of a spectrum. On one
end are programmable systems, which allow expert users (i.e. programmers) to
write software artifacts that describe complex abstractions, but programs are
disconnected from their eventual output. On the other end are domain-specific
graphical user interfaces (GUIs), which allow end users (i.e. non-programmers)
to easily create varied content but present insurmountable walls when a desired
feature is not built-in. Both programmatic and direct manipulation have
distinct strengths, but users must typically choose one over the other or use
some ad-hoc combination of systems. Our goal, put simply, is to bridge this
divide.
  We envision novel software systems that tightly couple programmatic and
direct manipulation --- a combination we dub prodirect manipulation --- for a
variety of use cases. This will require advances in a broad range of software
engineering disciplines, from program analysis and program synthesis technology
to user interface design and evaluation. In this extended abstract, we propose
two general strategies --- real-time program synthesis and domain-specific
synthesis of general-purpose programs --- that may prove fruitful for
overcoming the technical challenges. We also discuss metrics that will be
important in evaluating the usability and utility of prodirect manipulation
systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1612.04003v2,"Avoiding communication in primal and dual block coordinate descent
  methods","Primal and dual block coordinate descent methods are iterative methods for
solving regularized and unregularized optimization problems. Distributed-memory
parallel implementations of these methods have become popular in analyzing
large machine learning datasets. However, existing implementations communicate
at every iteration which, on modern data center and supercomputing
architectures, often dominates the cost of floating-point computation. Recent
results on communication-avoiding Krylov subspace methods suggest that large
speedups are possible by re-organizing iterative algorithms to avoid
communication. We show how applying similar algorithmic transformations can
lead to primal and dual block coordinate descent methods that only communicate
every $s$ iterations--where $s$ is a tuning parameter--instead of every
iteration for the \textit{regularized least-squares problem}. We show that the
communication-avoiding variants reduce the number of synchronizations by a
factor of $s$ on distributed-memory parallel machines without altering the
convergence rate and attains strong scaling speedups of up to $6.1\times$ on a
Cray XC30 supercomputer.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1510.08419v3,Program Synthesis from Polymorphic Refinement Types,"We present a method for synthesizing recursive functions that provably
satisfy a given specification in the form of a polymorphic refinement type. We
observe that such specifications are particularly suitable for program
synthesis for two reasons. First, they offer a unique combination of expressive
power and decidability, which enables automatic verification---and hence
synthesis---of nontrivial programs. Second, a type-based specification for a
program can often be effectively decomposed into independent specifications for
its components, causing the synthesizer to consider fewer component
combinations and leading to a combinatorial reduction in the size of the search
space. At the core of our synthesis procedure is a new algorithm for refinement
type checking, which supports specification decomposition.
  We have evaluated our prototype implementation on a large set of synthesis
problems and found that it exceeds the state of the art in terms of both
scalability and usability. The tool was able to synthesize more complex
programs than those reported in prior work (several sorting algorithms and
operations on balanced search trees), as well as most of the benchmarks tackled
by existing synthesizers, often starting from a more concise and intuitive user
input.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1608.08704v2,"Near-Optimal Lower Bounds on Quantifier Depth and Weisfeiler-Leman
  Refinement Steps","We prove near-optimal trade-offs for quantifier depth versus number of
variables in first-order logic by exhibiting pairs of $n$-element structures
that can be distinguished by a $k$-variable first-order sentence but where
every such sentence requires quantifier depth at least $n^{\Omega(k/\log k)}$.
Our trade-offs also apply to first-order counting logic, and by the known
connection to the $k$-dimensional Weisfeiler--Leman algorithm imply
near-optimal lower bounds on the number of refinement iterations.
  A key component in our proof is the hardness condensation technique recently
introduced by [Razborov '16] in the context of proof complexity. We apply this
method to reduce the domain size of relational structures while maintaining the
minimal quantifier depth to distinguish them in finite variable logics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0202012v1,Logic program specialisation through partial deduction: Control issues,"Program specialisation aims at improving the overall performance of programs
by performing source to source transformations. A common approach within
functional and logic programming, known respectively as partial evaluation and
partial deduction, is to exploit partial knowledge about the input. It is
achieved through a well-automated application of parts of the
Burstall-Darlington unfold/fold transformation framework. The main challenge in
developing systems is to design automatic control that ensures correctness,
efficiency, and termination. This survey and tutorial presents the main
developments in controlling partial deduction over the past 10 years and
analyses their respective merits and shortcomings. It ends with an assessment
of current achievements and sketches some remaining research challenges.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.02019v1,A logic for n-dimensional hierarchical refinement,"Hierarchical transition systems provide a popular mathematical structure to
represent state-based software applications in which different layers of
abstraction are represented by inter-related state machines. The decomposition
of high level states into inner sub-states, and of their transitions into inner
sub-transitions is common refinement procedure adopted in a number of
specification formalisms.
  This paper introduces a hybrid modal logic for k-layered transition systems,
its first-order standard translation, a notion of bisimulation, and a modal
invariance result. Layered and hierarchical notions of refinement are also
discussed in this setting.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.03539v1,HASeparator: Hyperplane-Assisted Softmax,"Efficient feature learning with Convolutional Neural Networks (CNNs)
constitutes an increasingly imperative property since several challenging tasks
of computer vision tend to require cascade schemes and modalities fusion.
Feature learning aims at CNN models capable of extracting embeddings,
exhibiting high discrimination among the different classes, as well as
intra-class compactness. In this paper, a novel approach is introduced that has
separator, which focuses on an effective hyperplane-based segregation of the
classes instead of the common class centers separation scheme. Accordingly, an
innovatory separator, namely the Hyperplane-Assisted Softmax separator
(HASeparator), is proposed that demonstrates superior discrimination
capabilities, as evaluated on popular image classification benchmarks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0205043v1,Low-Degree Spanning Trees of Small Weight,"The degree-d spanning tree problem asks for a minimum-weight spanning tree in
which the degree of each vertex is at most d. When d=2 the problem is TSP, and
in this case, the well-known Christofides algorithm provides a
1.5-approximation algorithm (assuming the edge weights satisfy the triangle
inequality).
  In 1984, Christos Papadimitriou and Umesh Vazirani posed the challenge of
finding an algorithm with performance guarantee less than 2 for Euclidean
graphs (points in R^n) and d > 2. This paper gives the first answer to that
challenge, presenting an algorithm to compute a degree-3 spanning tree of cost
at most 5/3 times the MST. For points in the plane, the ratio improves to 3/2
and the algorithm can also find a degree-4 spanning tree of cost at most 5/4
times the MST.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.02423v5,Geometric Transformers for Protein Interface Contact Prediction,"Computational methods for predicting the interface contacts between proteins
come highly sought after for drug discovery as they can significantly advance
the accuracy of alternative approaches, such as protein-protein docking,
protein function analysis tools, and other computational methods for protein
bioinformatics. In this work, we present the Geometric Transformer, a novel
geometry-evolving graph transformer for rotation and translation-invariant
protein interface contact prediction, packaged within DeepInteract, an
end-to-end prediction pipeline. DeepInteract predicts partner-specific protein
interface contacts (i.e., inter-protein residue-residue contacts) given the 3D
tertiary structures of two proteins as input. In rigorous benchmarks,
DeepInteract, on challenging protein complex targets from the 13th and 14th
CASP-CAPRI experiments as well as Docking Benchmark 5, achieves 14% and 1.1%
top L/5 precision (L: length of a protein unit in a complex), respectively. In
doing so, DeepInteract, with the Geometric Transformer as its graph-based
backbone, outperforms existing methods for interface contact prediction in
addition to other graph-based neural network backbones compatible with
DeepInteract, thereby validating the effectiveness of the Geometric Transformer
for learning rich relational-geometric features for downstream tasks on 3D
protein structures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.12937v1,Predicting Game Difficulty and Churn Without Players,"We propose a novel simulation model that is able to predict the per-level
churn and pass rates of Angry Birds Dream Blast, a popular mobile free-to-play
game. Our primary contribution is to combine AI gameplay using Deep
Reinforcement Learning (DRL) with a simulation of how the player population
evolves over the levels. The AI players predict level difficulty, which is used
to drive a player population model with simulated skill, persistence, and
boredom. This allows us to model, e.g., how less persistent and skilled players
are more sensitive to high difficulty, and how such players churn early, which
makes the player population and the relation between difficulty and churn
evolve level by level. Our work demonstrates that player behavior predictions
produced by DRL gameplay can be significantly improved by even a very simple
population-level simulation of individual player differences, without requiring
costly retraining of agents or collecting new DRL gameplay data for each
simulated player.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0603025v2,Open Answer Set Programming with Guarded Programs,"Open answer set programming (OASP) is an extension of answer set programming
where one may ground a program with an arbitrary superset of the program's
constants. We define a fixed point logic (FPL) extension of Clark's completion
such that open answer sets correspond to models of FPL formulas and identify a
syntactic subclass of programs, called (loosely) guarded programs. Whereas
reasoning with general programs in OASP is undecidable, the FPL translation of
(loosely) guarded programs falls in the decidable (loosely) guarded fixed point
logic (mu(L)GF). Moreover, we reduce normal closed ASP to loosely guarded OASP,
enabling for the first time, a characterization of an answer set semantics by
muLGF formulas. We further extend the open answer set semantics for programs
with generalized literals. Such generalized programs (gPs) have interesting
properties, e.g., the ability to express infinity axioms. We restrict the
syntax of gPs such that both rules and generalized literals are guarded. Via a
translation to guarded fixed point logic, we deduce 2-exptime-completeness of
satisfiability checking in such guarded gPs (GgPs). Bound GgPs are restricted
GgPs with exptime-complete satisfiability checking, but still sufficiently
expressive to optimally simulate computation tree logic (CTL). We translate
Datalog lite programs to GgPs, establishing equivalence of GgPs under an open
answer set semantics, alternation-free muGF, and Datalog lite.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1908.06684v4,Directed Homotopy in Non-Positively Curved Spaces,"A semantics of concurrent programs can be given using precubical sets, in
order to study (higher) commutations between the actions, thus encoding the
""geometry"" of the space of possible executions of the program. Here, we study
the particular case of programs using only mutexes, which are the most widely
used synchronization primitive. We show that in this case, the resulting
programs have non-positive curvature, a notion that we introduce and study here
for precubical sets, and can be thought of as an algebraic analogue of the
well-known one for metric spaces. Using this it, as well as categorical
rewriting techniques, we are then able to show that directed and non-directed
homotopy coincide for directed paths in these precubical sets. Finally, we
study the geometric realization of precubical sets in metric spaces, to show
that our conditions on precubical sets actually coincide with those for metric
spaces. Since the category of metric spaces is not cocomplete, we are lead to
work with generalized metric spaces and study some of their properties.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1312.2177v2,Machine Learning Techniques for Intrusion Detection,"An Intrusion Detection System (IDS) is a software that monitors a single or a
network of computers for malicious activities (attacks) that are aimed at
stealing or censoring information or corrupting network protocols. Most
techniques used in today's IDS are not able to deal with the dynamic and
complex nature of cyber attacks on computer networks. Hence, efficient adaptive
methods like various techniques of machine learning can result in higher
detection rates, lower false alarm rates and reasonable computation and
communication costs. In this paper, we study several such schemes and compare
their performance. We divide the schemes into methods based on classical
artificial intelligence (AI) and methods based on computational intelligence
(CI). We explain how various characteristics of CI techniques can be used to
build efficient IDS.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1106.1862v1,The MathScheme Library: Some Preliminary Experiments,"We present some of the experiments we have performed to best test our design
for a library for MathScheme, the mechanized mathematics software system we are
building. We wish for our library design to use and reflect, as much as
possible, the mathematical structure present in the objects which populate the
library.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1303.6555v1,Index sets for Finite Normal Predicate Logic Programs,"<Q>_e is the effective list of all finite predicate logic programs. <T_e> is
the list of recursive trees. We modify constructions of Marek, Nerode, and
Remmel [25] to construct recursive functions f and g such that for all indices
e, (i) there is a one-to-one degree preserving correspondence between the set
of stable models of Q_e and the set of infinite paths through T_{f(e)} and (ii)
there is a one-to-one degree preserving correspondence between the set of
infinite paths through T_e and the set of stable models of Q_{g(e)}. We use
these two recursive functions to reduce the problem of finding the complexity
of the index set I_P for various properties P of normal finite predicate logic
programs to the problem of computing index sets for primitive recursive trees
for which there is a large variety of results [6], [8], [16], [17], [18], [19].
We use our correspondences to determine the complexity of the index sets of all
programs and of certain special classes of finite predicate logic programs of
properties such as (i) having no stable models, (ii) having at least one stable
model, (iii) having exactly c stable models for any given positive integer c,
(iv) having only finitely many stable models, or (vi) having infinitely many
stable models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.03331v2,Ostraka: Secure Blockchain Scaling by Node Sharding,"Cryptocurrencies, implemented with blockchain protocols, promise to become a
global payment system if they can overcome performance limitations. Rapidly
advancing architectures improve on latency and throughput, but most require all
participating servers to process all transactions. Several recent works propose
to shard the system, such that each machine would only process a subset of the
transactions. However, we identify a denial-of-service attack that is exposed
by these solutions - an attacker can generate transactions that would overload
a single shard, thus delaying processing in the entire system. Moreover, we
show that in common scenarios, these protocols require most node operators to
process almost all blockchain transactions. We present Ostraka, a blockchain
node architecture that shards (parallelizes) the nodes themselves. We prove
that replacing a unified node with an Ostraka node does not affect the security
of the underlying consensus mechanism. We evaluate analytically and
experimentally block propagation and processing in various settings. Ostraka
allows nodes in the network to scale, without costly coordination. In our
experiments, Ostraka nodes' transaction processing rate grows linearly with the
addition of resources.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.05893v1,"Computed Axial Lithography (CAL): Toward Single Step 3D Printing of
  Arbitrary Geometries","Most additive manufacturing processes today operate by printing voxels (3D
pixels) serially point-by-point to build up a 3D part. In some more
recently-developed techniques, for example optical printing methods such as
projection stereolithography [Zheng et al. 2012], [Tumbleston et al. 2015],
parts are printed layer-by-layer by curing full 2d (very thin in one dimension)
layers of the 3d part in each print step. There does not yet exist a technique
which is able to print arbitrarily-defined 3D geometries in a single print
step. If such a technique existed, it could be used to expand the range of
printable geometries in additive manufacturing and relax constraints on factors
such as overhangs in topology optimization. It could also vastly increase print
speed for 3D parts. In this work, we develop the principles for an approach for
single exposure 3D printing of arbitrarily defined geometries. The approach,
termed Computed Axial Lithgography (CAL), is based on tomographic
reconstruction, with mathematical optimization to generate a set of projections
to optically define an arbitrary dose distribution within a target volume. We
demonstrate the potential ability of the technique to print 3D parts using a
prototype CAL system based on sequential illumination from many angles. We also
propose new hardware designs which will help us to realize true single-shot
arbitrary-geometry 3D CAL.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.03594v3,Learning Combinatorial Node Labeling Algorithms,"We present a novel neural architecture to solve graph optimization problems
where the solution consists of arbitrary node labels, allowing us to solve hard
problems like graph coloring. We train our model using reinforcement learning,
specifically policy gradients, which gives us both a greedy and a probabilistic
policy. Our architecture builds on a graph attention network and uses several
inductive biases to improve solution quality. Our learned deterministic
heuristics for graph coloring give better solutions than classical degree-based
greedy heuristics and only take seconds to apply to graphs with tens of
thousands of vertices. Moreover, our probabilistic policies outperform all
greedy state-of-the-art coloring baselines and a machine learning baseline.
Finally, we show that our approach also generalizes to other problems by
evaluating it on minimum vertex cover and outperforming two greedy heuristics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1908.10406v1,"An Effective and Efficient Method for Detecting Hands in Egocentric
  Videos for Rehabilitation Applications","Objective: Individuals with spinal cord injury (SCI) report upper limb
function as their top recovery priority. To accurately represent the true
impact of new interventions on patient function and independence, evaluation
should occur in a natural setting. Wearable cameras can be used to monitor hand
function at home, using computer vision to automatically analyze the resulting
videos (egocentric video). A key step in this process, hand detection, is
difficult to do robustly and reliably, hindering deployment of a complete
monitoring system in the home and community. We propose an accurate and
efficient hand detection method that uses a simple combination of existing
detection and tracking algorithms. Methods: Detection, tracking, and
combination methods were evaluated on a new hand detection dataset, consisting
of 167,622 frames of egocentric videos collected on 17 individuals with SCI
performing activities of daily living in a home simulation laboratory. Results:
The F1-scores for the best detector and tracker alone (SSD and Median Flow)
were 0.90$\pm$0.07 and 0.42$\pm$0.18, respectively. The best combination
method, in which a detector was used to initialize and reset a tracker,
resulted in an F1-score of 0.87$\pm$0.07 while being two times faster than the
fastest detector alone. Conclusion: The combination of the fastest detector and
best tracker improved the accuracy over online trackers while improving the
speed of detectors. Significance: The method proposed here, in combination with
wearable cameras, will help clinicians directly measure hand function in a
patient's daily life at home, enabling independence after SCI.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1711.00048v2,"Adversarial Semi-Supervised Audio Source Separation applied to Singing
  Voice Extraction","The state of the art in music source separation employs neural networks
trained in a supervised fashion on multi-track databases to estimate the
sources from a given mixture. With only few datasets available, often extensive
data augmentation is used to combat overfitting. Mixing random tracks, however,
can even reduce separation performance as instruments in real music are
strongly correlated. The key concept in our approach is that source estimates
of an optimal separator should be indistinguishable from real source signals.
Based on this idea, we drive the separator towards outputs deemed as realistic
by discriminator networks that are trained to tell apart real from separator
samples. This way, we can also use unpaired source and mixture recordings
without the drawbacks of creating unrealistic music mixtures. Our framework is
widely applicable as it does not assume a specific network architecture or
number of sources. To our knowledge, this is the first adoption of adversarial
training for music source separation. In a prototype experiment for singing
voice separation, separation performance increases with our approach compared
to purely supervised training.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.12865v1,A Concise Guide to Elicitation Methodology,"One of the open questions in the field of interaction design is ""what inputs
or interaction techniques should be used with augmented reality devices?"" The
transition from a touchpad and a keyboard to a multi-touch device was
relatively small. The transition from a multi-touch device to an HMD with no
controllers or clear surface to interact with is more complicated. This book is
a guide for how to figure out what interaction techniques and modalities people
prefer when interacting with those devices. The name of the technique covered
here is Elicitation. Elicitation is a form of participatory design, meaning
design with direct end-user involvement. By running an elicitation study
researchers can observe unconstrained human interactions with emerging
technologies to help guide input design.",0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0712.3335v1,"A polynomial time $\frac 3 2$ -approximation algorithm for the vertex
  cover problem on a class of graphs","We develop a polynomial time 3/2-approximation algorithm to solve the vertex
cover problem on a class of graphs satisfying a property called ``active edge
hypothesis''. The algorithm also guarantees an optimal solution on specially
structured graphs. Further, we give an extended algorithm which guarantees a
vertex cover $S_1$ on an arbitrary graph such that $|S_1|\leq {3/2} |S^*|+\xi$
where $S^*$ is an optimal vertex cover and $\xi$ is an error bound identified
by the algorithm. We obtained $\xi = 0$ for all the test problems we have
considered which include specially constructed instances that were expected to
be hard. So far we could not construct a graph that gives $\xi \not= 0$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1501.00387v1,Campaign Management under Approval-Driven Voting Rules,"Approval-like voting rules, such as Sincere-Strategy Preference-Based
Approval voting (SP-AV), the Bucklin rule (an adaptive variant of $k$-Approval
voting), and the Fallback rule (an adaptive variant of SP-AV) have many
desirable properties: for example, they are easy to understand and encourage
the candidates to choose electoral platforms that have a broad appeal. In this
paper, we investigate both classic and parameterized computational complexity
of electoral campaign management under such rules. We focus on two methods that
can be used to promote a given candidate: asking voters to move this candidate
upwards in their preference order or asking them to change the number of
candidates they approve of. We show that finding an optimal campaign management
strategy of the first type is easy for both Bucklin and Fallback. In contrast,
the second method is computationally hard even if the degree to which we need
to affect the votes is small. Nevertheless, we identify a large class of
scenarios that admit fixed-parameter tractable algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1502.03942v2,Communication Efficient Algorithms for Top-k Selection Problems,"We present scalable parallel algorithms with sublinear per-processor
communication volume and low latency for several fundamental problems related
to finding the most relevant elements in a set, for various notions of
relevance: We begin with the classical selection problem with unsorted input.
We present generalizations with locally sorted inputs, dynamic content
(bulk-parallel priority queues), and multiple criteria. Then we move on to
finding frequent objects and top-k sum aggregation. Since it is unavoidable
that the output of these algorithms might be unevenly distributed over the
processors, we also explain how to redistribute this data with minimal
communication.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1102.2963v3,A Tight Lower Bound for Streett Complementation,"Finite automata on infinite words ($\omega$-automata) proved to be a powerful
weapon for modeling and reasoning infinite behaviors of reactive systems.
Complementation of $\omega$-automata is crucial in many of these applications.
But the problem is non-trivial; even after extensive study during the past four
decades, we still have an important type of $\omega$-automata, namely Streett
automata, for which the gap between the current best lower bound $2^{\Omega(n
\lg nk)}$ and upper bound $2^{\Omega(nk \lg nk)}$ is substantial, for the
Streett index size $k$ can be exponential in the number of states $n$. In
arXiv:1102.2960 we showed a construction for complementing Streett automata
with the upper bound $2^{O(n \lg n+nk \lg k)}$ for $k = O(n)$ and $2^{O(n^{2}
\lg n)}$ for $k=\omega(n)$. In this paper we establish a matching lower bound
$2^{\Omega(n \lg n+nk \lg k)}$ for $k = O(n)$ and $2^{\Omega(n^{2} \lg n)}$ for
$k = \omega(n)$, and therefore showing that the construction is asymptotically
optimal with respect to the $2^{\Theta(\cdot)}$ notation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1410.4113v2,"A Direct Algorithm to Compute the Topological Euler Characteristic and
  Chern-Schwartz-MacPherson Class of Projective Complete Intersection Varieties","Let $V$ be a possibly singular scheme-theoretic complete intersection
subscheme of $\mathbb{P}^n$ over an algebraically closed field of
characteristic zero. Using a recent result of Fullwood (""On Milnor classes via
invariants of singular subschemes"", Journal of Singularities) we develop an
algorithm to compute the Chern-Schwartz-MacPherson class and Euler
characteristic of $V$. This algorithm complements existing algorithms by
providing performance improvements in the computation of the
Chern-Schwartz-MacPherson class and Euler characteristic for certain types of
complete intersection subschemes of $\mathbb{P}^n$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.04325v3,Prophet Matching Meets Probing with Commitment,"We consider the online stochastic matching problem for bipartite graphs where
edges adjacent to an online node must be probed to determine if they exist,
based on known edge probabilities. Our algorithms respect commitment, in that
if a probed edge exists, it must be used in the matching. We study this
matching problem subject to a downward-closed constraint on each online node's
allowable edge probes. Our setting generalizes the commonly studied patience
(or time-out) constraint which limits the number of probes that can be made to
an online node's adjacent edges. We introduce a new LP that we prove is a
relaxation of an optimal offline probing algorithm (the adaptive benchmark) and
which overcomes the limitations of previous LP relaxations.
  (1) A tight $\frac{1}{2}$ ratio when the stochastic graph is generated from a
known stochastic type graph where the $t^{th}$ online node is drawn
independently from a known distribution $\scr{D}_{\pi(t)}$ and $\pi$ is chosen
adversarially. We refer to this setting as the known i.d. stochastic matching
problem with adversarial arrivals.
  (2) A $1-1/e$ ratio when the stochastic graph is generated from a known
stochastic type graph where the $t^{th}$ online node is drawn independently
from a known distribution $\scr{D}_{\pi(t)}$ and $\pi$ is a random permutation.
We refer to this setting as the known i.d. stochastic matching problem with
random order arrivals.
  Our results improve upon the previous best competitive ratio of $0.46$ in the
known i.i.d. setting against the standard adaptive benchmark. Moreover, we are
the first to study the prophet secretary matching problem in the context of
probing, where we match the best known classical result.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1002.0562v1,Minimum and maximum against k lies,"A neat 1972 result of Pohl asserts that [3n/2]-2 comparisons are sufficient,
and also necessary in the worst case, for finding both the minimum and the
maximum of an n-element totally ordered set. The set is accessed via an oracle
for pairwise comparisons. More recently, the problem has been studied in the
context of the Renyi-Ulam liar games, where the oracle may give up to k false
answers. For large k, an upper bound due to Aigner shows that (k+O(\sqrt{k}))n
comparisons suffice. We improve on this by providing an algorithm with at most
(k+1+C)n+O(k^3) comparisons for some constant C. The known lower bounds are of
the form (k+1+c_k)n-D, for some constant D, where c_0=0.5, c_1=23/32=0.71875,
and c_k=\Omega(2^{-5k/4}) as k goes to infinity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1304.8069v2,Fast Approximate Polynomial Multipoint Evaluation and Applications,"It is well known that, using fast algorithms for polynomial multiplication
and division, evaluation of a polynomial $F \in \mathbb{C}[x]$ of degree $n$ at
$n$ complex-valued points can be done with $\tilde{O}(n)$ exact field
operations in $\mathbb{C},$ where $\tilde{O}(\cdot)$ means that we omit
polylogarithmic factors. We complement this result by an analysis of
approximate multipoint evaluation of $F$ to a precision of $L$ bits after the
binary point and prove a bit complexity of $\tilde{O}(n(L + \tau + n\Gamma)),$
where $2^\tau$ and $2^\Gamma,$ with $\tau, \Gamma \in \mathbb{N}_{\ge 1},$ are
bounds on the magnitude of the coefficients of $F$ and the evaluation points,
respectively. In particular, in the important case where the precision demand
dominates the other input parameters, the complexity is soft-linear in $n$ and
$L$.
  Our result on approximate multipoint evaluation has some interesting
consequences on the bit complexity of further approximation algorithms which
all use polynomial evaluation as a key subroutine. Of these applications, we
discuss in detail an algorithm for polynomial interpolation and for computing a
Taylor shift of a polynomial. Furthermore, our result can be used to derive
improved complexity bounds for algorithms to refine isolating intervals for the
real roots of a polynomial. For all of the latter algorithms, we derive
near-optimal running times.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0103011v1,"A Machine-Learning Approach to Estimating the Referential Properties of
  Japanese Noun Phrases","The referential properties of noun phrases in the Japanese language, which
has no articles, are useful for article generation in Japanese-English machine
translation and for anaphora resolution in Japanese noun phrases. They are
generally classified as generic noun phrases, definite noun phrases, and
indefinite noun phrases. In the previous work, referential properties were
estimated by developing rules that used clue words. If two or more rules were
in conflict with each other, the category having the maximum total score given
by the rules was selected as the desired category. The score given by each rule
was established by hand, so the manpower cost was high. In this work, we
automatically adjusted these scores by using a machine-learning method and
succeeded in reducing the amount of manpower needed to adjust these scores.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1509.01978v1,"An Approach to the Analysis of the South Slavic Medieval Labels Using
  Image Texture","The paper presents a new script classification method for the discrimination
of the South Slavic medieval labels. It consists in the textural analysis of
the script types. In the first step, each letter is coded by the equivalent
script type, which is defined by its typographical features. Obtained coded
text is subjected to the run-length statistical analysis and to the adjacent
local binary pattern analysis in order to extract the features. The result
shows a diversity between the extracted features of the scripts, which makes
the feature classification more effective. It is the basis for the
classification process of the script identification by using an extension of a
state-of-the-art approach for document clustering. The proposed method is
evaluated on an example of hand-engraved in stone and hand-printed in paper
labels in old Cyrillic, angular and round Glagolitic. Experiments demonstrate
very positive results, which prove the effectiveness of the proposed method.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0805.3935v1,Fusion for Evaluation of Image Classification in Uncertain Environments,"We present in this article a new evaluation method for classification and
segmentation of textured images in uncertain environments. In uncertain
environments, real classes and boundaries are known with only a partial
certainty given by the experts. Most of the time, in many presented papers,
only classification or only segmentation are considered and evaluated. Here, we
propose to take into account both the classification and segmentation results
according to the certainty given by the experts. We present the results of this
method on a fusion of classifiers of sonar images for a seabed
characterization.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1708.02710v1,From Reversible Programs to Univalent Universes and Back,"We establish a close connection between a reversible programming language
based on type isomorphisms and a formally presented univalent universe. The
correspondence relates combinators witnessing type isomorphisms in the
programming language to paths in the univalent universe; and combinator
optimizations in the programming language to 2-paths in the univalent universe.
The result suggests a simple computational interpretation of paths and of
univalence in terms of familiar programming constructs whenever the universe in
question is computable.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0207087v1,Axiomatic Aspects of Default Inference,"This paper studies axioms for nonmonotonic consequences from a
semantics-based point of view, focusing on a class of mathematical structures
for reasoning about partial information without a predefined syntax/logic. This
structure is called a default structure. We study axioms for the nonmonotonic
consequence relation derived from extensions as in Reiter's default logic,
using skeptical reasoning, but extensions are now used for the construction of
possible worlds in a default information structure.
  In previous work we showed that skeptical reasoning arising from
default-extensions obeys a well-behaved set of axioms including the axiom of
cautious cut. We show here that, remarkably, the converse is also true: any
consequence relation obeying this set of axioms can be represented as one
constructed from skeptical reasoning. We provide representation theorems to
relate axioms for nonmonotonic consequence relation and properties about
extensions, and provide one-to-one correspondence between nonmonotonic systems
which satisfies the law of cautious monotony and default structures with unique
extensions. Our results give a theoretical justification for a set of basic
rules governing the update of nonmonotonic knowledge bases, demonstrating the
derivation of them from the more concrete and primitive construction of
extensions. It is also striking to note that proofs of the representation
theorems show that only shallow extensions are necessary, in the sense that the
number of iterations needed to achieve an extension is at most three. All of
these developments are made possible by taking a more liberal view of
consistency: consistency is a user defined predicate, satisfying some basic
properties.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.06682v1,"On the efficient parallel computing of long term reliable trajectories
  for the Lorenz system","In this work we propose an efficient parallelization of multiple-precision
Taylor series method with variable stepsize and fixed order. For given level of
accuracy the optimal variable stepsize determines higher order of the method
than in the case of optimal fixed stepsize. Although the used order of the
method is greater then that in the case of fixed stepsize, and hence the
computational work per step is greater, the reduced number of steps gives less
overall work. Also the greater order of the method is beneficial in the sense
that it increases the parallel efficiency. As a model problem we use the
paradigmatic Lorenz system. With 256 CPU cores in Nestum cluster, Sofia,
Bulgaria, we succeed to obtain a correct reference solution in the rather long
time interval - [0,11000]. To get this solution we performed two large
computations: one computation with 4566 decimal digits of precision and 5240-th
order method, and second computation for verification - with 4778 decimal
digits of precision and 5490-th order method.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.07271v1,"Geofreebie: A Location-Based Freecycling App to Support Forced Migrant
  Resettlement","Germany has witnessed an influx of forced migrants in recent years. Promoting
social interaction with the local community is key to supporting the
resettlement of these newcomers. Location-based freecycling services present
important benefits due to freecycling's potential to bolster social engagement
and location-based services' ability to adapt to the user's context. Yet, their
potential to support forced migrants' resettlement is yet to be examined. We
conducted needs assessment interviews with 11 participants in Muenster,
Germany. We analyzed the interview results to develop user requirements for
location-based freecycling services. We then implemented a subset of the user
requirements as a prototype mobile app called Geofreebie. The evaluation of the
app with 22 participants showed that Geofreebie offered two key advantages for
forced migrants' resettlement: it increased the size of their social network,
and created a sense of community on their side. These findings can benefit
researchers and developers of location-based services to support forced migrant
resettlement.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1005.0665v2,"Software Design Document, Testing, Deployment and Configuration
  Management of the UUIS--a Team 2 COMP5541-W10 Project Approach","The Software Design Document of UUIS describes the prototype design details
of the system architecture, database layer, deployment and configuration
details as well as test cases produced while working the design and
implementation of the prototype. The requirements specification of UUIS are
detailed in arXiv:1005.0783.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2112.02478v2,"Classification of COVID-19 on chest X-Ray images using Deep Learning
  model with Histogram Equalization and Lungs Segmentation","Background and Objective: Artificial intelligence (AI) methods coupled with
biomedical analysis has a critical role during pandemics as it helps to release
the overwhelming pressure from healthcare systems and physicians. As the
ongoing COVID-19 crisis worsens in countries having dense populations and
inadequate testing kits like Brazil and India, radiological imaging can act as
an important diagnostic tool to accurately classify covid-19 patients and
prescribe the necessary treatment in due time. With this motivation, we present
our study based on deep learning architecture for detecting covid-19 infected
lungs using chest X-rays. Dataset: We collected a total of 2470 images for
three different class labels, namely, healthy lungs, ordinary pneumonia, and
covid-19 infected pneumonia, out of which 470 X-ray images belong to the
covid-19 category. Methods: We first pre-process all the images using histogram
equalization techniques and segment them using U-net architecture. VGG-16
network is then used for feature extraction from the pre-processed images which
is further sampled by SMOTE oversampling technique to achieve a balanced
dataset. Finally, the class-balanced features are classified using a support
vector machine (SVM) classifier with 10-fold cross-validation and the accuracy
is evaluated. Result and Conclusion: Our novel approach combining well-known
pre-processing techniques, feature extraction methods, and dataset balancing
method, lead us to an outstanding rate of recognition of 98% for COVID-19
images over a dataset of 2470 X-ray images. Our model is therefore fit to be
utilized in healthcare facilities for screening purposes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0210004v1,Revising Partially Ordered Beliefs,"This paper deals with the revision of partially ordered beliefs. It proposes
a semantic representation of epistemic states by partial pre-orders on
interpretations and a syntactic representation by partially ordered belief
bases. Two revision operations, the revision stemming from the history of
observations and the possibilistic revision, defined when the epistemic state
is represented by a total pre-order, are generalized, at a semantic level, to
the case of a partial pre-order on interpretations, and at a syntactic level,
to the case of a partially ordered belief base. The equivalence between the two
representations is shown for the two revision operations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1612.07771v3,Highway and Residual Networks learn Unrolled Iterative Estimation,"The past year saw the introduction of new architectures such as Highway
networks and Residual networks which, for the first time, enabled the training
of feedforward networks with dozens to hundreds of layers using simple gradient
descent. While depth of representation has been posited as a primary reason for
their success, there are indications that these architectures defy a popular
view of deep learning as a hierarchical computation of increasingly abstract
features at each layer.
  In this report, we argue that this view is incomplete and does not adequately
explain several recent findings. We propose an alternative viewpoint based on
unrolled iterative estimation -- a group of successive layers iteratively
refine their estimates of the same features instead of computing an entirely
new representation. We demonstrate that this viewpoint directly leads to the
construction of Highway and Residual networks. Finally we provide preliminary
experiments to discuss the similarities and differences between the two
architectures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1307.7231v1,Competitive MAC under Adversarial SINR,"This paper considers the problem of how to efficiently share a wireless
medium which is subject to harsh external interference or even jamming. While
this problem has already been studied intensively for simplistic single-hop or
unit disk graph models, we make a leap forward and study MAC protocols for the
SINR interference model (a.k.a. the physical model).
  We make two contributions. First, we introduce a new adversarial SINR model
which captures a wide range of interference phenomena. Concretely, we consider
a powerful, adaptive adversary which can jam nodes at arbitrary times and which
is only limited by some energy budget. The second contribution of this paper is
a distributed MAC protocol which provably achieves a constant competitive
throughput in this environment: we show that, with high probability, the
protocol ensures that a constant fraction of the non-blocked time periods is
used for successful transmissions.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.09326v5,"Gesticulator: A framework for semantically-aware speech-driven gesture
  generation","During speech, people spontaneously gesticulate, which plays a key role in
conveying information. Similarly, realistic co-speech gestures are crucial to
enable natural and smooth interactions with social agents. Current end-to-end
co-speech gesture generation systems use a single modality for representing
speech: either audio or text. These systems are therefore confined to producing
either acoustically-linked beat gestures or semantically-linked gesticulation
(e.g., raising a hand when saying ""high""): they cannot appropriately learn to
generate both gesture types. We present a model designed to produce arbitrary
beat and semantic gestures together. Our deep-learning based model takes both
acoustic and semantic representations of speech as input, and generates
gestures as a sequence of joint angle rotations as output. The resulting
gestures can be applied to both virtual agents and humanoid robots. Subjective
and objective evaluations confirm the success of our approach. The code and
video are available at the project page
https://svito-zar.github.io/gesticulator .",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.00246v2,Explainability Case Studies,"Explainability is one of the key ethical concepts in the design of AI
systems. However, attempts to operationalize this concept thus far have tended
to focus on approaches such as new software for model interpretability or
guidelines with checklists. Rarely do existing tools and guidance incentivize
the designers of AI systems to think critically and strategically about the
role of explanations in their systems. We present a set of case studies of a
hypothetical AI-enabled product, which serves as a pedagogical tool to empower
product designers, developers, students, and educators to develop a holistic
explainability strategy for their own products.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0
http://arxiv.org/abs/1305.5946v4,Optimal Groupon Allocations,"Group-buying websites represented by Groupon.com are very popular in
electronic commerce and online shopping nowadays. They have multiple slots to
provide deals with significant discounts to their visitors every day. The
current user traffic allocation mostly relies on human decisions. We study the
problem of automatically allocating the user traffic of a group-buying website
to different deals to maximize the total revenue and refer to it as the
Group-buying Allocation Problem (\GAP). The key challenge of \GAP\ is how to
handle the tipping point (lower bound) and the purchase limit (upper bound) of
each deal. We formulate \GAP\ as a knapsack-like problem with variable-sized
items and majorization constraints. Our main results for \GAP\ can be
summarized as follows. (1) We first show that for a special case of \GAP, in
which the lower bound equals the upper bound for each deal, there is a simple
dynamic programming-based algorithm that can find an optimal allocation in
pseudo-polynomial time. (2) The general case of \GAP\ is much more difficult
than the special case. To solve the problem, we first discover several
structural properties of the optimal allocation, and then design a two-layer
dynamic programming-based algorithm leveraging those properties. This algorithm
can find an optimal allocation in pseudo-polynomial time. (3) We convert the
two-layer dynamic programming based algorithm to a fully polynomial time
approximation scheme (FPTAS), using the technique developed in
\cite{ibarra1975fast}, combined with some careful modifications of the dynamic
programs. Besides these results, we further investigate some natural
generalizations of \GAP, and propose effective algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0203025v1,Sufficiently Fat Polyhedra are not 2-castable,"In this note we consider the problem of manufacturing a convex polyhedral
object via casting. We consider a generalization of the sand casting process
where the object is manufactured by gluing together two identical faces of
parts cast with a single piece mold. In this model we show that the class of
convex polyhedra which can be enclosed between two concentric spheres of the
ratio of their radii less than 1.07 cannot be manufactured using only two cast
parts.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.08291v1,A Manifesto for Web Science @ 10,"Twenty-seven years ago, one of the biggest societal changes in human history
began slowly when the technical foundations for the World Wide Web were defined
by Tim Berners-Lee. Ever since, the Web has grown exponentially, reaching far
beyond its original technical foundations and deeply affecting the world today
- and even more so the society of the future. We have seen that the Web can
influence the realization of human rights and even the pursuit of happiness.
The Web provides an infrastructure to help us to learn, to work, to communicate
with loved ones, and to provide entertainment. However, it also creates an
environment affected by the digital divide between those who have and those who
do not have access. Additionally, the Web provides challenges we must
understand if we are to find a viable balance between data ownership and
privacy protection, between over-whelming surveillance and the prevention of
terrorism. For the Web to succeed, we need to understand its societal
challenges including increased crime, the impact of social platforms and
socio-economic discrimination, and we must work towards fairness, social
inclusion, and open governance.
  Ten Yars ago, the field of Web Science was created to explore the science
underlying the Web from a socio-technical perspective including its
mathematical properties, engineering principles, and social impacts. Ten years
later, we are learning much as the interdisciplinary endeavor to understand the
Web's global information space continues to grow.
  In this article we want to elicit the major lessons we have learned through
Web Science and make some cautious predictions of what to expect next.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1306.4427v1,Multidimensional User Data Model for Web Personalization,"Personalization is being applied to great extend in many systems. This paper
presents a multi-dimensional user data model and its application in web search.
Online and Offline activities of the user are tracked for creating the user
model. The main phases are identification of relevant documents and the
representation of relevance and similarity of the documents. The concepts
Keywords, Topics, URLs and clusters are used in the implementation. The
algorithms for profiling, grading and clustering the concepts in the user model
and algorithm for determining the personalized search results by re-ranking the
results in a search bank are presented in this paper. Simple experiments for
evaluation of the model and their results are described.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.00826v2,WhatTheWikiFact: Fact-Checking Claims Against Wikipedia,"The rise of Internet has made it a major source of information.
Unfortunately, not all information online is true, and thus a number of
fact-checking initiatives have been launched, both manual and automatic, to
deal with the problem. Here, we present our contribution in this regard:
\emph{WhatTheWikiFact}, a system for automatic claim verification using
Wikipedia. The system can predict the veracity of an input claim, and it
further shows the evidence it has retrieved as part of the verification
process. It shows confidence scores and a list of relevant Wikipedia articles,
together with detailed information about each article, including the phrase
used to retrieve it, the most relevant sentences extracted from it and their
stance with respect to the input claim, as well as the associated
probabilities. The system supports several languages: Bulgarian, English, and
Russian.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1408.4978v1,The Empirical Commit Frequency Distribution of Open Source Projects,"A fundamental unit of work in programming is the code contribution (""commit"")
that a developer makes to the code base of the project in work. An author's
commit frequency describes how often that author commits. Knowing the
distribution of all commit frequencies is a fundamental part of understanding
software development processes. This paper presents a detailed quantitative
analysis of commit frequencies in open-source software development. The
analysis is based on a large sample of open source projects, and presents the
overall distribution of commit frequencies. We analyze the data to show the
differences between authors and projects by project size; we also includes a
comparison of successful and non successful projects and we derive an activity
indicator from these analyses. By measuring a fundamental dimension of
programming we help improve software development tools and our understanding of
software development. We also validate some fundamental assumptions about
software development.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.00104v3,"Performance of the low-rank tensor-train SVD (TT-SVD) for large dense
  tensors on modern multi-core CPUs","There are several factorizations of multi-dimensional tensors into
lower-dimensional components, known as `tensor networks'. We consider the
popular `tensor-train' (TT) format and ask: How efficiently can we compute a
low-rank approximation from a full tensor on current multi-core CPUs?
  Compared to sparse and dense linear algebra, kernel libraries for
multi-linear algebra are rare and typically not as well optimized. Linear
algebra libraries like BLAS and LAPACK may provide the required operations in
principle, but often at the cost of additional data movements for rearranging
memory layouts. Furthermore, these libraries are typically optimized for the
compute-bound case (e.g.\ square matrix operations) whereas low-rank tensor
decompositions lead to memory bandwidth limited operations.
  We propose a `tensor-train singular value decomposition' (TT-SVD) algorithm
based on two building blocks: a `Q-less tall-skinny QR' factorization, and a
fused tall-skinny matrix-matrix multiplication and reshape operation. We
analyze the performance of the resulting TT-SVD algorithm using the Roofline
performance model. In addition, we present performance results for different
algorithmic variants for shared-memory as well as distributed-memory
architectures. Our experiments show that commonly used TT-SVD implementations
suffer severe performance penalties. We conclude that a dedicated library for
tensor factorization kernels would benefit the community: Computing a low-rank
approximation can be as cheap as reading the data twice from main memory. As a
consequence, an implementation that achieves realistic performance will move
the limit at which one has to resort to randomized methods that only process
part of the data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1610.09908v1,Joint Large-Scale Motion Estimation and Image Reconstruction,"This article describes the implementation of the joint motion estimation and
image reconstruction framework presented by Burger, Dirks and Sch\""onlieb and
extends this framework to large-scale motion between consecutive image frames.
The variational framework uses displacements between consecutive frames based
on the optical flow approach to improve the image reconstruction quality on the
one hand and the motion estimation quality on the other. The energy functional
consists of a data-fidelity term with a general operator that connects the
input sequence to the solution, it has a total variation term for the image
sequence and is connected to the underlying flow using an optical flow term.
Additional spatial regularity for the flow is modeled by a total variation
regularizer for both components of the flow. The numerical minimization is
performed in an alternating manner using primal-dual techniques. The resulting
schemes are presented as pseudo-code together with a short numerical
evaluation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.08957v1,AI Accelerator Survey and Trends,"Over the past several years, new machine learning accelerators were being
announced and released every month for a variety of applications from speech
recognition, video object detection, assisted driving, and many data center
applications. This paper updates the survey of AI accelerators and processors
from past two years. This paper collects and summarizes the current commercial
accelerators that have been publicly announced with peak performance and power
consumption numbers. The performance and power values are plotted on a scatter
graph, and a number of dimensions and observations from the trends on this plot
are again discussed and analyzed. This year, we also compile a list of
benchmarking performance results and compute the computational efficiency with
respect to peak performance.",0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.05492v1,"BCD: A Cross-Architecture Binary Comparison Database Experiment Using
  Locality Sensitive Hashing Algorithms","Given a binary executable without source code, it is difficult to determine
what each function in the binary does by reverse engineering it, and even
harder without prior experience and context. In this paper, we performed a
comparison of different hashing functions' effectiveness at detecting similar
lifted snippets of LLVM IR code, and present the design and implementation of a
framework for cross-architecture binary code similarity search database using
MinHash as the chosen hashing algorithm, over SimHash, SSDEEP and TLSH. The
motivation is to help reverse engineers to quickly gain context of functions in
an unknown binary by comparing it against a database of known functions. The
code for this project is open source and can be found at
https://github.com/h4sh5/bcddb",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.3633v2,Logic of temporal attribute implications,"We study logic for reasoning with if-then formulas describing dependencies
between attributes of objects which are observed in consecutive points in time.
We introduce semantic entailment of the formulas, show its fixed-point
characterization, investigate closure properties of model classes, present an
axiomatization and prove its completeness, and investigate alternative
axiomatizations and normalized proofs. We investigate decidability and
complexity issues of the logic and prove that the entailment problem is NP-hard
and belongs to EXPSPACE. We show that by restricting to predictive formulas,
the entailment problem is decidable in pseudo-linear time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9906018v1,"Reconstructing Polyatomic Structures from Discrete X-Rays:
  NP-Completeness Proof for Three Atoms","We address a discrete tomography problem that arises in the study of the
atomic structure of crystal lattices. A polyatomic structure T can be defined
as an integer lattice in dimension D>=2, whose points may be occupied by $c$
distinct types of atoms. To ``analyze'' T, we conduct ell measurements that we
call_discrete X-rays_. A discrete X-ray in direction xi determines the number
of atoms of each type on each line parallel to xi. Given ell such non-parallel
X-rays, we wish to reconstruct T.
  The complexity of the problem for c=1 (one atom type) has been completely
determined by Gardner, Gritzmann and Prangenberg, who proved that the problem
is NP-complete for any dimension D>=2 and ell>=3 non-parallel X-rays, and that
it can be solved in polynomial time otherwise.
  The NP-completeness result above clearly extends to any c>=2, and therefore
when studying the polyatomic case we can assume that ell=2. As shown in another
article by the same authors, this problem is also NP-complete for c>=6 atoms,
even for dimension D=2 and axis-parallel X-rays. They conjecture that the
problem remains NP-complete for c=3,4,5, although, as they point out, the proof
idea does not seem to extend to c<=5.
  We resolve the conjecture by proving that the problem is indeed NP-complete
for c>=3 in 2D, even for axis-parallel X-rays. Our construction relies heavily
on some structure results for the realizations of 0-1 matrices with given row
and column sums.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.13418v1,Neuro-Symbolic Hierarchical Rule Induction,"We propose an efficient interpretable neuro-symbolic model to solve Inductive
Logic Programming (ILP) problems. In this model, which is built from a set of
meta-rules organised in a hierarchical structure, first-order rules are
invented by learning embeddings to match facts and body predicates of a
meta-rule. To instantiate it, we specifically design an expressive set of
generic meta-rules, and demonstrate they generate a consequent fragment of Horn
clauses. During training, we inject a controlled \pw{Gumbel} noise to avoid
local optima and employ interpretability-regularization term to further guide
the convergence to interpretable rules. We empirically validate our model on
various tasks (ILP, visual genome, reinforcement learning) against several
state-of-the-art methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1605.00532v1,"Parameterized complexity of the MINCCA problem on graphs of bounded
  decomposability","In an edge-colored graph, the cost incurred at a vertex on a path when two
incident edges with different colors are traversed is called reload or
changeover cost. The ""Minimum Changeover Cost Arborescence"" (MINCCA) problem
consists in finding an arborescence with a given root vertex such that the
total changeover cost of the internal vertices is minimized. It has been
recently proved by G\""oz\""upek et al. [TCS 2016] that the problem is FPT when
parameterized by the treewidth and the maximum degree of the input graph. In
this article we present the following results for the MINCCA problem:
  - the problem is W[1]-hard parameterized by the treedepth of the input graph,
even on graphs of average degree at most 8. In particular, it is W[1]-hard
parameterized by the treewidth of the input graph, which answers the main open
problem of G\""oz\""upek et al. [TCS 2016];
  - it is W[1]-hard on multigraphs parameterized by the tree-cutwidth of the
input multigraph;
  - it is FPT parameterized by the star tree-cutwidth of the input graph, which
is a slightly restricted version of tree-cutwidth. This result strictly
generalizes the FPT result given in G\""oz\""upek et al. [TCS 2016];
  - it remains NP-hard on planar graphs even when restricted to instances with
at most 6 colors and 0/1 symmetric costs, or when restricted to instances with
at most 8 colors, maximum degree bounded by 4, and 0/1 symmetric costs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0806.2274v2,"Exposing Multi-Relational Networks to Single-Relational Network Analysis
  Algorithms","Many, if not most network analysis algorithms have been designed specifically
for single-relational networks; that is, networks in which all edges are of the
same type. For example, edges may either represent ""friendship,"" ""kinship,"" or
""collaboration,"" but not all of them together. In contrast, a multi-relational
network is a network with a heterogeneous set of edge labels which can
represent relationships of various types in a single data structure. While
multi-relational networks are more expressive in terms of the variety of
relationships they can capture, there is a need for a general framework for
transferring the many single-relational network analysis algorithms to the
multi-relational domain. It is not sufficient to execute a single-relational
network analysis algorithm on a multi-relational network by simply ignoring
edge labels. This article presents an algebra for mapping multi-relational
networks to single-relational networks, thereby exposing them to
single-relational network analysis algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1906.04611v1,"New dynamic and verifiable multi-secret sharing schemes based on LFSR
  public key cryptosystem","A verifiable multi-secret sharing (VMSS) scheme enables the dealer to share
multiple secrets, and the deception of both participants and the dealer can be
detected. After analyzing the security of VMSS schemes proposed by Mashhadi and
Dehkordi in 2015, we illustrate that they cannot detect some deception of the
dealer. By using nonhomogeneous linear recursion and LFSR public key
cryptosystem, we introduce two new VMSS schemes. Our schemes can not only
overcome the drawback mentioned above, but also have shorter private/public key
length at the same safety level. Besides, our schemes have dynamism.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1512.08867v1,Modelling and Verifying the AODV Routing Protocol,"This paper presents a formal specification of the Ad hoc On-Demand Distance
Vector (AODV) routing protocol using AWN (Algebra for Wireless Networks), a
recent process algebra which has been tailored for the modelling of Mobile Ad
Hoc Networks and Wireless Mesh Network protocols. Our formalisation models the
exact details of the core functionality of AODV, such as route discovery, route
maintenance and error handling. We demonstrate how AWN can be used to reason
about critical protocol properties by providing detailed proofs of loop freedom
and route correctness.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0402047v1,"Parameter-less Optimization with the Extended Compact Genetic Algorithm
  and Iterated Local Search","This paper presents a parameter-less optimization framework that uses the
extended compact genetic algorithm (ECGA) and iterated local search (ILS), but
is not restricted to these algorithms. The presented optimization algorithm
(ILS+ECGA) comes as an extension of the parameter-less genetic algorithm (GA),
where the parameters of a selecto-recombinative GA are eliminated. The approach
that we propose is tested on several well known problems. In the absence of
domain knowledge, it is shown that ILS+ECGA is a robust and easy-to-use
optimization method.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9311103v1,Set Theory for Verification: I. From Foundations to Functions,"A logic for specification and verification is derived from the axioms of
Zermelo-Fraenkel set theory. The proofs are performed using the proof assistant
Isabelle. Isabelle is generic, supporting several different logics. Isabelle
has the flexibility to adapt to variants of set theory. Its higher-order syntax
supports the definition of new binding operators. Unknowns in subgoals can be
instantiated incrementally. The paper describes the derivation of rules for
descriptions, relations and functions, and discusses interactive proofs of
Cantor's Theorem, the Composition of Homomorphisms challenge [9], and Ramsey's
Theorem [5]. A generic proof assistant can stand up against provers dedicated
to particular logics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.02027v1,Packing: Towards 2x NLP BERT Acceleration,"We find that at sequence length 512 padding tokens represent in excess of 50%
of the Wikipedia dataset used for pretraining BERT (Bidirectional Encoder
Representations from Transformers). Therefore by removing all padding we
achieve a 2x speed-up in terms of sequences/sec. To exploit this characteristic
of the dataset, we develop and contrast two deterministic packing algorithms.
Both algorithms rely on the assumption that sequences are interchangeable and
therefore packing can be performed on the histogram of sequence lengths, rather
than per sample. This transformation of the problem leads to algorithms which
are fast and have linear complexity in dataset size. The shortest-pack-first
histogram-packing (SPFHP) algorithm determines the packing order for the
Wikipedia dataset of over 16M sequences in 0.02 seconds. The non-negative
least-squares histogram-packing (NNLSHP) algorithm converges in 28.4 seconds
but produces solutions which are more depth efficient, managing to get near
optimal packing by combining a maximum of 3 sequences in one sample. Using the
dataset with multiple sequences per sample requires additional masking in the
attention layer and a modification of the MLM loss function. We demonstrate
that both of these changes are straightforward to implement and have relatively
little impact on the achievable performance gain on modern hardware. Finally,
we pretrain BERT-Large using the packed dataset, demonstrating no loss of
convergence and the desired 2x speed-up.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1712.02542v1,Fair mixing: the case of dichotomous preferences,"Agents vote to choose a fair mixture of public outcomes; each agent likes or
dislikes each outcome. We discuss three outstanding voting rules. The
Conditional Utilitarian rule, a variant of the random dictator, is
Strategyproof and guarantees to any group of like-minded agents an influence
proportional to its size. It is easier to compute and more efficient than the
familiar Random Priority rule. Its worst case (resp. average) inefficiency is
provably (resp. in numerical experiments) low if the number of agents is low.
The efficient Egalitarian rule protects similarly individual agents but not
coalitions. It is Excludable Strategyproof: I do not want to lie if I cannot
consume outcomes I claim to dislike. The efficient Nash Max Product rule offers
the strongest welfare guarantees to coalitions, who can force any outcome with
a probability proportional to their size. But it fails even the excludable form
of Strategyproofness.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1910.09266v1,"Multi-Band Multi-Resolution Fully Convolutional Neural Networks for
  Singing Voice Separation","Deep neural networks with convolutional layers usually process the entire
spectrogram of an audio signal with the same time-frequency resolutions, number
of filters, and dimensionality reduction scale. According to the constant-Q
transform, good features can be extracted from audio signals if the low
frequency bands are processed with high frequency resolution filters and the
high frequency bands with high time resolution filters. In the spectrogram of a
mixture of singing voices and music signals, there is usually more information
about the voice in the low frequency bands than the high frequency bands. These
raise the need for processing each part of the spectrogram differently. In this
paper, we propose a multi-band multi-resolution fully convolutional neural
network (MBR-FCN) for singing voice separation. The MBR-FCN processes the
frequency bands that have more information about the target signals with more
filters and smaller dimentionality reduction scale than the bands with less
information. Furthermore, the MBR-FCN processes the low frequency bands with
high frequency resolution filters and the high frequency bands with high time
resolution filters. Our experimental results show that the proposed MBR-FCN
with very few parameters achieves better singing voice separation performance
than other deep neural networks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.05445v2,"Fast Optimization with Zeroth-Order Feedback in Distributed, Multi-User
  MIMO Systems","In this paper, we develop a gradient-free optimization methodology for
efficient resource allocation in Gaussian MIMO multiple access channels. Our
approach combines two main ingredients: (i) an entropic semidefinite
optimization based on matrix exponential learning (MXL); and (ii) a one-shot
gradient estimator which achieves low variance through the reuse of past
information. This novel algorithm, which we call gradient-free MXL algorithm
with callbacks (MXL0$^{+}$), retains the convergence speed of gradient-based
methods while requiring minimal feedback per iteration$-$a single scalar. In
more detail, in a MIMO multiple access channel with $K$ users and $M$ transmit
antennas per user, the MXL0$^{+}$ algorithm achieves $\epsilon$-optimality
within $\text{poly}(K,M)/\epsilon^2$ iterations (on average and with high
probability), even when implemented in a fully distributed, asynchronous
manner. For cross-validation, we also perform a series of numerical experiments
in medium- to large-scale MIMO networks under realistic channel conditions.
Throughout our experiments, the performance of MXL0$^{+}$ matches$-$and
sometimes exceeds$-$that of gradient-based MXL methods, all the while operating
with a vastly reduced communication overhead. In view of these findings, the
MXL0$^{+}$ algorithm appears to be uniquely suited for distributed massive MIMO
systems where gradient calculations can become prohibitively expensive.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0204015v1,Design Patterns for Functional Strategic Programming,"In previous work, we introduced the fundamentals and a supporting combinator
library for \emph{strategic programming}. This an idiom for generic programming
based on the notion of a \emph{functional strategy}: a first-class generic
function that cannot only be applied to terms of any type, but which also
allows generic traversal into subterms and can be customized with type-specific
behaviour.
  This paper seeks to provide practicing functional programmers with pragmatic
guidance in crafting their own strategic programs. We present the fundamentals
and the support from a user's perspective, and we initiate a catalogue of
\emph{strategy design patterns}. These design patterns aim at consolidating
strategic programming expertise in accessible form.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0111043v1,"Prototyping CLP(FD) Tracers: a Trace Model and an Experimental
  Validation Environment","Developing and maintaining CLP programs requires visualization and
explanation tools. However, existing tools are built in an ad hoc way.
Therefore porting tools from one platform to another is very difficult. We have
shown in previous work that, from a fine-grained execution trace, a number of
interesting views about logic program executions could be generated by trace
analysis.
  In this article, we propose a trace model for constraint solving by
narrowing. This trace model is the first one proposed for CLP(FD) and does not
pretend to be the ultimate one. We also propose an instrumented
meta-interpreter in order to experiment with the model. Furthermore, we show
that the proposed trace model contains the necessary information to build known
and useful execution views. This work sets the basis for generic execution
analysis of CLP(FD) programs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1209.6013v3,Abelian Repetitions in Sturmian Words,"We investigate abelian repetitions in Sturmian words. We exploit a bijection
between factors of Sturmian words and subintervals of the unitary segment that
allows us to study the periods of abelian repetitions by using classical
results of elementary Number Theory. We prove that in any Sturmian word the
superior limit of the ratio between the maximal exponent of an abelian
repetition of period $m$ and $m$ is a number $\geq\sqrt{5}$, and the equality
holds for the Fibonacci infinite word. We further prove that the longest prefix
of the Fibonacci infinite word that is an abelian repetition of period $F_j$,
$j>1$, has length $F_j(F_{j+1}+F_{j-1} +1)-2$ if $j$ is even or
$F_j(F_{j+1}+F_{j-1})-2$ if $j$ is odd. This allows us to give an exact formula
for the smallest abelian periods of the Fibonacci finite words. More precisely,
we prove that for $j\geq 3$, the Fibonacci word $f_j$ has abelian period equal
to $F_n$, where $n = \lfloor{j/2}\rfloor$ if $j = 0, 1, 2\mod{4}$, or $n = 1 +
\lfloor{j/2}\rfloor$ if $ j = 3\mod{4}$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1608.04529v2,"A Proposal for the Measurement and Documentation of Research Software
  Sustainability in Interactive Metadata Repositories","This paper proposes an interactive repository type for research software
metadata which measures and documents software sustainability by accumulating
metadata, and computing sustainability metrics over them. Such a repository
would help to overcome technical barriers to software sustainability by
furthering the discovery and identification of sustainable software, thereby
also facilitating documentation of research software within the framework of
software management plans.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/cs/9910011v1,A statistical model for word discovery in child directed speech,"A statistical model for segmentation and word discovery in child directed
speech is presented. An incremental unsupervised learning algorithm to infer
word boundaries based on this model is described and results of empirical tests
showing that the algorithm is competitive with other models that have been used
for similar tasks are also presented.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.03645v1,"Detection of Anomalies and Faults in Industrial IoT Systems by Data
  Mining: Study of CHRIST Osmotron Water Purification System","Industry 4.0 will make manufacturing processes smarter but this smartness
requires more environmental awareness, which in case of Industrial Internet of
Things, is realized by the help of sensors. This article is about industrial
pharmaceutical systems and more specifically, water purification systems.
Purified water which has certain conductivity is an important ingredient in
many pharmaceutical products. Almost every pharmaceutical company has a water
purifying unit as a part of its interdependent systems. Early detection of
faults right at the edge can significantly decrease maintenance costs and
improve safety and output quality, and as a result, lead to the production of
better medicines. In this paper, with the help of a few sensors and data mining
approaches, an anomaly detection system is built for CHRIST Osmotron water
purifier. This is a practical research with real-world data collected from
SinaDarou Labs Co. Data collection was done by using six sensors over two-week
intervals before and after system overhaul. This gave us normal and faulty
operation samples. Given the data, we propose two anomaly detection approaches
to build up our edge fault detection system. The first approach is based on
supervised learning and data mining e.g. by support vector machines. However,
since we cannot collect all possible faults data, an anomaly detection approach
is proposed based on normal system identification which models the system
components by artificial neural networks. Extensive experiments are conducted
with the dataset generated in this study to show the accuracy of the
data-driven and model-based anomaly detection methods.",0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0405040v1,Supervisory Control of Fuzzy Discrete Event Systems,"In order to cope with situations in which a plant's dynamics are not
precisely known, we consider the problem of supervisory control for a class of
discrete event systems modelled by fuzzy automata. The behavior of such
discrete event systems is described by fuzzy languages; the supervisors are
event feedback and can disable only controllable events with any degree. The
concept of discrete event system controllability is thus extended by
incorporating fuzziness. In this new sense, we present a necessary and
sufficient condition for a fuzzy language to be controllable. We also study the
supremal controllable fuzzy sublanguage and the infimal controllable fuzzy
superlanguage when a given pre-specified desired fuzzy language is
uncontrollable. Our framework generalizes that of Ramadge-Wonham and reduces to
Ramadge-Wonham framework when membership grades in all fuzzy languages must be
either 0 or 1. The theoretical development is accompanied by illustrative
numerical examples.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1105.2292v1,Analysis of Power-aware Buffering Schemes in Wireless Sensor Networks,"We study the power-aware buffering problem in battery-powered sensor
networks, focusing on the fixed-size and fixed-interval buffering schemes. The
main motivation is to address the yet poorly understood size variation-induced
effect on power-aware buffering schemes. Our theoretical analysis elucidates
the fundamental differences between the fixed-size and fixed-interval buffering
schemes in the presence of data size variation. It shows that data size
variation has detrimental effects on the power expenditure of the fixed-size
buffering in general, and reveals that the size variation induced effects can
be either mitigated by a positive skewness or promoted by a negative skewness
in size distribution. By contrast, the fixed-interval buffering scheme has an
obvious advantage of being eminently immune to the data-size variation. Hence
the fixed-interval buffering scheme is a risk-averse strategy for its
robustness in a variety of operational environments. In addition, based on the
fixed-interval buffering scheme, we establish the power consumption
relationship between child nodes and parent node in a static data collection
tree, and give an in-depth analysis of the impact of child bandwidth
distribution on parent's power consumption.
  This study is of practical significance: it sheds new light on the
relationship among power consumption of buffering schemes, power parameters of
radio module and memory bank, data arrival rate and data size variation,
thereby providing well-informed guidance in determining an optimal buffer size
(interval) to maximize the operational lifespan of sensor networks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1508.05143v2,A Discrete and Bounded Envy-free Cake Cutting Protocol for Four Agents,"We consider the well-studied cake cutting problem in which the goal is to
identify a fair allocation based on a minimal number of queries from the
agents. The problem has attracted considerable attention within various
branches of computer science, mathematics, and economics. Although, the elegant
Selfridge-Conway envy-free protocol for three agents has been known since 1960,
it has been a major open problem for the last fifty years to obtain a bounded
envy-free protocol for more than three agents. We propose a discrete and
bounded envy-free protocol for four agents.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1001.4251v1,A Decidable Class of Nested Iterated Schemata (extended version),"Many problems can be specified by patterns of propositional formulae
depending on a parameter, e.g. the specification of a circuit usually depends
on the number of bits of its input. We define a logic whose formulae, called
""iterated schemata"", allow to express such patterns. Schemata extend
propositional logic with indexed propositions, e.g. P_i, P_i+1, P_1, and with
generalized connectives, e.g. /\i=1..n or i=1..n (called ""iterations"") where n
is an (unbound) integer variable called a ""parameter"". The expressive power of
iterated schemata is strictly greater than propositional logic: it is even out
of the scope of first-order logic. We define a proof procedure, called DPLL*,
that can prove that a schema is satisfiable for at least one value of its
parameter, in the spirit of the DPLL procedure. However the converse problem,
i.e. proving that a schema is unsatisfiable for every value of the parameter,
is undecidable so DPLL* does not terminate in general. Still, we prove that it
terminates for schemata of a syntactic subclass called ""regularly nested"". This
is the first non trivial class for which DPLL* is proved to terminate.
Furthermore the class of regularly nested schemata is the first decidable class
to allow nesting of iterations, i.e. to allow schemata of the form /\i=1..n
(/\j=1..n ...).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0607009v1,"Almost Periodicity, Finite Automata Mappings and Related Effectiveness
  Issues","The paper studies different variants of almost periodicity notion. We
introduce the class of eventually strongly almost periodic sequences where some
suffix is strongly almost periodic (=uniformly recurrent). The class of almost
periodic sequences includes the class of eventually strongly almost periodic
sequences, and we prove this inclusion to be strict. We prove that the class of
eventually strongly almost periodic sequences is closed under finite automata
mappings and finite transducers. Moreover, an effective form of this result is
presented. Finally we consider some algorithmic questions concerning almost
periodicity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.06009v3,Verification of the Incremental Merkle Tree Algorithm with Dafny,"The Deposit Smart Contract (DSC) is an instrumental component of the Ethereum
2.0 Phase 0 infrastructure. We have developed the first machine-checkable
version of the incremental Merkle tree algorithm used in the DSC. We present
our new and original correctness proof of the algorithm along with the Dafny
machine-checkable version. The main results are: 1) a new proof of total
correctness; 2) a software artefact with the proof in the form of the complete
Dafny code base and 3) new provably correct optimisations of the algorithm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0101010v2,"An Even Faster and More Unifying Algorithm for Comparing Trees via
  Unbalanced Bipartite Matchings","A widely used method for determining the similarity of two labeled trees is
to compute a maximum agreement subtree of the two trees. Previous work on this
similarity measure is only concerned with the comparison of labeled trees of
two special kinds, namely, uniformly labeled trees (i.e., trees with all their
nodes labeled by the same symbol) and evolutionary trees (i.e., leaf-labeled
trees with distinct symbols for distinct leaves). This paper presents an
algorithm for comparing trees that are labeled in an arbitrary manner. In
addition to this generality, this algorithm is faster than the previous
algorithms.
  Another contribution of this paper is on maximum weight bipartite matchings.
We show how to speed up the best known matching algorithms when the input
graphs are node-unbalanced or weight-unbalanced. Based on these enhancements,
we obtain an efficient algorithm for a new matching problem called the
hierarchical bipartite matching problem, which is at the core of our maximum
agreement subtree algorithm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0609112v1,A Richer Understanding of the Complexity of Election Systems,"We provide an overview of some recent progress on the complexity of election
systems. The issues studied include the complexity of the winner, manipulation,
bribery, and control problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.10572v1,"Realism of Simulation Models in Serious Gaming: Two case studies from
  Urban Water Management Higher Education","For games used in educational contexts, realism, i.e., the degree of
congruence between the simulation models used in the games and the real-world
systems represented, is an important characteristic for achieving learning
goals well. However, in the past, the realism of especially entertainment games
has often been identified as insufficient. Thus, this study is investigating
the degree of realism provided by current games. To this purpose, two games in
the domain urban water management, a subdomain of environmental engineering
(EE), are examined. One is ANAWAK, a web-based serious game on water management
and climate change. For ANAWAK, an analysis of the simulation model is
conducted. Second, the simulation model of the entertainment game Cities:
Skylines (CS) is analyzed. In addition, a survey among CS players (N=61) is
conducted. Thereby, different degrees of realism in various EE subdomains are
revealed. All in all, there are still considerable deficits regarding the
degree of realism in the CS simulation model. However, modding as a means of
achieving more realistic simulation models is more widely supported than in the
past.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/cs/0702048v1,Finding Community Structure in Mega-scale Social Networks,"Community analysis algorithm proposed by Clauset, Newman, and Moore (CNM
algorithm) finds community structure in social networks. Unfortunately, CNM
algorithm does not scale well and its use is practically limited to networks
whose sizes are up to 500,000 nodes. The paper identifies that this
inefficiency is caused from merging communities in unbalanced manner. The paper
introduces three kinds of metrics (consolidation ratio) to control the process
of community analysis trying to balance the sizes of the communities being
merged. Three flavors of CNM algorithms are built incorporating those metrics.
The proposed techniques are tested using data sets obtained from existing
social networking service that hosts 5.5 million users. All the methods exhibit
dramatic improvement of execution efficiency in comparison with the original
CNM algorithm and shows high scalability. The fastest method processes a
network with 1 million nodes in 5 minutes and a network with 4 million nodes in
35 minutes, respectively. Another one processes a network with 500,000 nodes in
50 minutes (7 times faster than the original algorithm), finds community
structures that has improved modularity, and scales to a network with 5.5
million.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.08406v2,A SAT attack on higher dimensional Erds--Szekeres numbers,"A famous result by Erd\H{o}s and Szekeres (1935) asserts that, for all $k,d
\in \mathbb{N}$, there is a smallest integer $n = g^{(d)}(k)$ such that every
set of at least $n$ points in $\mathbb{R}^d$ in general position contains a
$k$-gon, that is, a subset of $k$ points which is in convex position. In this
article, we present a SAT model based on acyclic chirotopes (oriented matroids)
to investigate Erd\H{o}s--Szekeres numbers in small dimensions. To solve the
SAT instances we use modern SAT solvers and all our unsatisfiability results
are verified using DRAT certificates. We show $g^{(3)}(7) = 13$, $g^{(4)}(8)
\le 13$, and $g^{(5)}(9) \le 13$, which are the first improvements for decades.
For the setting of $k$-holes (i.e., $k$-gons with no other points in the convex
hull), where $h^{(d)}(k)$ denotes the minimum number $n$ such that every set of
at least $n$ points in $\mathbb{R}^d$ in general position contains a $k$-hole,
we show $h^{(3)}(7) \le 14$, $h^{(4)}(8) \le 13$, and $h^{(5)}(9) \le 13$.
Moreover, all obtained bounds are sharp in the setting of acyclic chirotopes
and we conjecture them to be sharp also in the original setting of point sets.
As a byproduct, we verify previously known bounds. In particular, we present
the first computer-assisted proof of the upper bound $h^{(2)}(6)\le g^{(2)}(9)
\le 1717$ by Gerken (2008).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0210024v1,The Lazy Bureaucrat Scheduling Problem,"We introduce a new class of scheduling problems in which the optimization is
performed by the worker (single ``machine'') who performs the tasks. A typical
worker's objective is to minimize the amount of work he does (he is ``lazy''),
or more generally, to schedule as inefficiently (in some sense) as possible.
The worker is subject to the constraint that he must be busy when there is work
that he can do; we make this notion precise both in the preemptive and
nonpreemptive settings. The resulting class of ``perverse'' scheduling
problems, which we denote ``Lazy Bureaucrat Problems,'' gives rise to a rich
set of new questions that explore the distinction between maximization and
minimization in computing optimal schedules.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1504.04517v1,Speeding up Glauber Dynamics for Random Generation of Independent Sets,"The maximum independent set (MIS) problem is a well-studied combinatorial
optimization problem that naturally arises in many applications, such as
wireless communication, information theory and statistical mechanics.
  MIS problem is NP-hard, thus many results in the literature focus on fast
generation of maximal independent sets of high cardinality. One possibility is
to combine Gibbs sampling with coupling from the past arguments to detect
convergence to the stationary regime. This results in a sampling procedure with
time complexity that depends on the mixing time of the Glauber dynamics Markov
chain.
  We propose an adaptive method for random event generation in the Glauber
dynamics that considers only the events that are effective in the coupling from
the past scheme, accelerating the convergence time of the Gibbs sampling
algorithm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.11760v1,Automaton of molecular perceptions in biochemical reactions,"Local interactions among biomolecules, and the role played by their
environment, have gained increasing attention in modelling biochemical
reactions. By defining the automaton of molecular perceptions, we explore an
agent-based representation of the behaviour of biomolecules in living cells.
Our approach considers the capability of a molecule to perceive its
surroundings a key property of bimolecular interactions, which we investigate
from a theoretical perspective. Graph-based reaction systems are then leveraged
to abstract enzyme regulation as a result of the influence exerted by the
environment on a catalysed reaction. By combining these methods, we aim at
overcoming some limitations of current kinetic models, which do not take into
account local molecular interactions and the way they are affected by the
reaction environment.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0401007v1,Design of a Community-based Translation Center,"Interfaces that support multi-lingual content can reach a broader community.
We wish to extend the reach of CITIDEL, a digital library for computing
education materials, to support multiple languages. By doing so, we hope that
it will increase the number of users, and in turn the number of resources. This
paper discusses three approaches to translation (automated translation,
developer-based, and community-based), and a brief evaluation of these
approaches. It proposes a design for an online community translation center
where volunteers help translate interface components and educational materials
available in CITIDEL.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1402.0197v1,Measuring the Complexity of Self-organizing Traffic Lights,"We apply measures of complexity, emergence and self-organization to an
abstract city traffic model for comparing a traditional traffic coordination
method with a self-organizing method in two scenarios: cyclic boundaries and
non-orientable boundaries. We show that the measures are useful to identify and
characterize different dynamical phases. It becomes clear that different
operation regimes are required for different traffic demands. Thus, not only
traffic is a non-stationary problem, which requires controllers to adapt
constantly. Controllers must also change drastically the complexity of their
behavior depending on the demand. Based on our measures, we can say that the
self-organizing method achieves an adaptability level comparable to a living
system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1901.10453v2,Simulating the DNA String Graph in Succinct Space,"Converting a set of sequencing reads into a lossless compact data structure
that encodes all the relevant biological information is a major challenge. The
classical approaches are to build the string graph or the de Bruijn graph. Each
has advantages over the other depending on the application. Still, the ideal
setting would be to have an index of the reads that is easy to build and can be
adapted to any type of biological analysis. In this paper, we propose a new
data structure we call rBOSS, which gets close to that ideal. Our rBOSS is a de
Bruijn graph in practice, but it simulates any length up to k and can compute
overlaps of size at least m between the labels of the nodes, with k and m being
parameters. If we choose the parameter k equal to the size of the reads, then
we can simulate a complete string graph. As most BWT-based structures, rBOSS is
unidirectional, but it exploits the property of the DNA reverse complements to
simulate bi-directionality with some time-space trade-offs. We implemented a
genome assembler on top of rBOSS to demonstrate its usefulness. Our
experimental results show that using k = 100, rBOSS can assemble 185 MB of
reads in less than 15 minutes and using 110 MB in total. It produces contigs of
mean sizes over 10,000, which is twice the size obtained by using a pure de
Bruijn graph of fixed length k.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1906.11941v2,Learning Policies through Quantile Regression,"Policy gradient based reinforcement learning algorithms coupled with neural
networks have shown success in learning complex policies in the model free
continuous action space control setting. However, explicitly parameterized
policies are limited by the scope of the chosen parametric probability
distribution. We show that alternatively to the likelihood based policy
gradient, a related objective can be optimized through advantage weighted
quantile regression. Our approach models the policy implicitly in the network,
which gives the agent the freedom to approximate any distribution in each
action dimension, not limiting its capabilities to the commonly used unimodal
Gaussian parameterization. This broader spectrum of policies makes our
algorithm suitable for problems where Gaussian policies cannot fit the optimal
policy. Moreover, our results on the MuJoCo physics simulator benchmarks are
comparable or superior to state-of-the-art on-policy methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1105.0322v4,"A Computational Model for the Direct Execution of General Specifications
  with Multi-way Constraints","In this paper, we propose a computational model for the direct execution of
general specifications with multi-way constraints. Although this computational
model has a similar structure to existing constraint programming models, it is
not meant for solving constraint satisfaction problems but rather for the
simulation of social systems and to continue to execute assigned processes.
Because of this similar structure, it is applicable to the spectrum of the
constraint solver, which is purple in this model. Essentially, it is a
technology that can speed up the construction of large-scale network systems.
This model can be efficiently executed to directly describe design content in a
simple way.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1005.2277v1,"A Simple Computational Model for Acceptance/Rejection of Binary Sequence
  Generators","A simple binary model to compute the degree of balancedness in the output
sequence of LFSR-combinational generators has been developed. The computational
method is based exclusively on the handling of binary strings by means of logic
operations. The proposed model can serve as a deterministic alternative to
existing probabilistic methods for checking balancedness in binary sequence
generators. The procedure here described can be devised as a first selective
criterium for acceptance/rejection of this type of generators.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.00229v4,"Frequency Fitness Assignment: Optimization without Bias for Good
  Solutions can be Efficient","A fitness assignment process transforms the features (such as the objective
value) of a candidate solution to a scalar fitness, which then is the basis for
selection. Under Frequency Fitness Assignment (FFA), the fitness corresponding
to an objective value is its encounter frequency in selection steps and is
subject to minimization. FFA creates algorithms that are not biased towards
better solutions and are invariant under all injective transformations of the
objective function value. We investigate the impact of FFA on the performance
of two theory-inspired, state-of-the-art EAs, the Greedy (2+1) GA and the
Self-Adjusting (1+(lambda,lambda)) GA. FFA improves their performance
significantly on some problems that are hard for them. In our experiments, one
FFA-based algorithm exhibited mean runtimes that appear to be polynomial on the
theory-based benchmark problems in our study, including traps, jumps, and
plateaus. We propose two hybrid approaches that use both direct and FFA-based
optimization and find that they perform well. All FFA-based algorithms also
perform better on satisfiability problems than any of the pure algorithm
variants.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0912.2174v1,Renewal theory in analysis of tries and strings,"We give a survey of a number of simple applications of renewal theory to
problems on random strings and tries: insertion depth, size, insertion mode and
imbalance of tries; variations for b-tries and Patricia tries; Khodak and
Tunstall codes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.02861v1,Learning the Markov order of paths in a network,"We study the problem of learning the Markov order in categorical sequences
that represent paths in a network, i.e. sequences of variable lengths where
transitions between states are constrained to a known graph. Such data pose
challenges for standard Markov order detection methods and demand modelling
techniques that explicitly account for the graph constraint. Adopting a
multi-order modelling framework for paths, we develop a Bayesian learning
technique that (i) more reliably detects the correct Markov order compared to a
competing method based on the likelihood ratio test, (ii) requires considerably
less data compared to methods using AIC or BIC, and (iii) is robust against
partial knowledge of the underlying constraints. We further show that a
recently published method that uses a likelihood ratio test has a tendency to
overfit the true Markov order of paths, which is not the case for our Bayesian
technique. Our method is important for data scientists analyzing patterns in
categorical sequence data that are subject to (partially) known constraints,
e.g. sequences with forbidden words, mobility trajectories and click stream
data, or sequence data in bioinformatics. Addressing the key challenge of model
selection, our work is further relevant for the growing body of research that
emphasizes the need for higher-order models in network analysis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.09367v1,On the Computational Complexities of Various Geography Variants,"Generalized Geography is a combinatorial game played on a directed graph.
Players take turns moving a token from vertex to vertex, deleting a vertex
after moving the token away from it. A player unable to move loses. It is well
known that the computational complexity of determining which player should win
from a given position of Generalized Geography is PSPACE-complete. We introduce
several rule variants to Generalized Geography, and we explore the
computational complexity of determining the winner of positions of many
resulting games. Among our results is a proof that determining the winner of a
game known in the literature as Undirected Partizan Geography is
PSPACE-complete, even when restricted to being played on a bipartite graph.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1105.3324v1,Hierarchies in Dependence Logic,"We study fragments of dependence logic defined either by restricting the
number k of universal quantifiers or the width of dependence atoms in formulas.
We find the sublogics of existential second-order logic corresponding to these
fragments of dependence logic. We also show that these both ways of defining
fragments of dependence logic give rise to a hierarchy in expressive power with
respect to k.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0806.0103v2,A note on clique-width and tree-width for structures,"We give a simple proof that the straightforward generalisation of
clique-width to arbitrary structures can be unbounded on structures of bounded
tree-width. This can be corrected by allowing fusion of elements.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.09540v2,StressNet: Detecting Stress in Thermal Videos,"Precise measurement of physiological signals is critical for the effective
monitoring of human vital signs. Recent developments in computer vision have
demonstrated that signals such as pulse rate and respiration rate can be
extracted from digital video of humans, increasing the possibility of
contact-less monitoring. This paper presents a novel approach to obtaining
physiological signals and classifying stress states from thermal video. The
proposed network--""StressNet""--features a hybrid emission representation model
that models the direct emission and absorption of heat by the skin and
underlying blood vessels. This results in an information-rich feature
representation of the face, which is used by spatio-temporal network for
reconstructing the ISTI ( Initial Systolic Time Interval: a measure of change
in cardiac sympathetic activity that is considered to be a quantitative index
of stress in humans ). The reconstructed ISTI signal is fed into a
stress-detection model to detect and classify the individual's stress state (
i.e. stress or no stress ). A detailed evaluation demonstrates that StressNet
achieves estimated the ISTI signal with 95% accuracy and detect stress with
average precision of 0.842. The source code is available on Github.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.09710v1,A Block-Sensitivity Lower Bound for Quantum Testing Hamming Distance,"The Gap-Hamming distance problem is the promise problem of deciding if the
Hamming distance $h$ between two strings of length $n$ is greater than $a$ or
less than $b$, where the gap $g=|a-b|\geq 1$ and $a$ and $b$ could depend on
$n$. In this short note, we give a lower bound of $\Omega( \sqrt{n/g})$ on the
quantum query complexity of computing the Gap-Hamming distance between two
given strings of lenght $n$. The proof is a combinatorial argument based on
block sensitivity and a reduction from a threshold function.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0903.3311v3,Cartesian effect categories are Freyd-categories,"Most often, in a categorical semantics for a programming language, the
substitution of terms is expressed by composition and finite products. However
this does not deal with the order of evaluation of arguments, which may have
major consequences when there are side-effects. In this paper Cartesian effect
categories are introduced for solving this issue, and they are compared with
strong monads, Freyd-categories and Haskell's Arrows. It is proved that a
Cartesian effect category is a Freyd-category where the premonoidal structure
is provided by a kind of binary product, called the sequential product. The
universal property of the sequential product provides Cartesian effect
categories with a powerful tool for constructions and proofs. To our knowledge,
both effect categories and sequential products are new notions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0305023v1,Fast Dempster-Shafer clustering using a neural network structure,"In this paper we study a problem within Dempster-Shafer theory where 2**n - 1
pieces of evidence are clustered by a neural structure into n clusters. The
clustering is done by minimizing a metaconflict function. Previously we
developed a method based on iterative optimization. However, for large scale
problems we need a method with lower computational complexity. The neural
structure was found to be effective and much faster than iterative optimization
for larger problems. While the growth in metaconflict was faster for the neural
structure compared with iterative optimization in medium sized problems, the
metaconflict per cluster and evidence was moderate. The neural structure was
able to find a global minimum over ten runs for problem sizes up to six
clusters.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1811.03966v2,"A Complexity Dichotomy for Critical Values of the b-Chromatic Number of
  Graphs","A $b$-coloring of a graph $G$ is a proper coloring of its vertices such that
each color class contains a vertex that has at least one neighbor in all the
other color classes. The b-Coloring problem asks whether a graph $G$ has a
$b$-coloring with $k$ colors. The $b$-chromatic number of a graph $G$, denoted
by $\chi_b(G)$, is the maximum number $k$ such that $G$ admits a $b$-coloring
with $k$ colors. We consider the complexity of the b-Coloring problem, whenever
the value of $k$ is close to one of two upper bounds on $\chi_b(G)$: The
maximum degree $\Delta(G)$ plus one, and the $m$-degree, denoted by $m(G)$,
which is defined as the maximum number $i$ such that $G$ has $i$ vertices of
degree at least $i-1$. We obtain a dichotomy result stating that for fixed $k
\in \{\Delta(G) + 1 - p, m(G) - p\}$, the problem is polynomial-time solvable
whenever $p \in \{0, 1\}$ and, even when $k = 3$, it is NP-complete whenever $p
\ge 2$. We furthermore consider parameterizations of the b-Coloring problem
that involve the maximum degree $\Delta(G)$ of the input graph $G$ and give two
FPT-algorithms. First, we show that deciding whether a graph $G$ has a
$b$-coloring with $m(G)$ colors is FPT parameterized by $\Delta(G)$. Second, we
show that b-Coloring is FPT parameterized by $\Delta(G) + \ell_k(G)$, where
$\ell_k(G)$ denotes the number of vertices of degree at least $k$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.07996v1,"Mining Large Quasi-cliques with Quality Guarantees from Vertex
  Neighborhoods","Mining dense subgraphs is an important primitive across a spectrum of
graph-mining tasks. In this work, we formally establish that two recurring
characteristics of real-world graphs, namely heavy-tailed degree distributions
and large clustering coefficients, imply the existence of substantially large
vertex neighborhoods with high edge-density. This observation suggests a very
simple approach for extracting large quasi-cliques: simply scan the vertex
neighborhoods, compute the clustering coefficient of each vertex, and output
the best such subgraph. The implementation of such a method requires counting
the triangles in a graph, which is a well-studied problem in graph mining. When
empirically tested across a number of real-world graphs, this approach reveals
a surprise: vertex neighborhoods include maximal cliques of non-trivial sizes,
and the density of the best neighborhood often compares favorably to subgraphs
produced by dedicated algorithms for maximizing subgraph density. For graphs
with small clustering coefficients, we demonstrate that small vertex
neighborhoods can be refined using a local-search method to ``grow'' larger
cliques and near-cliques. Our results indicate that contrary to worst-case
theoretical results, mining cliques and quasi-cliques of non-trivial sizes from
real-world graphs is often not a difficult problem, and provides motivation for
further work geared towards a better explanation of these empirical successes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.04111v2,Critical Graphs for Minimum Vertex Cover,"In the context of the chromatic-number problem, a critical graph is an
instance where the deletion of any element would decrease the graph's chromatic
number. Such instances have shown to be interesting objects of study for deepen
the understanding of the optimization problem.
  This work introduces critical graphs in context of Minimum Vertex Cover. We
demonstrate their potential for the generation of larger graphs with hidden a
priori known solutions. Firstly, we propose a parametrized graph-generation
process which preserves the knowledge of the minimum cover. Secondly, we
conduct a systematic search for small critical graphs. Thirdly, we illustrate
the applicability for benchmarking purposes by reporting on a series of
experiments using the state-of-the-art heuristic solver NuMVC.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.03458v1,"Horus: Using Sensor Fusion to Combine Infrastructure and On-board
  Sensing to Improve Autonomous Vehicle Safety","Studies predict that demand for autonomous vehicles will increase tenfold
between 2019 and 2026. However, recent high-profile accidents have
significantly impacted consumer confidence in this technology. The cause for
many of these accidents can be traced back to the inability of these vehicles
to correctly sense the impending danger. In response, manufacturers have been
improving the already extensive on-vehicle sensor packages to ensure that the
system always has access to the data necessary to ensure safe navigation.
However, these sensor packages only provide a view from the vehicle's
perspective and, as a result, autonomous vehicles still require frequent human
intervention to ensure safety.
  To address this issue, I developed a system, called Horus, that combines
on-vehicle and infrastructure-based sensors to provide a more complete view of
the environment, including areas not visible from the vehicle. I built a
small-scale experimental testbed as a proof of concept. My measurements of the
impact of sensor failures showed that even short outages (1 second) at slow
speeds (25 km/hr scaled velocity) prevents vehicles that rely on on-vehicle
sensors from navigating properly. My experiments also showed that Horus
dramatically improves driving safety and that the sensor fusion algorithm
selected plays a significant role in the quality of the navigation. With just a
pair of infrastructure sensors, Horus could tolerate sensors that fail 40% of
the time and still navigate safely. These results are a promising first step
towards safer autonomous vehicles.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.15606v1,"Multimodal Approaches for Indoor Localization for Ambient Assisted
  Living in Smart Homes","This work makes multiple scientific contributions to the field of Indoor
Localization for Ambient Assisted Living in Smart Homes. First, it presents a
Big-Data driven methodology that studies the multimodal components of user
interactions and analyzes the data from Bluetooth Low Energy (BLE) beacons and
BLE scanners to detect a user's indoor location in a specific activity-based
zone during Activities of Daily Living. Second, it introduces a context
independent approach that can interpret the accelerometer and gyroscope data
from diverse behavioral patterns to detect the zone-based indoor location of a
user in any Internet of Things (IoT)-based environment. These two approaches
achieved performance accuracies of 81.36% and 81.13%, respectively, when tested
on a dataset. Third, it presents a methodology to detect the spatial
coordinates of a user's indoor position that outperforms all similar works in
this field, as per the associated root mean squared error - one of the
performance evaluation metrics in ISO/IEC18305:2016- an international standard
for testing Localization and Tracking Systems. Finally, it presents a
comprehensive comparative study that includes Random Forest, Artificial Neural
Network, Decision Tree, Support Vector Machine, k-NN, Gradient Boosted Trees,
Deep Learning, and Linear Regression, to address the challenge of identifying
the optimal machine learning approach for Indoor Localization.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1001.5019v2,Lower Bounds for the Complexity of Monadic Second-Order Logic,"Courcelle's famous theorem from 1990 states that any property of graphs
definable in monadic second-order logic (MSO) can be decided in linear time on
any class of graphs of bounded treewidth, or in other words, MSO is
fixed-parameter tractable in linear time on any such class of graphs. From a
logical perspective, Courcelle's theorem establishes a sufficient condition, or
an upper bound, for tractability of MSO-model checking.
  Whereas such upper bounds on the complexity of logics have received
significant attention in the literature, almost nothing is known about
corresponding lower bounds. In this paper we establish a strong lower bound for
the complexity of monadic second-order logic. In particular, we show that if C
is any class of graphs which is closed under taking subgraphs and whose
treewidth is not bounded by a polylogarithmic function (in fact, $\log^c n$ for
some small c suffices) then MSO-model checking is intractable on C (under a
suitable assumption from complexity theory).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1501.07800v4,"Locality-aware parallel block-sparse matrix-matrix multiplication using
  the Chunks and Tasks programming model","We present a method for parallel block-sparse matrix-matrix multiplication on
distributed memory clusters. By using a quadtree matrix representation, data
locality is exploited without prior information about the matrix sparsity
pattern. A distributed quadtree matrix representation is straightforward to
implement due to our recent development of the Chunks and Tasks programming
model [Parallel Comput. 40, 328 (2014)]. The quadtree representation combined
with the Chunks and Tasks model leads to favorable weak and strong scaling of
the communication cost with the number of processes, as shown both
theoretically and in numerical experiments.
  Matrices are represented by sparse quadtrees of chunk objects. The leaves in
the hierarchy are block-sparse submatrices. Sparsity is dynamically detected by
the matrix library and may occur at any level in the hierarchy and/or within
the submatrix leaves. In case graphics processing units (GPUs) are available,
both CPUs and GPUs are used for leaf-level multiplication work, thus making use
of the full computing capacity of each node.
  The performance is evaluated for matrices with different sparsity structures,
including examples from electronic structure calculations. Compared to methods
that do not exploit data locality, our locality-aware approach reduces
communication significantly, achieving essentially constant communication per
node in weak scaling tests.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.11501v2,Extensional Higher-Order Paramodulation in Leo-III,"Leo-III is an automated theorem prover for extensional type theory with
Henkin semantics and choice. Reasoning with primitive equality is enabled by
adapting paramodulation-based proof search to higher-order logic. The prover
may cooperate with multiple external specialist reasoning systems such as
first-order provers and SMT solvers. Leo-III is compatible with the TPTP/TSTP
framework for input formats, reporting results and proofs, and standardized
communication between reasoning systems, enabling e.g. proof reconstruction
from within proof assistants such as Isabelle/HOL. Leo-III supports reasoning
in polymorphic first-order and higher-order logic, in all normal quantified
modal logics, as well as in different deontic logics. Its development had
initiated the ongoing extension of the TPTP infrastructure to reasoning within
non-classical logics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1710.03148v3,The complexity of general-valued CSPs seen from the other side,"The constraint satisfaction problem (CSP) is concerned with homomorphisms
between two structures. For CSPs with restricted left-hand side structures, the
results of Dalmau, Kolaitis, and Vardi [CP'02], Grohe [FOCS'03/JACM'07], and
Atserias, Bulatov, and Dalmau [ICALP'07] establish the precise borderline of
polynomial-time solvability (subject to complexity-theoretic assumptions) and
of solvability by bounded-consistency algorithms (unconditionally) as bounded
treewidth modulo homomorphic equivalence.
  The general-valued constraint satisfaction problem (VCSP) is a generalisation
of the CSP concerned with homomorphisms between two valued structures. For
VCSPs with restricted left-hand side valued structures, we establish the
precise borderline of polynomial-time solvability (subject to
complexity-theoretic assumptions) and of solvability by the $k$-th level of the
Sherali-Adams LP hierarchy (unconditionally). We also obtain results on related
problems concerned with finding a solution and recognising the tractable cases;
the latter has an application in database theory.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1402.6500v2,"Social Bootstrapping: How Pinterest and Last.fm Social Communities
  Benefit by Borrowing Links from Facebook","How does one develop a new online community that is highly engaging to each
user and promotes social interaction? A number of websites offer friend-finding
features that help users bootstrap social networks on the website by copying
links from an established network like Facebook or Twitter. This paper
quantifies the extent to which such social bootstrapping is effective in
enhancing a social experience of the website. First, we develop a stylised
analytical model that suggests that copying tends to produce a giant connected
component (i.e., a connected community) quickly and preserves properties such
as reciprocity and clustering, up to a linear multiplicative factor. Second, we
use data from two websites, Pinterest and Last.fm, to empirically compare the
subgraph of links copied from Facebook to links created natively. We find that
the copied subgraph has a giant component, higher reciprocity and clustering,
and confirm that the copied connections see higher social interactions.
However, the need for copying diminishes as users become more active and
influential. Such users tend to create links natively on the website, to users
who are more similar to them than their Facebook friends. Our findings give new
insights into understanding how bootstrapping from established social networks
can help engage new users by enhancing social interactivity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.05133v2,Contestable Black Boxes,"The right to contest a decision with consequences on individuals or the
society is a well-established democratic right. Despite this right also being
explicitly included in GDPR in reference to automated decision-making, its
study seems to have received much less attention in the AI literature compared,
for example, to the right for explanation. This paper investigates the type of
assurances that are needed in the contesting process when algorithmic
black-boxes are involved, opening new questions about the interplay of
contestability and explainability. We argue that specialised complementary
methodologies to evaluate automated decision-making in the case of a particular
decision being contested need to be developed. Further, we propose a
combination of well-established software engineering and rule-based approaches
as a possible socio-technical solution to the issue of contestability, one of
the new democratic challenges posed by the automation of decision making.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1809.05352v1,"Put a Ring on It: Text Entry Performance on a Grip Ring Attached
  Smartphone","This paper presents results of a study investing effects of grip rings on
text entry. Results revealed that grip rings do not affect text entry
performance in terms of speed, accuracy, or keystrokes per character. It then
reflects on future research directions based on the results and observations
from the study. The purpose of this work is to stress the necessity of
classifying and evaluating low-cost mobile phone accessories.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1901.07064v1,Predictive Indexing,"There has been considerable research on automated index tuning in database
management systems (DBMSs). But the majority of these solutions tune the index
configuration by retrospectively making computationally expensive physical
design changes all at once. Such changes degrade the DBMS's performance during
the process, and have reduced utility during subsequent query processing due to
the delay between a workload shift and the associated change. A better approach
is to generate small changes that tune the physical design over time, forecast
the utility of these changes, and apply them ahead of time to maximize their
impact.
  This paper presents predictive indexing that continuously improves a
database's physical design using lightweight physical design changes. It uses a
machine learning model to forecast the utility of these changes, and
continuously refines the index configuration of the database to handle evolving
workloads. We introduce a lightweight hybrid scan operator with which a DBMS
can make use of partially-built indexes for query processing. Our evaluation
shows that predictive indexing improves the throughput of a DBMS by 3.5--5.2x
compared to other state-of-the-art indexing approaches. We demonstrate that
predictive indexing works seamlessly with other lightweight automated physical
design tuning methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1806.10925v1,TheoryGuru: A Mathematica Package to apply Quantifier Elimination,"We consider the use of Quantifier Elimination (QE) technology for automated
reasoning in economics. There is a great body of work considering QE
applications in science and engineering but we demonstrate here that it also
has use in the social sciences. We explain how many suggested theorems in
economics could either be proven, or even have their hypotheses shown to be
inconsistent, automatically via QE.
  However, economists who this technology could benefit are usually unfamiliar
with QE, and the use of mathematical software generally. This motivated the
development of a Mathematica Package TheoryGuru, whose purpose is to lower the
costs of applying QE to economics. We describe the package's functionality and
give examples of its use.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.11416v1,COVID-19 Contact Tracing: Eight Privacy Questions Explored,"We respond to a recent short paper by de Motjoye et el. on privacy issues
with Covid-19 tracking. Their paper, which we discuss here, is structured
around three ""toy protocols"" for the design of an app which can maximise the
utility of contact tracing information while minimising the more general risk
to privacy. On this basis, the paper proceeds to introduce eight questions
against which they should be assessed. The questions raised and the protocols
proposed effectively amount to the creation of a game with different categories
of players able to make different moves. It is therefore possible to analyse
the model in terms of optimal game design.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/cs/0106030v1,"Logic, Individuals and Concepts","This extended abstract gives a brief outline of the connections between the
descriptions and variable concepts. Thus, the notion of a concept is extended
to include both the syntax and semantics features. The evaluation map in use is
parameterized by a kind of computational environment, the index, giving rise to
indexed concepts. The concepts are inhabited into language by the descriptions
from the higher order logic. In general the idea of object-as-functor should
assist the designer to outline a programming tool in conceptual shell style.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1706.06368v3,FA*IR: A Fair Top-k Ranking Algorithm,"In this work, we define and solve the Fair Top-k Ranking problem, in which we
want to determine a subset of k candidates from a large pool of n >> k
candidates, maximizing utility (i.e., select the ""best"" candidates) subject to
group fairness criteria. Our ranked group fairness definition extends group
fairness using the standard notion of protected groups and is based on ensuring
that the proportion of protected candidates in every prefix of the top-k
ranking remains statistically above or indistinguishable from a given minimum.
  Utility is operationalized in two ways: (i) every candidate included in the
top-$k$ should be more qualified than every candidate not included; and (ii)
for every pair of candidates in the top-k, the more qualified candidate should
be ranked above. An efficient algorithm is presented for producing the Fair
Top-k Ranking, and tested experimentally on existing datasets as well as new
datasets released with this paper, showing that our approach yields small
distortions with respect to rankings that maximize utility without considering
fairness criteria.
  To the best of our knowledge, this is the first algorithm grounded in
statistical tests that can mitigate biases in the representation of an
under-represented group along a ranked list.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.12546v1,"A light-weight method to foster the (Grad)CAM interpretability and
  explainability of classification networks","We consider a light-weight method which allows to improve the explainability
of localized classification networks. The method considers (Grad)CAM maps
during the training process by modification of the training loss and does not
require additional structural elements. It is demonstrated that the (Grad)CAM
interpretability, as measured by several indicators, can be improved in this
way. Since the method shall be applicable on embedded systems and on standard
deeper architectures, it essentially takes advantage of second order
derivatives during the training and does not require additional model layers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0701082v1,Recurrence with affine level mappings is P-time decidable for CLP(R),"In this paper we introduce a class of constraint logic programs such that
their termination can be proved by using affine level mappings. We show that
membership to this class is decidable in polynomial time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1001.1686v2,Combinatorial Auctions with Budgets,"We consider budget constrained combinatorial auctions where bidder $i$ has a
private value $v_i$, a budget $b_i$, and is interested in all the items in
$S_i$. The value to agent $i$ of a set of items $R$ is $|R \cap S_i| \cdot
v_i$. Such auctions capture adword auctions, where advertisers offer a bid for
ads in response to an advertiser-dependent set of adwords, and advertisers have
budgets. It is known that even of all items are identical and all budgets are
public it is not possible to be truthful and efficient. Our main result is a
novel auction that runs in polynomial time, is incentive compatible, and
ensures Pareto-optimality for such auctions when the valuations are private and
the budgets are public knowledge. This extends the result of Dobzinski et al.
(FOCS 2008) for auctions of multiple {\sl identical} items and public budgets
to single-valued {\sl combinatorial} auctions with public budgets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0908.3652v1,"Optimal Geometric Partitions, Covers and K-Centers","In this paper we present some new, practical, geometric optimization
techniques for computing polygon partitions, 1D and 2D point, interval, square
and rectangle covers, as well as 1D and 2D interval and rectangle K-centers.
All the techniques we present have immediate applications to several cost
optimization and facility location problems which are quite common in practice.
The main technique employed is dynamic programming, but we also make use of
efficient data structures and fast greedy algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1701.00117v1,"An initial performance review of software components for a heterogeneous
  computing platform","The design of embedded systems is a complex activity that involves a lot of
decisions. With high performance demands of present day usage scenarios and
software, they often involve energy hungry state-of-the-art computing units.
While focusing on power consumption of computing units, the physical properties
of software are often ignored. Recently, there has been a growing interest to
quantify and model the physical footprint of software (e.g. consumed power,
generated heat, execution time, etc.), and a component based approach
facilitates methods for describing such properties. Based on these, software
architects can make energy-efficient software design solutions. This paper
presents power consumption and execution time profiling of a component software
that can be allocated on heterogeneous computing units (CPU, GPU, FPGA) of a
tracked robot.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.05940v1,TransMed: Transformers Advance Multi-modal Medical Image Classification,"Over the past decade, convolutional neural networks (CNN) have shown very
competitive performance in medical image analysis tasks, such as disease
classification, tumor segmentation, and lesion detection. CNN has great
advantages in extracting local features of images. However, due to the locality
of convolution operation, it can not deal with long-range relationships well.
Recently, transformers have been applied to computer vision and achieved
remarkable success in large-scale datasets. Compared with natural images,
multi-modal medical images have explicit and important long-range dependencies,
and effective multi-modal fusion strategies can greatly improve the performance
of deep models. This prompts us to study transformer-based structures and apply
them to multi-modal medical images. Existing transformer-based network
architectures require large-scale datasets to achieve better performance.
However, medical imaging datasets are relatively small, which makes it
difficult to apply pure transformers to medical image analysis. Therefore, we
propose TransMed for multi-modal medical image classification. TransMed
combines the advantages of CNN and transformer to efficiently extract low-level
features of images and establish long-range dependencies between modalities. We
evaluated our model for the challenging problem of preoperative diagnosis of
parotid gland tumors, and the experimental results show the advantages of our
proposed method. We argue that the combination of CNN and transformer has
tremendous potential in a large number of medical image analysis tasks. To our
best knowledge, this is the first work to apply transformers to medical image
classification.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1306.6839v1,"W3-Scrape - A Windows based Reconnaissance Tool for Web Application
  Fingerprinting","Web Application finger printing is a quintessential part of the Information
Gathering phase of (ethical) hacking. It allows narrowing down the specifics
instead of looking for all clues. Also an application that has been correctly
recognized can help in quickly analyzing known weaknesses and then moving ahead
with remaining aspects. This step is also essential to allow a pen tester to
customize its payload or exploitation techniques based on the identification so
to increase the chances of successful intrusion. This paper presents a new tool
""W3-Scrape"" for the relatively nascent field of Web Application finger printing
that helps automate web application fingerprinting when performed in the
current scenarios.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1202.1148v1,Regular Languages are Church-Rosser Congruential,"This paper proves a long standing conjecture in formal language theory. It
shows that all regular languages are Church-Rosser congruential. The class of
Church-Rosser congruential languages was introduced by McNaughton, Narendran,
and Otto in 1988. A language L is Church-Rosser congruential, if there exists a
finite confluent, and length-reducing semi-Thue system S such that L is a
finite union of congruence classes modulo S. It was known that there are
deterministic linear context-free languages which are not Church-Rosser
congruential, but on the other hand it was strongly believed that all regular
language are of this form. Actually, this paper proves a more general result.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0804.0876v2,Semi-continuous Sized Types and Termination,"Some type-based approaches to termination use sized types: an ordinal bound
for the size of a data structure is stored in its type. A recursive function
over a sized type is accepted if it is visible in the type system that
recursive calls occur just at a smaller size. This approach is only sound if
the type of the recursive function is admissible, i.e., depends on the size
index in a certain way. To explore the space of admissible functions in the
presence of higher-kinded data types and impredicative polymorphism, a
semantics is developed where sized types are interpreted as functions from
ordinals into sets of strongly normalizing terms. It is shown that upper
semi-continuity of such functions is a sufficient semantic criterion for
admissibility. To provide a syntactical criterion, a calculus for
semi-continuous functions is developed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.01676v2,Automated Market Makers for Decentralized Finance (DeFi),"This paper compares mathematical models for automated market makers including
logarithmic market scoring rule (LMSR), liquidity sensitive LMSR (LS-LMSR),
constant product/mean/sum, and others. It is shown that though LMSR may not be
a good model for Decentralized Finance (DeFi) applications, LS-LMSR has several
advantages over constant product/mean based automated market makers. However,
LS-LMSR requires complicated computation (i.e., logarithm and exponentiation)
and the cost function curve is concave. In certain DeFi applications, it is
preferred to have computationally efficient cost functions with convex curves
to conform with the principle of supply and demand. This paper proposes and
analyzes constant circle/ellipse based cost functions for automated market
makers. The proposed cost functions are computationally efficient (only
requires multiplication and square root calculation) and have several
advantages over widely deployed constant product cost functions. For example,
the proposed market makers are more robust against front-runner (slippage)
attacks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2107.02248v1,A comparison of LSTM and GRU networks for learning symbolic sequences,"We explore relations between the hyper-parameters of a recurrent neural
network (RNN) and the complexity of string sequences it is able to memorize. We
compare long short-term memory (LSTM) networks and gated recurrent units
(GRUs). We find that an increase of RNN depth does not necessarily result in
better memorization capability when the training time is constrained. Our
results also indicate that the learning rate and the number of units per layer
are among the most important hyper-parameters to be tuned. Generally, GRUs
outperform LSTM networks on low complexity sequences while on high complexity
sequences LSTMs perform better.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.01451v5,"Comparison of a Head-Mounted Display and a Curved Screen in a
  Multi-Talker Audiovisual Listening Task","Objectives: Virtual audiovisual technology and its methodology has yet to be
established for psychoacoustic research. This study examined the effects of
different audiovisual conditions on head-yaw and eye-gaze direction as well as
on preference when listening to multi-talker conversations. The study's goal is
to explore and assess audiovisual technologies in the context of hearing
research. Design: The participants listened to audiovisual conversations
between four talkers. Two displays were tested and compared: a curved screen
(CS) and a head-mounted display (HMD). Using three visual conditions
(audio-only, virtual characters and video recordings), three groups of
participants were tested: seventeen young normal-hearing, eleven older
normal-hearing, and ten older hearing-impaired listeners. Results: When there
were no visual cues, participants tended to look ahead; when visual information
was available, they looked at the target talker. Participants generally turned
their head close towards the target talkers when presented as video than
virtual characters. In addition, an effect of display showed that in the most
extreme case when the talker was at 45 degrees, they turned closer when wearing
the HMD than when viewing the curved screen. Open interviews showed that the CS
was preferred over the HMD for older participants and that video recordings
were the preferred visual condition. Conclusions: Using different audiovisual
setups may lead to slightly different movement behavior in terms of head yaw
and horizontal gaze. These differences, as well as preference for specific
audiovisual technologies, should be taken into account when designing and
comparing experiments.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0102016v1,A Scientific Data Management System for Irregular Applications,"Many scientific applications are I/O intensive and generate or access large
data sets, spanning hundreds or thousands of ""files."" Management, storage,
efficient access, and analysis of this data present an extremely challenging
task. We have developed a software system, called Scientific Data Manager
(SDM), that uses a combination of parallel file I/O and database support for
high-performance scientific data management. SDM provides a high-level API to
the user and internally, uses a parallel file system to store real data and a
database to store application-related metadata. In this paper, we describe how
we designed and implemented SDM to support irregular applications. SDM can
efficiently handle the reading and writing of data in an irregular mesh as well
as the distribution of index values. We describe the SDM user interface and how
we implemented it to achieve high performance. SDM makes extensive use of
MPI-IO's noncontiguous collective I/O functions. SDM also uses the concept of a
history file to optimize the cost of the index distribution using the metadata
stored in the database. We present performance results with two irregular
applications, a CFD code called FUN3D and a Rayleigh-Taylor instability code,
on the SGI Origin2000 at Argonne National Laboratory.",0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0807.0807v2,Shortest Paths Avoiding Forbidden Subpaths,"In this paper we study a variant of the shortest path problem in graphs:
given a weighted graph G and vertices s and t, and given a set X of forbidden
paths in G, find a shortest s-t path P such that no path in X is a subpath of
P. Path P is allowed to repeat vertices and edges. We call each path in X an
exception, and our desired path a shortest exception-avoiding path. We
formulate a new version of the problem where the algorithm has no a priori
knowledge of X, and finds out about an exception x in X only when a path
containing x fails. This situation arises in computing shortest paths in
optical networks. We give an algorithm that finds a shortest exception avoiding
path in time polynomial in |G| and |X|. The main idea is to run Dijkstra's
algorithm incrementally after replicating vertices when an exception is
discovered.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1106.3448v3,Type classes for efficient exact real arithmetic in Coq,"Floating point operations are fast, but require continuous effort on the part
of the user in order to ensure that the results are correct. This burden can be
shifted away from the user by providing a library of exact analysis in which
the computer handles the error estimates. Previously, we [Krebbers/Spitters
2011] provided a fast implementation of the exact real numbers in the Coq proof
assistant. Our implementation improved on an earlier implementation by O'Connor
by using type classes to describe an abstract specification of the underlying
dense set from which the real numbers are built. In particular, we used dyadic
rationals built from Coq's machine integers to obtain a 100 times speed up of
the basic operations already. This article is a substantially expanded version
of [Krebbers/Spitters 2011] in which the implementation is extended in the
various ways. First, we implement and verify the sine and cosine function.
Secondly, we create an additional implementation of the dense set based on
Coq's fast rational numbers. Thirdly, we extend the hierarchy to capture order
on undecidable structures, while it was limited to decidable structures before.
This hierarchy, based on type classes, allows us to share theory on the
naturals, integers, rationals, dyadics, and reals in a convenient way. Finally,
we obtain another dramatic speed-up by avoiding evaluation of termination
proofs at runtime.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9907002v1,"The Distribution of Cycle Lengths in Graphical Models for Iterative
  Decoding","This paper analyzes the distribution of cycle lengths in turbo decoding and
low-density parity check (LDPC) graphs. The properties of such cycles are of
significant interest in the context of iterative decoding algorithms which are
based on belief propagation or message passing. We estimate the probability
that there exist no simple cycles of length less than or equal to k at a
randomly chosen node in a turbo decoding graph using a combination of counting
arguments and independence assumptions. For large block lengths n, this
probability is approximately e^{-{2^{k-1}-4}/n}, k>=4. Simulation results
validate the accuracy of the various approximations. For example, for turbo
codes with a block length of 64000, a randomly chosen node has a less than 1%
chance of being on a cycle of length less than or equal to 10, but has a
greater than 99.9% chance of being on a cycle of length less than or equal to
20. The effect of the ""S-random"" permutation is also analyzed and it is shown
that while it eliminates short cycles of length k<8, it does not significantly
affect the overall distribution of cycle lengths. Similar analyses and
simulations are also presented for graphs for LDPC codes. The paper concludes
by commenting briefly on how these results may provide insight into the
practical success of iterative decoding methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.06694v1,"I Probe, Therefore I Am: Designing a Virtual Journalist with Human
  Emotions","By utilizing different communication channels, such as verbal language,
gestures or facial expressions, virtually embodied interactive humans hold a
unique potential to bridge the gap between human-computer interaction and
actual interhuman communication. The use of virtual humans is consequently
becoming increasingly popular in a wide range of areas where such a natural
communication might be beneficial, including entertainment, education, mental
health research and beyond. Behind this development lies a series of
technological advances in a multitude of disciplines, most notably natural
language processing, computer vision, and speech synthesis. In this paper we
discuss a Virtual Human Journalist, a project employing a number of novel
solutions from these disciplines with the goal to demonstrate their viability
by producing a humanoid conversational agent capable of naturally eliciting and
reacting to information from a human user. A set of qualitative and
quantitative evaluation sessions demonstrated the technical feasibility of the
system whilst uncovering a number of deficits in its capacity to engage users
in a way that would be perceived as natural and emotionally engaging. We argue
that naturalness should not always be seen as a desirable goal and suggest that
deliberately suppressing the naturalness of virtual human interactions, such as
by altering its personality cues, might in some cases yield more desirable
results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.01529v1,Learning Complex Users' Preferences for Recommender Systems,"Recommender systems (RSs) have emerged as very useful tools to help customers
with their decision-making process, find items of their interest, and alleviate
the information overload problem. There are two different lines of approaches
in RSs: (1) general recommenders with the main goal of discovering long-term
users' preferences, and (2) sequential recommenders with the main focus of
capturing short-term users' preferences in a session of user-item interaction
(here, a session refers to a record of purchasing multiple items in one
shopping event). While considering short-term users' preferences may satisfy
their current needs and interests, long-term users' preferences provide users
with the items that they may interact with, eventually. In this thesis, we
first focus on improving the performance of general RSs. Most of the existing
general RSs tend to exploit the users' rating patterns on common items to
detect similar users. The data sparsity problem (i.e. the lack of available
information) is one of the major challenges for the current general RSs, and
they may fail to have any recommendations when there are no common items of
interest among users. We call this problem data sparsity with no feedback on
common items (DSW-n-FCI). To overcome this problem, we propose a
personality-based RS in which similar users are identified based on the
similarity of their personality traits.",0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.13860v1,Active Tree Search in Large POMDPs,"Model-based planning and prospection are widely studied in both cognitive
neuroscience and artificial intelligence (AI), but from different perspectives
- and with different desiderata in mind (biological realism versus scalability)
that are difficult to reconcile. Here, we introduce a novel method to plan in
large POMDPs - Active Tree Search - that combines the normative character and
biological realism of a leading planning theory in neuroscience (Active
Inference) and the scalability of Monte-Carlo methods in AI. This unification
is beneficial for both approaches. On the one hand, using Monte-Carlo planning
permits scaling up the biologically grounded approach of Active Inference to
large-scale problems. On the other hand, the theory of Active Inference
provides a principled solution to the balance of exploration and exploitation,
which is often addressed heuristically in Monte-Carlo methods. Our simulations
show that Active Tree Search successfully navigates binary trees that are
challenging for sampling-based methods, problems that require adaptive
exploration, and the large POMDP problem Rocksample. Furthermore, we illustrate
how Active Tree Search can be used to simulate neurophysiological responses
(e.g., in the hippocampus and prefrontal cortex) of humans and other animals
that contain large planning problems. These simulations show that Active Tree
Search is a principled realisation of neuroscientific and AI theories of
planning, which offers both biological realism and scalability.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0405076v1,An Abductive Framework For Computing Knowledge Base Updates,"This paper introduces an abductive framework for updating knowledge bases
represented by extended disjunctive programs. We first provide a simple
transformation from abductive programs to update programs which are logic
programs specifying changes on abductive hypotheses. Then, extended abduction,
which was introduced by the same authors as a generalization of traditional
abduction, is computed by the answer sets of update programs. Next, different
types of updates, view updates and theory updates are characterized by
abductive programs and computed by update programs. The task of consistency
restoration is also realized as special cases of these updates. Each update
problem is comparatively assessed from the computational complexity viewpoint.
The result of this paper provides a uniform framework for different types of
knowledge base updates, and each update is computed using existing procedures
of logic programming.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0611103v1,"Barriers and local minima in energy landscapes of stochastic local
  search","A local search algorithm operating on an instance of a Boolean constraint
satisfaction problem (in particular, k-SAT) can be viewed as a stochastic
process traversing successive adjacent states in an ``energy landscape''
defined by the problem instance on the n-dimensional Boolean hypercube. We
investigate analytically the worst-case topography of such landscapes in the
context of satisfiable k-SAT via a random ensemble of satisfiable ``k-regular''
linear equations modulo 2.
  We show that for each fixed k=3,4,..., the typical k-SAT energy landscape
induced by an instance drawn from the ensemble has a set of 2^{\Omega(n)} local
energy minima, each separated by an unconditional \Omega(n) energy barrier from
each of the O(1) ground states, that is, solution states with zero energy. The
main technical aspect of the analysis is that a random k-regular 0/1 matrix
constitutes a strong boundary expander with almost full GF(2)-linear rank, a
property which also enables us to prove a 2^{\Omega(n)} lower bound for the
expected number of steps required by the focused random walk heuristic to solve
typical instances drawn from the ensemble. These results paint a grim picture
of the worst-case topography of k-SAT for local search, and constitute
apparently the first rigorous analysis of the growth of energy barriers in a
random ensemble of k-SAT landscapes as the number of variables n is increased.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0405083v1,The Design of a COM-Oriented Module System,"We present in this paper the preliminary design of a module system based on a
notion of components such as they are found in COM. This module system is
inspired from that of Standard ML, and features first-class instances of
components, first-class interfaces, and interface-polymorphic functions, as
well as allowing components to be both imported from the environment and
exported to the environment using simple mechanisms. The module system
automates the memory management of interfaces and hides the IUnknown interface
and QueryInterface mechanisms from the programmer, favoring instead a
higher-level approach to handling interfaces.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0711.0840v2,A thread calculus with molecular dynamics,"We present a theory of threads, interleaving of threads, and interaction
between threads and services with features of molecular dynamics, a model of
computation that bears on computations in which dynamic data structures are
involved. Threads can interact with services of which the states consist of
structured data objects and computations take place by means of actions which
may change the structure of the data objects. The features introduced include
restriction of the scope of names used in threads to refer to data objects.
Because that feature makes it troublesome to provide a model based on
structural operational semantics and bisimulation, we construct a projective
limit model for the theory.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.03727v1,Defect Detection on Semiconductor Wafers by Distribution Analysis,"A method for object classification that is based on distribution analysis is
proposed. In addition, a method for finding relevant features and the
unification of this algorithm with another classification algorithm is
proposed. The presented classification algorithm has been applied successfully
to real-world measurement data from wafer fabrication of close to hundred
thousand chips of several product types. The presented algorithm prefers
finding the best rater in a low-dimensional search space over finding a good
rater in a high-dimensional search space. Our approach is interesting in that
it is fast (quasi-linear) and reached good to excellent prediction or detection
quality for real-world wafer data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.6599v3,Hot Swapping for Online Adaptation of Optimization Hyperparameters,"We describe a general framework for online adaptation of optimization
hyperparameters by `hot swapping' their values during learning. We investigate
this approach in the context of adaptive learning rate selection using an
explore-exploit strategy from the multi-armed bandit literature. Experiments on
a benchmark neural network show that the hot swapping approach leads to
consistently better solutions compared to well-known alternatives such as
AdaDelta and stochastic gradient with exhaustive hyperparameter search.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.05343v3,"Electrotactile feedback applications for hand and arm interactions: A
  systematic review, meta-analysis, and future directions","Haptic feedback is critical in a broad range of
human-machine/computer-interaction applications. However, the high cost and low
portability/wearability of haptic devices remain unresolved issues, severely
limiting the adoption of this otherwise promising technology. Electrotactile
interfaces have the advantage of being more portable and wearable due to their
reduced actuators' size, as well as their lower power consumption and
manufacturing cost. The applications of electrotactile feedback have been
explored in human-computer interaction and human-machine-interaction for
facilitating hand-based interactions in applications such as prosthetics,
virtual reality, robotic teleoperation, surface haptics, portable devices, and
rehabilitation. This paper presents a technological overview of electrotactile
feedback, as well a systematic review and meta-analysis of its applications for
hand-based interactions. We discuss the different electrotactile systems
according to the type of application. We also discuss over a quantitative
congregation of the findings, to offer a high-level overview into the
state-of-art and suggest future directions. Electrotactile feedback systems
showed increased portability/wearability, and they were successful in rendering
and/or augmenting most tactile sensations, eliciting perceptual processes, and
improving performance in many scenarios. However, knowledge gaps (e.g.,
embodiment), technical (e.g., recurrent calibration, electrodes' durability)
and methodological (e.g., sample size) drawbacks were detected, which should be
addressed in future studies.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0
http://arxiv.org/abs/1210.1100v2,Confluence by Decreasing Diagrams -- Formalized,"This paper presents a formalization of decreasing diagrams in the theorem
prover Isabelle. It discusses mechanical proofs showing that any locally
decreasing abstract rewrite system is confluent. The valley and the conversion
version of decreasing diagrams are considered.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0911.3195v2,Efficient Distributed Random Walks with Applications,"We focus on the problem of performing random walks efficiently in a
distributed network. Given bandwidth constraints, the goal is to minimize the
number of rounds required to obtain a random walk sample. We first present a
fast sublinear time distributed algorithm for performing random walks whose
time complexity is sublinear in the length of the walk. Our algorithm performs
a random walk of length $\ell$ in $\tilde{O}(\sqrt{\ell D})$ rounds (with high
probability) on an undirected network, where $D$ is the diameter of the
network. This improves over the previous best algorithm that ran in
$\tilde{O}(\ell^{2/3}D^{1/3})$ rounds (Das Sarma et al., PODC 2009). We further
extend our algorithms to efficiently perform $k$ independent random walks in
$\tilde{O}(\sqrt{k\ell D} + k)$ rounds. We then show that there is a
fundamental difficulty in improving the dependence on $\ell$ any further by
proving a lower bound of $\Omega(\sqrt{\frac{\ell}{\log \ell}} + D)$ under a
general model of distributed random walk algorithms. Our random walk algorithms
are useful in speeding up distributed algorithms for a variety of applications
that use random walks as a subroutine. We present two main applications. First,
we give a fast distributed algorithm for computing a random spanning tree (RST)
in an arbitrary (undirected) network which runs in $\tilde{O}(\sqrt{m}D)$
rounds (with high probability; here $m$ is the number of edges). Our second
application is a fast decentralized algorithm for estimating mixing time and
related parameters of the underlying network. Our algorithm is fully
decentralized and can serve as a building block in the design of
topologically-aware networks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1309.1270v1,"Satisfiability of cross product terms is complete for real
  nondeterministic polytime Blum-Shub-Smale machines","Nondeterministic polynomial-time Blum-Shub-Smale Machines over the reals give
rise to a discrete complexity class between NP and PSPACE. Several problems,
mostly from real algebraic geometry / polynomial systems, have been shown
complete (under many-one reduction by polynomial-time Turing machines) for this
class. We exhibit a new one based on questions about expressions built from
cross products only.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1108.1862v1,Input-output Conformance Testing for Channel-based Service Connectors,"Service-based systems are software systems composed of autonomous components
or services provided by different vendors, deployed on remote machines and
accessible through the web. One of the challenges of modern software
engineering is to ensure that such a system behaves as intended by its
designer. The Reo coordination language is an extensible notation for formal
modeling and execution of service compositions. Services that have no prior
knowledge about each other communicate through advanced channel connectors
which guarantee that each participant, service or client, receives the right
data at the right time. Each channel is a binary relation that imposes
synchronization and data constraints on input and output messages. Furthermore,
channels are composed together to realize arbitrarily complex behavioral
protocols. During this process, a designer may introduce errors into the
connector model or the code for their execution, and thus affect the behavior
of a composed service. In this paper, we present an approach for model-based
testing of coordination protocols designed in Reo. Our approach is based on the
input-output conformance (ioco) testing theory and exploits the mapping of
automata-based semantic models for Reo to equivalent process algebra
specifications.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1411.4619v1,"Aggregating partial rankings with applications to peer grading in
  massive online open courses","We investigate the potential of using ordinal peer grading for the evaluation
of students in massive online open courses (MOOCs). According to such grading
schemes, each student receives a few assignments (by other students) which she
has to rank. Then, a global ranking (possibly translated into numerical scores)
is produced by combining the individual ones. This is a novel application area
for social choice concepts and methods where the important problem to be solved
is as follows: how should the assignments be distributed so that the collected
individual rankings can be easily merged into a global one that is as close as
possible to the ranking that represents the relative performance of the
students in the assignment? Our main theoretical result suggests that using
very simple ways to distribute the assignments so that each student has to rank
only $k$ of them, a Borda-like aggregation method can recover a $1-O(1/k)$
fraction of the true ranking when each student correctly ranks the assignments
she receives. Experimental results strengthen our analysis further and also
demonstrate that the same method is extremely robust even when students have
imperfect capabilities as graders. We believe that our results provide strong
evidence that ordinal peer grading can be a highly effective and scalable
solution for evaluation in MOOCs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1711.08226v1,"Proportionally Representative Participatory Budgeting: Axioms and
  Algorithms","Participatory budgeting is one of the exciting developments in deliberative
grassroots democracy. We concentrate on approval elections and propose
proportional representation axioms in participatory budgeting, by generalizing
relevant axioms for approval-based multi-winner elections. We observe a rich
landscape with respect to the computational complexity of identifying
proportional budgets and computing such, and present budgeting methods that
satisfy these axioms by identifying budgets that are representative to the
demands of vast segments of the voters.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1203.6127v6,"List Decoding Algorithm based on Voting in Groebner Bases for General
  One-Point AG Codes","We generalize the unique decoding algorithm for one-point AG codes over the
Miura-Kamiya Cab curves proposed by Lee, Bras-Amor\'os and O'Sullivan (2012) to
general one-point AG codes, without any assumption. We also extend their unique
decoding algorithm to list decoding, modify it so that it can be used with the
Feng-Rao improved code construction, prove equality between its error
correcting capability and half the minimum distance lower bound by Andersen and
Geil (2008) that has not been done in the original proposal except for
one-point Hermitian codes, remove the unnecessary computational steps so that
it can run faster, and analyze its computational complexity in terms of
multiplications and divisions in the finite field. As a unique decoding
algorithm, the proposed one is empirically and theoretically as fast as the BMS
algorithm for one-point Hermitian codes. As a list decoding algorithm,
extensive experiments suggest that it can be much faster for many moderate
size/usual inputs than the algorithm by Beelen and Brander (2010). It should be
noted that as a list decoding algorithm the proposed method seems to have
exponential worst-case computational complexity while the previous proposals
(Beelen and Brander, 2010; Guruswami and Sudan, 1999) have polynomial ones, and
that the proposed method is expected to be slower than the previous proposals
for very large/special inputs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0803.0803v1,"Un Algorithme de Gestion des Adjacences bas sur la Puissance du
  Signal","In this proposition, we present a link management technique for pro-active
routing protocols for ad-hoc networks. This new mechanism is based on signal
strength hence cross layer approach is used. The hysteresis mechanism provided
by OLSR is improved upon by using signal strength in combination with the hello
loss based hysteresis. The signal power is used to determine if the
link-quality is improving or deteriorating while packet losses are handled
through the hysteresis mechanism specified in OLSR RFC. This not only makes the
link management more robust but also helps in anticipating link breakages
thereby greatly improving the performance.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.09362v2,"On Linear Solution of ""Cherry Pickup II"". Max Weight of Two Disjoint
  Paths in Node-Weighted Gridlike DAG","""Minimum Falling Path Sum"" (MFPS) is classic question in programming - ""Given
a grid of size $N{\times}N$ with integers in cells, return the minimum sum of a
falling path through grid. A falling path starts at any cell in the first row
and ends in last row, with the rule of motion - the next element after the cell
$(i,j)$ is one of the cells $(i+1,j-1),(i+1,j)$ and $(i+1,j+1)$"". This problem
has linear solution (LS) (i.e. O($N^2$)) using dynamic programming method
(DPM). There is an Multi-Agent version of MFPS called ""Cherry Pickup II"" (CP2).
CP2 is a search for the maximum sum of 2 falling paths started from top
corners, where each covered cell summed up one time. All known fast solutions
of CP2 uses DPM, but have O($N^3$) time complexity on grid $N{\times}N$. Here
we offer a LS of CP2 (also using DPM) as finding maximum total weight of 2
vertex-disjoint paths. Also, we extend this LS for some extended version of CP2
with wider motion rules.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1908.06095v1,Recommendations and specifications for data scope analysis tools,"This document is one of the deliverable reports created for the ESCAPE
project. ESCAPE stands for Energy-efficient Scalable Algorithms for Weather
Prediction at Exascale. The project develops world-class, extreme-scale
computing capabilities for European operational numerical weather prediction
and future climate models. This is done by identifying Weather & Climate dwarfs
which are key patterns in terms of computation and communication (in the spirit
of the Berkeley dwarfs). These dwarfs are then optimised for different hardware
architectures (single and multi-node) and alternative algorithms are explored.
Performance portability is addressed through the use of domain specific
languages.
  In today's computer architectures, moving data is considerably more time- and
energy consuming than computing on this data. One of the key performance
optimizations for any application is therefore to minimize data motion and
maximize data reuse. Especially on modern supercomputers with very complex and
deep memory hierarchies, it is mandatory to take data locality into account.
Especially when targeting accelerators with directive systems like OpenACC or
OpenMP, identifying data scope, access type and data reuse are critical to
minimize the data transfers from and to the accelerator. Unfortunately,
manually identifying data locality information in complex code bases can be a
time consuming task and tool support is therefore desirable.
  In this report we summarize the results of a survey of currently available
tools that support software developers and performance engineers with data
locality information in complex code bases like numerical weather prediction
(NWP) or climate simulation applications. Based on the survey results we then
recommend a tool and specify some extensions for a tool to solve the problems
encountered in an NWP application.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9810004v1,"The Design of EzWindows: A Graphics API for an Introductory Programming
  Course","Teaching object-oriented programming in an introductory programming course
poses considerable challenges to the instructor. An often advocated approach to
meeting this challenge is the use of a simple, object-oriented graphics
library. We have developed a simple, portable graphics library for teaching
object-oriented programming using C++. The library, EzWindows, allows beginning
programmers to design and write programs that use the graphical display found
on all modern desktop computers. In addition to providing simple graphical
objects such as windows, geometric shapes, and bitmaps, EzWindows provides
facilities for introducing event-based programming using the mouse and timers.
EzWindows has proven to be extremely popular; it is currently in use at over
200 universities, colleges, and high schools. This paper describes the
rationale for EzWindows and its high-level design.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/2011.03123v2,"PubSqueezer: A Text-Mining Web Tool to Transform Unstructured Documents
  into Structured Data","The amount of scientific papers published every day is daunting and
constantly increasing. Keeping up with literature represents a challenge. If
one wants to start exploring new topics it is hard to have a big picture
without reading lots of articles. Furthermore, as one reads through literature,
making mental connections is crucial to ask new questions which might lead to
discoveries. In this work, I present a web tool which uses a Text Mining
strategy to transform large collections of unstructured biomedical articles
into structured data. Generated results give a quick overview on complex topics
which can possibly suggest not explicitly reported information. In particular,
I show two Data Science analyses. First, I present a literature based rare
diseases network build using this tool in the hope that it will help clarify
some aspects of these less popular pathologies. Secondly, I show how a
literature based analysis conducted with PubSqueezer results allows to describe
known facts about SARS-CoV-2. In one sentence, data generated with PubSqueezer
make it easy to use scientific literate in any computational analysis such as
machine learning, natural language processing etc.
  Availability: http://www.pubsqueezer.com",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.02077v1,Deep Optimization of Parametric IIR Filters for Audio Equalization,"This paper describes a novel Deep Learning method for the design of IIR
parametric filters for automatic audio equalization. A simple and effective
neural architecture, named BiasNet, is proposed to determine the IIR equalizer
parameters. An output denormalization technique is used to obtain accurate
tuning of the IIR filters center frequency, quality factor and gain. All layers
involved in the proposed method are shown to be differentiable, allowing
backpropagation to optimize the network weights and achieve, after a number of
training iterations, the optimal output. The parameters are optimized with
respect to a loss function based on a spectral distance between the measured
and desired magnitude response, and a regularization term used to achieve a
spatialization of the acoustc scene. Two scenarios with different
characteristics were considered for the experimental evaluation: a room and a
car cabin. The performance of the proposed method improves over the baseline
techniques and achieves an almost flat band. Moreover IIR filters provide a
consistently lower computational cost during runtime with respect to FIR
filters.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.12174v1,"Deep Neural Network Based Respiratory Pathology Classification Using
  Cough Sounds","Intelligent systems are transforming the world, as well as our healthcare
system. We propose a deep learning-based cough sound classification model that
can distinguish between children with healthy versus pathological coughs such
as asthma, upper respiratory tract infection (URTI), and lower respiratory
tract infection (LRTI). In order to train a deep neural network model, we
collected a new dataset of cough sounds, labelled with clinician's diagnosis.
The chosen model is a bidirectional long-short term memory network (BiLSTM)
based on Mel Frequency Cepstral Coefficients (MFCCs) features. The resulting
trained model when trained for classifying two classes of coughs -- healthy or
pathology (in general or belonging to a specific respiratory pathology),
reaches accuracy exceeding 84\% when classifying cough to the label provided by
the physicians' diagnosis. In order to classify subject's respiratory pathology
condition, results of multiple cough epochs per subject were combined. The
resulting prediction accuracy exceeds 91\% for all three respiratory
pathologies. However, when the model is trained to classify and discriminate
among the four classes of coughs, overall accuracy dropped: one class of
pathological coughs are often misclassified as other. However, if one consider
the healthy cough classified as healthy and pathological cough classified to
have some kind of pathologies, then the overall accuracy of four class model is
above 84\%. A longitudinal study of MFCC feature space when comparing
pathological and recovered coughs collected from the same subjects revealed the
fact that pathological cough irrespective of the underlying conditions occupy
the same feature space making it harder to differentiate only using MFCC
features.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1803.07333v1,"Statistical evaluation of the azimuth and elevation angles seen at the
  output of the receiving antenna","A method to evaluate the statistical properties of the reception angle seen
at the input receiver that considers the receiving antenna pattern is
presented. In particular, the impact of the direction and beamwidth of the
antenna pattern on distribution of the reception angle is shown on the basis of
3D simulation studies. The obtained results show significant differences
between distributions of angle of arrival and angle of reception. This means
that the presented new method allows assessing the impact of the receiving
antenna pattern on the correlation and spectral characteristics at the receiver
input in simulation studies of wireless channel. The use of this method also
provides an opportunity for analysis of a co-existence between small cells and
wireless backhaul, what is currently a significant problem in designing 5G
networks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1406.0993v2,"Latent Kullback Leibler Control for Continuous-State Systems using
  Probabilistic Graphical Models","Kullback Leibler (KL) control problems allow for efficient computation of
optimal control by solving a principal eigenvector problem. However, direct
applicability of such framework to continuous state-action systems is limited.
In this paper, we propose to embed a KL control problem in a probabilistic
graphical model where observed variables correspond to the continuous (possibly
high-dimensional) state of the system and latent variables correspond to a
discrete (low-dimensional) representation of the state amenable for KL control
computation. We present two examples of this approach. The first one uses
standard hidden Markov models (HMMs) and computes exact optimal control, but is
only applicable to low-dimensional systems. The second one uses factorial HMMs,
it is scalable to higher dimensional problems, but control computation is
approximate. We illustrate both examples in several robot motor control tasks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1011.2973v1,The Hamiltonian Syllogistic,"This paper undertakes a re-examination of Sir William Hamilton's doctrine of
the quantification of the predicate. Hamilton's doctrine comprises two theses.
First, the predicates of traditional syllogistic sentence-forms contain
implicit existential quantifiers, so that, for example, ""All p are q"" is to be
understood as ""All p are some q"". Second, these implicit quantifiers can be
meaningfully dualized to yield novel sentence-forms, such as, for example, ""All
p are all q"". Hamilton attempted to provide a deductive system for his
language, along the lines of the classical syllogisms. We show, using
techniques unavailable to Hamilton, that such a system does exist, though with
qualifications that distinguish it from its classical counterpart.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1105.6190v1,Construction of fuzzy automata from fuzzy regular expressions,"Li and Pedrycz [Y. M. Li, W. Pedrycz, Fuzzy finite automata and fuzzy regular
expressions with membership values in lattice ordered monoids, Fuzzy Sets and
Systems 156 (2005) 68--92] have proved fundamental results that provide
different equivalent ways to represent fuzzy languages with membership values
in a lattice-ordered monoid, and generalize the well-known results of the
classical theory of formal languages. In particular, they have shown that a
fuzzy language over an integral lattice-ordered monoid can be represented by a
fuzzy regular expression if and only if it can be recognized by a fuzzy finite
automaton. However, they did not give any effective method for constructing an
equivalent fuzzy finite automaton from a given fuzzy regular expression. In
this paper we provide such an effective method. Transforming scalars appearing
in a fuzzy regular expression {\alpha} into letters of the new extended
alphabet, we convert the fuzzy regular expression {\alpha} to an ordinary
regular expression {\alpha}_{R}. Then, starting from an arbitrary
nondeterministic finite automaton A that recognizes the language ||{\alpha}_R||
represented by the regular expression {\alpha}_R, we construct fuzzy finite
automata A_{\alpha} and A_{\alpha}^r with the same or even less number of
states than the automaton A, which recognize the fuzzy language ||{\alpha}||
represented by the fuzzy regular expression {\alpha}. The starting
nondeterministic finite automaton A can be obtained from {\alpha}_R using any
of the well-known constructions for converting regular expressions to
nondeterministic finite automata, such as Glushkov-McNaughton-Yamada's position
automaton, Brzozowski's derivative automaton, Antimirov's partial derivative
automaton, or Ilie-Yu's follow automaton.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.11007v1,"Pseudo-Haptic Button for Improving User Experience of Mid-Air
  Interaction in VR","Mid-air interaction is one of the promising interaction modalities in virtual
reality (VR) due to its merits in naturalness and intuitiveness, but the
interaction suffers from the lack of haptic feedback as no force or
vibrotactile feedback can be provided in mid-air. As a breakthrough to
compensate for this insufficiency, the application of pseudo-haptic features
which create the visuo-haptic illusion without actual physical haptic stimulus
can be explored. Therefore, this study aimed to investigate the effect of four
pseudo-haptic features: proximity feedback, protrusion, hit effect, and
penetration blocking on user experience for free-hand mid-air button
interaction in VR. We conducted a user study on 21 young subjects to collect
user ratings on various aspects of user experience while users were freely
interacting with 16 buttons with different combinations of four features.
Results indicated that all investigated features significantly improved user
experience in terms of haptic illusion, embodiment, sense of reality,
spatiotemporal perception, satisfaction, and hedonic quality. In addition,
protrusion and hit effect were more beneficial in comparison with the other two
features. It is recommended to utilize the four proposed pseudo-haptic features
in 3D user interfaces (UIs) to make users feel more pleased and amused, but
caution is needed when using proximity feedback together with other features.
The findings of this study could be helpful for VR developers and UI designers
in providing better interactive buttons in the 3D interfaces.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1506.01652v4,"Polynomial Fixed-Parameter Algorithms: A Case Study for Longest Path on
  Interval Graphs","We study the design of fixed-parameter algorithms for problems already known
to be solvable in polynomial time. The main motivation is to get more efficient
algorithms for problems with unattractive polynomial running times. Here, we
focus on a fundamental graph problem: Longest Path, that is, given an
undirected graph, find a maximum-length path in $G$. Longest Path is NP-hard in
general but known to be solvable in $O(n^{4})$ time on $n$-vertex interval
graphs. We show how to solve Longest Path on Interval Graphs, parameterized by
vertex deletion number $k$ to proper interval graphs, in $O(k^{9}n)$ time.
Notably, Longest Path is trivially solvable in linear time on proper interval
graphs, and the parameter value $k$ can be approximated up to a factor of 4 in
linear time. From a more general perspective, we believe that using
parameterized complexity analysis may enable a refined understanding of
efficiency aspects for polynomial-time solvable problems similarly to what
classical parameterized complexity analysis does for NP-hard problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.01826v1,"Simulating and classifying behavior in adversarial environments based on
  action-state traces: an application to money laundering","Many business applications involve adversarial relationships in which both
sides adapt their strategies to optimize their opposing benefits. One of the
key characteristics of these applications is the wide range of strategies that
an adversary may choose as they adapt their strategy dynamically to sustain
benefits and evade authorities. In this paper, we present a novel way of
approaching these types of applications, in particular in the context of
Anti-Money Laundering. We provide a mechanism through which diverse, realistic
and new unobserved behavior may be generated to discover potential unobserved
adversarial actions to enable organizations to preemptively mitigate these
risks. In this regard, we make three main contributions. (a) Propose a novel
behavior-based model as opposed to individual transactions-based models
currently used by financial institutions. We introduce behavior traces as
enriched relational representation to represent observed human behavior. (b) A
modelling approach that observes these traces and is able to accurately infer
the goals of actors by classifying the behavior into money laundering or
standard behavior despite significant unobserved activity. And (c) a synthetic
behavior simulator that can generate new previously unseen traces. The
simulator incorporates a high level of flexibility in the behavioral parameters
so that we can challenge the detection algorithm. Finally, we provide
experimental results that show that the learning module (automated
investigator) that has only partial observability can still successfully infer
the type of behavior, and thus the simulated goals, followed by customers based
on traces - a key aspiration for many applications today.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9301106v1,Isabelle: The Next 700 Theorem Provers,"Isabelle is a generic theorem prover, designed for interactive reasoning in a
variety of formal theories. At present it provides useful proof procedures for
Constructive Type Theory, various first-order logics, Zermelo-Fraenkel set
theory, and higher-order logic. This survey of Isabelle serves as an
introduction to the literature. It explains why generic theorem proving is
beneficial. It gives a thorough history of Isabelle, beginning with its origins
in the LCF system. It presents an account of how logics are represented,
illustrated using classical logic. The approach is compared with the Edinburgh
Logical Framework. Several of the Isabelle object-logics are presented.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0911.2899v3,Coding Guidelines for Prolog,"Coding standards and good practices are fundamental to a disciplined approach
to software projects, whatever programming languages they employ. Prolog
programming can benefit from such an approach, perhaps more than programming in
other languages. Despite this, no widely accepted standards and practices seem
to have emerged up to now. The present paper is a first step towards filling
this void: it provides immediate guidelines for code layout, naming
conventions, documentation, proper use of Prolog features, program development,
debugging and testing. Presented with each guideline is its rationale and,
where sensible options exist, illustrations of the relative pros and cons for
each alternative. A coding standard should always be selected on a per-project
basis, based on a host of issues pertinent to any given programming project;
for this reason the paper goes beyond the mere provision of normative
guidelines by discussing key factors and important criteria that should be
taken into account when deciding on a fully-fledged coding standard for the
project.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/cs/0106054v1,"Software Toolkit for Building Embedded and Distributed Knowledge-based
  Systems","The paper discusses the basic principles and the architecture of the software
toolkit for constructing knowledge-based systems which can be used
cooperatively over computer networks and also embedded into larger software
systems in different ways. Presented architecture is based on frame knowledge
representation and production rules, which also allows to interface high-level
programming languages and relational databases by exposing corresponding
classes or database tables as frames. Frames located on the remote computers
can also be transparently accessed and used in inference, and the dynamic
knowledge for specific frames can also be transferred over the network. The
issues of implementation of such a system are addressed, which use Java
programming language, CORBA and XML for external knowledge representation.
Finally, some applications of the toolkit are considered, including e-business
approach to knowledge sharing, intelligent web behaviours, etc.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1712.05197v2,Towards Deep Modeling of Music Semantics using EEG Regularizers,"Modeling of music audio semantics has been previously tackled through
learning of mappings from audio data to high-level tags or latent unsupervised
spaces. The resulting semantic spaces are theoretically limited, either because
the chosen high-level tags do not cover all of music semantics or because audio
data itself is not enough to determine music semantics. In this paper, we
propose a generic framework for semantics modeling that focuses on the
perception of the listener, through EEG data, in addition to audio data. We
implement this framework using a novel end-to-end 2-view Neural Network (NN)
architecture and a Deep Canonical Correlation Analysis (DCCA) loss function
that forces the semantic embedding spaces of both views to be maximally
correlated. We also detail how the EEG dataset was collected and use it to
train our proposed model. We evaluate the learned semantic space in a transfer
learning context, by using it as an audio feature extractor in an independent
dataset and proxy task: music audio-lyrics cross-modal retrieval. We show that
our embedding model outperforms Spotify features and performs comparably to a
state-of-the-art embedding model that was trained on 700 times more data. We
further discuss improvements to the model that are likely to improve its
performance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1305.0453v1,Complexity Theory for Operators in Analysis,"We propose an extension of the framework for discussing the computational
complexity of problems involving uncountably many objects, such as real
numbers, sets and functions, that can be represented only through
approximation. The key idea is to use (a certain class of) string functions as
names representing these objects. These are more expressive than infinite
sequences, which served as names in prior work that formulated complexity in
more restricted settings. An advantage of using string functions is that we can
define their ""size"" in the way inspired by higher-type complexity theory. This
enables us to talk about computation on string functions whose time or space is
bounded polynomially in the input size, giving rise to more general analogues
of the classes P, NP, and PSPACE. We also define NP- and PSPACE-completeness
under suitable many-one reductions.
  Because our framework separates machine computation and semantics, it can be
applied to problems on sets of interest in analysis once we specify a suitable
representation (encoding). As prototype applications, we consider the
complexity of functions (operators) on real numbers, real sets, and real
functions. For example, the task of numerical algorithms for solving a certain
class of differential equations is naturally viewed as an operator taking real
functions to real functions. As there was no complexity theory for operators,
previous results only stated how complex the solution can be. We now
reformulate them and show that the operator itself is polynomial-space
complete.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1407.6832v1,Faster Fully-Dynamic Minimum Spanning Forest,"We give a new data structure for the fully-dynamic minimum spanning forest
problem in simple graphs. Edge updates are supported in $O(\log^4n/\log\log n)$
amortized time per operation, improving the $O(\log^4n)$ amortized bound of
Holm et al. (STOC'98, JACM'01). We assume the Word-RAM model with standard
instructions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.07030v3,"Clustering with fair-center representation: parameterized approximation
  algorithms and heuristics","We study a variant of classical clustering formulations in the context of
algorithmic fairness, known as diversity-aware clustering. In this variant we
are given a collection of facility subsets, and a solution must contain at
least a specified number of facilities from each subset while simultaneously
minimizing the clustering objective ($k$-median or $k$-means). We investigate
the fixed-parameter tractability of these problems and show several negative
hardness and inapproximability results, even when we afford exponential running
time with respect to some parameters.
  Motivated by these results we identify natural parameters of the problem, and
present fixed-parameter approximation algorithms with approximation ratios
$\big(1 + \frac{2}{e} +\epsilon \big)$ and $\big(1 + \frac{8}{e}+ \epsilon
\big)$ for diversity-aware $k$-median and diversity-aware $k$-means
respectively, and argue that these ratios are essentially tight assuming the
gap-exponential time hypothesis. We also present a simple and more practical
bicriteria approximation algorithm with better running time bounds. We finally
propose efficient and practical heuristics. We evaluate the scalability and
effectiveness of our methods in a wide variety of rigorously conducted
experiments, on both real and synthetic data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.05847v1,"A Survey and Implementation of Performance Metrics for Self-Organized
  Maps","Self-Organizing Map algorithms have been used for almost 40 years across
various application domains such as biology, geology, healthcare, industry and
humanities as an interpretable tool to explore, cluster and visualize
high-dimensional data sets. In every application, practitioners need to know
whether they can \textit{trust} the resulting mapping, and perform model
selection to tune algorithm parameters (e.g. the map size). Quantitative
evaluation of self-organizing maps (SOM) is a subset of clustering validation,
which is a challenging problem as such. Clustering model selection is typically
achieved by using clustering validity indices. While they also apply to
self-organized clustering models, they ignore the topology of the map, only
answering the question: do the SOM code vectors approximate well the data
distribution? Evaluating SOM models brings in the additional challenge of
assessing their topology: does the mapping preserve neighborhood relationships
between the map and the original data? The problem of assessing the performance
of SOM models has already been tackled quite thoroughly in literature, giving
birth to a family of quality indices incorporating neighborhood constraints,
called \textit{topographic} indices. Commonly used examples of such metrics are
the topographic error, neighborhood preservation or the topographic product.
However, open-source implementations are almost impossible to find. This is the
issue we try to solve in this work: after a survey of existing SOM performance
metrics, we implemented them in Python and widely used numerical libraries, and
provide them as an open-source library, SOMperf. This paper introduces each
metric available in our module along with usage examples.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0510085v1,"Canonical time-frequency, time-scale, and frequency-scale
  representations of time-varying channels","Mobile communication channels are often modeled as linear time-varying
filters or, equivalently, as time-frequency integral operators with finite
support in time and frequency. Such a characterization inherently assumes the
signals are narrowband and may not be appropriate for wideband signals. In this
paper time-scale characterizations are examined that are useful in wideband
time-varying channels, for which a time-scale integral operator is physically
justifiable. A review of these time-frequency and time-scale characterizations
is presented. Both the time-frequency and time-scale integral operators have a
two-dimensional discrete characterization which motivates the design of
time-frequency or time-scale rake receivers. These receivers have taps for both
time and frequency (or time and scale) shifts of the transmitted signal. A
general theory of these characterizations which generates, as specific cases,
the discrete time-frequency and time-scale models is presented here. The
interpretation of these models, namely, that they can be seen to arise from
processing assumptions on the transmit and receive waveforms is discussed. Out
of this discussion a third model arises: a frequency-scale continuous channel
model with an associated discrete frequency-scale characterization.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0508100v1,A primer on Answer Set Programming,"A introduction to the syntax and Semantics of Answer Set Programming intended
as an handout to [under]graduate students taking Artificial Intlligence or
Logic Programming classes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1510.04469v3,A Formal System: Rigorous Constructions of Computer Models,"This book explores an alternative to the current dominant paradigm where a
discrete computer model is constructed as an attempt to approximate some
continuum theory. We focus on a class of discrete computer models that are
based on simple deterministic rules and finite state arithmetic. Such models
are highly compatible with the operational parameters of the real world
computer on which they are executed and hence their validation can be
associated with the allowable computations on the machine. A simple formal
system based on a language of functional programs is employed as a tool of
analysis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1706.03949v1,On Generalizing Decidable Standard Prefix Classes of First-Order Logic,"Recently, the separated fragment (SF) of first-order logic has been
introduced. Its defining principle is that universally and existentially
quantified variables may not occur together in atoms. SF properly generalizes
both the Bernays-Sch\""onfinkel-Ramsey (BSR) fragment and the relational monadic
fragment. In this paper the restrictions on variable occurrences in SF
sentences are relaxed such that universally and existentially quantified
variables may occur together in the same atom under certain conditions. Still,
satisfiability can be decided. This result is established in two ways: firstly,
by an effective equivalence-preserving translation into the BSR fragment, and,
secondly, by a model-theoretic argument.
  Slight modifications to the described concepts facilitate the definition of
other decidable classes of first-order sentences. The paper presents a second
fragment which is novel, has a decidable satisfiability problem, and properly
contains the Ackermann fragment and---once more---the relational monadic
fragment. The definition is again characterized by restrictions on the
occurrences of variables in atoms. More precisely, after certain
transformations, Skolemization yields only unary functions and constants, and
every atom contains at most one universally quantified variable. An effective
satisfiability-preserving translation into the monadic fragment is devised and
employed to prove decidability of the associated satisfiability problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.10648v1,The Logarithmic Funnel Heap: A Statistically Self-Similar Priority Queue,"The present work contains the design and analysis of a statistically
self-similar data structure using linear space and supporting the operations,
insert, search, remove, increase-key and decrease-key for a deterministic
priority queue in expected O(1) time. Extract-max runs in O(log N) time. The
depth of the data structure is at most log* N. On the highest level, each
element acts as the entrance of a discrete, log* N-level funnel with a
logarithmically decreasing stem diameter, where the stem diameter denotes a
metric for the expected number of items maintained on a given level.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.03937v1,"May I Take Your Order? On the Interplay Between Time and Order in
  Process Mining","Process mining starts from event data. The ordering of events is vital for
the discovery of process models. However, the timestamps of events may be
unreliable or imprecise. To further complicate matters, also causally unrelated
events may be ordered in time. The fact that one event is followed by another
does not imply that the former causes the latter. This paper explores the
relationship between time and order. Moreover, it describes an approach to
preprocess event data having timestamp-related problems. This approach avoids
using accidental or unreliable orders and timestamps, creates partial orders to
capture uncertainty, and allows for exploiting domain knowledge to (re)order
events. Optionally, the approach also generates interleavings to be able to use
existing process mining techniques that cannot handle partially ordered event
data. The approach has been implemented using ProM and can be applied to any
event log.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.06536v1,"Hitting minors on bounded treewidth graphs. II. Single-exponential
  algorithms","For a finite collection of graphs ${\cal F}$, the ${\cal F}$-M-DELETION
(resp. ${\cal F}$-TM-DELETION) problem consists in, given a graph $G$ and an
integer $k$, decide whether there exists $S \subseteq V(G)$ with $|S| \leq k$
such that $G \setminus S$ does not contain any of the graphs in ${\cal F}$ as a
minor (resp. topological minor). We are interested in the parameterized
complexity of both problems when the parameter is the treewidth of $G$, denoted
by $tw$, and specifically in the cases where ${\cal F}$ contains a single
connected planar graph $H$. We present algorithms running in time $2^{O(tw)}
\cdot n^{O(1)}$, called single-exponential, when $H$ is either $P_3$, $P_4$,
$C_4$, the paw, the chair, and the banner for both $\{H\}$-M-DELETION and
$\{H\}$-TM-DELETION, and when $H=K_{1,i}$, with $i \geq 1$, for
$\{H\}$-TM-DELETION. Some of these algorithms use the rank-based approach
introduced by Bodlaender et al. [Inform Comput, 2015]. This is the second of a
series of articles on this topic, and the results given here together with
other ones allow us, in particular, to provide a tight dichotomy on the
complexity of $\{H\}$-M-DELETION in terms of $H$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.06488v1,"Multiple Node Immunisation for Preventing Epidemics on Networks by Exact
  Multiobjective Optimisation of Cost and Shield-Value","The general problem in this paper is vertex (node) subset selection with the
goal to contain an infection that spreads in a network. Instead of selecting
the single most important node, this paper deals with the problem of selecting
multiple nodes for removal. As compared to previous work on multiple-node
selection, the trade-off between cost and benefit is considered. The benefit is
measured in terms of increasing the epidemic threshold which is a measure of
how difficult it is for an infection to spread in a network. The cost is
measured in terms of the number and size of nodes to be removed or controlled.
Already in its single-objective instance with a fixed number of $k$ nodes to be
removed, the multiple vertex immunisation problems have been proven to be
NP-hard. Several heuristics have been developed to approximate the problem. In
this work, we compare meta-heuristic techniques with exact methods on the
Shield-value, which is a sub-modular proxy for the maximal eigenvalue and used
in the current state-of-the-art greedy node-removal strategies. We generalise
it to the multi-objective case and replace the greedy algorithm by a quadratic
program (QP), which then can be solved with exact QP solvers. The main
contribution of this paper is the insight that, if time permits, exact and
problem-specific methods approximation should be used, which are often far
better than Pareto front approximations obtained by general meta-heuristics.
Based on these, it will be more effective to develop strategies for controlling
real-world networks when the goal is to prevent or contain epidemic outbreaks.
This paper is supported by ready to use Python implementation of the
optimization methods and datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.04269v3,"On Time Synchronization Issues in Time-Sensitive Networks with
  Regulators and Nonideal Clocks","Flow reshaping is used in time-sensitive networks (as in the context of IEEE
TSN and IETF Detnet) in order to reduce burstiness inside the network and to
support the computation of guaranteed latency bounds. This is performed using
per-flow regulators (such as the Token Bucket Filter) or interleaved regulators
(as with IEEE TSN Asynchronous Traffic Shaping). Both types of regulators are
beneficial as they cancel the increase of burstiness due to multiplexing inside
the network. It was demonstrated, by using network calculus, that they do not
increase the worst-case latency. However, the properties of regulators were
established assuming that time is perfect in all network nodes. In reality,
nodes use local, imperfect clocks. Time-sensitive networks exist in two
flavours: (1) in non-synchronized networks, local clocks run independently at
every node and their deviations are not controlled and (2) in synchronized
networks, the deviations of local clocks are kept within very small bounds
using for example a synchronization protocol (such as PTP) or a satellite based
geo-positioning system (such as GPS). We revisit the properties of regulators
in both cases. In non-synchronized networks, we show that ignoring the timing
inaccuracies can lead to network instability due to unbounded delay in per-flow
or interleaved regulators. We propose and analyze two methods (rate and burst
cascade, and asynchronous dual arrival-curve method) for avoiding this problem.
In synchronized networks, we show that there is no instability with per-flow
regulators but, surprisingly, interleaved regulators can lead to instability.
To establish these results, we develop a new framework that captures industrial
requirements on clocks in both non-synchronized and synchronized networks, and
we develop a toolbox that extends network calculus to account for clock
imperfections.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1311.1632v2,"Persistence, Change, and the Integration of Objects and Processes in the
  Framework of the General Formal Ontology","In this paper we discuss various problems, associated to temporal phenomena.
These problems include persistence and change, the integration of objects and
processes, and truth-makers for temporal propositions. We propose an approach
which interprets persistence as a phenomenon emanating from the activity of the
mind, and which, additionally, postulates that persistence, finally, rests on
personal identity. The General Formal Ontology (GFO) is a top level ontology
being developed at the University of Leipzig. Top level ontologies can be
roughly divided into 3D-ontologies, and 4D-ontologies. GFO is the only top
level ontology, used in applications, which is a 4D-ontology admitting
additionally 3D objects. Objects and processes are integrated in a natural way.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0205007v1,On-Line Paging against Adversarially Biased Random Inputs,"In evaluating an algorithm, worst-case analysis can be overly pessimistic.
Average-case analysis can be overly optimistic. An intermediate approach is to
show that an algorithm does well on a broad class of input distributions.
Koutsoupias and Papadimitriou recently analyzed the least-recently-used (LRU)
paging strategy in this manner, analyzing its performance on an input sequence
generated by a so-called diffuse adversary -- one that must choose each request
probabilitistically so that no page is chosen with probability more than some
fixed epsilon>0. They showed that LRU achieves the optimal competitive ratio
(for deterministic on-line algorithms), but they didn't determine the actual
ratio.
  In this paper we estimate the optimal ratios within roughly a factor of two
for both deterministic strategies (e.g. least-recently-used and
first-in-first-out) and randomized strategies. Around the threshold epsilon ~
1/k (where k is the cache size), the optimal ratios are both Theta(ln k). Below
the threshold the ratios tend rapidly to O(1). Above the threshold the ratio is
unchanged for randomized strategies but tends rapidly to Theta(k) for
deterministic ones.
  We also give an alternate proof of the optimality of LRU.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0110037v1,"Practical Aspects for a Working Compile Time Garbage Collection System
  for Mercury","Compile-time garbage collection (CTGC) is still a very uncommon feature
within compilers. In previous work we have developed a compile-time structure
reuse system for Mercury, a logic programming language. This system indicates
which datastructures can safely be reused at run-time. As preliminary
experiments were promising, we have continued this work and have now a working
and well performing near-to-ship CTGC-system built into the Melbourne Mercury
Compiler (MMC).
  In this paper we present the multiple design decisions leading to this
system, we report the results of using CTGC for a set of benchmarks, including
a real-world program, and finally we discuss further possible improvements.
Benchmarks show substantial memory savings and a noticeable reduction in
execution time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0903.1878v1,Contracting preference relations for database applications,"The binary relation framework has been shown to be applicable to many
real-life preference handling scenarios. Here we study preference contraction:
the problem of discarding selected preferences. We argue that the property of
minimality and the preservation of strict partial orders are crucial for
contractions. Contractions can be further constrained by specifying which
preferences should be protected. We consider two classes of preference
relations: finite and finitely representable. We present algorithms for
computing minimal and preference-protecting minimal contractions for finite as
well as finitely representable preference relations. We study relationships
between preference change in the binary relation framework and belief change in
the belief revision theory. We also introduce some preference query
optimization techniques which can be used in the presence of contraction. We
evaluate the proposed algorithms experimentally and present the results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.06823v1,Erlang Redux: An Ansatz Method for Solving the M/M/m Queue,"This exposition presents a novel approach to solving an M/M/m queue for the
waiting time and the residence time. The motivation comes from an algebraic
solution for the residence time of the M/M/1 queue. The key idea is the
introduction of an ansatz transformation, defined in terms of the Erlang B
function, that avoids the more opaque derivation based on applied probability
theory. The only prerequisite is an elementary knowledge of the Poisson
distribution, which is already necessary for understanding the M/M/1 queue. The
approach described here supersedes our earlier approximate morphing
transformation.",0,0,0,0,0,0,0,0,1,0,0,1,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1801.07215v1,"Get Your Workload in Order: Game Theoretic Prioritization of Database
  Auditing","For enhancing the privacy protections of databases, where the increasing
amount of detailed personal data is stored and processed, multiple mechanisms
have been developed, such as audit logging and alert triggers, which notify
administrators about suspicious activities; however, the two main limitations
in common are: 1) the volume of such alerts is often substantially greater than
the capabilities of resource-constrained organizations, and 2) strategic
attackers may disguise their actions or carefully choosing which records they
touch, making incompetent the statistical detection models. For solving them,
we introduce a novel approach to database auditing that explicitly accounts for
adversarial behavior by 1) prioritizing the order in which types of alerts are
investigated and 2) providing an upper bound on how much resource to allocate
for each type. We model the interaction between a database auditor and
potential attackers as a Stackelberg game in which the auditor chooses an
auditing policy and attackers choose which records to target. A corresponding
approach combining linear programming, column generation, and heuristic search
is proposed to derive an auditing policy. For testing the policy-searching
performance, a publicly available credit card application dataset are adopted,
on which it shows that our methods produce high-quality mixed strategies as
database audit policies, and our general approach significantly outperforms
non-game-theoretic baselines.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1411.5573v1,Description and Optimization of Abstract Machines in a Dialect of Prolog,"In order to achieve competitive performance, abstract machines for Prolog and
related languages end up being large and intricate, and incorporate
sophisticated optimizations, both at the design and at the implementation
levels. At the same time, efficiency considerations make it necessary to use
low-level languages in their implementation. This makes them laborious to code,
optimize, and, especially, maintain and extend. Writing the abstract machine
(and ancillary code) in a higher-level language can help tame this inherent
complexity. We show how the semantics of most basic components of an efficient
virtual machine for Prolog can be described using (a variant of) Prolog. These
descriptions are then compiled to C and assembled to build a complete bytecode
emulator. Thanks to the high level of the language used and its closeness to
Prolog, the abstract machine description can be manipulated using standard
Prolog compilation and optimization techniques with relative ease. We also show
how, by applying program transformations selectively, we obtain abstract
machine implementations whose performance can match and even exceed that of
state-of-the-art, highly-tuned, hand-crafted emulators.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1604.01378v5,"Isolate First, Then Share: a New OS Architecture for Datacenter
  Computing","This paper presents the ""isolate first, then share"" OS model in which the
processor cores, memory, and devices are divided up between disparate OS
instances and a new abstraction, subOS, is proposed to encapsulate an OS
instance that can be created, destroyed, and resized on-the-fly. The intuition
is that this avoids shared kernel states between applications, which in turn
reduces performance loss caused by contention. We decompose the OS into the
supervisor and several subOSes running at the same privilege level: a subOS
directly manages physical resources, while the supervisor can create, destroy,
resize a subOS on-the-fly. The supervisor and subOSes have few state sharing,
but fast inter-subOS communication mechanisms are provided on demand.
  We present the first implementation, RainForest, which supports unmodified
Linux binaries. Our comprehensive evaluation shows RainForest outperforms Linux
with four different kernels, LXC, and Xen in terms of worst-case and average
performance most of time when running a large number of benchmarks. The source
code is available soon.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1507.01585v1,Finite-Horizon Markov Decision Processes with State Constraints,"Markov Decision Processes (MDPs) have been used to formulate many
decision-making problems in science and engineering. The objective is to
synthesize the best decision (action selection) policies to maximize expected
rewards (minimize costs) in a given stochastic dynamical environment. In many
practical scenarios (multi-agent systems, telecommunication, queuing, etc.),
the decision-making problem can have state constraints that must be satisfied,
which leads to Constrained MDP (CMDP) problems. In the presence of such state
constraints, the optimal policies can be very hard to characterize. This paper
introduces a new approach for finding non-stationary randomized policies for
finite-horizon CMDPs. An efficient algorithm based on Linear Programming (LP)
and duality theory is proposed, which gives the convex set of feasible policies
and ensures that the expected total reward is above a computable lower-bound.
The resulting decision policy is a randomized policy, which is the projection
of the unconstrained deterministic MDP policy on this convex set. To the best
of our knowledge, this is the first result in state constrained MDPs to give an
efficient algorithm for generating finite horizon randomized policies for CMDP
with optimality guarantees. A simulation example of a swarm of autonomous
agents running MDPs is also presented to demonstrate the proposed CMDP solution
algorithm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.04599v1,Mega Key Authentication Mechanism,"For secure communication it is not just sufficient to use strong cryptography
with good and strong keys, but to actually have the assurance, that the keys in
use for it are authentic and from the contact one is expecting to communicate
with. Without that, it is possible to be subject to impersonation or
man-in-the-middle (MitM) attacks.
  Mega meets this problem by providing a hierarchical authentication mechanism
for contacts and their keys. To avoid any hassle when using multiple types of
keys and key pairs for different purposes, the whole authentication mechanism
is brought down to a single ""identity key"".",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/cs/0001010v1,A Real World Implementation of Answer Extraction,"In this paper we describe ExtrAns, an answer extraction system. Answer
extraction (AE) aims at retrieving those exact passages of a document that
directly answer a given user question. AE is more ambitious than information
retrieval and information extraction in that the retrieval results are phrases,
not entire documents, and in that the queries may be arbitrarily specific. It
is less ambitious than full-fledged question answering in that the answers are
not generated from a knowledge base but looked up in the text of documents. The
current version of ExtrAns is able to parse unedited Unix ""man pages"", and
derive the logical form of their sentences. User queries are also translated
into logical forms. A theorem prover then retrieves the relevant phrases, which
are presented through selective highlighting in their context.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0805.3462v2,Enriched MU-Calculi Module Checking,"The model checking problem for open systems has been intensively studied in
the literature, for both finite-state (module checking) and infinite-state
(pushdown module checking) systems, with respect to Ctl and Ctl*. In this
paper, we further investigate this problem with respect to the \mu-calculus
enriched with nominals and graded modalities (hybrid graded Mu-calculus), in
both the finite-state and infinite-state settings. Using an automata-theoretic
approach, we show that hybrid graded \mu-calculus module checking is solvable
in exponential time, while hybrid graded \mu-calculus pushdown module checking
is solvable in double-exponential time. These results are also tight since they
match the known lower bounds for Ctl. We also investigate the module checking
problem with respect to the hybrid graded \mu-calculus enriched with inverse
programs (Fully enriched \mu-calculus): by showing a reduction from the domino
problem, we show its undecidability. We conclude with a short overview of the
model checking problem for the Fully enriched Mu-calculus and the fragments
obtained by dropping at least one of the additional constructs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1309.7099v1,On the Internal Dynamics of the Shanghai Ranking,"The Academic Ranking of World Universities (ARWU) published by researchers at
Shanghai Jiao Tong University has become a major source of information for
university administrators, country officials, students and the public at large.
Recent discoveries regarding its internal dynamics allow the inversion of
published ARWU indicator scores to reconstruct raw scores for five hundred
world class universities. This paper explores raw scores in the ARWU and in
other contests to contrast the dynamics of rank-driven and score-driven tables,
and to explain why the ARWU ranking is a score-driven procedure. We show that
the ARWU indicators constitute sub-scales of a single factor accounting for
research performance, and provide an account of the system of gains and
non-linearities used by ARWU. The paper discusses the non-linearities selected
by ARWU, concluding that they are designed to represent the regressive
character of indicators measuring research performance. We propose that the
utility and usability of the ARWU could be greatly improved by replacing the
unwanted dynamical effects of the annual re-scaling based on raw scores of the
best performers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1309.3611v1,"Ultrametric Component Analysis with Application to Analysis of Text and
  of Emotion","We review the theory and practice of determining what parts of a data set are
ultrametric. It is assumed that the data set, to begin with, is endowed with a
metric, and we include discussion of how this can be brought about if a
dissimilarity, only, holds. The basis for part of the metric-endowed data set
being ultrametric is to consider triplets of the observables (vectors). We
develop a novel consensus of hierarchical clusterings. We do this in order to
have a framework (including visualization and supporting interpretation) for
the parts of the data that are determined to be ultrametric. Furthermore a
major objective is to determine locally ultrametric relationships as opposed to
non-local ultrametric relationships. As part of this work, we also study a
particular property of our ultrametricity coefficient, namely, it being a
function of the difference of angles of the base angles of the isosceles
triangle. This work is completed by a review of related work, on consensus
hierarchies, and of a major new application, namely quantifying and
interpreting the emotional content of narrative.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1006.3506v1,Action Recognition in Videos: from Motion Capture Labs to the Web,"This paper presents a survey of human action recognition approaches based on
visual data recorded from a single video camera. We propose an organizing
framework which puts in evidence the evolution of the area, with techniques
moving from heavily constrained motion capture scenarios towards more
challenging, realistic, ""in the wild"" videos. The proposed organization is
based on the representation used as input for the recognition task, emphasizing
the hypothesis assumed and thus, the constraints imposed on the type of video
that each technique is able to address. Expliciting the hypothesis and
constraints makes the framework particularly useful to select a method, given
an application. Another advantage of the proposed organization is that it
allows categorizing newest approaches seamlessly with traditional ones, while
providing an insightful perspective of the evolution of the action recognition
task up to now. That perspective is the basis for the discussion in the end of
the paper, where we also present the main open issues in the area.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.01119v1,"Anyone Can Become a Troll: Causes of Trolling Behavior in Online
  Discussions","In online communities, antisocial behavior such as trolling disrupts
constructive discussion. While prior work suggests that trolling behavior is
confined to a vocal and antisocial minority, we demonstrate that ordinary
people can engage in such behavior as well. We propose two primary trigger
mechanisms: the individual's mood, and the surrounding context of a discussion
(e.g., exposure to prior trolling behavior). Through an experiment simulating
an online discussion, we find that both negative mood and seeing troll posts by
others significantly increases the probability of a user trolling, and together
double this probability. To support and extend these results, we study how
these same mechanisms play out in the wild via a data-driven, longitudinal
analysis of a large online news discussion community. This analysis reveals
temporal mood effects, and explores long range patterns of repeated exposure to
trolling. A predictive model of trolling behavior shows that mood and
discussion context together can explain trolling behavior better than an
individual's history of trolling. These results combine to suggest that
ordinary people can, under the right circumstances, behave like trolls.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0405042v2,"A Distributed TDMA Slot Assignment Algorithm for Wireless Sensor
  Networks","Wireless sensor networks benefit from communication protocols that reduce
power requirements by avoiding frame collision. Time Division Media Access
methods schedule transmission in slots to avoid collision, however these
methods often lack scalability when implemented in \emph{ad hoc} networks
subject to node failures and dynamic topology. This paper reports a distributed
algorithm for TDMA slot assignment that is self-stabilizing to transient faults
and dynamic topology change. The expected local convergence time is O(1) for
any size network satisfying a constant bound on the size of a node
neighborhood.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1211.0331v1,Sylvester-Gallai type theorems for approximate collinearity,"We study questions in incidence geometry where the precise position of points
is `blurry' (e.g. due to noise, inaccuracy or error). Thus lines are replaced
by narrow tubes, and more generally affine subspaces are replaced by their
small neighborhood. We show that the presence of a sufficiently large number of
approximately collinear triples in a set of points in d dimensional complex
space implies that the points are close to a low dimensional affine subspace.
This can be viewed as a stable variant of the Sylvester-Gallai theorem and its
extensions.
  Building on the recently found connection between Sylvester-Gallai type
theorems and complex Locally Correctable Codes (LCCs), we define the new notion
of stable LCCs, in which the (local) correction procedure can also handle small
perturbations in the euclidean metric. We prove that such stable codes with
constant query complexity do not exist. No impossibility results were known in
any such local setting for more than 2 queries.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1104.3148v1,Complexity of two-variable Dependence Logic and IF-Logic,"We study the two-variable fragments D^2 and IF^2 of dependence logic and
independence-friendly logic. We consider the satisfiability and finite
satisfiability problems of these logics and show that for D^2, both problems
are NEXPTIME-complete, whereas for IF^2, the problems are undecidable. We also
show that D^2 is strictly less expressive than IF^2 and that already in D^2,
equicardinality of two unary predicates and infinity can be expressed (the
latter in the presence of a constant symbol). This is an extended version of a
publication in the proceedings of the 26th Annual IEEE Symposium on Logic in
Computer Science (LICS 2011).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.5769v2,"Descriptor Matching with Convolutional Neural Networks: a Comparison to
  SIFT","Latest results indicate that features learned via convolutional neural
networks outperform previous descriptors on classification tasks by a large
margin. It has been shown that these networks still work well when they are
applied to datasets or recognition tasks different from those they were trained
on. However, descriptors like SIFT are not only used in recognition but also
for many correspondence problems that rely on descriptor matching. In this
paper we compare features from various layers of convolutional neural nets to
standard SIFT descriptors. We consider a network that was trained on ImageNet
and another one that was trained without supervision. Surprisingly,
convolutional neural networks clearly outperform SIFT on descriptor matching.
This paper has been merged with arXiv:1406.6909",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.07989v1,On Misbehaviour and Fault Tolerance in Machine Learning Systems,"Machine learning (ML) provides us with numerous opportunities, allowing ML
systems to adapt to new situations and contexts. At the same time, this
adaptability raises uncertainties concerning the run-time product quality or
dependability, such as reliability and security, of these systems. Systems can
be tested and monitored, but this does not provide protection against faults
and failures in adapted ML systems themselves. We studied software designs that
aim at introducing fault tolerance in ML systems so that possible problems in
ML components of the systems can be avoided. The research was conducted as a
case study, and its data was collected through five semi-structured interviews
with experienced software architects. We present a conceptualisation of the
misbehaviour of ML systems, the perceived role of fault tolerance, and the
designs used. Common patterns to incorporating ML components in design in a
fault tolerant fashion have started to emerge. ML models are, for example,
guarded by monitoring the inputs and their distribution, and enforcing business
rules on acceptable outputs. Multiple, specialised ML models are used to adapt
to the variations and changes in the surrounding world, and simpler fall-over
techniques like default outputs are put in place to have systems up and running
in the face of problems. However, the general role of these patterns is not
widely acknowledged. This is mainly due to the relative immaturity of using ML
as part of a complete software system: the field still lacks established
frameworks and practices beyond training to implement, operate, and maintain
the software that utilises ML. ML software engineering needs further analysis
and development on all fronts.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.01089v1,selp: A Single-Shot Epistemic Logic Program Solver,"Epistemic Logic Programs (ELPs) are an extension of Answer Set Programming
(ASP) with epistemic operators that allow for a form of meta-reasoning, that
is, reasoning over multiple possible worlds. Existing ELP solving approaches
generally rely on making multiple calls to an ASP solver in order to evaluate
the ELP. However, in this paper, we show that there also exists a direct
translation from ELPs into non-ground ASP with bounded arity. The resulting ASP
program can thus be solved in a single shot. We then implement this encoding
method, using recently proposed techniques to handle large, non-ground ASP
rules, into the prototype ELP solving system ""selp"", which we present in this
paper. This solver exhibits competitive performance on a set of ELP benchmark
instances. Under consideration in Theory and Practice of Logic Programming
(TPLP).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.13341v1,0-Gaps on 3D Digital Curves,"In Digital Geometry, gaps are some basic portion of a digital object that a
discrete ray can cross without intersecting any voxel of the object itself.
Such a notion is quite important in combinatorial image analysis and it is
strictly connected with some applications in fields as CAD and Computer
graphics. In this paper we prove that the number of $0$-gaps of a $3$D digital
curve can be expressed as a linear combination of the number of its $i$-cells
(with $i = 0, \ldots, 3$).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.11210v1,"Efficient Global Optimization of Non-differentiable, Symmetric
  Objectives for Multi Camera Placement","We propose a novel iterative method for optimally placing and orienting
multiple cameras in a 3D scene. Sample applications include improving the
accuracy of 3D reconstruction, maximizing the covered area for surveillance, or
improving the coverage in multi-viewpoint pedestrian tracking. Our algorithm is
based on a block-coordinate ascent combined with a surrogate function and an
exclusion area technique. This allows to flexibly handle difficult objective
functions that are often expensive and quantized or non-differentiable. The
solver is globally convergent and easily parallelizable. We show how to
accelerate the optimization by exploiting special properties of the objective
function, such as symmetry. Additionally, we discuss the trade-off between
non-optimal stationary points and the cost reduction when optimizing the
viewpoints consecutively.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.05304v1,"Inferring Invariants with Quantifier Alternations: Taming the Search
  Space Explosion","We present a PDR/IC3 algorithm for finding inductive invariants with
quantifier alternations. We tackle scalability issues that arise due to the
large search space of quantified invariants by combining a breadth-first search
strategy and a new syntactic form for quantifier-free bodies. The breadth-first
strategy prevents inductive generalization from getting stuck in regions of the
search space that are expensive to search and focuses instead on lemmas that
are easy to discover. The new syntactic form is well-suited to lemmas with
quantifier alternations by allowing both limited conjunction and disjunction in
the quantifier-free body, while carefully controlling the size of the search
space. Combining the breadth-first strategy with the new syntactic form results
in useful inductive bias by prioritizing lemmas according to: (i) well-defined
syntactic metrics for simple quantifier structures and quantifier-free bodies,
and (ii) the empirically useful heuristic of preferring lemmas that are fast to
discover. On a benchmark suite of primarily distributed protocols and complex
Paxos variants, we demonstrate that our algorithm can solve more of the most
complicated examples than state-of-the-art techniques.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1801.01165v5,Automorphism groups and Ramsey properties of sparse graphs,"We study automorphism groups of sparse graphs from the viewpoint of
topological dynamics and the Kechris, Pestov, Todor\v{c}evi\'c correspondence.
We investigate amenable and extremely amenable subgroups of these groups using
the space of orientations of the graph and results from structural Ramsey
theory. Resolving one of the open questions in the area, we show that
Hrushovski's example of an $\omega$-categorical sparse graph has no
$\omega$-categorical expansion with extremely amenable automorphism group.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.08270v3,Language Models as a Knowledge Source for Cognitive Agents,"Language models (LMs) are sentence-completion engines trained on massive
corpora. LMs have emerged as a significant breakthrough in natural-language
processing, providing capabilities that go far beyond sentence completion
including question answering, summarization, and natural-language inference.
While many of these capabilities have potential application to cognitive
systems, exploiting language models as a source of task knowledge, especially
for task learning, offers significant, near-term benefits. We introduce
language models and the various tasks to which they have been applied and then
review methods of knowledge extraction from language models. The resulting
analysis outlines both the challenges and opportunities for using language
models as a new knowledge source for cognitive systems. It also identifies
possible ways to improve knowledge extraction from language models using the
capabilities provided by cognitive systems. Central to success will be the
ability of a cognitive agent to itself learn an abstract model of the knowledge
implicit in the LM as well as methods to extract high-quality knowledge
effectively and efficiently. To illustrate, we introduce a hypothetical robot
agent and describe how language models could extend its task knowledge and
improve its performance and the kinds of knowledge and methods the agent can
use to exploit the knowledge within a language model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1006.5442v1,Static and Dynamic Quality Assurance by Aspect Oriented Techniques,"The overall goal of the described research project was to create applicable
quality assurance patterns for Java software systems using the aspect-oriented
programming language extension AspectJ 5. We tried to develop aspects to check
static quality criteria as a variable mutator convention and architectural
layering rules. We successfully developed aspects for automating the following
dynamic quality criteria: Parameterized Exception Chaining, Comfortable
Declaration of Parameterized Exceptions, Not-Null Checking of Reference
Variables.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1905.13038v3,"Memory-efficient and fast implementation of local adaptive binarization
  methods","Binarization is widely used as an image preprocessing step to separate object
especially text from background before recognition. For noisy images with
uneven illumination such as degraded documents, threshold values need to be
computed pixel by pixel to obtain a good segmentation. Since local threshold
values typically depend on moment-based statistics such as mean and variance of
gray levels inside rectangular windows, integral images which are memory
consuming are commonly used to accelerate the calculation. Observed that
moment-based statistics as well as quantiles in a sliding window can be
computed recursively, integral images can be avoided without neglecting speed,
more binarization methods can be accelerated too. In particular, given a
$H\times W$ input image, Sauvola's method and alike can run in $\Theta (HW)$
time independent of window size, while only around $6\min\{H,W\}$ bytes of
auxiliary space is needed, which is significantly lower than the $16HW$ bytes
occupied by the two integral images. Since the proposed technique enable
various well-known local adaptive binarization methods to be applied in
real-time use cases on devices with limited resources, it has the potential of
wide application.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.11443v3,Star-Struck by Fixed Embeddings: Modern Crossing Number Heuristics,"We present a thorough experimental evaluation of several crossing
minimization heuristics that are based on the construction and iterative
improvement of a planarization, i.e., a planar representation of a graph with
crossings replaced by dummy vertices. The evaluated heuristics include
variations and combinations of the well-known planarization method, the
recently implemented star reinsertion method, and a new approach proposed
herein: the mixed insertion method. Our experiments reveal the importance of
several implementation details such as the detection of non-simple crossings
(i.e., crossings between adjacent edges or multiple crossings between the same
two edges). The most notable finding, however, is that the insertion of stars
in a fixed embedding setting is not only significantly faster than the
insertion of edges in a variable embedding setting, but also leads to solutions
of higher quality.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1312.7646v1,Short random circuits define good quantum error correcting codes,"We study the encoding complexity for quantum error correcting codes with
large rate and distance. We prove that random Clifford circuits with $O(n
\log^2 n)$ gates can be used to encode $k$ qubits in $n$ qubits with a distance
$d$ provided $\frac{k}{n} < 1 - \frac{d}{n} \log_2 3 - h(\frac{d}{n})$. In
addition, we prove that such circuits typically have a depth of $O( \log^3 n)$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.01621v1,"The Composability of Intermediate Values in Composable Inductive
  Programming","It is believed that mechanisms including intermediate values enable
composable inductive programming (CIP) to be used to produce software of any
size. We present the results of a study that investigated the relationships
between program size, the number of intermediate values and the number of test
cases used to specify programs using CIP. In the study 96,000 programs of
various sizes were randomly generated, decomposed into fragments and
transformed into test cases. The test cases were then used to regenerate new
versions of the original programs using Zoea. The results show linear
relationships between the number of intermediate values and regenerated program
size, and between the number of test cases and regenerated program size within
the size range studied. In addition, as program size increases there is
increasing scope for trading off the number of test cases against the number of
intermediate values and vice versa.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.01958v2,"ASMD: an automatic framework for compiling multimodal datasets with
  audio and scores","This paper describes an open-source Python framework for handling datasets
for music processing tasks, built with the aim of improving the reproducibility
of research projects in music computing and assessing the generalization
abilities of machine learning models. The framework enables the automatic
download and installation of several commonly used datasets for multimodal
music processing. Specifically, we provide a Python API to access the datasets
through Boolean set operations based on particular attributes, such as
intersections and unions of composers, instruments, and so on. The framework is
designed to ease the inclusion of new datasets and the respective ground-truth
annotations so that one can build, convert, and extend one's own collection as
well as distribute it by means of a compliant format to take advantage of the
API. All code and ground-truth are released under suitable open licenses.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1701.00441v1,"Collecting a Swarm in a Grid Environment Using Shared, Global Inputs","This paper investigates efficient techniques to collect and concentrate an
under-actuated particle swarm despite obstacles. Concentrating a swarm of
particles is of critical importance in health-care for targeted drug delivery,
where micro-scale particles must be steered to a goal location. Individual
particles must be small in order to navigate through micro-vasculature, but
decreasing size brings new challenges. Individual particles are too small to
contain on-board power or computation and are instead controlled by a global
input, such as an applied fluidic flow or electric field.
  To make progress, this paper considers a swarm of robots initialized in a
grid world in which each position is either free-space or obstacle. This paper
provides algorithms that collect all the robots to one position and compares
these algorithms on the basis of efficiency and implementation time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1005.0600v1,When can we decide that a P-finite sequence is positive?,"We consider two algorithms which can be used for proving positivity of
sequences that are defined by a linear recurrence equation with polynomial
coefficients (P-finite sequences). Both algorithms have in common that while
they do succeed on a great many examples, there is no guarantee for them to
terminate, and they do in fact not terminate for every input. For some
restricted classes of P-finite recurrence equations of order up to three we
provide a priori criteria that assert the termination of the algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1512.07505v1,"A short proof of the first selection lemma and weak $\frac{1}{r}$-nets
  for moving points","(i) We provide a short and simple proof of the first selection lemma. (ii) We
also prove a selection lemma of a new type in $\Re^d$. For example, when $d=2$
assuming $n$ is large enough we prove that for any set $P$ of $n$ points in
general position there are $\Omega(n^4)$ pairs of segments spanned by $P$ all
of which intersect in some fixed triangle spanned by $P$. (iii) Finally, we
extend the weak $\frac{1}{r}$-net theorem to a kinetic setting where the
underlying set of points is moving polynomially with bounded description
complexity. We establish that one can find a kinetic analog $N$ of a weak
$\frac{1}{r}$-net of cardinality $O(r^{\frac{d(d+1)}{2}}\log^{d}r)$ whose
points are moving with coordinates that are rational functions with bounded
description complexity. Moreover, each member of $N$ has one polynomial
coordinate.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0511016v1,How to make the top ten: Approximating PageRank from in-degree,"PageRank has become a key element in the success of search engines, allowing
to rank the most important hits in the top screen of results. One key aspect
that distinguishes PageRank from other prestige measures such as in-degree is
its global nature. From the information provider perspective, this makes it
difficult or impossible to predict how their pages will be ranked. Consequently
a market has emerged for the optimization of search engine results. Here we
study the accuracy with which PageRank can be approximated by in-degree, a
local measure made freely available by search engines. Theoretical and
empirical analyses lead to conclude that given the weak degree correlations in
the Web link graph, the approximation can be relatively accurate, giving
service and information providers an effective new marketing tool.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1506.02516v3,Learning to Transduce with Unbounded Memory,"Recently, strong results have been demonstrated by Deep Recurrent Neural
Networks on natural language transduction problems. In this paper we explore
the representational power of these models using synthetic grammars designed to
exhibit phenomena similar to those found in real transduction problems such as
machine translation. These experiments lead us to propose new memory-based
recurrent networks that implement continuously differentiable analogues of
traditional data structures such as Stacks, Queues, and DeQues. We show that
these architectures exhibit superior generalisation performance to Deep RNNs
and are often able to learn the underlying generating algorithms in our
transduction experiments.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0505071v1,Summarization Techniques for Pattern Collections in Data Mining,"Discovering patterns from data is an important task in data mining. There
exist techniques to find large collections of many kinds of patterns from data
very efficiently. A collection of patterns can be regarded as a summary of the
data. A major difficulty with patterns is that pattern collections summarizing
the data well are often very large.
  In this dissertation we describe methods for summarizing pattern collections
in order to make them also more understandable. More specifically, we focus on
the following themes: 1) Quality value simplifications. 2) Pattern orderings.
3) Pattern chains and antichains. 4) Change profiles. 5) Inverse pattern
discovery.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.07397v1,"Level generation and style enhancement -- deep learning for game
  development overview","We present practical approaches of using deep learning to create and enhance
level maps and textures for video games -- desktop, mobile, and web. We aim to
present new possibilities for game developers and level artists. The task of
designing levels and filling them with details is challenging. It is both
time-consuming and takes effort to make levels rich, complex, and with a
feeling of being natural. Fortunately, recent progress in deep learning
provides new tools to accompany level designers and visual artists. Moreover,
they offer a way to generate infinite worlds for game replayability and adjust
educational games to players' needs. We present seven approaches to create
level maps, each using statistical methods, machine learning, or deep learning.
In particular, we include:
  - Generative Adversarial Networks for creating new images from existing
examples (e.g. ProGAN).
  - Super-resolution techniques for upscaling images while preserving crisp
detail (e.g. ESRGAN).
  - Neural style transfer for changing visual themes.
  - Image translation - turning semantic maps into images (e.g. GauGAN).
  - Semantic segmentation for turning images into semantic masks (e.g. U-Net).
  - Unsupervised semantic segmentation for extracting semantic features (e.g.
Tile2Vec).
  - Texture synthesis - creating large patterns based on a smaller sample (e.g.
InGAN).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1503.07082v2,Recognition and Complexity of Point Visibility Graphs,"A point visibility graph is a graph induced by a set of points in the plane,
where every vertex corresponds to a point, and two vertices are adjacent
whenever the two corresponding points are visible from each other, that is, the
open segment between them does not contain any other point of the set. We study
the recognition problem for point visibility graphs: given a simple undirected
graph, decide whether it is the visibility graph of some point set in the
plane. We show that the problem is complete for the existential theory of the
reals. Hence the problem is as hard as deciding the existence of a real
solution to a system of polynomial inequalities. The proof involves simple
substructures forcing collinearities in all realizations of some visibility
graphs, which are applied to the algebraic universality constructions of Mn\""ev
and Richter-Gebert. This solves a longstanding open question and paves the way
for the analysis of other classes of visibility graphs. Furthermore, as a
corollary of one of our construction, we show that there exist point visibility
graphs that do not admit any geometric realization with points having integer
coordinates.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1611.09774v1,"Serving the Grid: an Experimental Study of Server Clusters as Real-Time
  Demand Response Resources","Demand response is a crucial technology to allow large-scale penetration of
intermittent renewable energy sources in the electric grid. This paper is based
on the thesis that datacenters represent especially attractive candidates for
providing flexible, real-time demand response services to the grid; they are
capable of finely-controllable power consumption, fast power ramp-rates, and
large dynamic range. This paper makes two main contributions: (a) it provides
detailed experimental evidence justifying this thesis, and (b) it presents a
comparative investigation of three candidate software interfaces for power
control within the servers. All of these results are based on a series of
experiments involving real-time power measurements on a lab-scale server
cluster. This cluster was specially instrumented for accurate and fast power
measurements on a time-scale of 100 ms or less. Our results provide preliminary
evidence for the feasibility of large scale demand response using datacenters,
and motivates future work on exploiting this capability.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2102.11991v2,"Being correct is not enough: efficient verification using robust linear
  temporal logic","While most approaches in formal methods address system correctness, ensuring
robustness has remained a challenge. In this paper we present and study the
logic rLTL which provides a means to formally reason about both correctness and
robustness in system design. Furthermore, we identify a large fragment of rLTL
for which the verification problem can be efficiently solved, i.e.,
verification can be done by using an automaton, recognizing the behaviors
described by the rLTL formula $\varphi$, of size at most $\mathcal{O} \left(
3^{ |\varphi|} \right)$, where $|\varphi|$ is the length of $\varphi$. This
result improves upon the previously known bound of
$\mathcal{O}\left(5^{|\varphi|} \right)$ for rLTL verification and is closer to
the LTL bound of $\mathcal{O}\left( 2^{|\varphi|} \right)$. The usefulness of
this fragment is demonstrated by a number of case studies showing its practical
significance in terms of expressiveness, the ability to describe robustness,
and the fine-grained information that rLTL brings to the process of system
verification. Moreover, these advantages come at a low computational overhead
with respect to LTL verification.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.08446v1,Heterogeneous download times in bandwidth-homogeneous BitTorrent swarms,"Modeling and understanding BitTorrent (BT) dynamics is a recurrent research
topic mainly due to its high complexity and tremendous practical efficiency.
Over the years, different models have uncovered various phenomena exhibited by
the system, many of which have direct impact on its performance. In this paper
we identify and characterize a phenomenon that has not been previously
observed: homogeneous peers (with respect to their upload capacities)
experience heterogeneous download times. This behavior has direct impact on
peer and system performance, such as high variability of download times,
unfairness with respect to peer arrival order, bursty departures and content
synchronization. Detailed packet-level simulations and prototype-based
experiments on the Internet were performed to characterize this phenomenon. We
also develop a mathematical model that accurately predicts the heterogeneous
download rates of the homogeneous peers as a function of their content. In
addition, we apply the model to calculate lower and upper bounds to the number
of departures that occur in a burst. The heterogeneous download rates are more
prevalent in unpopular swarms (very few peers). Although few works have
addressed this kind of swarm, these by far represent the most common type of
swarm in BT.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1209.3793v1,Polynomial Path Orders: A Maximal Model,"This paper is concerned with the automated complexity analysis of term
rewrite systems (TRSs for short) and the ramification of these in implicit
computational complexity theory (ICC for short). We introduce a novel path
order with multiset status, the polynomial path order POP*. Essentially relying
on the principle of predicative recursion as proposed by Bellantoni and Cook,
its distinct feature is the tight control of resources on compatible TRSs: The
(innermost) runtime complexity of compatible TRSs is polynomially bounded. We
have implemented the technique, as underpinned by our experimental evidence our
approach to the automated runtime complexity analysis is not only feasible, but
compared to existing methods incredibly fast. As an application in the context
of ICC we provide an order-theoretic characterisation of the polytime
computable functions. To be precise, the polytime computable functions are
exactly the functions computable by an orthogonal constructor TRS compatible
with POP*.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1611.09718v2,Efficient Linear Programming for Dense CRFs,"The fully connected conditional random field (CRF) with Gaussian pairwise
potentials has proven popular and effective for multi-class semantic
segmentation. While the energy of a dense CRF can be minimized accurately using
a linear programming (LP) relaxation, the state-of-the-art algorithm is too
slow to be useful in practice. To alleviate this deficiency, we introduce an
efficient LP minimization algorithm for dense CRFs. To this end, we develop a
proximal minimization framework, where the dual of each proximal problem is
optimized via block coordinate descent. We show that each block of variables
can be efficiently optimized. Specifically, for one block, the problem
decomposes into significantly smaller subproblems, each of which is defined
over a single pixel. For the other block, the problem is optimized via
conditional gradient descent. This has two advantages: 1) the conditional
gradient can be computed in a time linear in the number of pixels and labels;
and 2) the optimal step size can be computed analytically. Our experiments on
standard datasets provide compelling evidence that our approach outperforms all
existing baselines including the previous LP based approach for dense CRFs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1104.1466v1,Logical Varieties in Normative Reasoning,"Although conventional logical systems based on logical calculi have been
successfully used in mathematics and beyond, they have definite limitations
that restrict their application in many cases. For instance, the principal
condition for any logical calculus is its consistency. At the same time,
knowledge about large object domains (in science or in practice) is essentially
inconsistent. Logical prevarieties and varieties were introduced to eliminate
these limitations in a logically correct way. In this paper, the Logic of
Reasonable Inferences is described. This logic has been applied successfully to
model legal reasoning with inconsistent knowledge. It is demonstrated that this
logic is a logical variety and properties of logical varieties related to legal
reasoning are developed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1104.4229v2,Data Reduction for Graph Coloring Problems,"This paper studies the kernelization complexity of graph coloring problems
with respect to certain structural parameterizations of the input instances. We
are interested in how well polynomial-time data reduction can provably shrink
instances of coloring problems, in terms of the chosen parameter. It is well
known that deciding 3-colorability is already NP-complete, hence parameterizing
by the requested number of colors is not fruitful. Instead, we pick up on a
research thread initiated by Cai (DAM, 2003) who studied coloring problems
parameterized by the modification distance of the input graph to a graph class
on which coloring is polynomial-time solvable; for example parameterizing by
the number k of vertex-deletions needed to make the graph chordal. We obtain
various upper and lower bounds for kernels of such parameterizations of
q-Coloring, complementing Cai's study of the time complexity with respect to
these parameters.
  Our results show that the existence of polynomial kernels for q-Coloring
parameterized by the vertex-deletion distance to a graph class F is strongly
related to the existence of a function f(q) which bounds the number of vertices
which are needed to preserve the NO-answer to an instance of q-List-Coloring on
F.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0305036v4,Using Dynamic Simulation in the Development of Construction Machinery,"As in the car industry for quite some time, dynamic simulation of complete
vehicles is being practiced more and more in the development of off-road
machinery. However, specific questions arise due not only to company structure
and size, but especially to the type of product. Tightly coupled, non-linear
subsystems of different domains make prediction and optimisation of the
complete system's dynamic behaviour a challenge. Furthermore, the demand for
versatile machines leads to sometimes contradictory target requirements and can
turn the design process into a hunt for the least painful compromise. This can
be avoided by profound system knowledge, assisted by simulation-driven product
development. This paper gives an overview of joint research into this issue by
Volvo Wheel Loaders and Linkoping University on that matter, lists the results
of a related literature review and introduces the term ""operateability"". Rather
than giving detailed answers, the problem space for ongoing and future research
is examined and possible solutions are sketched.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.0271v6,"Stable marriage and roommates problems with restricted edges: complexity
  and approximability","In the stable marriage and roommates problems, a set of agents is given, each
of them having a strictly ordered preference list over some or all of the other
agents. A matching is a set of disjoint pairs of mutually accepted agents. If
any two agents mutually prefer each other to their partner, then they block the
matching, otherwise, the matching is said to be stable. In this paper we
investigate the complexity of finding a solution satisfying additional
constraints on restricted pairs of agents. Restricted pairs can be either
forced or forbidden. A stable solution must contain all of the forced pairs,
while it must contain none of the forbidden pairs.
  Dias et al. gave a polynomial-time algorithm to decide whether such a
solution exists in the presence of restricted edges. If the answer is no, one
might look for a solution close to optimal. Since optimality in this context
means that the matching is stable and satisfies all constraints on restricted
pairs, there are two ways of relaxing the constraints by permitting a solution
to: (1) be blocked by some pairs (as few as possible), or (2) violate some
constraints on restricted pairs (again as few as possible).
  Our main theorems prove that for the (bipartite) stable marriage problem,
case (1) leads to NP-hardness and inapproximability results, whilst case (2)
can be solved in polynomial time. For the non-bipartite stable roommates
instances, case (2) yields an NP-hard problem. In the case of NP-hard problems,
we also discuss polynomially solvable special cases, arising from restrictions
on the lengths of the preference lists, or upper bounds on the numbers of
restricted pairs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0609067v1,"A tool set for the quick and efficient exploration of large document
  collections","We are presenting a set of multilingual text analysis tools that can help
analysts in any field to explore large document collections quickly in order to
determine whether the documents contain information of interest, and to find
the relevant text passages. The automatic tool, which currently exists as a
fully functional prototype, is expected to be particularly useful when users
repeatedly have to sieve through large collections of documents such as those
downloaded automatically from the internet. The proposed system takes a whole
document collection as input. It first carries out some automatic analysis
tasks (named entity recognition, geo-coding, clustering, term extraction),
annotates the texts with the generated meta-information and stores the
meta-information in a database. The system then generates a zoomable and
hyperlinked geographic map enhanced with information on entities and terms
found. When the system is used on a regular basis, it builds up a historical
database that contains information on which names have been mentioned together
with which other names or places, and users can query this database to retrieve
information extracted in the past.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1910.11143v1,The Economics of Smart Contracts,"Ethereum is a distributed blockchain that can execute smart contracts, which
inter-communicate and perform transactions automatically. The execution of
smart contracts is paid in the form of gas, which is a monetary unit used in
the Ethereum blockchain. The Ethereum Virtual Machine (EVM) provides the
metering capability for smart contract execution. Instruction costs vary
depending on the instruction type and the approximate computational resources
required to execute the instruction on the network. The cost of gas is adjusted
using transaction fees to ensure adequate payment of the network. In this work,
we highlight the ""real"" economics of smart contracts. We show that the actual
costs of executing smart contracts are disproportionate to the computational
costs and that this gap is continuously widening. We show that the gas
cost-model of the underlying EVM instruction-set is wrongly modeled.
Specifically, the computational cost for the SLOAD instruction increases with
the length of the blockchain. Our proposed performance model estimates gas
usage and execution time of a smart contract at a given block-height. The new
gas-cost model incorporates the block-height to eliminate irregularities in the
Ethereum gas calculations. Our findings are based on extensive experiments over
the entire history of the EVM blockchain.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0306131v1,Complexity of Cycle Length Modularity Problems in Graphs,"The even cycle problem for both undirected and directed graphs has been the
topic of intense research in the last decade. In this paper, we study the
computational complexity of \emph{cycle length modularity problems}. Roughly
speaking, in a cycle length modularity problem, given an input (undirected or
directed) graph, one has to determine whether the graph has a cycle $C$ of a
specific length (or one of several different lengths), modulo a fixed integer.
We denote the two families (one for undirected graphs and one for directed
graphs) of problems by $(S,m)\hbox{-}{\rm UC}$ and $(S,m)\hbox{-}{\rm DC}$,
where $m \in \mathcal{N}$ and $S \subseteq \{0,1, ..., m-1\}$.
$(S,m)\hbox{-}{\rm UC}$ (respectively, $(S,m)\hbox{-}{\rm DC}$) is defined as
follows: Given an undirected (respectively, directed) graph $G$, is there a
cycle in $G$ whose length, modulo $m$, is a member of $S$? In this paper, we
fully classify (i.e., as either polynomial-time solvable or as ${\rm
NP}$-complete) each problem $(S,m)\hbox{-}{\rm UC}$ such that $0 \in S$ and
each problem $(S,m)\hbox{-}{\rm DC}$ such that $0 \notin S$. We also give a
sufficient condition on $S$ and $m$ for the following problem to be
polynomial-time computable: $(S,m)\hbox{-}{\rm UC}$ such that $0 \notin S$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1005.0418v1,Lower Bounds on Near Neighbor Search via Metric Expansion,"In this paper we show how the complexity of performing nearest neighbor (NNS)
search on a metric space is related to the expansion of the metric space. Given
a metric space we look at the graph obtained by connecting every pair of points
within a certain distance $r$ . We then look at various notions of expansion in
this graph relating them to the cell probe complexity of NNS for randomized and
deterministic, exact and approximate algorithms. For example if the graph has
node expansion $\Phi$ then we show that any deterministic $t$-probe data
structure for $n$ points must use space $S$ where $(St/n)^t > \Phi$. We show
similar results for randomized algorithms as well. These relationships can be
used to derive most of the known lower bounds in the well known metric spaces
such as $l_1$, $l_2$, $l_\infty$ by simply computing their expansion. In the
process, we strengthen and generalize our previous results (FOCS 2008).
Additionally, we unify the approach in that work and the communication
complexity based approach. Our work reduces the problem of proving cell probe
lower bounds of near neighbor search to computing the appropriate expansion
parameter. In our results, as in all previous results, the dependence on $t$ is
weak; that is, the bound drops exponentially in $t$. We show a much stronger
(tight) time-space tradeoff for the class of dynamic low contention data
structures. These are data structures that supports updates in the data set and
that do not look up any single cell too often.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1105.1364v3,"Achieving Data Privacy through Secrecy Views and Null-Based Virtual
  Updates","There may be sensitive information in a relational database, and we might
want to keep it hidden from a user or group thereof. In this work, sensitive
data is characterized as the contents of a set of secrecy views. For a user
without permission to access that sensitive data, the database instance he
queries is updated to make the contents of the views empty or contain only
tuples with null values. In particular, if this user poses a query about any of
these views, no meaningful information is returned. Since the database is not
expected to be physically changed to produce this result, the updates are only
virtual. And also minimal in a precise way. These minimal updates are reflected
in the secrecy view contents, and also in the fact that query answers, while
being privacy preserving, are also maximally informative. Virtual updates are
based on the use of null values as used in the SQL standard. We provide the
semantics of secrecy views and the virtual updates. The different ways in which
the underlying database is virtually updated are specified as the models of a
logic program with stable model semantics. The program becomes the basis for
the computation of the ""secret answers"" to queries, i.e. those that do not
reveal the sensitive information.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.05244v1,"Finite Automata Intersection Non-Emptiness: Parameterized Complexity
  Revisited","The problem DFA-Intersection-Nonemptiness asks if a given number of
deterministic automata accept a common word. In general, this problem is
PSPACE-complete. Here, we investigate this problem for the subclasses of
commutative automata and automata recognizing sparse languages. We show that in
both cases DFA-Intersection-Nonemptiness is complete for NP and for the
parameterized class $W[1]$, where the number of input automata is the
parameter, when the alphabet is fixed. Additionally, we establish the same
result for Tables Non-Empty Join, a problem that asks if the join of several
tables (possibly containing null values) in a database is non-empty. Lastly, we
show that Bounded NFA-Intersection-Nonemptiness, parameterized by the length
bound, is $\mbox{co-}W[2]$-hard with a variable input alphabet and for
nondeterministic automata recognizing finite strictly bounded languages,
yielding a variant leaving the realm of $W[1]$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1502.02585v2,"Core Higher-Order Session Processes: Tractable Equivalences and Relative
  Expressiveness","This work proposes tractable bisimulations for the higher-order pi-calculus
with session primitives (HOpi) and offers a complete study of the expressivity
of its most significant subcalculi. First we develop three typed bisimulations,
which are shown to coincide with contextual equivalence. These
characterisations demonstrate that observing as inputs only a specific finite
set of higher-order values (which inhabit session types) suffices to reason
about HOp} processes. Next, we identify HO, a minimal, second-order subcalculus
of HOpi in which higher-order applications/abstractions, name-passing, and
recursion are absent. We show that HO can encode HOpi extended with
higher-order applications and abstractions and that a first-order session
pi-calculus can encode HOpi. Both encodings are fully abstract. We also prove
that the session pi-calculus with passing of shared names cannot be encoded
into HOpi without shared names. We show that HOpi, HO, and pi are equally
expressive; the expressivity of HO enables effective reasoning about typed
equivalences for higher-order processes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0801.4716v1,"Methods to integrate a language model with semantic information for a
  word prediction component","Most current word prediction systems make use of n-gram language models (LM)
to estimate the probability of the following word in a phrase. In the past
years there have been many attempts to enrich such language models with further
syntactic or semantic information. We want to explore the predictive powers of
Latent Semantic Analysis (LSA), a method that has been shown to provide
reliable information on long-distance semantic dependencies between words in a
context. We present and evaluate here several methods that integrate LSA-based
information with a standard language model: a semantic cache, partial
reranking, and different forms of interpolation. We found that all methods show
significant improvements, compared to the 4-gram baseline, and most of them to
a simple cache model as well.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1301.4258v1,"Design Pattern-Based Extension of Class Hierarchies to Support Runtime
  Invariant Checks","We present a technique for automatically weaving structural invariant checks
into an existing collection of classes. Using variations on existing design
patterns, we use a concise specification to generate from this collection a new
set of classes that implement the interfaces of the originals, but with the
addition of user-specified class invariant checks. Our work is notable in the
scarcity of assumptions made. Unlike previous design pattern approaches to this
problem, our technique requires no modification of the original source code,
relies only on single inheritance, and does not require that the attributes
used in the checks be publicly visible. We are able to instrument a wide
variety of class hierarchies, including those with pure interfaces, abstract
classes and classes with type parameters. We have implemented the construction
as an Eclipse plug-in for Java development.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.06177v3,The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence,"Recent research in artificial intelligence and machine learning has largely
emphasized general-purpose learning and ever-larger training sets and more and
more compute. In contrast, I propose a hybrid, knowledge-driven,
reasoning-based approach, centered around cognitive models, that could provide
the substrate for a richer, more robust AI than is currently possible.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0710.3603v3,"On a Clique-Based Integer Programming Formulation of Vertex Colouring
  with Applications in Course Timetabling","Vertex colouring is a well-known problem in combinatorial optimisation, whose
alternative integer programming formulations have recently attracted
considerable attention. This paper briefly surveys seven known formulations of
vertex colouring and introduces a formulation of vertex colouring using a
suitable clique partition of the graph. This formulation is applicable in
timetabling applications, where such a clique partition of the conflict graph
is given implicitly. In contrast with some alternatives, the presented
formulation can also be easily extended to accommodate complex performance
indicators (``soft constraints'') imposed in a number of real-life course
timetabling applications. Its performance depends on the quality of the clique
partition, but encouraging empirical results for the Udine Course Timetabling
problem are reported.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9811030v1,"Generating Segment Durations in a Text-To-Speech System: A Hybrid
  Rule-Based/Neural Network Approach","A combination of a neural network with rule firing information from a
rule-based system is used to generate segment durations for a text-to-speech
system. The system shows a slight improvement in performance over a neural
network system without the rule firing information. Synthesized speech using
segment durations was accepted by listeners as having about the same quality as
speech generated using segment durations extracted from natural speech.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/0805.4211v1,Managing Critical Spreadsheets in a Compliant Environment,"The use of uncontrolled financial spreadsheets can expose organizations to
unacceptable business and compliance risks, including errors in the financial
reporting process, spreadsheet misuse and fraud, or even significant
operational errors. These risks have been well documented and thoroughly
researched. With the advent of regulatory mandates such as SOX 404 and FDICIA
in the U.S., and MiFID, Basel II and Combined Code in the UK and Europe,
leading tax and audit firms are now recommending that organizations automate
their internal controls over critical spreadsheets and other end-user computing
applications, including Microsoft Access databases. At a minimum, auditors
mandate version control, change control and access control for operational
spreadsheets, with more advanced controls for critical financial spreadsheets.
This paper summarises the key issues regarding the establishment and
maintenance of control of Business Critical spreadsheets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1
http://arxiv.org/abs/cs/0110048v1,"Multivariant Branching Prediction, Reflection, and Retrospection","In branching simulation, a novel approach to simulation presented in this
paper, a multiplicity of plausible scenarios are concurrently developed and
implemented. In conventional simulations of complex systems, there arise from
time to time uncertainties as to which of two or more alternatives are more
likely to be pursued by the system being simulated. Under these conditions the
simulationist makes a judicious choice of one of these alternatives and embeds
this choice in the simulation model. By contrast, in the branching approach,
two or more of such alternatives (or branches) are included in the model and
implemented for concurrent computer solution. The theoretical foundations for
branching simulation as a computational process are in the domains of
alternating Turing machines, molecular computing, and E-machines. Branching
simulations constitute the development of diagrams of scenarios representing
significant, alternative flows of events. Logical means for interpretation and
investigation of the branching simulation and prediction are provided by the
logical theories of possible worlds, which have been formalized by the
construction of logical varieties. Under certain conditions, the branching
approach can considerably enhance the efficiency of computer simulations and
provide more complete insights into the interpretation of predictions based on
simulations. As an example, the concepts developed in this paper have been
applied to a simulation task that plays an important role in radiology - the
noninvasive treatment of brain aneurysms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0007001v1,Constraint Exploration and Envelope of Simulation Trajectories,"The implicit theory that a simulation represents is precisely not in the
individual choices but rather in the 'envelope' of possible trajectories - what
is important is the shape of the whole envelope. Typically a huge amount of
computation is required when experimenting with factors bearing on the dynamics
of a simulation to tease out what affects the shape of this envelope. In this
paper we present a methodology aimed at systematically exploring this envelope.
We propose a method for searching for tendencies and proving their necessity
relative to a range of parameterisations of the model and agents' choices, and
to the logic of the simulation language. The exploration consists of a forward
chaining generation of the trajectories associated to and constrained by such a
range of parameterisations and choices. Additionally, we propose a
computational procedure that helps implement this exploration by translating a
Multi Agent System simulation into a constraint-based search over possible
trajectories by 'compiling' the simulation rules into a more specific form,
namely by partitioning the simulation rules using appropriate modularity in the
simulation. An example of this procedure is exhibited.
  Keywords: Constraint Search, Constraint Logic Programming, Proof, Emergence,
Tendencies",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.09662v3,Attention Augmented ConvLSTM for Environment Prediction,"Safe and proactive planning in robotic systems generally requires accurate
predictions of the environment. Prior work on environment prediction applied
video frame prediction techniques to bird's-eye view environment
representations, such as occupancy grids. ConvLSTM-based frameworks used
previously often result in significant blurring and vanishing of moving
objects, thus hindering their applicability for use in safety-critical
applications. In this work, we propose two extensions to the ConvLSTM to
address these issues. We present the Temporal Attention Augmented ConvLSTM
(TAAConvLSTM) and Self-Attention Augmented ConvLSTM (SAAConvLSTM) frameworks
for spatiotemporal occupancy prediction, and demonstrate improved performance
over baseline architectures on the real-world KITTI and Waymo datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.01226v1,"Parameterized complexity of Bandwidth of Caterpillars and Weighted Path
  Emulation","In this paper, we show that Bandwidth is hard for the complexity class $W[t]$
for all $t\in {\bf N}$, even for caterpillars with hair length at most three.
As intermediate problem, we introduce the Weighted Path Emulation problem:
given a vertex-weighted path $P_N$ and integer $M$, decide if there exists a
mapping of the vertices of $P_N$ to a path $P_M$, such that adjacent vertices
are mapped to adjacent or equal vertices, and such that the total weight of the
image of a vertex from $P_M$ equals an integer $c$. We show that {\sc Weighted
Path Emulation}, with $c$ as parameter, is hard for $W[t]$ for all $t\in {\bf
N}$, and is strongly NP-complete. We also show that Directed Bandwidth is hard
for $W[t]$ for all $t\in {\bf N}$, for directed acyclic graphs whose underlying
undirected graph is a caterpillar.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.08980v4,Sampling Polynomial Trajectories for LTL Verification,"This paper concerns the verification of continuous-time polynomial spline
trajectories against linear temporal logic specifications (LTL without 'next').
Each atomic proposition is assumed to represent a state space region described
by a multivariate polynomial inequality. The proposed approach samples a
trajectory strategically, to capture every one of its region transitions. This
yields a discrete word called a trace, which is amenable to established formal
methods for path checking. The original continuous-time trajectory is shown to
satisfy the specification if and only if its trace does. General topological
conditions on the sample points are derived that ensure a trace is recorded for
arbitrary continuous paths, given arbitrary region descriptions. Using
techniques from computer algebra, a trace generation algorithm is developed to
satisfy these conditions when the path and region boundaries are defined by
polynomials. The proposed PolyTrace algorithm has polynomial complexity in the
number of atomic propositions, and is guaranteed to produce a trace of any
polynomial path. Its performance is demonstrated via numerical examples and a
case study from robotics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1006.5845v1,Dynamic and Transparent Analysis of Commodity Production Systems,"We propose a framework that provides a programming interface to perform
complex dynamic system-level analyses of deployed production systems. By
leveraging hardware support for virtualization available nowadays on all
commodity machines, our framework is completely transparent to the system under
analysis and it guarantees isolation of the analysis tools running on its top.
Thus, the internals of the kernel of the running system needs not to be
modified and the whole platform runs unaware of the framework. Moreover, errors
in the analysis tools do not affect the running system and the framework. This
is accomplished by installing a minimalistic virtual machine monitor and
migrating the system, as it runs, into a virtual machine. In order to
demonstrate the potentials of our framework we developed an interactive kernel
debugger, nicknamed HyperDbg. HyperDbg can be used to debug any critical kernel
component, and even to single step the execution of exception and interrupt
handlers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1706.00034v1,Improved Algorithms for MST and Metric-TSP Interdiction,"We consider the {\em MST-interdiction} problem: given a multigraph $G = (V,
E)$, edge weights $\{w_e\geq 0\}_{e \in E}$, interdiction costs $\{c_e\geq
0\}_{e \in E}$, and an interdiction budget $B\geq 0$, the goal is to remove a
set $R\subseteq E$ of edges of total interdiction cost at most $B$ so as to
maximize the $w$-weight of an MST of $G-R:=(V,E\setminus R)$.
  Our main result is a $4$-approximation algorithm for this problem. This
improves upon the previous-best $14$-approximation~\cite{Zenklusen15}. Notably,
our analysis is also significantly simpler and cleaner than the one
in~\cite{Zenklusen15}. Whereas~\cite{Zenklusen15} uses a greedy algorithm with
an involved analysis to extract a good interdiction set from an over-budget
set, we utilize a generalization of knapsack called the {\em tree knapsack
problem} that nicely captures the key combinatorial aspects of this ""extraction
problem."" We prove a simple, yet strong, LP-relative approximation bound for
tree knapsack, which leads to our improved guarantees for MST interdiction. Our
algorithm and analysis are nearly tight, as we show that one cannot achieve an
approximation ratio better than 3 relative to the upper bound used in our
analysis (and the one in~\cite{Zenklusen15}).
  Our guarantee for MST-interdiction yields an $8$-approximation for {\em
metric-TSP interdiction} (improving over the $28$-approximation
in~\cite{Zenklusen15}). We also show that the {\em maximum-spanning-tree
interdiction} problem is at least as hard to approximate as the minimization
version of densest-$k$-subgraph.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.06536v1,Lets Make A Story Measuring MR Child Engagement,"We present the result of a pilot study measuring child engagement with the
Lets Make A Story system, a novel mixed reality, MR, collaborative storytelling
system designed for grandparents and grandchildren. We compare our MR
experience against an equivalent paper story experience. The goal of our pilot
was to test the system with actual child users and assess the goodness of using
metrics of time, user generated story content and facial expression analysis as
metrics of child engagement. We find that multiple confounding variables make
these metrics problematic including attribution of engagement time, spontaneous
non-story related conversation and having the childs full forward face
continuously in view during the story. We present our platform and experiences
and our finding that the strongest metric was user comments in the
post-experiential interview.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.11186v1,Logic Programming and Machine Ethics,"Transparency is a key requirement for ethical machines. Verified ethical
behavior is not enough to establish justified trust in autonomous intelligent
agents: it needs to be supported by the ability to explain decisions. Logic
Programming (LP) has a great potential for developing such perspective ethical
systems, as in fact logic rules are easily comprehensible by humans.
Furthermore, LP is able to model causality, which is crucial for ethical
decision making.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/cs/0411042v1,An Empirical Analysis of Internet Protocol Version 6 (IPv6),"Although the current Internet Protocol known as IPv4 has served its purpose
for over 20 years, its days are numbered. With IPv6 reaching a mature enough
level, there is a need to evaluate the performance benefits or drawbacks that
the new IPv6 protocol will have in comparison to the well established IPv4
protocol. Theoretically, the overhead between the two different protocols
should be directly proportional to the difference in the packet's header size,
however according to our findings, the empirical performance difference between
IPv4 and IPv6, especially when the transition mechanisms are taken into
consideration, is much larger than anticipated. We first examine the
performance of each protocol independently. We then examined two transition
mechanisms which perform the encapsulation at various points in the network:
host-to-host and router-to-router (tunneling). Our experiments were conducted
using two dual stack (IPv4/IPv6) routers using end nodes running both Windows
2000 and Solaris 8.0 in order to compare two different IPv6 implementations
side by side. Our tests were written in C++ and utilized metrics such as
latency, throughput, CPU utilization, socket creation time, socket connection
time, web server simulation, and a video client/server application for TCP/UDP
in IPv4/IPv6 under both Windows 2000 and Solaris 8.0. Our empirical evaluation
proved that IPv6 is not yet a mature enough technology and that it is still
years away from having consistent and good enough implementations, as the
performance of IPv6 in many cases proved to be significantly worse than IPv4.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9809049v1,Aspects of Evolutionary Design by Computers,"This paper examines the four main types of Evolutionary Design by computers:
Evolutionary Design Optimisation, Evolutionary Art, Evolutionary Artificial
Life Forms and Creative Evolutionary Design. Definitions for all four areas are
provided. A review of current work in each of these areas is given, with
examples of the types of applications that have been tackled. The different
properties and requirements of each are examined. Descriptions of typical
representations and evolutionary algorithms are provided and examples of
designs evolved using these techniques are shown. The paper then discusses how
the boundaries of these areas are beginning to merge, resulting in four new
'overlapping' types of Evolutionary Design: Integral Evolutionary Design,
Artificial Life Based Evolutionary Design, Aesthetic Evolutionary AL and
Aesthetic Evolutionary Design. Finally, the last part of the paper discusses
some common problems faced by creators of Evolutionary Design systems,
including: interdependent elements in designs, epistasis, and constraint
handling.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0409030v1,Automatic Generation of CHR Constraint Solvers,"In this paper, we present a framework for automatic generation of CHR solvers
given the logical specification of the constraints. This approach takes
advantage of the power of tabled resolution for constraint logic programming,
in order to check the validity of the rules. Compared to previous works where
different methods for automatic generation of constraint solvers have been
proposed, our approach enables the generation of more expressive rules (even
recursive and splitting rules) that can be used directly as CHR solvers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1909.12256v1,Circuit equivalence in 2-nilpotent algebras,"The circuit equivalence problem of a finite algebra $\mathbf A$ is the
computational problem of deciding whether two circuits over $\mathbf A$ define
the same function or not. This problem not just generalises the equivalence
problem for Boolean circuits, but is also of high interest in universal
algebra, as it models the problems of checking identities in $\mathbf A$. In
this paper we discuss the complexity for algebras from congruence modular
varieties. A partial classification was already given by Idziak and
Krzaczkowski, leaving essentially only a gap for nilpotent but not
supernilpotent algebras. We start a systematic study of this open case, proving
that the circuit equivalence problem is in P for $2$-nilpotent such algebras.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0610128v3,"Hierarchical Bin Buffering: Online Local Moments for Dynamic External
  Memory Arrays","Local moments are used for local regression, to compute statistical measures
such as sums, averages, and standard deviations, and to approximate probability
distributions. We consider the case where the data source is a very large I/O
array of size n and we want to compute the first N local moments, for some
constant N. Without precomputation, this requires O(n) time. We develop a
sequence of algorithms of increasing sophistication that use precomputation and
additional buffer space to speed up queries. The simpler algorithms partition
the I/O array into consecutive ranges called bins, and they are applicable not
only to local-moment queries, but also to algebraic queries (MAX, AVERAGE, SUM,
etc.). With N buffers of size sqrt{n}, time complexity drops to O(sqrt n). A
more sophisticated approach uses hierarchical buffering and has a logarithmic
time complexity (O(b log_b n)), when using N hierarchical buffers of size n/b.
Using Overlapped Bin Buffering, we show that only a single buffer is needed, as
with wavelet-based algorithms, but using much less storage. Applications exist
in multidimensional and statistical databases over massive data sets,
interactive image processing, and visualization.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.08816v1,Answering Regular Path Queries Over SQ Ontologies,"We study query answering in the description logic $\mathcal{SQ}$ supporting
qualified number restrictions on both transitive and non-transitive roles. Our
main contributions are a tree-like model property for $\mathcal{SQ}$ knowledge
bases and, building upon this, an optimal automata-based algorithm for
answering positive existential regular path queries in 2ExpTime.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.04198v1,Sequence-to-Sequence Natural Language to Humanoid Robot Sign Language,"This paper presents a study on natural language to sign language translation
with human-robot interaction application purposes. By means of the presented
methodology, the humanoid robot TEO is expected to represent Spanish sign
language automatically by converting text into movements, thanks to the
performance of neural networks. Natural language to sign language translation
presents several challenges to developers, such as the discordance between the
length of input and output data and the use of non-manual markers. Therefore,
neural networks and, consequently, sequence-to-sequence models, are selected as
a data-driven system to avoid traditional expert system approaches or temporal
dependencies limitations that lead to limited or too complex translation
systems. To achieve these objectives, it is necessary to find a way to perform
human skeleton acquisition in order to collect the signing input data. OpenPose
and skeletonRetriever are proposed for this purpose and a 3D sensor
specification study is developed to select the best acquisition hardware.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1002.0412v1,"SIFT-based Ear Recognition by Fusion of Detected Keypoints from Color
  Similarity Slice Regions","Ear biometric is considered as one of the most reliable and invariant
biometrics characteristics in line with iris and fingerprint characteristics.
In many cases, ear biometrics can be compared with face biometrics regarding
many physiological and texture characteristics. In this paper, a robust and
efficient ear recognition system is presented, which uses Scale Invariant
Feature Transform (SIFT) as feature descriptor for structural representation of
ear images. In order to make it more robust to user authentication, only the
regions having color probabilities in a certain ranges are considered for
invariant SIFT feature extraction, where the K-L divergence is used for keeping
color consistency. Ear skin color model is formed by Gaussian mixture model and
clustering the ear color pattern using vector quantization. Finally, K-L
divergence is applied to the GMM framework for recording the color similarity
in the specified ranges by comparing color similarity between a pair of
reference model and probe ear images. After segmentation of ear images in some
color slice regions, SIFT keypoints are extracted and an augmented vector of
extracted SIFT features are created for matching, which is accomplished between
a pair of reference model and probe ear images. The proposed technique has been
tested on the IITK Ear database and the experimental results show improvements
in recognition accuracy while invariant features are extracted from color slice
regions to maintain the robustness of the system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0103022v1,"Secure, Efficient Data Transport and Replica Management for
  High-Performance Data-Intensive Computing","An emerging class of data-intensive applications involve the geographically
dispersed extraction of complex scientific information from very large
collections of measured or computed data. Such applications arise, for example,
in experimental physics, where the data in question is generated by
accelerators, and in simulation science, where the data is generated by
supercomputers. So-called Data Grids provide essential infrastructure for such
applications, much as the Internet provides essential services for applications
such as e-mail and the Web. We describe here two services that we believe are
fundamental to any Data Grid: reliable, high-speed transporet and replica
management. Our high-speed transport service, GridFTP, extends the popular FTP
protocol with new features required for Data Grid applciations, such as
striping and partial file access. Our replica management service integrates a
replica catalog with GridFTP transfers to provide for the creation,
registration, location, and management of dataset replicas. We present the
design of both services and also preliminary performance results. Our
implementations exploit security and other services provided by the Globus
Toolkit.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.12154v3,Neural Fashion Image Captioning : Accounting for Data Diversity,"Image captioning has increasingly large domains of application, and fashion
is not an exception. Having automatic item descriptions is of great interest
for fashion web platforms, sometimes hosting hundreds of thousands of images.
This paper is one of the first to tackle image captioning for fashion images.
To address dataset diversity issues, we introduced the InFashAIv1 dataset
containing almost 16.000 African fashion item images with their titles, prices,
and general descriptions. We also used the well-known DeepFashion dataset in
addition to InFashAIv1. Captions are generated using the Show and Tell model
made of CNN encoder and RNN Decoder. We showed that jointly training the model
on both datasets improves captions quality for African style fashion images,
suggesting a transfer learning from Western style data. The InFashAIv1 dataset
is released on Github to encourage works with more diversity inclusion.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0302001v5,"Many Hard Examples in Exact Phase Transitions with Application to
  Generating Hard Satisfiable Instances","This paper first analyzes the resolution complexity of two random CSP models
(i.e. Model RB/RD) for which we can establish the existence of phase
transitions and identify the threshold points exactly. By encoding CSPs into
CNF formulas, it is proved that almost all instances of Model RB/RD have no
tree-like resolution proofs of less than exponential size. Thus, we not only
introduce new families of CNF formulas hard for resolution, which is a central
task of Proof-Complexity theory, but also propose models with both many hard
instances and exact phase transitions. Then, the implications of such models
are addressed. It is shown both theoretically and experimentally that an
application of Model RB/RD might be in the generation of hard satisfiable
instances, which is not only of practical importance but also related to some
open problems in cryptography such as generating one-way functions.
Subsequently, a further theoretical support for the generation method is shown
by establishing exponential lower bounds on the complexity of solving random
satisfiable and forced satisfiable instances of RB/RD near the threshold.
Finally, conclusions are presented, as well as a detailed comparison of Model
RB/RD with the Hamiltonian cycle problem and random 3-SAT, which, respectively,
exhibit three different kinds of phase transition behavior in NP-complete
problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0406019v1,"Providing Service Guarantees in High-Speed Switching Systems with
  Feedback Output Queuing","We consider the problem of providing service guarantees in a high-speed
packet switch. As basic requirements, the switch should be scalable to high
speeds per port, a large number of ports and a large number of traffic flows
with independent guarantees. Existing scalable solutions are based on Virtual
Output Queuing, which is computationally complex when required to provide
service guarantees for a large number of flows.
  We present a novel architecture for packet switching that provides support
for such service guarantees. A cost-effective fabric with small external
speedup is combined with a feedback mechanism that enables the fabric to be
virtually lossless, thus avoiding packet drops indiscriminate of flows. Through
analysis and simulation, we show that this architecture provides accurate
support for service guarantees, has low computational complexity and is
scalable to very high port speeds.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1905.06406v2,A Development of Continuous-Time Transfer Entropy,"Transfer entropy (TE) was introduced by Schreiber in 2000 as a measurement of
the predictive capacity of one stochastic process with respect to another.
Originally stated for discrete time processes, we expand the theory in line
with recent work of Spinney, Prokopenko, and Lizier to define TE for stochastic
processes indexed over a compact interval taking values in a Polish state
space. We provide a definition for continuous time TE using the Radon-Nikodym
Theorem, random measures, and projective limits of probability spaces. As our
main result, we provide necessary and sufficient conditions to obtain this
definition as a limit of discrete time TE, as well as illustrate its
application via an example involving Poisson point processes. As a derivative
of continuous time TE, we also define the transfer entropy rate between two
processes and show that (under mild assumptions) their stationarity implies a
constant rate. We also investigate TE between homogeneous Markov jump processes
and discuss some open problems and possible future directions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.10545v2,Two-Step Meta-Learning for Time-Series Forecasting Ensemble,"Amounts of historical data collected increase and business intelligence
applicability with automatic forecasting of time series are in high demand.
While no single time series modeling method is universal to all types of
dynamics, forecasting using an ensemble of several methods is often seen as a
compromise. Instead of fixing ensemble diversity and size, we propose to
predict these aspects adaptively using meta-learning. Meta-learning here
considers two separate random forest regression models, built on 390
time-series features, to rank 22 univariate forecasting methods and recommend
ensemble size. The forecasting ensemble is consequently formed from methods
ranked as the best, and forecasts are pooled using either simple or weighted
average (with a weight corresponding to reciprocal rank). The proposed approach
was tested on 12561 micro-economic time-series (expanded to 38633 for various
forecasting horizons) of M4 competition where meta-learning outperformed Theta
and Comb benchmarks by relative forecasting errors for all data types and
horizons. Best overall results were achieved by weighted pooling with a
symmetric mean absolute percentage error of 9.21% versus 11.05% obtained using
the Theta method.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1103.4914v1,Generating contour lines using different elevation data file formats,"In terrain mapping, there are so many ways to measure and estimate the
terrain measurements like contouring, vertical profiling, hill shading,
hypsometric tinting, perspective view, etc. Here in this paper we are using the
contouring techniques to generate the contours for the different digital
elevation data like DEM, HGT, IMG etc. The elevation data is captured in dem,
hgt and img formats of the same projected area and the contour is generated
using the existing techniques and applications. The exact differences, errors
of elevation (contour) intervals, slopes and heights are analyzed and
recovered.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.00544v1,Rank Maximal Equal Contribution: a Probabilistic Social Choice Function,"When aggregating preferences of agents via voting, two desirable goals are to
incentivize agents to participate in the voting process and then identify
outcomes that are Pareto efficient. We consider participation as formalized by
Brandl, Brandt, and Hofbauer (2015) based on the stochastic dominance (SD)
relation. We formulate a new rule called RMEC (Rank Maximal Equal Contribution)
that satisfies the strongest notion of participation and is also ex post
efficient. The rule is polynomial-time computable and also satisfies many other
desirable fairness properties. The rule suggests a general approach to
achieving ex post efficiency and very strong participation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1604.04827v3,h-Index Manipulation by Undoing Merges,"The h-index is an important bibliographic measure used to assess the
performance of researchers. Dutiful researchers merge different versions of
their articles in their Google Scholar profile even though this can decrease
their h-index. In this article, we study the manipulation of the h-index by
undoing such merges. In contrast to manipulation by merging articles (van
Bevern et al. [Artif. Intel. 240:19-35, 2016]) such manipulation is harder to
detect. We present numerous results on computational complexity (from
linear-time algorithms to parameterized computational hardness results) and
empirically indicate that at least small improvements of the h-index by
splitting merged articles are unfortunately easily achievable.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.06126v1,EAGER: Embedding-Assisted Entity Resolution for Knowledge Graphs,"Entity Resolution (ER) is a constitutional part for integrating different
knowledge graphs in order to identify entities referring to the same real-world
object. A promising approach is the use of graph embeddings for ER in order to
determine the similarity of entities based on the similarity of their graph
neighborhood. The similarity computations for such embeddings translates to
calculating the distance between them in the embedding space which is
comparatively simple. However, previous work has shown that the use of graph
embeddings alone is not sufficient to achieve high ER quality. We therefore
propose a more comprehensive ER approach for knowledge graphs called EAGER
(Embedding-Assisted Knowledge Graph Entity Resolution) to flexibly utilize both
the similarity of graph embeddings and attribute values within a supervised
machine learning approach. We evaluate our approach on 23 benchmark datasets
with differently sized and structured knowledge graphs and use hypothesis tests
to ensure statistical significance of our results. Furthermore we compare our
approach with state-of-the-art ER solutions, where our approach yields
competitive results for table-oriented ER problems and shallow knowledge graphs
but much better results for deeper knowledge graphs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.03050v1,Dropout Prediction in Crowdsourcing Markets,"Crowdsourcing environments have shown promise in solving diverse tasks in
limited cost and time. This type of business model involves both the expert and
non-expert workers. Interestingly, the success of such models depends on the
volume of the total number of workers. But, the survival of the fittest
controls the stability of these workers. Here, we show that the crowd workers
who fail to win jobs successively loose interest and might dropout over time.
Therefore, dropout prediction in such environments is a promising task. In this
paper, we establish that it is possible to predict the dropouts in a
crowdsourcing market from the success rate based on the arrival pattern of
workers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.04294v1,"Using Network Analysis on Twitter Data to Identify Threats on Indonesian
  Digital Activism","In this study, we tried to see and characterize potential threats to digital
activism in the internet-active nation of Indonesia by doing network analysis
on a recent digital activism event on Twitter, which protested against a recent
law related to alcoholic beverage investment. We hoped insights from the study
can help the nation moving forward as public discourses are likely to stay
online post-COVID. From this study, we found that threats in form of hashtag
hijackings happen often in digital activism, and there were traces of a
systematic information campaign in our observed case. We also found that the
usage of bots is prevalent in and they showed significant activity, although
the extent to which they influenced the conversation needs to be followed
through more. These threats are something to think about as activism goes
increasingly digital after COVID-19 as it can imbue unwanted messages, sow
polarization, and distract the conversation from the real issue.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2111.06290v1,"Fairness, Integrity, and Privacy in a Scalable Blockchain-based
  Federated Learning System","Federated machine learning (FL) allows to collectively train models on
sensitive data as only the clients' models and not their training data need to
be shared. However, despite the attention that research on FL has drawn, the
concept still lacks broad adoption in practice. One of the key reasons is the
great challenge to implement FL systems that simultaneously achieve fairness,
integrity, and privacy preservation for all participating clients. To
contribute to solving this issue, our paper suggests a FL system that
incorporates blockchain technology, local differential privacy, and
zero-knowledge proofs. Our implementation of a proof-of-concept with multiple
linear regression illustrates that these state-of-the-art technologies can be
combined to a FL system that aligns economic incentives, trust, and
confidentiality requirements in a scalable and transparent system.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1111.6808v1,Packet flow analysis in IP networks via abstract interpretation,"Static analysis (aka offline analysis) of a model of an IP network is useful
for understanding, debugging, and verifying packet flow properties of the
network. There have been static analysis approaches proposed in the literature
for networks based on model checking as well as graph reachability. Abstract
interpretation is a method that has typically been applied to static analysis
of programs. We propose a new, abstract-interpretation based approach for
analysis of networks. We formalize our approach, mention its correctness
guarantee, and demonstrate its flexibility in addressing multiple
network-analysis problems that have been previously solved via tailor-made
approaches. Finally, we investigate an application of our analysis to a novel
problem -- inferring a high-level policy for the network -- which has been
addressed in the past only in the restricted single-router setting.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1506.01976v1,Eye-Tracking Metrics for Task-Based Supervisory Control,"Task-based, rather than vehicle-based, control architectures have been shown
to provide superior performance in certain human supervisory control missions.
These results motivate the need for the development of robust, reliable
usability metrics to aid in creating interfaces for use in this domain. To this
end, we conduct a pilot usability study of a particular task-based supervisory
control interface called the Research Environment for Supervisory Control of
Heterogenous Unmanned Vehicles (RESCHU). In particular, we explore the use of
eye-tracking metrics as an objective means of evaluating the RESCHU interface
and providing guidance in improving usability. Our main goals for this study
are to 1) better understand how eye-tracking can augment standard usability
metrics, 2) formulate initial models of operator behavior, and 3) identify
interesting areas of future research.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.07763v1,Comparative Study of Learning Outcomes for Online Learning Platforms,"Personalization and active learning are key aspects to successful learning.
These aspects are important to address in intelligent educational applications,
as they help systems to adapt and close the gap between students with varying
abilities, which becomes increasingly important in the context of online and
distance learning. We run a comparative head-to-head study of learning outcomes
for two popular online learning platforms: Platform A, which follows a
traditional model delivering content over a series of lecture videos and
multiple-choice quizzes, and Platform B, which creates a personalized learning
environment and provides problem-solving exercises and personalized feedback.
We report on the results of our study using pre- and post-assessment quizzes
with participants taking courses on an introductory data science topic on two
platforms. We observe a statistically significant increase in the learning
outcomes on Platform B, highlighting the impact of well-designed and
well-engineered technology supporting active learning and problem-based
learning in online education. Moreover, the results of the self-assessment
questionnaire, where participants reported on perceived learning gains, suggest
that participants using Platform B improve their metacognition.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/cs/0206015v1,"Japanese/English Cross-Language Information Retrieval: Exploration of
  Query Translation and Transliteration","Cross-language information retrieval (CLIR), where queries and documents are
in different languages, has of late become one of the major topics within the
information retrieval community. This paper proposes a Japanese/English CLIR
system, where we combine a query translation and retrieval modules. We
currently target the retrieval of technical documents, and therefore the
performance of our system is highly dependent on the quality of the translation
of technical terms. However, the technical term translation is still
problematic in that technical terms are often compound words, and thus new
terms are progressively created by combining existing base words. In addition,
Japanese often represents loanwords based on its special phonogram.
Consequently, existing dictionaries find it difficult to achieve sufficient
coverage. To counter the first problem, we produce a Japanese/English
dictionary for base words, and translate compound words on a word-by-word
basis. We also use a probabilistic method to resolve translation ambiguity. For
the second problem, we use a transliteration method, which corresponds words
unlisted in the base word dictionary to their phonetic equivalents in the
target language. We evaluate our system using a test collection for CLIR, and
show that both the compound word translation and transliteration methods
improve the system performance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.11061v1,Visualization of Deep Transfer Learning In SAR Imagery,"Synthetic Aperture Radar (SAR) imagery has diverse applications in land and
marine surveillance. Unlike electro-optical (EO) systems, these systems are not
affected by weather conditions and can be used in the day and night times. With
the growing importance of SAR imagery, it would be desirable if models trained
on widely available EO datasets can also be used for SAR images. In this work,
we consider transfer learning to leverage deep features from a network trained
on an EO ships dataset and generate predictions on SAR imagery. Furthermore, by
exploring the network activations in the form of class-activation maps (CAMs),
we visualize the transfer learning process to SAR imagery and gain insight on
how a deep network interprets a new modality.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0909.1102v1,Branching-time model checking of one-counter processes,"One-counter processes (OCPs) are pushdown processes which operate only on a
unary stack alphabet. We study the computational complexity of model checking
computation tree logic (CTL) over OCPs. A PSPACE upper bound is inherited from
the modal mu-calculus for this problem. First, we analyze the periodic
behaviour of CTL over OCPs and derive a model checking algorithm whose running
time is exponential only in the number of control locations and a syntactic
notion of the formula that we call leftward until depth. Thus, model checking
fixed OCPs against CTL formulas with a fixed leftward until depth is in P. This
generalizes a result of the first author, Mayr, and To for the expression
complexity of CTL's fragment EF. Second, we prove that already over some fixed
OCP, CTL model checking is PSPACE-hard. Third, we show that there already
exists a fixed CTL formula for which model checking of OCPs is PSPACE-hard. To
obtain the latter result, we employ two results from complexity theory: (i)
Converting a natural number in Chinese remainder presentation into binary
presentation is in logspace-uniform NC^1 and (ii) PSPACE is AC^0-serializable.
We demonstrate that our approach can be used to obtain further results. We show
that model-checking CTL's fragment EF over OCPs is hard for P^NP, thus
establishing a matching lower bound and answering an open question of the first
author, Mayr, and To. We moreover show that the following problem is hard for
PSPACE: Given a one-counter Markov decision process, a set of target states
with counter value zero each, and an initial state, to decide whether the
probability that the initial state will eventually reach one of the target
states is arbitrarily close to 1. This improves a previously known lower bound
for every level of the Boolean hierarchy by Brazdil et al.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0109002v1,Probabilistic asynchronous pi-calculus,"We propose an extension of the asynchronous pi-calculus with a notion of
random choice. We define an operational semantics which distinguishes between
probabilistic choice, made internally by the process, and nondeterministic
choice, made externally by an adversary scheduler. This distinction will allow
us to reason about the probabilistic correctness of algorithms under certain
schedulers. We show that in this language we can solve the electoral problem,
which was proved not possible in the asynchronous $\pi$-calculus. Finally, we
show an implementation of the probabilistic asynchronous pi-calculus in a
Java-like language.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1006.1413v1,"Coinductive subtyping for abstract compilation of object-oriented
  languages into Horn formulas","In recent work we have shown how it is possible to define very precise type
systems for object-oriented languages by abstractly compiling a program into a
Horn formula f. Then type inference amounts to resolving a certain goal w.r.t.
the coinductive (that is, the greatest) Herbrand model of f.
  Type systems defined in this way are idealized, since in the most interesting
instantiations both the terms of the coinductive Herbrand universe and goal
derivations cannot be finitely represented. However, sound and quite expressive
approximations can be implemented by considering only regular terms and
derivations. In doing so, it is essential to introduce a proper subtyping
relation formalizing the notion of approximation between types.
  In this paper we study a subtyping relation on coinductive terms built on
union and object type constructors. We define an interpretation of types as set
of values induced by a quite intuitive relation of membership of values to
types, and prove that the definition of subtyping is sound w.r.t. subset
inclusion between type interpretations. The proof of soundness has allowed us
to simplify the notion of contractive derivation and to discover that the
previously given definition of subtyping did not cover all possible
representations of the empty type.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1302.6808v3,Learning Gaussian Networks,"We describe algorithms for learning Bayesian networks from a combination of
user knowledge and statistical data. The algorithms have two components: a
scoring metric and a search procedure. The scoring metric takes a network
structure, statistical data, and a user's prior knowledge, and returns a score
proportional to the posterior probability of the network structure given the
data. The search procedure generates networks for evaluation by the scoring
metric. Previous work has concentrated on metrics for domains containing only
discrete variables, under the assumption that data represents a multinomial
sample. In this paper, we extend this work, developing scoring metrics for
domains containing all continuous variables or a mixture of discrete and
continuous variables, under the assumption that continuous data is sampled from
a multivariate normal distribution. Our work extends traditional statistical
approaches for identifying vanishing regression coefficients in that we
identify two important assumptions, called event equivalence and parameter
modularity, that when combined allow the construction of prior distributions
for multivariate normal parameters from a single prior Bayesian network
specified by a user.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.11218v2,"A novel structure preserving semi-implicit finite volume method for
  viscous and resistive magnetohydrodynamics","In this work we introduce a novel semi-implicit structure-preserving
finite-volume/finite-difference scheme for the viscous and resistive equations
of magnetohydrodynamics (MHD) based on an appropriate 3-split of the governing
PDE system, which is decomposed into a first convective subsystem, a second
subsystem involving the coupling of the velocity field with the magnetic field
and a third subsystem involving the pressure-velocity coupling. The nonlinear
convective terms are discretized explicitly, while the remaining two subsystems
accounting for the Alfven waves and the magneto-acoustic waves are treated
implicitly. The final algorithm is at least formally constrained only by a mild
CFL stability condition depending on the velocity field of the pure
hydrodynamic convection. To preserve the divergence-free constraint of the
magnetic field exactly at the discrete level, a proper set of overlapping dual
meshes is employed. The resulting linear algebraic systems are shown to be
symmetric and therefore can be solved by means of an efficient standard
matrix-free conjugate gradient algorithm. One of the peculiarities of the
presented algorithm is that the magnetic field is defined on the edges of the
main grid, while the electric field is on the faces. The final scheme can be
regarded as a novel shock-capturing, conservative and structure preserving
semi-implicit scheme for the nonlinear viscous and resistive MHD equations.
Several numerical tests are presented to show the main features of our novel
solver: linear-stability in the sense of Lyapunov is verified at a prescribed
constant equilibrium solution; a 2nd-order of convergence is numerically
estimated; shock-capturing capabilities are proven against a standard set of
stringent MHD shock-problems; accuracy and robustness are verified against a
nontrivial set of 2- and 3-dimensional MHD problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1604.06961v1,"Sparse p-Adic Data Coding for Computationally Efficient and Effective
  Big Data Analytics","We develop the theory and practical implementation of p-adic sparse coding of
data. Rather than the standard, sparsifying criterion that uses the $L_0$
pseudo-norm, we use the p-adic norm. We require that the hierarchy or tree be
node-ranked, as is standard practice in agglomerative and other hierarchical
clustering, but not necessarily with decision trees. In order to structure the
data, all computational processing operations are direct reading of the data,
or are bounded by a constant number of direct readings of the data, implying
linear computational time. Through p-adic sparse data coding, efficient storage
results, and for bounded p-adic norm stored data, search and retrieval are
constant time operations. Examples show the effectiveness of this new approach
to content-driven encoding and displaying of data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1508.04403v1,"Synthesizing and tuning chemical reaction networks with specified
  behaviours","We consider how to generate chemical reaction networks (CRNs) from functional
specifications. We propose a two-stage approach that combines synthesis by
satisfiability modulo theories and Markov chain Monte Carlo based optimisation.
First, we identify candidate CRNs that have the possibility to produce correct
computations for a given finite set of inputs. We then optimise the reaction
rates of each CRN using a combination of stochastic search techniques applied
to the chemical master equation, simultaneously improving the of correct
behaviour and ruling out spurious solutions. In addition, we use techniques
from continuous time Markov chain theory to study the expected termination time
for each CRN. We illustrate our approach by identifying CRNs for majority
decision-making and division computation, which includes the identification of
both known and unknown networks.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1710.06339v1,Understanding the Correlation Gap for Matchings,"Given a set of vertices $V$ with $|V| = n$, a weight vector $w \in
(\mathbb{R}^+ \cup \{ 0 \})^{\binom{V}{2}}$, and a probability vector $x \in
[0, 1]^{\binom{V}{2}}$ in the matching polytope, we study the quantity
$\frac{E_{G}[ \nu_w(G)]}{\sum_{(u, v) \in \binom{V}{2}} w_{u, v} x_{u, v}}$
where $G$ is a random graph where each edge $e$ with weight $w_e$ appears with
probability $x_e$ independently, and let $\nu_w(G)$ denotes the weight of the
maximum matching of $G$. This quantity is closely related to correlation gap
and contention resolution schemes, which are important tools in the design of
approximation algorithms, algorithmic game theory, and stochastic optimization.
  We provide lower bounds for the above quantity for general and bipartite
graphs, and for weighted and unweighted settings. he best known upper bound is
$0.54$ by Karp and Sipser, and the best lower bound is $0.4$. We show that it
is at least $0.47$ for unweighted bipartite graphs, at least $0.45$ for
weighted bipartite graphs, and at lea st $0.43$ for weighted general graphs. To
achieve our results, we construct local distribution schemes on the dual which
may be of independent interest.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0806.4221v1,Localized Spanners for Wireless Networks,"We present a new efficient localized algorithm to construct, for any given
quasi-unit disk graph G=(V,E) and any e > 0, a (1+e)-spanner for G of maximum
degree O(1) and total weight O(w(MST)), where w(MST) denotes the weight of a
minimum spanning tree for V. We further show that similar localized techniques
can be used to construct, for a given unit disk graph G = (V, E), a planar
Cdel(1+e)(1+pi/2)-spanner for G of maximum degree O(1) and total weight
O(w(MST)). Here Cdel denotes the stretch factor of the unit Delaunay
triangulation for V. Both constructions can be completed in O(1) communication
rounds, and require each node to know its own coordinates.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0609081v1,Recurrence relations and fast algorithms,"We construct fast algorithms for evaluating transforms associated with
families of functions which satisfy recurrence relations. These include
algorithms both for computing the coefficients in linear combinations of the
functions, given the values of these linear combinations at certain points,
and, vice versa, for evaluating such linear combinations at those points, given
the coefficients in the linear combinations; such procedures are also known as
analysis and synthesis of series of certain special functions. The algorithms
of the present paper are efficient in the sense that their computational costs
are proportional to n (ln n) (ln(1/epsilon))^3, where n is the amount of input
and output data, and epsilon is the precision of computations. Stated somewhat
more precisely, we find a positive real number C such that, for any positive
integer n > 10, the algorithms require at most C n (ln n) (ln(1/epsilon))^3
floating-point operations and words of memory to evaluate at n appropriately
chosen points any linear combination of n special functions, given the
coefficients in the linear combination, where epsilon is the precision of
computations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.00891v1,"Towards Fine-grained Image Classification with Generative Adversarial
  Networks and Facial Landmark Detection","Fine-grained classification remains a challenging task because distinguishing
categories needs learning complex and local differences. Diversity in the pose,
scale, and position of objects in an image makes the problem even more
difficult. Although the recent Vision Transformer models achieve high
performance, they need an extensive volume of input data. To encounter this
problem, we made the best use of GAN-based data augmentation to generate extra
dataset instances. Oxford-IIIT Pets was our dataset of choice for this
experiment. It consists of 37 breeds of cats and dogs with variations in scale,
poses, and lighting, which intensifies the difficulty of the classification
task. Furthermore, we enhanced the performance of the recent Generative
Adversarial Network (GAN), StyleGAN2-ADA model to generate more realistic
images while preventing overfitting to the training set. We did this by
training a customized version of MobileNetV2 to predict animal facial
landmarks; then, we cropped images accordingly. Lastly, we combined the
synthetic images with the original dataset and compared our proposed method
with standard GANs augmentation and no augmentation with different subsets of
training data. We validated our work by evaluating the accuracy of fine-grained
image classification on the recent Vision Transformer (ViT) Model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1610.02995v1,Extrapolation and learning equations,"In classical machine learning, regression is treated as a black box process
of identifying a suitable function from a hypothesis set without attempting to
gain insight into the mechanism connecting inputs and outputs. In the natural
sciences, however, finding an interpretable function for a phenomenon is the
prime goal as it allows to understand and generalize results. This paper
proposes a novel type of function learning network, called equation learner
(EQL), that can learn analytical expressions and is able to extrapolate to
unseen domains. It is implemented as an end-to-end differentiable feed-forward
network and allows for efficient gradient based training. Due to sparsity
regularization concise interpretable expressions can be obtained. Often the
true underlying source expression is identified.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0906.0052v1,A Minimum Description Length Approach to Multitask Feature Selection,"Many regression problems involve not one but several response variables
(y's). Often the responses are suspected to share a common underlying
structure, in which case it may be advantageous to share information across
them; this is known as multitask learning. As a special case, we can use
multiple responses to better identify shared predictive features -- a project
we might call multitask feature selection.
  This thesis is organized as follows. Section 1 introduces feature selection
for regression, focusing on ell_0 regularization methods and their
interpretation within a Minimum Description Length (MDL) framework. Section 2
proposes a novel extension of MDL feature selection to the multitask setting.
The approach, called the ""Multiple Inclusion Criterion"" (MIC), is designed to
borrow information across regression tasks by more easily selecting features
that are associated with multiple responses. We show in experiments on
synthetic and real biological data sets that MIC can reduce prediction error in
settings where features are at least partially shared across responses. Section
3 surveys hypothesis testing by regression with a single response, focusing on
the parallel between the standard Bonferroni correction and an MDL approach.
Mirroring the ideas in Section 2, Section 4 proposes a novel MIC approach to
hypothesis testing with multiple responses and shows that on synthetic data
with significant sharing of features across responses, MIC sometimes
outperforms standard FDR-controlling methods in terms of finding true positives
for a given level of false positives. Section 5 concludes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.05964v1,"CCS: It's not Fair! Fair Schedulers cannot be implemented in CCS-like
  languages even under progress and certain fairness assumptions","In the process algebra community it is sometimes suggested that, on some
level of abstraction, any distributed system can be modelled in standard
process-algebraic specification formalisms like CCS. This sentiment is
strengthened by results testifying that CCS, like many similar formalisms, is
Turing powerful and provides a mechanism for interaction. This paper counters
that sentiment by presenting a simple fair scheduler---one that in suitable
variations occurs in many distributed systems---of which no implementation can
be expressed in CCS, unless CCS is enriched with a fairness assumption.
  Since Dekker's and Peterson's mutual exclusion protocols implement fair
schedulers, it follows that these protocols cannot be rendered correctly in CCS
without imposing a fairness assumption. Peterson expressed this algorithm
correctly in pseudocode without resorting to a fairness assumption, so it
furthermore follows that CCS lacks the expressive power to accurately capture
such pseudocode.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0801.2618v1,Survey of Technologies for Web Application Development,"Web-based application developers face a dizzying array of platforms,
languages, frameworks and technical artifacts to choose from. We survey,
classify, and compare technologies supporting Web application development. The
classification is based on (1) foundational technologies; (2)integration with
other information sources; and (3) dynamic content generation. We further
survey and classify software engineering techniques and tools that have been
adopted from traditional programming into Web programming. We conclude that,
although the infrastructure problems of the Web have largely been solved, the
cacophony of technologies for Web-based applications reflects the lack of a
solid model tailored for this domain.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1403.6025v4,Web-Based Visualization of Very Large Scientific Astronomy Imagery,"Visualizing and navigating through large astronomy images from a remote
location with current astronomy display tools can be a frustrating experience
in terms of speed and ergonomics, especially on mobile devices. In this paper,
we present a high performance, versatile and robust client-server system for
remote visualization and analysis of extremely large scientific images.
Applications of this work include survey image quality control, interactive
data query and exploration, citizen science, as well as public outreach. The
proposed software is entirely open source and is designed to be generic and
applicable to a variety of datasets. It provides access to floating point data
at terabyte scales, with the ability to precisely adjust image settings in
real-time. The proposed clients are light-weight, platform-independent web
applications built on standard HTML5 web technologies and compatible with both
touch and mouse-based devices. We put the system to the test and assess the
performance of the system and show that a single server can comfortably handle
more than a hundred simultaneous users accessing full precision 32 bit
astronomy data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1804.02929v1,First Experiments with a Flexible Infrastructure for Normative Reasoning,"A flexible infrastructure for normative reasoning is outlined. A small-scale
demonstrator version of the envisioned system has been implemented in the proof
assistant Isabelle/HOL by utilising the first authors universal logical
reasoning approach based on shallow semantical embeddings in meta-logic HOL.
The need for such a flexible reasoning infrastructure is motivated and
illustrated with a contrary-to-duty example scenario selected from the General
Data Protection Regulation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.01039v3,Improving reference mining in patents with BERT,"In this paper we address the challenge of extracting scientific references
from patents. We approach the problem as a sequence labelling task and
investigate the merits of BERT models to the extraction of these long
sequences. References in patents to scientific literature are relevant to study
the connection between science and industry. Most prior work only uses the
front-page citations for this analysis, which are provided in the metadata of
patent archives. In this paper we build on prior work using Conditional Random
Fields (CRF) and Flair for reference extraction. We improve the quality of the
training data and train three BERT-based models on the labelled data (BERT,
bioBERT, sciBERT). We find that the improved training data leads to a large
improvement in the quality of the trained models. In addition, the BERT models
beat CRF and Flair, with recall scores around 97% obtained with cross
validation. With the best model we label a large collection of 33 thousand
patents, extract the citations, and match them to publications in the Web of
Science database. We extract 50% more references than with the old training
data and methods: 735 thousand references in total. With these
patent-publication links, follow-up research will further analyze which types
of scientific work lead to inventions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.04317v4,Machine Learning Approaches For Motor Learning: A Short Review,"Machine learning approaches have seen considerable applications in human
movement modeling, but remain limited for motor learning. Motor learning
requires accounting for motor variability, and poses new challenges as the
algorithms need to be able to differentiate between new movements and variation
of known ones. In this short review, we outline existing machine learning
models for motor learning and their adaptation capabilities. We identify and
describe three types of adaptation: Parameter adaptation in probabilistic
models, Transfer and meta-learning in deep neural networks, and Planning
adaptation by reinforcement learning. To conclude, we discuss challenges for
applying these models in the domain of motor learning support systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1404.7456v1,Automatic Differentiation of Algorithms for Machine Learning,"Automatic differentiation---the mechanical transformation of numeric computer
programs to calculate derivatives efficiently and accurately---dates to the
origin of the computer age. Reverse mode automatic differentiation both
antedates and generalizes the method of backwards propagation of errors used in
machine learning. Despite this, practitioners in a variety of fields, including
machine learning, have been little influenced by automatic differentiation, and
make scant use of available tools. Here we review the technique of automatic
differentiation, describe its two main modes, and explain how it can benefit
machine learning practitioners. To reach the widest possible audience our
treatment assumes only elementary differential calculus, and does not assume
any knowledge of linear algebra.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1307.2467v1,The Irreducible Spine(s) of Undirected Networks,"Using closure concepts, we show that within every undirected network, or
graph, there is a unique irreducible subgraph which we call its ""spine"". The
chordless cycles which comprise this irreducible core effectively characterize
the connectivity structure of the network as a whole. In particular, it is
shown that the center of the network, whether defined by distance or
betweenness centrality, is effectively contained in this spine. By counting the
number of cycles of length 3 <= k <= max_length, we can also create a kind of
signature that can be used to identify the network. Performance is analyzed,
and the concepts we develop are illurstrated by means of a relatively small
running sample network of about 400 nodes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0912.4226v2,Computing Least Fixed Points of Probabilistic Systems of Polynomials,"We study systems of equations of the form X1 = f1(X1, ..., Xn), ..., Xn =
fn(X1, ..., Xn), where each fi is a polynomial with nonnegative coefficients
that add up to 1. The least nonnegative solution, say mu, of such equation
systems is central to problems from various areas, like physics, biology,
computational linguistics and probabilistic program verification. We give a
simple and strongly polynomial algorithm to decide whether mu=(1, ..., 1)
holds. Furthermore, we present an algorithm that computes reliable sequences of
lower and upper bounds on mu, converging linearly to mu. Our algorithm has
these features despite using inexact arithmetic for efficiency. We report on
experiments that show the performance of our algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.11836v2,"A Novel Framework for Spatio-Temporal Prediction of Environmental Data
  Using Deep Learning","As the role played by statistical and computational sciences in climate and
environmental modelling and prediction becomes more important, Machine Learning
researchers are becoming more aware of the relevance of their work to help
tackle the climate crisis. Indeed, being universal nonlinear function
approximation tools, Machine Learning algorithms are efficient in analysing and
modelling spatially and temporally variable environmental data. While Deep
Learning models have proved to be able to capture spatial, temporal, and
spatio-temporal dependencies through their automatic feature representation
learning, the problem of the interpolation of continuous spatio-temporal fields
measured on a set of irregular points in space is still under-investigated. To
fill this gap, we introduce here a framework for spatio-temporal prediction of
climate and environmental data using deep learning. Specifically, we show how
spatio-temporal processes can be decomposed in terms of a sum of products of
temporally referenced basis functions, and of stochastic spatial coefficients
which can be spatially modelled and mapped on a regular grid, allowing the
reconstruction of the complete spatio-temporal signal. Applications on two case
studies based on simulated and real-world data will show the effectiveness of
the proposed framework in modelling coherent spatio-temporal fields.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.05029v2,"Adversarial examples attack based on random warm restart mechanism and
  improved Nesterov momentum","The deep learning algorithm has achieved great success in the field of
computer vision, but some studies have pointed out that the deep learning model
is vulnerable to attacks adversarial examples and makes false decisions. This
challenges the further development of deep learning, and urges researchers to
pay more attention to the relationship between adversarial examples attacks and
deep learning security. This work focuses on adversarial examples, optimizes
the generation of adversarial examples from the view of adversarial robustness,
takes the perturbations added in adversarial examples as the optimization
parameter. We propose RWR-NM-PGD attack algorithm based on random warm restart
mechanism and improved Nesterov momentum from the view of gradient
optimization. The algorithm introduces improved Nesterov momentum, using its
characteristics of accelerating convergence and improving gradient update
direction in optimization algorithm to accelerate the generation of adversarial
examples. In addition, the random warm restart mechanism is used for
optimization, and the projected gradient descent algorithm is used to limit the
range of the generated perturbations in each warm restart, which can obtain
better attack effect. Experiments on two public datasets show that the
algorithm proposed in this work can improve the success rate of attacking deep
learning models without extra time cost. Compared with the benchmark attack
method, the algorithm proposed in this work can achieve better attack success
rate for both normal training model and defense model. Our method has average
attack success rate of 46.3077%, which is 27.19% higher than I-FGSM and 9.27%
higher than PGD. The attack results in 13 defense models show that the attack
algorithm proposed in this work is superior to the benchmark algorithm in
attack universality and transferability.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1909.07433v4,"KRNC: New Foundations for Permissionless Byzantine Consensus and Global
  Monetary Stability","This paper applies biomimetic engineering to the problem of permissionless
Byzantine consensus and achieves results that surpass the prior state of the
art by four orders of magnitude. It introduces a biologically inspired
asymmetric Sybil-resistance mechanism, Proof-of-Balance, which can replace
symmetric Proof-of-Work and Proof-of-Stake weighting schemes.
  The biomimetic mechanism is incorporated into a permissionless blockchain
protocol, Key Retroactivity Network Consensus (""KRNC""), which delivers ~40,000
times the security and speed of today's decentralized ledgers. KRNC allows the
fiat money that the public already owns to be upgraded with cryptographic
inflation protection, eliminating the problems inherent in bootstrapping new
currencies like Bitcoin and Ethereum.
  The paper includes two independently significant contributions to the
literature. First, it replaces the non-structural axioms invoked in prior work
with a new formal method for reasoning about trust, liveness, and safety from
first principles. Second, it demonstrates how two previously overlooked
exploits, book-prize attacks and pseudo-transfer attacks, collectively
undermine the security guarantees of all prior permissionless ledgers.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0
http://arxiv.org/abs/cs/0605075v1,"On the Capacity and Mutual Information of Memoryless Noncoherent
  Rayleigh-Fading Channels","The memoryless noncoherent single-input single-output (SISO) Rayleigh-fading
channel is considered. Closed-form expressions for the mutual information
between the output and the input of this channel when the input magnitude
distribution is discrete and restricted to having two mass points are derived,
and it is subsequently shown how these expressions can be used to obtain
closed-form expressions for the capacity of this channel for signal to noise
ratio (SNR) values of up to approximately 0 dB, and a tight capacity lower
bound for SNR values between 0 dB and 10 dB. The expressions for the channel
capacity and its lower bound are given as functions of a parameter which can be
obtained via numerical root-finding algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1506.02082v2,"A Real-time Cargo Damage Management System via a Sorting Array
  Triangulation Technique","This report covers an intelligent decision support system (IDSS), which
handles an efficient and effective way to rapidly inspect containerized cargos
for defection. Defection is either cargo exposure to radiation, physical
damages such as holes, punctured surfaces, iron surface oxidation, etc. The
system uses a sorting array triangulation technique (SAT) and surface damage
detection (SDD) to conduct the inspection. This new technique saves time and
money on finding damaged goods during transportation such that, instead of
running $n$ inspections on $n$ containers, only 3 inspections per triangulation
or a ratio of $3:n$ is required, assuming $n > 3$ containers. The damaged stack
in the array is virtually detected contiguous to an actually-damaged cargo by
calculating nearby distances of such cargos, delivering reliable estimates for
the whole local stack population. The estimated values on damaged, somewhat
damaged and undamaged cargo stacks, are listed and profiled after being sorted
by the program, thereby submitted to the manager for a final decision. The
report describes the problem domain and the implementation of the simulator
prototype, showing how the system operates via software, hardware with/without
human agents, conducting real-time inspections and management per se.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.04090v1,"Model Checking the Logic of Allen's Relations Meets and Started-by is
  $P^NP$-Complete","In the plethora of fragments of Halpern and Shoham's modal logic of time
intervals (HS), the logic AB of Allen's relations Meets and Started-by is at a
central position. Statements that may be true at certain intervals, but at no
sub-interval of them, such as accomplishments, as well as metric constraints
about the length of intervals, that force, for instance, an interval to be at
least (resp., at most, exactly) k points long, can be expressed in AB.
Moreover, over the linear order of the natural numbers N, it subsumes the
(point-based) logic LTL, as it can easily encode the next and until modalities.
Finally, it is expressive enough to capture the {\omega}-regular languages,
that is, for each {\omega}-regular expression R there exists an AB formula
{\phi} such that the language defined by R coincides with the set of models of
{\phi} over N. It has been shown that the satisfiability problem for AB over N
is EXPSPACE-complete. Here we prove that, under the homogeneity assumption, its
model checking problem is {\Delta}^p_2 = P^NP-complete (for the sake of
comparison, the model checking problem for full HS is EXPSPACE-hard, and the
only known decision procedure is nonelementary). Moreover, we show that the
modality for the Allen relation Met-by can be added to AB at no extra cost
(AA'B is P^NP-complete as well).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.13603v1,Integrating LHCb workflows on HPC resources: status and strategies,"High Performance Computing (HPC) supercomputers are expected to play an
increasingly important role in HEP computing in the coming years. While HPC
resources are not necessarily the optimal fit for HEP workflows, computing time
at HPC centers on an opportunistic basis has already been available to the LHC
experiments for some time, and it is also possible that part of the pledged
computing resources will be offered as CPU time allocations at HPC centers in
the future. The integration of the experiment workflows to make the most
efficient use of HPC resources is therefore essential. This paper describes the
work that has been necessary to integrate LHCb workflows at a specific HPC
site, the Marconi-A2 system at CINECA in Italy, where LHCb benefited from a
joint PRACE (Partnership for Advanced Computing in Europe) allocation with the
other Large Hadron Collider (LHC) experiments. This has required addressing two
types of challenges: on the software application workloads, for optimising
their performance on a many-core hardware architecture that differs
significantly from those traditionally used in WLCG (Worldwide LHC Computing
Grid), by reducing memory footprint using a multi-process approach; and in the
distributed computing area, for submitting these workloads using more than one
logical processor per job, which had never been done yet in LHCb.",0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1010.3133v2,"Probabilistic cellular automata, invariant measures, and perfect
  sampling","A probabilistic cellular automaton (PCA) can be viewed as a Markov chain. The
cells are updated synchronously and independently, according to a distribution
depending on a finite neighborhood. We investigate the ergodicity of this
Markov chain. A classical cellular automaton is a particular case of PCA. For a
1-dimensional cellular automaton, we prove that ergodicity is equivalent to
nilpotency, and is therefore undecidable. We then propose an efficient perfect
sampling algorithm for the invariant measure of an ergodic PCA. Our algorithm
does not assume any monotonicity property of the local rule. It is based on a
bounding process which is shown to be also a PCA. Last, we focus on the PCA
Majority, whose asymptotic behavior is unknown, and perform numerical
experiments using the perfect sampling procedure.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.01367v1,"""Alexa, Can I Program You?"": Student Perceptions of Conversational
  Artificial Intelligence Before and After Programming Alexa","Growing up in an artificial intelligence-filled world, with Siri and Amazon
Alexa often within arm's - or speech's - reach, could have significant impact
on children. Conversational agents could influence how students
anthropomorphize computer systems or develop a theory of mind. Previous
research has explored how conversational agents are used and perceived by
children within and outside of learning contexts. This study investigates how
middle and high school students' perceptions of Alexa change through
programming their own conversational agents in week-long AI education
workshops. Specifically, we investigate the workshops' influence on student
perceptions of Alexa's intelligence, friendliness, aliveness, safeness,
trustworthiness, human-likeness, and feelings of closeness. We found that
students felt Alexa was more intelligent and felt closer to Alexa after the
workshops. We also found strong correlations between students' perceptions of
Alexa's friendliness and trustworthiness, and safeness and trustworthiness.
Finally, we explored how students tended to more frequently use computer
science-related diction and ideas after the workshops. Based on our findings,
we recommend designers carefully consider personification, transparency,
playfulness and utility when designing CAs for learning contexts.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1310.0530v2,On the group-theoretic structure of lifted filter banks,"The polyphase-with-advance matrix representations of whole-sample symmetric
(WS) unimodular filter banks form a multiplicative matrix Laurent polynomial
group. Elements of this group can always be factored into lifting matrices with
half-sample symmetric (HS) off-diagonal lifting filters; such linear phase
lifting factorizations are specified in the ISO/IEC JPEG 2000 image coding
standard. Half-sample symmetric unimodular filter banks do not form a group,
but such filter banks can be partially factored into a cascade of whole-sample
antisymmetric (WA) lifting matrices starting from a concentric, equal-length HS
base filter bank. An algebraic framework called a group lifting structure has
been introduced to formalize the group-theoretic aspects of matrix lifting
factorizations. Despite their pronounced differences, it has been shown that
the group lifting structures for both the WS and HS classes satisfy a polyphase
order-increasing property that implies uniqueness (""modulo rescaling"") of
irreducible group lifting factorizations in both group lifting structures.
These unique factorization results can in turn be used to characterize the
group-theoretic structure of the groups generated by the WS and HS group
lifting structures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1512.02379v2,Crossing Number is Hard for Kernelization,"The graph crossing number problem, cr(G)<=k, asks for a drawing of a graph G
in the plane with at most k edge crossings. Although this problem is in general
notoriously difficult, it is fixed- parameter tractable for the parameter k
[Grohe]. This suggests a closely related question of whether this problem has a
polynomial kernel, meaning whether every instance of cr(G)<=k can be in
polynomial time reduced to an equivalent instance of size polynomial in k (and
independent of |G|). We answer this question in the negative. Along the proof
we show that the tile crossing number problem of twisted planar tiles is
NP-hard, which has been an open problem for some time, too, and then employ the
complexity technique of cross-composition. Our result holds already for the
special case of graphs obtained from planar graphs by adding one edge.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.07650v2,"Altruist: Argumentative Explanations through Local Interpretations of
  Predictive Models","Explainable AI is an emerging field providing solutions for acquiring
insights into automated systems' rationale. It has been put on the AI map by
suggesting ways to tackle key ethical and societal issues. Existing explanation
techniques are often not comprehensible to the end user. Lack of evaluation and
selection criteria also makes it difficult for the end user to choose the most
suitable technique. In this study, we combine logic-based argumentation with
Interpretable Machine Learning, introducing a preliminary meta-explanation
methodology that identifies the truthful parts of feature importance oriented
interpretations. This approach, in addition to being used as a meta-explanation
technique, can be used as an evaluation or selection tool for multiple feature
importance techniques. Experimentation strongly indicates that an ensemble of
multiple interpretation techniques yields considerably more truthful
explanations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.04958v1,"Hallmarks of Human-Machine Collaboration: A framework for assessment in
  the DARPA Communicating with Computers Program","There is a growing desire to create computer systems that can communicate
effectively to collaborate with humans on complex, open-ended activities.
Assessing these systems presents significant challenges. We describe a
framework for evaluating systems engaged in open-ended complex scenarios where
evaluators do not have the luxury of comparing performance to a single right
answer. This framework has been used to evaluate human-machine creative
collaborations across story and music generation, interactive block building,
and exploration of molecular mechanisms in cancer. These activities are
fundamentally different from the more constrained tasks performed by most
contemporary personal assistants as they are generally open-ended, with no
single correct solution, and often no obvious completion criteria.
  We identified the Key Properties that must be exhibited by successful
systems. From there we identified ""Hallmarks"" of success -- capabilities and
features that evaluators can observe that would be indicative of progress
toward achieving a Key Property. In addition to being a framework for
assessment, the Key Properties and Hallmarks are intended to serve as goals in
guiding research direction.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.05864v1,Searching for Designs in-between,"The use of evolutionary methods in design and art is increasing in diversity
and popularity. Approaches to using these methods for creative production
typically focus either on optimisation or exploration. In this paper we
introduce an evolutionary system for design that combines these two approaches,
enabling users to explore landscapes of design alternatives using
design-oriented measures of fitness, along with their own aesthetic
preferences. We test our methods using a biologically-inspired generative
system capable of producing 3D objects that can be exported directly as 3D
printing toolpath instructions. For the search stage of our system we combine
the use of the CMA-ES algorithm for optimisation and linear interpolation
between generated objects for feature exploration. We investigate the system`s
capabilities by evolving highly fit artefacts and then combining them with
aesthetically interesting ones.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.04791v2,"ANTASID: A Novel Temporal Adjustment to Shannon's Index of Difficulty
  for Quantifying the Perceived Difficulty of Uncontrolled Pointing Tasks","Shannon's Index of Difficulty ($ID$), reputable for quantifying the perceived
difficulty of pointing tasks as a logarithmic relationship between
movement-amplitude ($A$) and target-width ($W$), is used for modelling the
corresponding observed movement-times ($MT_O$) in such tasks in controlled
experimental setup. However, real-life pointing tasks are both spatially and
temporally uncontrolled, being influenced by factors such as - human aspects,
subjective behavior, the context of interaction, the inherent speed-accuracy
trade-off where, emphasizing accuracy compromises speed of interaction and vice
versa, and so on. Effective target-width ($W_e$) is considered as spatial
adjustment for compensating accuracy. However, no significant adjustment exists
in the literature for compensating speed in different contexts of interaction
in these tasks. As a result, without any temporal adjustment, the true
difficulty of an uncontrolled pointing task may be inaccurately quantified
using Shannon's ID. To verify this, we propose the ANTASID (A Novel Temporal
Adjustment to Shannon's ID) formulation with detailed performance analysis. We
hypothesized a temporal adjustment factor ($t$) as a binary logarithm of
$MT_O$, compensating for speed due to contextual differences and minimizing the
non-linearity between movement-amplitude and target-width. Considering spatial
and/or temporal adjustments to ID, we conducted regression analysis using our
own and Benchmark datasets in both controlled and uncontrolled scenarios of
pointing tasks with a generic mouse.ANTASID formulation showed significantly
superior fitness values and throughput in all the scenarios while reducing the
standard error. Furthermore, the quantification of ID with ANTASID varied
significantly compared to the classical formulations of Shannon's ID,
validating the purpose of this study.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.07440v1,LitterBox: A Linter for Scratch Programs,"Creating programs with block-based programming languages like Scratch is easy
and fun. Block-based programs can nevertheless contain bugs, in particular when
learners have misconceptions about programming. Even when they do not, Scratch
code is often of low quality and contains code smells, further inhibiting
understanding, reuse, and fun. To address this problem, in this paper we
introduce LitterBox, a linter for Scratch programs. Given a program or its
public project ID, LitterBox checks the program against patterns of known bugs
and code smells. For each issue identified, LitterBox provides not only the
location in the code, but also a helpful explanation of the underlying reason
and possible misconceptions. Learners can access LitterBox through an easy to
use web interface with visual information about the errors in the block-code,
while for researchers LitterBox provides a general, open source, and extensible
framework for static analysis of Scratch programs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/0705.4442v2,World-set Decompositions: Expressiveness and Efficient Algorithms,"Uncertain information is commonplace in real-world data management scenarios.
The ability to represent large sets of possible instances (worlds) while
supporting efficient storage and processing is an important challenge in this
context. The recent formalism of world-set decompositions (WSDs) provides a
space-efficient representation for uncertain data that also supports scalable
processing. WSDs are complete for finite world-sets in that they can represent
any finite set of possible worlds. For possibly infinite world-sets, we show
that a natural generalization of WSDs precisely captures the expressive power
of c-tables. We then show that several important decision problems are
efficiently solvable on WSDs while they are NP-hard on c-tables. Finally, we
give a polynomial-time algorithm for factorizing WSDs, i.e. an efficient
algorithm for minimizing such representations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0909.4021v1,Beyond O*(2^n) in domination-type problems,"In this paper we provide algorithms faster than O*(2^n) for several
NP-complete domination-type problems. More precisely, we provide: an algorithm
for CAPACITATED DOMINATING SET that solves it in O(1.89^n), a branch-and-reduce
algorithm solving LARGEST IRREDUNDANT SET in O(1.9657^n) time and a simple
iterative-DFS algorithm for SMALLEST INCLUSION-MAXIMAL IRREDUNDANT SET that
solves it in O(1.999956^n) time.
  We also provide an exponential approximation scheme for CAPACITATED
DOMINATING SET. All algorithms require polynomial space. Despite the fact that
the discussed problems are quite similar to the DOMINATING SET problem, we are
not aware of any published algorithms solving these problems faster than the
obvious O*(2^n) solution prior to this paper.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.00173v2,"Emotion Recognition of the Singing Voice: Toward a Real-Time Analysis
  Tool for Singers","Current computational-emotion research has focused on applying acoustic
properties to analyze how emotions are perceived mathematically or used in
natural language processing machine learning models. While recent interest has
focused on analyzing emotions from the spoken voice, little experimentation has
been performed to discover how emotions are recognized in the singing voice --
both in noiseless and noisy data (i.e., data that is either inaccurate,
difficult to interpret, has corrupted/distorted/nonsense information like
actual noise sounds in this case, or has a low ratio of usable/unusable
information). Not only does this ignore the challenges of training machine
learning models on more subjective data and testing them with much noisier
data, but there is also a clear disconnect in progress between advancing the
development of convolutional neural networks and the goal of emotionally
cognizant artificial intelligence. By training a new model to include this type
of information with a rich comprehension of psycho-acoustic properties, not
only can models be trained to recognize information within extremely noisy
data, but advancement can be made toward more complex biofeedback applications
-- including creating a model which could recognize emotions given any human
information (language, breath, voice, body, posture) and be used in any
performance medium (music, speech, acting) or psychological assistance for
patients with disorders such as BPD, alexithymia, autism, among others. This
paper seeks to reflect and expand upon the findings of related research and
present a stepping-stone toward this end goal.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,1,0,1,0,1,0,0,0,0,0
http://arxiv.org/abs/1704.07716v1,System of unbiased representatives for a collection of bicolorings,"Let $\mathcal{B}$ denote a set of bicolorings of $[n]$, where each bicoloring
is a mapping of the points in $[n]$ to $\{-1,+1\}$.
  For each $B \in \mathcal{B}$, let $Y_B=(B(1),\ldots,B(n))$.
  For each $A \subseteq [n]$, let $X_A \in \{0,1\}^n$ denote the incidence
vector of $A$.
  A non-empty set $A$ is said to be an `unbiased representative' for a
bicoloring $B \in \mathcal{B}$ if $\left\langle X_A,Y_B\right\rangle =0$.
  Given a set $\mathcal{B}$ of bicolorings, we study the minimum cardinality of
a family $\mathcal{A}$ consisting of subsets of $[n]$ such that every
bicoloring in $\mathcal{B}$ has an unbiased representative in $\mathcal{A}$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.07180v1,"Small hitting-sets for tiny arithmetic circuits or: How to turn bad
  designs into good","We show that if we can design poly($s$)-time hitting-sets for
$\Sigma\wedge^a\Sigma\Pi^{O(\log s)}$ circuits of size $s$, where $a=\omega(1)$
is arbitrarily small and the number of variables, or arity $n$, is $O(\log s)$,
then we can derandomize blackbox PIT for general circuits in quasipolynomial
time. This also establishes that either E$\not\subseteq$\#P/poly or that
VP$\ne$VNP. In fact, we show that one only needs a poly($s$)-time hitting-set
against individual-degree $a'=\omega(1)$ polynomials that are computable by a
size-$s$ arity-$(\log s)$ $\Sigma\Pi\Sigma$ circuit (note: $\Pi$ fanin may be
$s$). Alternatively, we claim that, to understand VP one only needs to find
hitting-sets, for depth-$3$, that have a small parameterized complexity.
Another tiny family of interest is when we restrict the arity $n=\omega(1)$ to
be arbitrarily small. We show that if we can design poly($s,\mu(n)$)-time
hitting-sets for size-$s$ arity-$n$ $\Sigma\Pi\Sigma\wedge$ circuits
(resp.~$\Sigma\wedge^a\Sigma\Pi$), where function $\mu$ is arbitrary, then we
can solve PIT for VP in quasipoly-time, and prove the corresponding lower
bounds. Our methods are strong enough to prove a surprising {\em arity
reduction} for PIT-- to solve the general problem completely it suffices to
find a blackbox PIT with time-complexity $sd2^{O(n)}$. We give several examples
of ($\log s$)-variate circuits where a new measure (called cone-size) helps in
devising poly-time hitting-sets, but the same question for their $s$-variate
versions is open till date: For eg., diagonal depth-$3$ circuits, and in
general, models that have a {\em small} partial derivative space. We also
introduce a new concept, called cone-closed basis isolation, and provide
example models where it occurs, or can be achieved by a small shift.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1408.4364v1,"The Structure of Optimal and Near Optimal Target Sets in Consensus
  Models","We consider the problem of identifying a subset of nodes in a network that
will enable the fastest spread of information in a decentralized environment.In
a model of communication based on a random walk on an undirected graph, the
optimal set over all sets of the same or smaller cardinality minimizes the sum
of the mean first arrival times to the set by walkers starting at nodes outside
the set. The problem originates from the study of the spread of information or
consensus in a network and was introduced in this form by V.Borkar et al. in
2010. More generally, the work of A. Clark et al. in 2012 showed that
estimating the fastest rate of convergence to consensus of so-called leader
follower systems leads to a consideration of the same optimization problem.
  The set function $F$ to be minimized is supermodular and therefore the greedy
algorithm is commonly used to construct optimal sets or their approximations.
In this paper, the problem is reformulated so that the search for solutions is
restricted to optimal and near optimal subsets of the graph. We prove
sufficient conditions for the existence of a greedoid structure that contains
feasible optimal and near optimal sets. It is therefore possible we conjecture,
to search for optimal or near optimal sets by local moves in a stepwise manner
to obtain near optimal sets that are better approximations than the factor
$(1-1/e)$ degree of optimality guaranteed by the use of the greedy algorithm. A
simple example illustrates aspects of the method.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1707.03186v3,Community Discovery in Dynamic Networks: a Survey,"Networks built to model real world phenomena are characeterised by some
properties that have attracted the attention of the scientific community: (i)
they are organised according to community structure and (ii) their structure
evolves with time. Many researchers have worked on methods that can efficiently
unveil substructures in complex networks, giving birth to the field of
community discovery. A novel and challenging problem started capturing
researcher interest recently: the identification of evolving communities. To
model the evolution of a system, dynamic networks can be used: nodes and edges
are mutable and their presence, or absence, deeply impacts the community
structure that composes them. The aim of this survey is to present the
distinctive features and challenges of dynamic community discovery, and propose
a classification of published approaches. As a ""user manual"", this work
organizes state of art methodologies into a taxonomy, based on their rationale,
and their specific instanciation. Given a desired definition of network
dynamics, community characteristics and analytical needs, this survey will
support researchers to identify the set of approaches that best fit their
needs. The proposed classification could also help researchers to choose in
which direction should future research be oriented.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1202.5216v1,Identifying Discriminating Network Motifs in YouTube Spam,"Like other social media websites, YouTube is not immune from the attention of
spammers. In particular, evidence can be found of attempts to attract users to
malicious third-party websites. As this type of spam is often associated with
orchestrated campaigns, it has a discernible network signature, based on
networks derived from comments posted by users to videos. In this paper, we
examine examples of different YouTube spam campaigns of this nature, and use a
feature selection process to identify network motifs that are characteristic of
the corresponding campaign strategies. We demonstrate how these discriminating
motifs can be used as part of a network motif profiling process that tracks the
activity of spam user accounts over time, enabling the process to scale to
larger networks.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0412047v1,A Social Network for Societal-Scale Decision-Making Systems,"In societal-scale decision-making systems the collective is faced with the
problem of ensuring that the derived group decision is in accord with the
collective's intention. In modern systems, political institutions have
instatiated representative forms of decision-making to ensure that every
individual in the society has a participatory voice in the decision-making
behavior of the whole--even if only indirectly through representation. An
agent-based simulation demonstrates that in modern representative systems, as
the ratio of representatives increases, there exists an exponential decrease in
the ability for the group to behave in accord with the desires of the whole. To
remedy this issue, this paper provides a novel representative power structure
for decision-making that utilizes a social network and power distribution
algorithm to maintain the collective's perspective over varying degrees of
participation and/or ratios of representation. This work shows promise for the
future development of policy-making systems that are supported by the computer
and network infrastructure of our society.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
http://arxiv.org/abs/cs/0204026v1,Querying Databases of Annotated Speech,"Annotated speech corpora are databases consisting of signal data along with
time-aligned symbolic `transcriptions'. Such databases are typically
multidimensional, heterogeneous and dynamic. These properties present a number
of tough challenges for representation and query. The temporal nature of the
data adds an additional layer of complexity. This paper presents and harmonises
two independent efforts to model annotated speech databases, one at Macquarie
University and one at the University of Pennsylvania. Various query languages
are described, along with illustrative applications to a variety of analytical
problems. The research reported here forms a part of several ongoing projects
to develop platform-independent open-source tools for creating, browsing,
searching, querying and transforming linguistic databases, and to disseminate
large linguistic databases over the internet.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1311.2850v1,"Virtual Modules in Discrete-Event Systems: Achieving Modular
  Diagnosability","This paper deals with the problem of enforcing modular diagnosability for
discrete-event systems that don't satisfy this property by their natural
modularity. We introduce an approach to achieve this property combining
existing modules into new virtual modules. An underlining mathematical problem
is to find a partition of a set, such that the partition satisfies the required
property. The time complexity of such problem is very high. To overcome it, the
paper introduces a structural analysis of the system's modules. In the analysis
we focus on the case when the modules participate in diagnosis with their
observations, rather then the case when indistinguishable observations are
blocked due to concurrency.",0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1006.3970v2,Approximating Sparsest Cut in Graphs of Bounded Treewidth,"We give the first constant-factor approximation algorithm for Sparsest Cut
with general demands in bounded treewidth graphs. In contrast to previous
algorithms, which rely on the flow-cut gap and/or metric embeddings, our
approach exploits the Sherali-Adams hierarchy of linear programming
relaxations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0509003v1,COMODI: Architecture for a Component-Based Scientific Computing System,"The COmputational MODule Integrator (COMODI) is an initiative aiming at a
component based framework, component developer tool and component repository
for scientific computing. We identify the main ingredients to a solution that
would be sufficiently appealing to scientists and engineers to consider
alternatives to their deeply rooted programming traditions. The overall
structure of the complete solution is sketched with special emphasis on the
Component Developer Tool standing at the basis of COMODI.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.13260v1,Reachability for Updatable Timed Automata made faster and more effective,"Updatable timed automata (UTA) are extensions of classic timed automata that
allow special updates to clock variables, like x:= x - 1, x := y + 2, etc., on
transitions. Reachability for UTA is undecidable in general. Various subclasses
with decidable reachability have been studied. A generic approach to UTA
reachability consists of two phases: first, a static analysis of the automaton
is performed to compute a set of clock constraints at each state; in the second
phase, reachable sets of configurations, called zones, are enumerated. In this
work, we improve the algorithm for the static analysis. Compared to the
existing algorithm, our method computes smaller sets of constraints and
guarantees termination for more UTA, making reachability faster and more
effective. As the main application, we get an alternate proof of decidability
and a more efficient algorithm for timed automata with bounded subtraction, a
class of UTA widely used for modelling scheduling problems. We have implemented
our procedure in the tool TChecker and conducted experiments that validate the
benefits of our approach.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.10803v1,A Type Checker for a Logical Framework with Union and Intersection Types,"We present the syntax, semantics, and typing rules of Bull, a prototype
theorem prover based on the Delta-Framework, i.e. a fully-typed lambda-calculus
decorated with union and intersection types, as described in previous papers by
the authors. Bull also implements a subtyping algorithm for the Type Theory Xi
of Barbanera-Dezani-de'Liguoro. Bull has a command-line interface where the
user can declare axioms, terms, and perform computations and some basic
terminal-style features like error pretty-printing, subexpressions
highlighting, and file loading. Moreover, it can typecheck a proof or normalize
it. These terms can be incomplete, therefore the typechecking algorithm uses
unification to try to construct the missing subterms. Bull uses the syntax of
Berardi's Pure Type Systems to improve the compactness and the modularity of
the kernel. Abstract and concrete syntax are mostly aligned and similar to the
concrete syntax of Coq. Bull uses a higher-order unification algorithm for
terms, while typechecking and partial type inference are done by a
bidirectional refinement algorithm, similar to the one found in Matita and
Beluga. The refinement can be split into two parts: the essence refinement and
the typing refinement. Binders are implemented using commonly-used de Bruijn
indices. We have defined a concrete language syntax that will allow the user to
write Delta-terms. We have defined the reduction rules and an evaluator. We
have implemented from scratch a refiner which does partial typechecking and
type reconstruction. We have experimented Bull with classical examples of the
intersection and union literature, such as the ones formalized by Pfenning with
his Refinement Types in LF. We hope that this research vein could be useful to
experiment, in a proof theoretical setting, forms of polymorphism alternatives
to Girard's parametric one.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1707.09467v1,Probabilistic Model Counting with Short XORs,"The idea of counting the number of satisfying truth assignments (models) of a
formula by adding random parity constraints can be traced back to the seminal
work of Valiant and Vazirani, showing that NP is as easy as detecting unique
solutions. While theoretically sound, the random parity constraints in that
construction have the following drawback: each constraint, on average, involves
half of all variables. As a result, the branching factor associated with
searching for models that also satisfy the parity constraints quickly gets out
of hand. In this work we prove that one can work with much shorter parity
constraints and still get rigorous mathematical guarantees, especially when the
number of models is large so that many constraints need to be added. Our work
is based on the realization that the essential feature for random systems of
parity constraints to be useful in probabilistic model counting is that the
geometry of their set of solutions resembles an error-correcting code.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0009022v1,"A Comparison between Supervised Learning Algorithms for Word Sense
  Disambiguation","This paper describes a set of comparative experiments, including cross-corpus
evaluation, between five alternative algorithms for supervised Word Sense
Disambiguation (WSD), namely Naive Bayes, Exemplar-based learning, SNoW,
Decision Lists, and Boosting. Two main conclusions can be drawn: 1) The
LazyBoosting algorithm outperforms the other four state-of-the-art algorithms
in terms of accuracy and ability to tune to new domains; 2) The domain
dependence of WSD systems seems very strong and suggests that some kind of
adaptation or tuning is required for cross-corpus application.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.08095v2,"How much does a treedepth modulator help to obtain polynomial kernels
  beyond sparse graphs?","In the last years, kernelization with structural parameters has been an
active area of research within the field of parameterized complexity. As a
relevant example, Gajarsk{\`y} et al. [ESA 2013] proved that every graph
problem satisfying a property called finite integer index admits a linear
kernel on graphs of bounded expansion and an almost linear kernel on nowhere
dense graphs, parameterized by the size of a $c$-treedepth modulator, which is
a vertex set whose removal results in a graph of treedepth at most $c$, where
$c \geq 1$ is a fixed integer. The authors left as further research to
investigate this parameter on general graphs, and in particular to find
problems that, while admitting polynomial kernels on sparse graphs, behave
differently on general graphs.
  In this article we answer this question by finding two very natural such
problems: we prove that Vertex Cover admits a polynomial kernel on general
graphs for any integer $c \geq 1$, and that Dominating Set does not for any
integer $c \geq 2$ even on degenerate graphs, unless $\text{NP} \subseteq
\text{coNP}/\text{poly}$. For the positive result, we build on the techniques
of Jansen and Bodlaender [STACS 2011], and for the negative result we use a
polynomial parameter transformation for $c\geq 3$ and an OR-cross-composition
for $c = 2$. As existing results imply that Dominating Set admits a polynomial
kernel on degenerate graphs for $c = 1$, our result provides a dichotomy about
the existence of polynomial kernels for Dominating Set on degenerate graphs
with this parameter.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.04577v1,"Online Test Vector Insertion: A Concurrent Built-In Self-Testing (CBIST)
  Approach for Asynchronous Logic","Complementing concurrent checking with online testing is crucial for
preventing fault accumulation in fault-tolerant systems with long mission
times. While implementing a non-intrusive online test is cumbersome in a
synchronous environment, this task becomes even more challenging in
asynchronous designs. The latter receive increasing attention, mainly due to
their elastic timing behaviour; however the issues related with their testing
remain a key obstacle for their wide adoption.
  In this paper we present a novel approach for testing of asynchronous
circuits that leverages the redundancy present in the conventional 4-phase
protocol for implementing a fully transparent and fully concurrent test
procedure. The key idea is to use the protocol's unproductive NULL phase for
processing test vectors, thus effectively interleaving the incoming 4-phase
data stream with a test data stream in a 2-phase fashion. We present
implementation templates for the fundamental building blocks required and give
a proof-of-concept by an example application that also serves as a platform for
evaluating the overheads of our solution which turn out to be moderate.",0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1602.02377v1,"Find an Optimal Path in Static System and Dynamical System within
  Polynomial Runtime","We study an ancient problem that in a static or dynamical system, sought an
optimal path, which the context always means within an extremal condition. In
fact, through those discussions about this theme, we established a universal
essential calculated model to serve for these complex systems. Meanwhile we
utilize the sample space to character the system. These contents in this paper
would involve in several major areas including the geometry, probability, graph
algorithms and some prior approaches, which stands the ultimately subtle linear
algorithm to solve this class problem. Along with our progress, our discussion
would demonstrate more general meaning and robust character, which provides
clear ideas or notion to support our concrete applications, who work in a more
popular complex system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
http://arxiv.org/abs/1210.0962v1,Breaking Monotony with Meaning: Motivation in Crowdsourcing Markets,"We conduct the first natural field experiment to explore the relationship
between the ""meaningfulness"" of a task and worker effort. We employed about
2,500 workers from Amazon's Mechanical Turk (MTurk), an online labor market, to
label medical images. Although given an identical task, we experimentally
manipulated how the task was framed. Subjects in the meaningful treatment were
told that they were labeling tumor cells in order to assist medical
researchers, subjects in the zero-context condition (the control group) were
not told the purpose of the task, and, in stark contrast, subjects in the
shredded treatment were not given context and were additionally told that their
work would be discarded. We found that when a task was framed more
meaningfully, workers were more likely to participate. We also found that the
meaningful treatment increased the quantity of output (with an insignificant
change in quality) while the shredded treatment decreased the quality of output
(with no change in quantity). We believe these results will generalize to other
short-term labor markets. Our study also discusses MTurk as an exciting
platform for running natural field experiments in economics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0306007v1,"The first deployment of workload management services on the EU DataGrid
  Testbed: feedback on design and implementation","Application users have now been experiencing for about a year with the
standardized resource brokering services provided by the 'workload management'
package of the EU DataGrid project (WP1). Understanding, shaping and pushing
the limits of the system has provided valuable feedback on both its design and
implementation. A digest of the lessons, and ""better practices"", that were
learned, and that were applied towards the second major release of the
software, is given.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0805.0585v1,"Discrete Mathematics for Computer Science, Some Notes","These are notes on discrete mathematics for computer scientists. The
presentation is somewhat unconventional. Indeed I begin with a discussion of
the basic rules of mathematical reasoning and of the notion of proof formalized
in a natural deduction system ``a la Prawitz''. The rest of the material is
more or less traditional but I emphasize partial functions more than usual
(after all, programs may not terminate for all input) and I provide a fairly
complete account of the basic concepts of graph theory.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1903.00507v3,Superseding traditional indexes by orchestrating learning and geometry,"We design the first learned index that solves the dictionary problem with
time and space complexity provably better than classic data structures for
hierarchical memories, such as B-trees, and modern learned indexes. We call our
solution the Piecewise Geometric Model index (PGM-index) because it turns the
indexing of a sequence of keys into the coverage of a sequence of 2D-points via
linear models (i.e. segments) suitably learned to trade query time vs space
efficiency. This idea comes from some known heuristic results which we
strengthen by showing that the minimal number of such segments can be computed
via known and optimal streaming algorithms. Our index is then obtained by
recursively applying this geometric idea that guarantees a smoothed adaptation
to the ""geometric complexity"" of the input data. Finally, we propose a variant
of the index that adapts not only to the distribution of the dictionary keys
but also to their access frequencies, thus obtaining the first
distribution-aware learned index.
  The second main contribution of this paper is the proposal and study of the
concept of Multicriteria Data Structure, namely one that asks a data structure
to adapt in an automatic way to the constraints imposed by the application of
use. We show that our index is a multicriteria data structure because its
significant flexibility in storage and query time can be exploited by a
properly designed optimisation algorithm that efficiently finds its best design
setting in order to match the input constraints.
  A thorough experimental analysis shows that our index and its multicriteria
variant improve uniformly, over both time and space, classic and learned
indexes up to several orders of magnitude.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1011.0673v3,Modeling the structure and evolution of discussion cascades,"We analyze the structure and evolution of discussion cascades in four popular
websites: Slashdot, Barrapunto, Meneame and Wikipedia. Despite the big
heterogeneities between these sites, a preferential attachment (PA) model with
bias to the root can capture the temporal evolution of the observed trees and
many of their statistical properties, namely, probability distributions of the
branching factors (degrees), subtree sizes and certain correlations. The
parameters of the model are learned efficiently using a novel maximum
likelihood estimation scheme for PA and provide a figurative interpretation
about the communication habits and the resulting discussion cascades on the
four different websites.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.13159v1,"New Accurate Approximation for Average Error Probability Under
  $-$ Shadowed Fading Channel","This paper proposes new accurate approximations for average error probability
(AEP) of a communication system employing either $M$-phase-shift keying (PSK)
or differential quaternary PSK with Gray coding (GC-DQPSK) modulation schemes
over $\kappa-\mu$ shadowed fading channel. Firstly, new accurate approximations
of error probability (EP) of both modulation schemes are derived over additive
white Gaussian noise (AWGN) channel. Leveraging the trapezoidal integral
method, a tight approximate expression of symbol error probability for $M$-PSK
modulation is presented, while new upper and lower bounds for Marcum
$Q$-function of the first order (MQF), and subsequently those for bit error
probability (BER) under DQPSK scheme, are proposed. Next, these bounds are
linearly combined to propose a highly refined and accurate BER's approximation.
The key idea manifested in the decrease property of modified Bessel function
$I_{v}$, strongly related to MQF, with its argument $v$. Finally, theses
approximations are used to tackle AEP's approximation under $\kappa-\mu$
shadowed fading. Numerical results show the accuracy of the presented
approximations compared to the exact ones.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.02435v2,Cloud for Gaming,"Cloud for Gaming refers to the use of cloud computing technologies to build
large-scale gaming infrastructures, with the goal of improving scalability and
responsiveness, improve the user's experience and enable new business models.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1201.0432v1,Testing Substitutability of Weak Preferences,"In many-to-many matching models, substitutable preferences constitute the
largest domain for which a pairwise stable matching is guaranteed to exist. In
this note, we extend the recently proposed algorithm of Hatfield et al. [3] to
test substitutability of weak preferences. Interestingly, the algorithm is
faster than the algorithm of Hatfield et al. by a linear factor on the domain
of strict preferences.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1709.01879v2,Progress-Space Tradeoffs in Single-Writer Memory Implementations,"Most algorithms designed for shared-memory distributed systems assume the
single-writer multi-reader (SWMR) setting where each process is provided with a
unique register readable by all. In a system where computation is performed by
a bounded number n of processes coming from a very large (possibly unbounded)
set of potential participants, the assumption of a SWMR memory is no longer
reasonable. If only a bounded number of multi-writer multi-reader (MWMR)
registers are provided, we cannot rely on an a priori assignment of processes
to registers. In this setting, simulating SWMR memory, or equivalently,
ensuring stable writing (i.e., every written value persists in the memory), is
desirable.
  In this paper, we propose a SWMR simulation that adapts the number of MWMR
registers used to the desired progress condition. For any given k from 1 to n,
we present an algorithm that uses only n+k-1 registers to simulate a
k-lock-free SWMR memory. We also give a matching lower bound of n+1 registers
required for the case of 2-lock-freedom, which supports our conjectures that
the algorithm is space-optimal. Our lower bound holds for the strictly weaker
progress condition of 2-obstruction-freedom, which suggests that the space
complexity for k-obstruction-free and k-lock-free SWMR simulations might
coincide.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.12308v1,"IAIA-BL: A Case-based Interpretable Deep Learning Model for
  Classification of Mass Lesions in Digital Mammography","Interpretability in machine learning models is important in high-stakes
decisions, such as whether to order a biopsy based on a mammographic exam.
Mammography poses important challenges that are not present in other computer
vision tasks: datasets are small, confounding information is present, and it
can be difficult even for a radiologist to decide between watchful waiting and
biopsy based on a mammogram alone. In this work, we present a framework for
interpretable machine learning-based mammography. In addition to predicting
whether a lesion is malignant or benign, our work aims to follow the reasoning
processes of radiologists in detecting clinically relevant semantic features of
each image, such as the characteristics of the mass margins. The framework
includes a novel interpretable neural network algorithm that uses case-based
reasoning for mammography. Our algorithm can incorporate a combination of data
with whole image labelling and data with pixel-wise annotations, leading to
better accuracy and interpretability even with a small number of images. Our
interpretable models are able to highlight the classification-relevant parts of
the image, whereas other methods highlight healthy tissue and confounding
information. Our models are decision aids, rather than decision makers, aimed
at better overall human-machine collaboration. We do not observe a loss in mass
margin classification accuracy over a black box neural network trained on the
same data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1407.0645v2,Branching Bisimilarity of Normed BPA Processes is in NEXPTIME,"Branching bisimilarity on normed BPA processes was recently shown to be
decidable by Yuxi Fu (ICALP 2013) but his proof has not provided any upper
complexity bound. We present a simpler approach based on relative prime
decompositions that leads to a nondeterministic exponential-time algorithm;
this is close to the known exponential-time lower bound.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1709.03701v2,FO model checking of geometric graphs,"Over the past two decades the main focus of research into first-order (FO)
model checking algorithms has been on sparse relational structures -
culminating in the FPT algorithm by Grohe, Kreutzer and Siebertz for FO model
checking of nowhere dense classes of graphs. On contrary to that, except the
case of locally bounded clique-width only little is currently known about FO
model checking of dense classes of graphs or other structures. We study the FO
model checking problem for dense graph classes definable by geometric means
(intersection and visibility graphs). We obtain new nontrivial FPT results,
e.g., for restricted subclasses of circular-arc, circle, box, disk, and
polygon-visibility graphs. These results use the FPT algorithm by Gajarsk\'y et
al. for FO model checking of posets of bounded width. We also complement the
tractability results by related hardness reductions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.01652v1,"Interpersonal distance in VR: reactions of older adults to the presence
  of a virtual agent","The rapid development of virtual reality technology has increased its
availability and, consequently, increased the number of its possible
applications. The interest in the new medium has grown due to the entertainment
industry (games, VR experiences and movies). The number of freely available
training and therapeutic applications is also increasing. Contrary to popular
opinion, new technologies are also adopted by older adults. Creating virtual
environments tailored to the needs and capabilities of older adults requires
intense research on the behaviour of these participants in the most common
situations, towards commonly used elements of the virtual environment, in
typical sceneries. Comfortable immersion in a virtual environment is key to
achieving the impression of presence. Presence is, in turn, necessary to obtain
appropriate training, persuasive and therapeutic effects. A virtual agent (a
humanoid representation of an algorithm or artificial intelligence) is often an
element of the virtual environment interface. Maintaining an appropriate
distance to the agent is, therefore, a key parameter for the creator of the VR
experience. Older (65+) participants maintain greater distance towards an agent
(a young white male) than younger ones (25-35). It may be caused by differences
in the level of arousal, but also cultural norms. As a consequence, VR
developers are advised to use algorithms that maintain the agent at the
appropriate distance, depending on the user's age.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0004012v1,"Assisted Video Sequences Indexing : Motion Analysis Based on Interest
  Points","This work deals with content-based video indexing. Our viewpoint is
semi-automatic analysis of compressed video. We consider the possible
applications of motion analysis and moving object detection : assisting moving
object indexing, summarising videos, and allowing image and motion queries. We
propose an approach based on interest points. As first results, we test and
compare the stability of different types of interest point detectors in
compressed sequences.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1707.09109v1,"The Convergence of Least-Squares Progressive Iterative Approximation
  with Singular Iterative Matrix","Developed in [Deng and Lin, 2014], Least-Squares Progressive Iterative
Approximation (LSPIA) is an efficient iterative method for solving B-spline
curve and surface least-squares fitting systems. In [Deng and Lin 2014], it was
shown that LSPIA is convergent when the iterative matrix is nonsingular. In
this paper, we will show that LSPIA is still convergent even the iterative
matrix is singular.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1608.00501v1,"Supervised Classification of RADARSAT-2 Polarimetric Data for Different
  Land Features","The pixel percentage belonging to the user defined area that are assigned to
cluster in a confusion matrix for RADARSAT-2 over Vancouver area has been
analysed for classification. In this study, supervised Wishart and Support
Vector Machine (SVM) classifiers over RADARSAT-2 (RS2) fine quadpol mode Single
Look Complex (SLC) product data is computed and compared. In comparison with
conventional single channel or dual channel polarization, RADARSAT-2 is fully
polarimetric, making it to offer better land feature contrast for
classification operation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1210.3241v1,"Distributional Framework for Emergent Knowledge Acquisition and its
  Application to Automated Document Annotation","The paper introduces a framework for representation and acquisition of
knowledge emerging from large samples of textual data. We utilise a
tensor-based, distributional representation of simple statements extracted from
text, and show how one can use the representation to infer emergent knowledge
patterns from the textual data in an unsupervised manner. Examples of the
patterns we investigate in the paper are implicit term relationships or
conjunctive IF-THEN rules. To evaluate the practical relevance of our approach,
we apply it to annotation of life science articles with terms from MeSH (a
controlled biomedical vocabulary and thesaurus).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1909.00201v1,"Software-Defined Network-Based Vehicular Networks: A Position Paper on
  Their Modeling and Implementation","There is a strong devotion in the automotive industry to be part of a wider
progression towards the Fifth Generation (5G) era. In-vehicle integration costs
between cellular and vehicle-to-vehicle networks using Dedicated Short Range
Communication could be avoided by adopting Cellular Vehicle-to-Everything
(C-V2X) technology with the possibility to re-use the existing mobile network
infrastructure. More and more, with the emergence of Software Defined Networks,
the flexibility and the programmability of the network have not only impacted
the design of new vehicular network architectures but also the implementation
of V2X services in future intelligent transportation systems. In this paper, we
define the concepts that help evaluate software-defined-based vehicular network
systems in the literature based on their modeling and implementation schemes.
We first overview the current studies available in the literature on C-V2X
technology in support of V2X applications. We then present the different
architectures and their underlying system models for LTE-V2X communications. We
later describe the key ideas of software-defined networks and their concepts
for V2X services. Lastly, we provide a comparative analysis of existing
SDN-based vehicular network system grouped according to their modeling and
simulation concepts. We provide a discussion and highlight vehicular ad-hoc
networks' challenges handled by SDN-based vehicular networks.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1106.1957v1,"Interdefinability of defeasible logic and logic programming under the
  well-founded semantics","We provide a method of translating theories of Nute's defeasible logic into
logic programs, and a corresponding translation in the opposite direction.
Under certain natural restrictions, the conclusions of defeasible theories
under the ambiguity propagating defeasible logic ADL correspond to those of the
well-founded semantics for normal logic programs, and so it turns out that the
two formalisms are closely related. Using the same translation of logic
programs into defeasible theories, the semantics for the ambiguity blocking
defeasible logic NDL can be seen as indirectly providing an ambiguity blocking
semantics for logic programs. We also provide antimonotone operators for both
ADL and NDL, each based on the Gelfond-Lifschitz (GL) operator for logic
programs. For defeasible theories without defeaters or priorities on rules, the
operator for ADL corresponds to the GL operator and so can be seen as partially
capturing the consequences according to ADL. Similarly, the operator for NDL
captures the consequences according to NDL, though in this case no restrictions
on theories apply. Both operators can be used to define stable model semantics
for defeasible theories.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0501092v1,Multi-Vehicle Cooperative Control Using Mixed Integer Linear Programming,"We present methods to synthesize cooperative strategies for multi-vehicle
control problems using mixed integer linear programming. Complex multi-vehicle
control problems are expressed as mixed logical dynamical systems. Optimal
strategies for these systems are then solved for using mixed integer linear
programming. We motivate the methods on problems derived from an adversarial
game between two teams of robots called RoboFlag. We assume the strategy for
one team is fixed and governed by state machines. The strategy for the other
team is generated using our methods. Finally, we perform an average case
computational complexity study on our approach.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0106049v2,Recursively Undecidable Properties of NP,"We show that there cannot be any algorithm that for a given nondeterministic
polynomial-time Turing machine determinates whether or not the language
recognized by this machine belongs to P",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1209.2379v3,"Reducing the size and number of linear programs in a dynamic Grbner
  basis algorithm","The dynamic algorithm to compute a Gr\""obner basis is nearly twenty years
old, yet it seems to have arrived stillborn; aside from two initial
publications, there have been no published followups. One reason for this may
be that, at first glance, the added overhead seems to outweigh the benefit; the
algorithm must solve many linear programs with many linear constraints. This
paper describes two methods of reducing the cost substantially, answering the
problem effectively.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1507.05875v2,"Efficient Dodgson-Score Calculation Using Heuristics and Parallel
  Computing","Conflict of interest is the permanent companion of any population of agents
(computational or biological). For that reason, the ability to compromise is of
paramount importance, making voting a key element of societal mechanisms. One
of the voting procedures most often discussed in the literature and, due to its
intuitiveness, also conceptually quite appealing is Charles Dodgson's scoring
rule, basically using the respective closeness to being a Condorcet winner for
evaluating competing alternatives. In this paper, we offer insights on the
practical limits of algorithms computing the exact Dodgson scores from a number
of votes. While the problem itself is theoretically intractable, this work
proposes and analyses five different solutions which try distinct approaches to
practically solve the issue in an effective manner. Additionally, three of the
discussed procedures can be run in parallel which has the potential of
drastically reducing the problem size.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1706.09249v3,"Logics and practices of transparency and opacity in real-world
  applications of public sector machine learning","Machine learning systems are increasingly used to support public sector
decision-making across a variety of sectors. Given concerns around
accountability in these domains, and amidst accusations of intentional or
unintentional bias, there have been increased calls for transparency of these
technologies. Few, however, have considered how logics and practices concerning
transparency have been understood by those involved in the machine learning
systems already being piloted and deployed in public bodies today. This short
paper distils insights about transparency on the ground from interviews with 27
such actors, largely public servants and relevant contractors, across 5 OECD
countries. Considering transparency and opacity in relation to trust and
buy-in, better decision-making, and the avoidance of gaming, it seeks to
provide useful insights for those hoping to develop socio-technical approaches
to transparency that might be useful to practitioners on-the-ground.
  An extended, archival version of this paper is available as Veale M., Van
Kleek M., & Binns R. (2018). `Fairness and accountability design needs for
algorithmic support in high-stakes public sector decision-making' Proceedings
of the 2018 CHI Conference on Human Factors in Computing Systems (CHI'18),
http://doi.org/10.1145/3173574.3174014.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,1,0,0,0
http://arxiv.org/abs/1509.07687v1,Practical Algorithms for Linear Boolean-width,"In this paper, we give a number of new exact algorithms and heuristics to
compute linear boolean decompositions, and experimentally evaluate these
algorithms. The experimental evaluation shows that significant improvements can
be made with respect to running time without increasing the width of the
generated decompositions. We also evaluated dynamic programming algorithms on
linear boolean decompositions for several vertex subset problems. This
evaluation shows that such algorithms are often much faster (up to several
orders of magnitude) compared to theoretical worst case bounds.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.13293v1,"Public sentiment analysis and topic modeling regarding COVID-19 vaccines
  on the Reddit social media platform: A call to action for strengthening
  vaccine confidence","The COVID-19 pandemic fueled one of the most rapid vaccine developments in
history. However, misinformation spread through online social media often leads
to negative vaccine sentiment and hesitancy. To investigate COVID-19
vaccine-related discussion in social media, we conducted a sentiment analysis
and Latent Dirichlet Allocation topic modeling on textual data collected from
13 Reddit communities focusing on the COVID-19 vaccine from Dec 1, 2020, to May
15, 2021. Data were aggregated and analyzed by month to detect changes in any
sentiment and latent topics. ty analysis suggested these communities expressed
more positive sentiment than negative regarding the vaccine-related discussions
and has remained static over time. Topic modeling revealed community members
mainly focused on side effects rather than outlandish conspiracy theories.
Covid-19 vaccine-related content from 13 subreddits show that the sentiments
expressed in these communities are overall more positive than negative and have
not meaningfully changed since December 2020. Keywords indicating vaccine
hesitancy were detected throughout the LDA topic modeling. Public sentiment and
topic modeling analysis regarding vaccines could facilitate the implementation
of appropriate messaging, digital interventions, and new policies to promote
vaccine confidence.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1904.03598v1,Markov Processes in Blockchain Systems,"In this paper, we develop a more general framework of block-structured Markov
processes in the queueing study of blockchain systems, which can provide
analysis both for the stationary performance measures and for the sojourn times
of any transaction and block. Note that an original aim of this paper is to
generalize the two-stage batch-service queueing model studied in Li et al.
\cite{Li:2018} both ``from exponential to phase-type"" service times and ``from
Poisson to MAP"" transaction arrivals. In general, the MAP transaction arrivals
and the two stages of PH service times make our blockchain queue more suitable
to various practical conditions of blockchain systems with crucial random
factors, for example, the mining processes, the block-generations, the
blockchain-building and so forth. For such a more general blockchain queueing
model, we focus on two basic research aspects: (1) By using the
matrix-geometric solution, we first obtain a sufficient stable condition of the
blockchain system. Then we provide simple expressions for the average number of
transactions in the queueing waiting room, and the average number of
transactions in the block. (2) However, comparing with Li et al.
\cite{Li:2018}, analysis of the transaction-confirmation time becomes very
difficult and challenging due to the complicated blockchain structure. To
overcome the difficulties, we develop a computational technique of the first
passage times by means of both the PH distributions of infinite sizes and the
$RG$-factorizations. Finally, we hope that the methodology and results given in
this paper will open a new avenue to queueing analysis of more general
blockchain systems in practice, and can motivate a series of promising future
research on development of lockchain technologies.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1407.3361v1,Faster polynomial multiplication over finite fields,"Let p be a prime, and let M_p(n) denote the bit complexity of multiplying two
polynomials in F_p[X] of degree less than n. For n large compared to p, we
establish the bound M_p(n) = O(n log n 8^(log^* n) log p), where log^* is the
iterated logarithm. This is the first known F\""urer-type complexity bound for
F_p[X], and improves on the previously best known bound M_p(n) = O(n log n log
log n log p).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0803.2306v2,"Tableau-based decision procedures for logics of strategic ability in
  multi-agent systems","We develop an incremental tableau-based decision procedures for the
  Alternating-time temporal logic ATL and some of its variants.
  While running within the theoretically established complexity upper bound, we
claim that our tableau is practically more efficient in the average case than
other decision procedures for ATL known so far. Besides, the ease of its
adaptation to variants of ATL demonstrates the flexibility of the proposed
procedure.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1510.04183v2,"Mathematical Foundations for Designing and Development of Intelligent
  Systems of Information Analysis","This article is an attempt to combine different ways of working with sets of
objects and their classes for designing and development of artificial
intelligent systems (AIS) of analysis information, using object-oriented
programming (OOP). This paper contains analysis of basic concepts of OOP and
their relation with set theory and artificial intelligence (AI). Process of
sets and multisets creation from different sides, in particular mathematical
set theory, OOP and AI is considered. Definition of object and its properties,
homogeneous and inhomogeneous classes of objects, set of objects, multiset of
objects and constructive methods of their creation and classification are
proposed. In addition, necessity of some extension of existing OOP tools for
the purpose of practical implementation AIS of analysis information, using
proposed approach, is shown.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1105.5032v2,"The Complexity of Manipulative Attacks in Nearly Single-Peaked
  Electorates","Many electoral bribery, control, and manipulation problems (which we will
refer to in general as ""manipulative actions"" problems) are NP-hard in the
general case. It has recently been noted that many of these problems fall into
polynomial time if the electorate is single-peaked (i.e., is polarized along
some axis/issue). However, real-world electorates are not truly single-peaked.
There are usually some mavericks, and so real-world electorates tend to merely
be nearly single-peaked. This paper studies the complexity of
manipulative-action algorithms for elections over nearly single-peaked
electorates, for various notions of nearness and various election systems. We
provide instances where even one maverick jumps the manipulative-action
complexity up to $\np$-hardness, but we also provide many instances where a
reasonable number of mavericks can be tolerated without increasing the
manipulative-action complexity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1712.04202v1,"Interactive graph query language for multidimensional data in
  Collaboration Spotting visual analytics framework","Human reasoning in visual analytics of data networks relies mainly on the
quality of visual perception and the capability of interactively exploring the
data from different facets. Visual quality strongly depends on networks' size
and dimensional complexity while network exploration capability on the
intuitiveness and expressiveness of user frontends. The approach taken in this
paper aims at addressing the above by decomposing data networks into multiple
networks of smaller dimensions and building an interactive graph query language
that supports full navigation across the sub-networks. Within sub-networks of
reduced dimensionality, structural abstraction and semantic techniques can then
be used to enhance visual perception further.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1304.2983v1,Centrality of Trees for Capacitated k-Center,"There is a large discrepancy in our understanding of uncapacitated and
capacitated versions of network location problems. This is perhaps best
illustrated by the classical k-center problem: there is a simple tight
2-approximation algorithm for the uncapacitated version whereas the first
constant factor approximation algorithm for the general version with capacities
was only recently obtained by using an intricate rounding algorithm that
achieves an approximation guarantee in the hundreds.
  Our paper aims to bridge this discrepancy. For the capacitated k-center
problem, we give a simple algorithm with a clean analysis that allows us to
prove an approximation guarantee of 9. It uses the standard LP relaxation and
comes close to settling the integrality gap (after necessary preprocessing),
which is narrowed down to either 7, 8 or 9. The algorithm proceeds by first
reducing to special tree instances, and then solves such instances optimally.
Our concept of tree instances is quite versatile, and applies to natural
variants of the capacitated k-center problem for which we also obtain improved
algorithms. Finally, we give evidence to show that more powerful preprocessing
could lead to better algorithms, by giving an approximation algorithm that
beats the integrality gap for instances where all non-zero capacities are
uniform.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1306.0400v2,The RAM equivalent of P vs. RP,"One of the fundamental open questions in computational complexity is whether
the class of problems solvable by use of stochasticity under the Random
Polynomial time (RP) model is larger than the class of those solvable in
deterministic polynomial time (P). However, this question is only open for
Turing Machines, not for Random Access Machines (RAMs).
  Simon (1981) was able to show that for a sufficiently equipped Random Access
Machine, the ability to switch states nondeterministically does not entail any
computational advantage. However, in the same paper, Simon describes a
different (and arguably more natural) scenario for stochasticity under the RAM
model. According to Simon's proposal, instead of receiving a new random bit at
each execution step, the RAM program is able to execute the pseudofunction
$\textit{RAND}(y)$, which returns a uniformly distributed random integer in the
range $[0,y)$. Whether the ability to allot a random integer in this fashion is
more powerful than the ability to allot a random bit remained an open question
for the last 30 years.
  In this paper, we close Simon's open problem, by fully characterising the
class of languages recognisable in polynomial time by each of the RAMs
regarding which the question was posed. We show that for some of these,
stochasticity entails no advantage, but, more interestingly, we show that for
others it does.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.08425v1,"Repurposing of Resources: from Everyday Problem Solving through to
  Crisis Management","The human ability to repurpose objects and processes is universal, but it is
not a well-understood aspect of human intelligence. Repurposing arises in
everyday situations such as finding substitutes for missing ingredients when
cooking, or for unavailable tools when doing DIY. It also arises in critical,
unprecedented situations needing crisis management. After natural disasters and
during wartime, people must repurpose the materials and processes available to
make shelter, distribute food, etc. Repurposing is equally important in
professional life (e.g. clinicians often repurpose medicines off-license) and
in addressing societal challenges (e.g. finding new roles for waste products,).
Despite the importance of repurposing, the topic has received little academic
attention. By considering examples from a variety of domains such as every-day
activities, drug repurposing and natural disasters, we identify some principle
characteristics of the process and describe some technical challenges that
would be involved in modelling and simulating it. We consider cases of both
substitution, i.e. finding an alternative for a missing resource, and
exploitation, i.e. identifying a new role for an existing resource. We argue
that these ideas could be developed into general formal theory of repurposing,
and that this could then lead to the development of AI methods based on
commonsense reasoning, argumentation, ontological reasoning, and various
machine learning methods, to develop tools to support repurposing in practice.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1001.0251v2,Ultimate Traces of Cellular Automata,"A cellular automaton (CA) is a parallel synchronous computing model, which
consists in a juxtaposition of finite automata (cells) whose state evolves
according to that of their neighbors. Its trace is the set of infinite words
representing the sequence of states taken by some particular cell. In this
paper we study the ultimate trace of CA and partial CA (a CA restricted to a
particular subshift). The ultimate trace is the trace observed after a long
time run of the CA. We give sufficient conditions for a set of infinite words
to be the trace of some CA and prove the undecidability of all properties over
traces that are stable by ultimate coincidence.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.11606v2,Homological Scaffold via Minimal Homology Bases,"The homological scaffold leverages persistent homology to construct a
topologically sound summary of a weighted network. However, its crucial
dependency on the choice of representative cycles hinders the ability to trace
back global features onto individual network components, unless one provides a
principled way to make such a choice. In this paper, we apply recent advances
in the computation of minimal homology bases to introduce a quasi-canonical
version of the scaffold, called minimal, and employ it to analyze data both
real and in silico. At the same time, we verify that, statistically, the
standard scaffold is a good proxy of the minimal one for sufficiently complex
networks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.06769v1,"Toward the Fully Physics-Informed Echo State Network -- an ODE
  Approximator Based on Recurrent Artificial Neurons","Inspired by recent theoretical arguments, physics-informed echo state network
(ESN) is discussed on the attempt to train a reservoir model absolutely in
physics-informed manner. As the plainest work on such a purpose, an ODE
(ordinary differential equation) approximator is designed to replicate the
solution in sequence with respect to the recurrent evaluations. On the
principal invariance of differential equations, the constraint in recurrence
just takes shape to secure a proper regression method for the ESN-based ODE
approximator. After then, the actual training process is established on the
idea of two-pass strategy for regression. Aiming at the fully physics-informed
reservoir model, a couple of nonlinear dynamical problems are demonstrated as
the computations obtained from the proposed method in this study.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.07434v1,"Regulating Reward Training by Means of Certainty Prediction in a Neural
  Network-Implemented Pong Game","We present the first reinforcement-learning model to self-improve its
reward-modulated training implemented through a continuously improving
""intuition"" neural network. An agent was trained how to play the arcade video
game Pong with two reward-based alternatives, one where the paddle was placed
randomly during training, and a second where the paddle was simultaneously
trained on three additional neural networks such that it could develop a sense
of ""certainty"" as to how probable its own predicted paddle position will be to
return the ball. If the agent was less than 95% certain to return the ball, the
policy used an intuition neural network to place the paddle. We trained both
architectures for an equivalent number of epochs and tested learning
performance by letting the trained programs play against a near-perfect
opponent. Through this, we found that the reinforcement learning model that
uses an intuition neural network for placing the paddle during reward training
quickly overtakes the simple architecture in its ability to outplay the
near-perfect opponent, additionally outscoring that opponent by an increasingly
wide margin after additional epochs of training.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.00261v1,ECLARE: Extreme Classification with Label Graph Correlations,"Deep extreme classification (XC) seeks to train deep architectures that can
tag a data point with its most relevant subset of labels from an extremely
large label set. The core utility of XC comes from predicting labels that are
rarely seen during training. Such rare labels hold the key to personalized
recommendations that can delight and surprise a user. However, the large number
of rare labels and small amount of training data per rare label offer
significant statistical and computational challenges. State-of-the-art deep XC
methods attempt to remedy this by incorporating textual descriptions of labels
but do not adequately address the problem. This paper presents ECLARE, a
scalable deep learning architecture that incorporates not only label text, but
also label correlations, to offer accurate real-time predictions within a few
milliseconds. Core contributions of ECLARE include a frugal architecture and
scalable techniques to train deep models along with label correlation graphs at
the scale of millions of labels. In particular, ECLARE offers predictions that
are 2 to 14% more accurate on both publicly available benchmark datasets as
well as proprietary datasets for a related products recommendation task sourced
from the Bing search engine. Code for ECLARE is available at
https://github.com/Extreme-classification/ECLARE.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1911.00433v1,Approximate Representer Theorems in Non-reflexive Banach Spaces,"The representer theorem is one of the most important mathematical foundations
for regularised learning and kernel methods. Classical formulations of the
theorem state sufficient conditions under which a regularisation problem on a
Hilbert space admits a solution in the subspace spanned by the representers of
the data points. This turns the problem into an equivalent optimisation problem
in a finite dimensional space, making it computationally tractable. Moreover,
Banach space methods for learning have been receiving more and more attention.
Considering the representer theorem in Banach spaces is hence of increasing
importance. Recently the question of the necessary condition for a representer
theorem to hold in Hilbert spaces and certain Banach spaces has been
considered. It has been shown that a classical representer theorem cannot exist
in general in non-reflexive Banach spaces. In this paper we propose a notion of
approximate solutions and approximate representer theorem to overcome this
problem. We show that for these notions we can indeed extend the previous
results to obtain a unified theory for the existence of representer theorems in
any general Banach spaces, in particular including $l_1$-type spaces. We give a
precise characterisation when a regulariser admits a classical representer
theorem and when only an approximate representer theorem is possible.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1802.00405v2,HOL Light QE,"We are interested in algorithms that manipulate mathematical expressions in
mathematically meaningful ways. Expressions are syntactic, but most logics do
not allow one to discuss syntax. ${\rm CTT}_{\rm qe}$ is a version of Church's
type theory that includes quotation and evaluation operators, akin to quote and
eval in the Lisp programming language. Since the HOL logic is also a version of
Church's type theory, we decided to add quotation and evaluation to HOL Light
to demonstrate the implementability of ${\rm CTT}_{\rm qe}$ and the benefits of
having quotation and evaluation in a proof assistant. The resulting system is
called HOL Light QE. Here we document the design of HOL Light QE and the
challenges that needed to be overcome. The resulting implementation is freely
available.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1101.0510v1,"Good Friends, Bad News - Affect and Virality in Twitter","The link between affect, defined as the capacity for sentimental arousal on
the part of a message, and virality, defined as the probability that it be sent
along, is of significant theoretical and practical importance, e.g. for viral
marketing. A quantitative study of emailing of articles from the NY Times finds
a strong link between positive affect and virality, and, based on psychological
theories it is concluded that this relation is universally valid. The
conclusion appears to be in contrast with classic theory of diffusion in news
media emphasizing negative affect as promoting propagation. In this paper we
explore the apparent paradox in a quantitative analysis of information
diffusion on Twitter. Twitter is interesting in this context as it has been
shown to present both the characteristics social and news media. The basic
measure of virality in Twitter is the probability of retweet. Twitter is
different from email in that retweeting does not depend on pre-existing social
relations, but often occur among strangers, thus in this respect Twitter may be
more similar to traditional news media. We therefore hypothesize that negative
news content is more likely to be retweeted, while for non-news tweets positive
sentiments support virality. To test the hypothesis we analyze three corpora: A
complete sample of tweets about the COP15 climate summit, a random sample of
tweets, and a general text corpus including news. The latter allows us to train
a classifier that can distinguish tweets that carry news and non-news
information. We present evidence that negative sentiment enhances virality in
the news segment, but not in the non-news segment. We conclude that the
relation between affect and virality is more complex than expected based on the
findings of Berger and Milkman (2010), in short 'if you want to be cited: Sweet
talk your friends or serve bad news to the public'.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.01098v2,"Garbage Collection Makes Rust Easier to Use: A Randomized Controlled
  Trial of the Bronze Garbage Collector","Rust is a general-purpose programming language that is both type- and
memory-safe. Rust does not use a garbage collector, but rather achieves these
properties through a sophisticated, but complex, type system. Doing so makes
Rust very efficient, but makes Rust relatively hard to learn and use. We
designed Bronze, an optional, library-based garbage collector for Rust. To see
whether Bronze could make Rust more usable, we conducted a randomized
controlled trial with volunteers from a 633-person class, collecting data from
428 students in total. We found that for a task that required managing complex
aliasing, Bronze users were more likely to complete the task in the time
available, and those who did so required only about a third as much time (4
hours vs. 12 hours). We found no significant difference in total time, even
though Bronze users re-did the task without Bronze afterward. Surveys indicated
that ownership, borrowing, and lifetimes were primary causes of the challenges
that users faced when using Rust.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1408.5751v1,First-Class Variability Modeling in Matlab/Simulink,"Modern cars exist in an vast number of variants. Thus, variability has to be
dealt with in all phases of the development process, in particular during
model-based development of software-intensive functionality using
Matlab/Simulink. Currently, variability is often encoded within a functional
model leading to so called 150%-models which easily become very complex and do
not scale for larger product lines. To counter these problems, we propose a
modular variability modeling approach for Matlab/Simulink based on the concept
of delta modeling [8, 9, 24]. A functional variant is described by a delta
encapsulating a set of modifications. A sequence of deltas can be applied to a
core product to derive the desired variant. We present a prototypical
implementation, which is integrated into Matlab/Simulink and offers graphical
editing of delta models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1905.11125v1,On Timed Scope-bounded Context-sensitive Languages,"In (DLT 2016) we studied timed context sensitive languages characterized by
multiple stack push down automata (MPA), with an explicit bound on number of
stages where in each stage at most one stack is used (k-round MPA).
  In this paper, we continue our work on timed MPA and study a subclass in
which a symbol corresponding to a stack being pushed in it must be popped
within fixed number of contexts of that stack---scope-bounded push-down
automata with multiple stacks (k-scope MPA). We use Visibly Push-down Alphabet
and Event Clocks to show that timed k-scope MPA have decidable reachability
problem; are closed under Boolean operations; and have an equivalent logical
characterization.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1711.00201v2,Credimus,"We believe that economic design and computational complexity---while already
important to each other---should become even more important to each other with
each passing year. But for that to happen, experts in on the one hand such
areas as social choice, economics, and political science and on the other hand
computational complexity will have to better understand each other's
worldviews.
  This article, written by two complexity theorists who also work in
computational social choice theory, focuses on one direction of that process by
presenting a brief overview of how most computational complexity theorists view
the world. Although our immediate motivation is to make the lens through which
complexity theorists see the world be better understood by those in the social
sciences, we also feel that even within computer science it is very important
for nontheoreticians to understand how theoreticians think, just as it is
equally important within computer science for theoreticians to understand how
nontheoreticians think.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.02460v1,"Deterministic Backbone Creation in an SINR Network without Knowledge of
  Location","For a given network, a backbone is an overlay network consisting of a
connected dominating set with additional accessibility properties. Once a
backbone is created for a network, it can be utilized for fast communication
amongst the nodes of the network.
  The Signal-to-Interference-plus-Noise-Ratio (SINR) model has become the
standard for modeling communication among devices in wireless networks. For
this model, the community has pondered what the most realistic solutions for
communication problems in wireless networks would look like. Such solutions
would have the characteristic that they would make the least number of
assumptions about the availability of information about the participating
nodes. Solving problems when nothing at all is known about the network and
having nodes just start participating would be ideal. However, this is quite
challenging and most likely not feasible. The pragmatic approach is then to
make meaningful assumptions about the available information and present
efficient solutions based on this information.
  We present a solution for creation of backbone in the SINR model, when nodes
do not have access to their physical coordinates or the coordinates of other
nodes in the network. This restriction models the deployment of nodes in
various situations for sensing hurricanes, cyclones, and so on, where only
information about nodes prior to their deployment may be known but not their
actual locations post deployment. We assume that nodes have access to knowledge
of their label, the labels of nodes within their neighborhood, the range from
which labels are taken $[N]$ and the total number of participating nodes $n$.
We also assume that nodes wake up spontaneously. We present an efficient
deterministic protocol to create a backbone with a round complexity of
$O(\Delta \lg^2 N)$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1905.06860v1,"Speaker-Independent Speech-Driven Visual Speech Synthesis using
  Domain-Adapted Acoustic Models","Speech-driven visual speech synthesis involves mapping features extracted
from acoustic speech to the corresponding lip animation controls for a face
model. This mapping can take many forms, but a powerful approach is to use deep
neural networks (DNNs). However, a limitation is the lack of synchronized
audio, video, and depth data required to reliably train the DNNs, especially
for speaker-independent models. In this paper, we investigate adapting an
automatic speech recognition (ASR) acoustic model (AM) for the visual speech
synthesis problem. We train the AM on ten thousand hours of audio-only data.
The AM is then adapted to the visual speech synthesis domain using ninety hours
of synchronized audio-visual speech. Using a subjective assessment test, we
compared the performance of the AM-initialized DNN to one with a random
initialization. The results show that viewers significantly prefer animations
generated from the AM-initialized DNN than the ones generated using the
randomly initialized model. We conclude that visual speech synthesis can
significantly benefit from the powerful representation of speech in the ASR
acoustic models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.07991v1,Towards Refinable Choreographies,"We investigate refinement in the context of choreographies. We introduce
refinable global choreographies allowing for the underspecification of
protocols, whose interactions can be refined into actual protocols. Arbitrary
refinements may spoil well-formedness, that is the sufficient conditions that
guarantee a protocol to be implementable. We introduce a typing discipline that
enforces well-formedness of typed choreographies. Then we unveil the relation
among refinable choregraphies and their admissible refinements in terms of an
axiom scheme.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1304.3480v1,Friendship Paradox Redux: Your Friends Are More Interesting Than You,"Feld's friendship paradox states that ""your friends have more friends than
you, on average."" This paradox arises because extremely popular people, despite
being rare, are overrepresented when averaging over friends. Using a sample of
the Twitter firehose, we confirm that the friendship paradox holds for >98% of
Twitter users. Because of the directed nature of the follower graph on Twitter,
we are further able to confirm more detailed forms of the friendship paradox:
everyone you follow or who follows you has more friends and followers than you.
This is likely caused by a correlation we demonstrate between Twitter activity,
number of friends, and number of followers. In addition, we discover two new
paradoxes: the virality paradox that states ""your friends receive more viral
content than you, on average,"" and the activity paradox, which states ""your
friends are more active than you, on average."" The latter paradox is important
in regulating online communication. It may result in users having difficulty
maintaining optimal incoming information rates, because following additional
users causes the volume of incoming tweets to increase super-linearly. While
users may compensate for increased information flow by increasing their own
activity, users become information overloaded when they receive more
information than they are able or willing to process. We compare the average
size of cascades that are sent and received by overloaded and underloaded
users. And we show that overloaded users post and receive larger cascades and
they are poor detector of small cascades.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.10685v1,"MILP, pseudo-boolean, and OMT solvers for optimal fault-tolerant
  placements of relay nodes in mission critical wireless networks","In critical infrastructures like airports, much care has to be devoted in
protecting radio communication networks from external electromagnetic
interference. Protection of such mission-critical radio communication networks
is usually tackled by exploiting radiogoniometers: at least three suitably
deployed radiogoniometers, and a gateway gathering information from them,
permit to monitor and localise sources of electromagnetic emissions that are
not supposed to be present in the monitored area. Typically, radiogoniometers
are connected to the gateway through relay nodes. As a result, some degree of
fault-tolerance for the network of relay nodes is essential in order to offer a
reliable monitoring. On the other hand, deployment of relay nodes is typically
quite expensive. As a result, we have two conflicting requirements: minimise
costs while guaranteeing a given fault-tolerance. In this paper, we address the
problem of computing a deployment for relay nodes that minimises the relay node
network cost while at the same time guaranteeing proper working of the network
even when some of the relay nodes (up to a given maximum number) become faulty
(fault-tolerance). We show that, by means of a computation-intensive
pre-processing on a HPC infrastructure, the above optimisation problem can be
encoded as a 0/1 Linear Program, becoming suitable to be approached with
standard Artificial Intelligence reasoners like MILP, PB-SAT, and SMT/OMT
solvers. Our problem formulation enables us to present experimental results
comparing the performance of these three solving technologies on a real case
study of a relay node network deployment in areas of the Leonardo da Vinci
Airport in Rome, Italy.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0006033v1,"Verifying Termination and Error-Freedom of Logic Programs with block
  Declarations","We present verification methods for logic programs with delay declarations.
The verified properties are termination and freedom from errors related to
built-ins. Concerning termination, we present two approaches. The first
approach tries to eliminate the well-known problem of speculative output
bindings. The second approach is based on identifying the predicates for which
the textual position of an atom using this predicate is irrelevant with respect
to termination. Three features are distinctive of this work: it allows for
predicates to be used in several modes; it shows that block declarations, which
are a very simple delay construct, are sufficient to ensure the desired
properties; it takes the selection rule into account, assuming it to be as in
most Prolog implementations. The methods can be used to verify existing
programs and assist in writing new programs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0812.2769v2,"Geometric scaling: a simple preconditioner for certain linear systems
  with discontinuous coefficients","Linear systems with large differences between coefficients (""discontinuous
coefficients"") arise in many cases in which partial differential
equations(PDEs) model physical phenomena involving heterogeneous media. The
standard approach to solving such problems is to use domain decomposition
techniques, with domain boundaries conforming to the boundaries between the
different media. This approach can be difficult to implement when the geometry
of the domain boundaries is complicated or the grid is unstructured. This work
examines the simple preconditioning technique of scaling the equations by
dividing each equation by the Lp-norm of its coefficients. This preconditioning
is called geometric scaling (GS). It has long been known that diagonal scaling
can be useful in improving convergence, but there is no study on the general
usefulness of this approach for discontinuous coefficients. GS was tested on
several nonsymmetric linear systems with discontinuous coefficients derived
from convection-diffusion elliptic PDEs with small to moderate convection
terms. It is shown that GS improved the convergence properties of restarted
GMRES and Bi-CGSTAB, with and without the ILUT preconditioner. GS was also
shown to improve the distribution of the eigenvalues by reducing their
concentration around the origin very significantly.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.09594v1,"Multi-Modal Super Resolution for Dense Microscopic Particle Size
  Estimation","Particle Size Analysis (PSA) is an important process carried out in a number
of industries, which can significantly influence the properties of the final
product. A ubiquitous instrument for this purpose is the Optical Microscope
(OM). However, OMs are often prone to drawbacks like low resolution, small
focal depth, and edge features being masked due to diffraction. We propose a
powerful application of a combination of two Conditional Generative Adversarial
Networks (cGANs) that Super Resolve OM images to look like Scanning Electron
Microscope (SEM) images. We further demonstrate the use of a custom object
detection module that can perform efficient PSA of the super-resolved particles
on both, densely and sparsely packed images. The PSA results obtained from the
super-resolved images have been benchmarked against human annotators, and
results obtained from the corresponding SEM images. The proposed models show a
generalizable way of multi-modal image translation and super-resolution for
accurate particle size estimation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.13563v2,"An arbitrary high-order Spectral Difference method for the induction
  equation","We study in this paper three variants of the high-order Discontinuous
Galerkin (DG) method with Runge-Kutta (RK) time integration for the induction
equation, analysing their ability to preserve the divergence free constraint of
the magnetic field. To quantify divergence errors, we use a norm based on both
a surface term, measuring global divergence errors, and a volume term,
measuring local divergence errors. This leads us to design a new, arbitrary
high-order numerical scheme for the induction equation in multiple space
dimensions, based on a modification of the Spectral Difference (SD) method [1]
with ADER time integration [2]. It appears as a natural extension of the
Constrained Transport (CT) method. We show that it preserves
$\nabla\cdot\vec{B}=0$ exactly by construction, both in a local and a global
sense. We compare our new method to the 3 RKDG variants and show that the
magnetic energy evolution and the solution maps of our new SD-ADER scheme are
qualitatively similar to the RKDG variant with divergence cleaning, but without
the need for an additional equation and an extra variable to control the
divergence errors.
  [1] Liu Y., Vinokur M., Wang Z.J. (2006) Discontinuous Spectral Difference
Method for Conservation Laws on Unstructured Grids. In: Groth C., Zingg D.W.
(eds) Computational Fluid Dynamics 2004. Springer, Berlin, Heidelberg
  [2] Dumbser M., Castro M., Par\'es C., Toro E.F (2009) ADER schemes on
unstructured meshes for nonconservative hyperbolic systems: Applications to
geophysical flows. In: Computers & Fluids, Volume 38, Issue 9",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0309046v1,"The Liar and Related Paradoxes: Fuzzy Truth Value Assignment for
  Collections of Self-Referential Sentences","We study self-referential sentences of the type related to the Liar paradox.
In particular, we consider the problem of assigning consistent fuzzy truth
values to collections of self-referential sentences. We show that the problem
can be reduced to the solution of a system of nonlinear equations. Furthermore,
we prove that, under mild conditions, such a system always has a solution (i.e.
a consistent truth value assignment) and that, for a particular implementation
of logical ``and'', ``or'' and ``negation'', the ``mid-point'' solution is
always consistent. Next we turn to computational issues and present several
truth-value assignment algorithms; we argue that these algorithms can be
understood as generalized sequential reasoning. In an Appendix we present a
large number of examples of self-referential collections (including the Liar
and the Strengthened Liar), we formulate the corresponding truth value
equations and solve them analytically and/ or numerically.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1411.7158v1,Cathoristic logic: A modal logic of incompatible propositions,"Cathoristic logic is a multi-modal logic where negation is replaced by a
novel operator allowing the expression of incompatible sentences. We present
the syntax and semantics of the logic including complete proof rules, and
establish a number of results such as compactness, a semantic characterisation
of elementary equivalence, the existence of a quadratic-time decision
procedure, and Brandom's incompatibility semantics property. We demonstrate the
usefulness of the logic as a language for knowledge representation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.15571v4,Learning Sub-Patterns in Piecewise Continuous Functions,"Most stochastic gradient descent algorithms can optimize neural networks that
are sub-differentiable in their parameters; however, this implies that the
neural network's activation function must exhibit a degree of continuity which
limits the neural network model's uniform approximation capacity to continuous
functions. This paper focuses on the case where the discontinuities arise from
distinct sub-patterns, each defined on different parts of the input space. We
propose a new discontinuous deep neural network model trainable via a decoupled
two-step procedure that avoids passing gradient updates through the network's
only and strategically placed, discontinuous unit. We provide approximation
guarantees for our architecture in the space of bounded continuous functions
and universal approximation guarantees in the space of piecewise continuous
functions which we introduced herein. We present a novel semi-supervised
two-step training procedure for our discontinuous deep learning model, tailored
to its structure, and we provide theoretical support for its effectiveness. The
performance of our model and trained with the propose procedure is evaluated
experimentally on both real-world financial datasets and synthetic datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0509046v2,On the number of t-ary trees with a given path length,"We show that the number of $t$-ary trees with path length equal to $p$ is
$\exp(h(t^{-1})t\log t \frac{p}{\log p}(1+o(1)))$, where
$\entropy(x){=}{-}x\log x {-}(1{-}x)\log (1{-}x)$ is the binary entropy
function. Besides its intrinsic combinatorial interest, the question recently
arose in the context of information theory, where the number of $t$-ary trees
with path length $p$ estimates the number of universal types, or, equivalently,
the number of different possible Lempel-Ziv'78 dictionaries for sequences of
length $p$ over an alphabet of size $t$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.13348v1,Graph? Yes! Which one? Help!,"Amazon Neptune is a graph database service that supports two graph
(meta)models: W3C's Resource Description Framework (RDF) and Labeled Property
Graphs (LPG). Customers opt in for one or the other model, and this choice
determines which data modeling features can be used, and - perhaps more
importantly - which query languages are available to query and manipulate the
graph. The choice between the two technology stacks is difficult and requires
consideration of data modeling aspects, query language features, their adequacy
for current and future use cases, as well as many other factors (including
developer preferences). Sometimes we see customers make the wrong choice with
no easy way to reverse it later.
  It is therefore highly desirable that the choice of the query language can be
made without consideration of what graph model is chosen, and can be easily
revised or complemented at a later point. In this paper, we advocate and
explore the idea of a single, unified graph data model that embraces both RDF
and LPGs, and naturally supports different graph query languages on top. We
investigate obstacles towards unifying the two graph data models, and propose
an initial unifying model, dubbed ""one graph"" (""1G"" for short), as the basis
for moving forward.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1212.2284v2,The Complexity of Planar Boolean #CSP with Complex Weights,"We prove a complexity dichotomy theorem for symmetric complex-weighted
Boolean #CSP when the constraint graph of the input must be planar. The
problems that are #P-hard over general graphs but tractable over planar graphs
are precisely those with a holographic reduction to matchgates. This
generalizes a theorem of Cai, Lu, and Xia for the case of real weights. We also
obtain a dichotomy theorem for a symmetric arity 4 signature with complex
weights in the planar Holant framework, which we use in the proof of our #CSP
dichotomy. In particular, we reduce the problem of evaluating the Tutte
polynomial of a planar graph at the point (3,3) to counting the number of
Eulerian orientations over planar 4-regular graphs to show the latter is
#P-hard. This strengthens a theorem by Huang and Lu to the planar setting. Our
proof techniques combine new ideas with refinements and extensions of existing
techniques. These include planar pairings, the recursive unary construction,
the anti-gadget technique, and pinning in the Hadamard basis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1607.05597v1,Distributed Construction of Purely Additive Spanners,"This paper studies the complexity of distributed construction of purely
additive spanners in the CONGEST model. We describe algorithms for building
such spanners in several cases. Because of the need to simultaneously make
decisions at far apart locations, the algorithms use additional mechanisms
compared to their sequential counterparts.
  We complement our algorithms with a lower bound on the number of rounds
required for computing pairwise spanners. The standard reductions from
set-disjointness and equality seem unsuitable for this task because no specific
edge needs to be removed from the graph. Instead, to obtain our lower bound, we
define a new communication complexity problem that reduces to computing a
sparse spanner, and prove a lower bound on its communication complexity using
information theory. This technique significantly extends the current toolbox
used for obtaining lower bounds for the CONGEST model, and we believe it may
find additional applications.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0510095v3,Rate Region of the Quadratic Gaussian Two-Encoder Source-Coding Problem,"We determine the rate region of the quadratic Gaussian two-encoder
source-coding problem. This rate region is achieved by a simple architecture
that separates the analog and digital aspects of the compression. Furthermore,
this architecture requires higher rates to send a Gaussian source than it does
to send any other source with the same covariance. Our techniques can also be
used to determine the sum rate of some generalizations of this classical
problem. Our approach involves coupling the problem to a quadratic Gaussian
``CEO problem.''",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1212.6500v1,"Quantifier Alternation in Two-Variable First-Order Logic with Successor
  Is Decidable","We consider the quantifier alternation hierarchy within two-variable
first-order logic FO^2[<,suc] over finite words with linear order and binary
successor predicate. We give a single identity of omega-terms for each level of
this hierarchy. This shows that it is decidable for a given regular language
and a non-negative integer m, whether the language is definable by a formula in
FO^2[<,suc] which has at most m quantifier alternations. We also consider the
alternation hierarchy of unary temporal logic TL[X,F,Y,P] defined by the
maximal number of nested negations. This hierarchy coincides with the
FO^2[<,suc] alternation hierarchy.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1810.06667v2,Deep Transfer Reinforcement Learning for Text Summarization,"Deep neural networks are data hungry models and thus face difficulties when
attempting to train on small text datasets. Transfer learning is a potential
solution but their effectiveness in the text domain is not as explored as in
areas such as image analysis. In this paper, we study the problem of transfer
learning for text summarization and discuss why existing state-of-the-art
models fail to generalize well on other (unseen) datasets. We propose a
reinforcement learning framework based on a self-critic policy gradient
approach which achieves good generalization and state-of-the-art results on a
variety of datasets. Through an extensive set of experiments, we also show the
ability of our proposed framework to fine-tune the text summarization model
using only a few training samples. To the best of our knowledge, this is the
first work that studies transfer learning in text summarization and provides a
generic solution that works well on unseen data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1410.7596v1,Fast Algorithms for Online Stochastic Convex Programming,"We introduce the online stochastic Convex Programming (CP) problem, a very
general version of stochastic online problems which allows arbitrary concave
objectives and convex feasibility constraints. Many well-studied problems like
online stochastic packing and covering, online stochastic matching with concave
returns, etc. form a special case of online stochastic CP. We present fast
algorithms for these problems, which achieve near-optimal regret guarantees for
both the i.i.d. and the random permutation models of stochastic inputs. When
applied to the special case online packing, our ideas yield a simpler and
faster primal-dual algorithm for this well studied problem, which achieves the
optimal competitive ratio. Our techniques make explicit the connection of
primal-dual paradigm and online learning to online stochastic CP.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.10492v1,A Game Characterization for Contrasimilarity,"We present the first game characterization of contrasimilarity, the weakest
form of bisimilarity. The game is finite for finite-state processes and can
thus be used for contrasimulation equivalence checking, of which no tool has
been capable to date. A machine-checked Isabelle/HOL formalization backs our
work and enables further use of contrasimilarity in verification contexts.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1208.2755v1,Two-Way Finite Automata: Old and Recent Results,"The notion of two-way automata was introduced at the very beginning of
automata theory. In 1959, Rabin and Scott and, independently, Shepherdson,
proved that these models, both in the deterministic and in the nondeterministic
versions, have the same power of one-way automata, namely, they characterize
the class of regular languages.
  In 1978, Sakoda and Sipser posed the question of the cost, in the number of
the states, of the simulation of one-way and two-way nondeterministic automata
by two-way deterministic automata. They conjectured that these costs are
exponential. In spite of all attempts to solve it, this question is still open.
  In the last ten years the problem of Sakoda and Sipser was widely
reconsidered and many new results related to it have been obtained. In this
work we discuss some of them. In particular, we focus on the restriction to the
unary case and on the connections with open questions in space complexity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.11459v1,Recoloring graphs of treewidth 2,"Two (proper) colorings of a graph are adjacent if they differ on exactly one
vertex. Jerrum proved that any $(d + 2)$-coloring of any d-degenerate graph can
be transformed into any other via a sequence of adjacent colorings. A result of
Bonamy et al. ensures that a shortest transformation can have a quadratic
length even for $d = 1$. Bousquet and Perarnau proved that a linear
transformation exists for between $(2d + 2)$-colorings. It is open to determine
if this bound can be reduced. In this note, we prove that it can be reduced for
graphs of treewidth 2, which are 2-degenerate. There exists a linear
transformation between 5-colorings. It completes the picture for graphs of
treewidth 2 since there exist graphs of treewidth 2 such a linear
transformation between 4-colorings does not exist.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.05851v1,Detecting Latency Degradation Patterns in Service-based Systems,"Performance in heterogeneous service-based systems shows non-determistic
trends. Even for the same request type, latency may vary from one request to
another. These variations can occur due to several reasons on different levels
of the software stack: operating system, network, software libraries,
application code or others. Furthermore, a request may involve several Remote
Procedure Calls (RPC), where each call can be subject to performance variation.
Performance analysts inspect distributed traces and seek for recurrent patterns
in trace attributes, such as RPCs execution time, in order to cluster traces in
which variations may be induced by the same cause. Clustering ""similar"" traces
is a prerequisite for effective performance debugging. Given the scale of the
problem, such activity can be tedious and expensive. In this paper, we present
an automated approach that detects relevant RPCs execution time patterns
associated to request latency degradation, i.e. latency degradation patterns.
The presented approach is based on a genetic search algorithm driven by an
information retrieval relevance metric and an optimized fitness evaluation.
Each latency degradation pattern identifies a cluster of requests subject to
latency degradation with similar patterns in RPCs execution time. We show on a
microservice-based application case study that the proposed approach can
effectively detect clusters identified by artificially injected latency
degradation patterns. Experimental results show that our approach outperforms
in terms of F-score a state-of-art approach for latency profile analysis and
widely popular machine learning clustering algorithms. We also show how our
approach can be easily extended to trace attributes other than RPC execution
time (e.g. HTTP headers, execution node, etc.).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.09790v1,Shallow2Deep: Indoor Scene Modeling by Single Image Understanding,"Dense indoor scene modeling from 2D images has been bottlenecked due to the
absence of depth information and cluttered occlusions. We present an automatic
indoor scene modeling approach using deep features from neural networks. Given
a single RGB image, our method simultaneously recovers semantic contents, 3D
geometry and object relationship by reasoning indoor environment context.
Particularly, we design a shallow-to-deep architecture on the basis of
convolutional networks for semantic scene understanding and modeling. It
involves multi-level convolutional networks to parse indoor semantics/geometry
into non-relational and relational knowledge. Non-relational knowledge
extracted from shallow-end networks (e.g. room layout, object geometry) is fed
forward into deeper levels to parse relational semantics (e.g. support
relationship). A Relation Network is proposed to infer the support relationship
between objects. All the structured semantics and geometry above are assembled
to guide a global optimization for 3D scene modeling. Qualitative and
quantitative analysis demonstrates the feasibility of our method in
understanding and modeling semantics-enriched indoor scenes by evaluating the
performance of reconstruction accuracy, computation performance and scene
complexity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1106.4063v1,Verification of Quantum Programs,"This paper develops verification methodology for quantum programs, and the
contribution of the paper is two-fold: 1. Sharir, Pnueli and Hart [SIAM J.
Comput. 13(1984)292-314] presented a general method for proving properties of
probabilistic programs, in which a probabilistic program is modeled by a Markov
chain and an assertion on the output distribution is extended into an invariant
assertion on all intermediate distributions. Their method is essentially a
probabilistic generalization of the classical Floyd inductive assertion method.
In this paper, we consider quantum programs modeled by quantum Markov chains
which are defined by super-operators. It is shown that the Sharir-Pnueli-Hart
method can be elegantly generalized to quantum programs by exploiting the
Schr\""odinger-Heisenberg duality between quantum states and observables. In
particular, a completeness theorem for the Sharir-Pnueli-Hart verification
method of quantum programs is established. 2. As indicated by the completeness
theorem, the Sharir-Pnueli-Hart method is in principle effective for verifying
all properties of quantum programs that can be expressed in terms of Hermitian
operators (observables). But it is not feasible for many practical applications
because of the complicated calculation involved in the verification. For the
case of finite-dimensional state spaces, we find a method for verification of
quantum programs much simpler than the Sharir-Pnueli-Hart method by employing
the matrix representation of super-operators and Jordan decomposition of
matrices. In particular, this method enables us to compute easily the average
running time and even to analyze some interesting long-run behaviors of quantum
programs in a finite-dimensional state space.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0310064v1,Satisfiability and computing van der Waerden numbers,"In this paper we bring together the areas of combinatorics and propositional
satisfiability. Many combinatorial theorems establish, often constructively,
the existence of positive integer functions, without actually providing their
closed algebraic form or tight lower and upper bounds. The area of Ramsey
theory is especially rich in such results. Using the problem of computing van
der Waerden numbers as an example, we show that these problems can be
represented by parameterized propositional theories in such a way that
decisions concerning their satisfiability determine the numbers (function) in
question. We show that by using general-purpose complete and local-search
techniques for testing propositional satisfiability, this approach becomes
effective -- competitive with specialized approaches. By following it, we were
able to obtain several new results pertaining to the problem of computing van
der Waerden numbers. We also note that due to their properties, especially
their structural simplicity and computational hardness, propositional theories
that arise in this research can be of use in development, testing and
benchmarking of SAT solvers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0502087v1,Self-Replicating Strands that Self-Assemble into User-Specified Meshes,"It has been argued that a central objective of nanotechnology is to make
products inexpensively, and that self-replication is an effective approach to
very low-cost manufacturing. The research presented here is intended to be a
step towards this vision. In previous work (JohnnyVon 1.0), we simulated
machines that bonded together to form self-replicating strands. There were two
types of machines (called types 0 and 1), which enabled strands to encode
arbitrary bit strings. However, the information encoded in the strands had no
functional role in the simulation. The information was replicated without being
interpreted, which was a significant limitation for potential manufacturing
applications. In the current work (JohnnyVon 2.0), the information in a strand
is interpreted as instructions for assembling a polygonal mesh. There are now
four types of machines and the information encoded in a strand determines how
it folds. A strand may be in an unfolded state, in which the bonds are straight
(although they flex slightly due to virtual forces acting on the machines), or
in a folded state, in which the bond angles depend on the types of machines. By
choosing the sequence of machine types in a strand, the user can specify a
variety of polygonal shapes. A simulation typically begins with an initial
unfolded seed strand in a soup of unbonded machines. The seed strand replicates
by bonding with free machines in the soup. The child strands fold into the
encoded polygonal shape, and then the polygons drift together and bond to form
a mesh. We demonstrate that a variety of polygonal meshes can be manufactured
in the simulation, by simply changing the sequence of machine types in the
seed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.2494v1,A Measure of Arbitrariness in Abductive Explanations,"We study the framework of abductive logic programming extended with integrity
constraints. For this framework, we introduce a new measure of the simplicity
of an explanation based on its degree of \emph{arbitrariness}: the more
arbitrary the explanation, the less appealing it is, with explanations having
no arbitrariness - they are called constrained - being the preferred ones. In
the paper, we study basic properties of constrained explanations. For the case
when programs in abductive theories are stratified we establish results
providing a detailed picture of the complexity of the problem to decide whether
constrained explanations exist. (To appear in Theory and Practice of Logic
Programming (TPLP).)",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1409.8072v1,"Revisiting the stability of computing the roots of a quadratic
  polynomial","We show in this paper that the roots $x_1$ and $x_2$ of a scalar quadratic
polynomial $ax^2+bx+c=0$ with real or complex coefficients $a$, $b$ $c$ can be
computed in a element-wise mixed stable manner, measured in a relative sense.
We also show that this is a stronger property than norm-wise backward
stability, but weaker than element-wise backward stability. We finally show
that there does not exist any method that can compute the roots in an
element-wise backward stable sense, which is also illustrated by some numerical
experiments.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.01040v1,"Distributional Offline Continuous-Time Reinforcement Learning with
  Neural Physics-Informed PDEs (SciPhy RL for DOCTR-L)","This paper addresses distributional offline continuous-time reinforcement
learning (DOCTR-L) with stochastic policies for high-dimensional optimal
control. A soft distributional version of the classical Hamilton-Jacobi-Bellman
(HJB) equation is given by a semilinear partial differential equation (PDE).
This `soft HJB equation' can be learned from offline data without assuming that
the latter correspond to a previous optimal or near-optimal policy. A
data-driven solution of the soft HJB equation uses methods of Neural PDEs and
Physics-Informed Neural Networks developed in the field of Scientific Machine
Learning (SciML). The suggested approach, dubbed `SciPhy RL', thus reduces
DOCTR-L to solving neural PDEs from data. Our algorithm called Deep DOCTR-L
converts offline high-dimensional data into an optimal policy in one step by
reducing it to supervised learning, instead of relying on value iteration or
policy iteration methods. The method enables a computable approach to the
quality control of obtained policies in terms of both their expected returns
and uncertainties about their values.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1201.1214v6,Statistical Algorithms and a Lower Bound for Detecting Planted Clique,"We introduce a framework for proving lower bounds on computational problems
over distributions against algorithms that can be implemented using access to a
statistical query oracle. For such algorithms, access to the input distribution
is limited to obtaining an estimate of the expectation of any given function on
a sample drawn randomly from the input distribution, rather than directly
accessing samples. Most natural algorithms of interest in theory and in
practice, e.g., moments-based methods, local search, standard iterative methods
for convex optimization, MCMC and simulated annealing can be implemented in
this framework. Our framework is based on, and generalizes, the statistical
query model in learning theory (Kearns, 1998).
  Our main application is a nearly optimal lower bound on the complexity of any
statistical query algorithm for detecting planted bipartite clique
distributions (or planted dense subgraph distributions) when the planted clique
has size $O(n^{1/2-\delta})$ for any constant $\delta > 0$. The assumed
hardness of variants of these problems has been used to prove hardness of
several other problems and as a guarantee for security in cryptographic
applications. Our lower bounds provide concrete evidence of hardness, thus
supporting these assumptions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.06515v1,"Physical Activity Analysis of College Students During the COVID-19
  Pandemic Using Smartphones","Owing to the pandemic caused by the coronavirus disease of 2019 (COVID-19),
several universities have closed their campuses for preventing the spread of
infection. Consequently, the university classes are being held over the
Internet, and students attend these classes from their homes. While the
COVID-19 pandemic is expected to be prolonged, the online-centric lifestyle has
raised concerns about secondary health issues caused by reduced physical
activity (PA). However, the actual status of PA among university students has
not yet been examined in Japan. Hence, in this study, we collected daily PA
data (including the data corresponding to the number of steps taken and the
data associated with six types of activities) by employing smartphones and
thereby analyzed the changes in the PA of university students. The PA data were
collected over a period of ten weeks from 305 first-year university students
who were attending a mandatory class of physical education at the university.
The obtained results indicate that compared to the average number of steps
taken before the COVID-19 pandemic (6474.87 steps), the average number of steps
taken after the COVID-19 pandemic (3522.5 steps) has decreased by 45.6%.
Furthermore, the decrease in commuting time (7 AM to 10 AM), classroom time,
and extracurricular activity time (11 AM to 12 AM) has led to a decrease in PA
on weekdays owing to reduced unplanned exercise opportunities and has caused an
increase in the duration of being in the stationary state in the course of
daily life.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1202.6168v1,D-iteration: Evaluation of the Asynchronous Distributed Computation,"The aim of this paper is to present a first evaluation of the potential of an
asynchronous distributed computation associated to the recently proposed
approach, D-iteration: the D-iteration is a fluid diffusion based iterative
method, which has the advantage of being natively distributive. It exploits a
simple intuitive decomposition of the matrix-vector product as elementary
operations of fluid diffusion associated to a new algebraic representation. We
show through experiments on real datasets how much this approach can improve
the computation efficiency when the parallelism is applied: with the proposed
solution, when the computation is distributed over $K$ virtual machines (PIDs),
the memory size to be handled by each virtual machine decreases linearly with
$K$ and the computation speed increases almost linearly with $K$ with a slope
becoming closer to one when the number $N$ of linear equations to be solved
increases.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1710.06278v1,"Solving nonlinear circuits with pulsed excitation by multirate partial
  differential equations","In this paper the concept of Multirate Partial Differential Equations (MPDEs)
is applied to obtain an efficient solution for nonlinear low-frequency
electrical circuits with pulsed excitation. The MPDEs are solved by a Galerkin
approach and a conventional time discretization. Nonlinearities are efficiently
accounted for by neglecting the high-frequency components (ripples) of the
state variables and using only their envelope for the evaluation. It is shown
that the impact of this approximation on the solution becomes increasingly
negligible for rising frequency and leads to significant performance gains.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1909.08255v1,Towards Ethical Machines Via Logic Programming,"Autonomous intelligent agents are playing increasingly important roles in our
lives. They contain information about us and start to perform tasks on our
behalves. Chatbots are an example of such agents that need to engage in a
complex conversations with humans. Thus, we need to ensure that they behave
ethically. In this work we propose a hybrid logic-based approach for ethical
chatbots.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.07462v3,"PL-VINS: Real-Time Monocular Visual-Inertial SLAM with Point and Line
  Features","Leveraging line features to improve localization accuracy of point-based
visual-inertial SLAM (VINS) is gaining interest as they provide additional
constraints on scene structure. However, real-time performance when
incorporating line features in VINS has not been addressed. This paper presents
PL-VINS, a real-time optimization-based monocular VINS method with point and
line features, developed based on the state-of-the-art point-based VINS-Mono
\cite{vins}. We observe that current works use the LSD \cite{lsd} algorithm to
extract line features; however, LSD is designed for scene shape representation
instead of the pose estimation problem, which becomes the bottleneck for the
real-time performance due to its high computational cost. In this paper, a
modified LSD algorithm is presented by studying a hidden parameter tuning and
length rejection strategy. The modified LSD can run at least three times as
fast as LSD. Further, by representing space lines with the Pl\""{u}cker
coordinates, the residual error in line estimation is modeled in terms of the
point-to-line distance, which is then minimized by iteratively updating the
minimum four-parameter orthonormal representation of the Pl\""{u}cker
coordinates. Experiments in a public benchmark dataset show that the
localization error of our method is 12-16\% less than that of VINS-Mono at the
same pose update frequency. %For the benefit of the community, The source code
of our method is available at: https://github.com/cnqiangfu/PL-VINS.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0007023v1,Towards a query language for annotation graphs,"The multidimensional, heterogeneous, and temporal nature of speech databases
raises interesting challenges for representation and query. Recently,
annotation graphs have been proposed as a general-purpose representational
framework for speech databases. Typical queries on annotation graphs require
path expressions similar to those used in semistructured query languages.
However, the underlying model is rather different from the customary graph
models for semistructured data: the graph is acyclic and unrooted, and both
temporal and inclusion relationships are important. We develop a query language
and describe optimization techniques for an underlying relational
representation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0709.4063v2,The Importance and Criticality of Spreadsheets in the City of London,"Spreadsheets have been with us in their present form for over a quarter of a
century. We have become so used to them that we forget that we are using them
at all. It may serve us well to stand back for a moment to review where, when
and how we use spreadsheets in the financial markets and elsewhere in order to
inform research that may guide their future development. In this article I
bring together the experiences of a number of senior practitioners who have
spent much of their careers working with large spreadsheets that have been and
continue to be used to support major financial transactions and manage large
institutions in the City of London. The author suggests that the City of London
is presently exposed to significant reputational risk through the continued
uncontrolled use of critical spreadsheets in the financial markets and
elsewhere.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1905.04800v1,"Quantitative Analysis of Cloud Function Evolution in the AWS Serverless
  Application Repository","The serverless computing ecosystem is growing due to interest by software
engineers. Beside Function-as-a-Service (FaaS) and Backend-as-a-Service (BaaS)
systems, developer-oriented tools such as deployment and debugging frameworks
as well as cloud function repositories enable the rapid creation of wholly or
partially serverless applications. This study presents first insights into how
cloud functions (Lambda functions) and composite serverless applications
offered through the AWS Serverless Application Repository have evolved over the
course of one year. Specifically, it outlines information on cloud function and
function-based application offering models and descriptions, high-level
implementation statistics, and evolution including change patterns over time.
Several results are presented in live paper style, offering hyperlinks to
continuously updated figures to follow the evolution after publication date.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1506.02717v4,"An Improved BKW Algorithm for LWE with Applications to Cryptography and
  Lattices","In this paper, we study the Learning With Errors problem and its binary
variant, where secrets and errors are binary or taken in a small interval. We
introduce a new variant of the Blum, Kalai and Wasserman algorithm, relying on
a quantization step that generalizes and fine-tunes modulus switching. In
general this new technique yields a significant gain in the constant in front
of the exponent in the overall complexity. We illustrate this by solving p
within half a day a LWE instance with dimension n = 128, modulus $q = n^2$,
Gaussian noise $\alpha = 1/(\sqrt{n/\pi} \log^2 n)$ and binary secret, using
$2^{28}$ samples, while the previous best result based on BKW claims a time
complexity of $2^{74}$ with $2^{60}$ samples for the same parameters. We then
introduce variants of BDD, GapSVP and UniqueSVP, where the target point is
required to lie in the fundamental parallelepiped, and show how the previous
algorithm is able to solve these variants in subexponential time. Moreover, we
also show how the previous algorithm can be used to solve the BinaryLWE problem
with n samples in subexponential time $2^{(\ln 2/2+o(1))n/\log \log n}$. This
analysis does not require any heuristic assumption, contrary to other algebraic
approaches; instead, it uses a variant of an idea by Lyubashevsky to generate
many samples from a small number of samples. This makes it possible to
asymptotically and heuristically break the NTRU cryptosystem in subexponential
time (without contradicting its security assumption). We are also able to solve
subset sum problems in subexponential time for density $o(1)$, which is of
independent interest: for such density, the previous best algorithm requires
exponential time. As a direct application, we can solve in subexponential time
the parameters of a cryptosystem based on this problem proposed at TCC 2010.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.08921v3,"Dormancy-aware timed branching bisimilarity for timed analysis of
  communication protocols","A variant of the standard notion of branching bisimilarity for processes with
discrete relative timing is proposed which is coarser than the standard notion.
Using a version of ACP (Algebra of Communicating Processes) with abstraction
for processes with discrete relative timing, it is shown that the proposed
variant allows of the functional correctness of the PAR (Positive
Acknowledgement with Retransmission) protocol as well as its performance
properties to be analyzed. In the version of ACP concerned, the difference
between the standard notion and its proposed variant is characterized by a
single axiom schema.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.05034v1,When is gray-box modeling advantageous for virtual flow metering?,"Integration of physics and machine learning in virtual flow metering
applications is known as gray-box modeling. The combination is believed to
enhance multiphase flow rate predictions. However, the superiority of gray-box
models is yet to be demonstrated in the literature. This article examines
scenarios where a gray-box model is expected to outperform physics-based and
data-driven models. The experiments are conducted with synthetic data where
properties of the underlying data generating process are known and controlled.
The results show that a gray-box model yields increased prediction accuracy
over a physics-based model in the presence of process-model mismatch. They also
show improvements over a data-driven model when the amount of available data is
small. On the other hand, gray-box and data-driven models are similarly
influenced by noisy measurements. Lastly, the results indicate that a gray-box
approach may be advantageous in nonstationary process conditions.
Unfortunately, choosing the best model prior to training is challenging, and
overhead on model development is unavoidable.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1710.07148v2,"A unified polynomial-time algorithm for Feedback Vertex Set on graphs of
  bounded mim-width","We give a first polynomial-time algorithm for (Weighted) Feedback Vertex Set
on graphs of bounded maximum induced matching width (mim-width). Explicitly,
given a branch decomposition of mim-width $w$, we give an
$n^{\mathcal{O}(w)}$-time algorithm that solves Feedback Vertex Set. This
provides a unified algorithm for many well-known classes, such as Interval
graphs and Permutation graphs, and furthermore, it gives the first
polynomial-time algorithms for other classes of bounded mim-width, such as
Circular Permutation and Circular $k$-Trapezoid graphs for fixed $k$. In all
these classes the decomposition is computable in polynomial time, as shown by
Belmonte and Vatshelle [Theor. Comput. Sci. 2013]. We show that powers of
graphs of tree-width $w - 1$ or path-width $w$ and powers of graphs of
clique-width $w$ have mim-width at most $w$. These results extensively provide
new classes of bounded mim-width. We prove a slight strengthening of the first
statement which implies that, surprisingly, Leaf Power graphs which are of
importance in the field of phylogenetic studies have mim-width at most $1$.
Given a tree decomposition of width $w-1$, a path decomposition of width $w$,
or a clique-width $w$-expression of a graph, one can for any value of $k$ find
a mim-width decomposition of its $k$-power in polynomial time, and apply our
algorithm to solve Feedback Vertex Set on the $k$-power in time
$n^{\mathcal{O}(w)}$. In contrast to Feedback Vertex Set, we show that
Hamiltonian Cycle is NP-complete even on graphs of linear mim-width $1$, which
further hints at the expressive power of the mim-width parameter.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.07495v4,Tameness and the power of programs over monoids in DA,"The program-over-monoid model of computation originates with Barrington's
proof that the model captures the complexity class $\mathsf{NC^1}$. Here we
make progress in understanding the subtleties of the model. First, we identify
a new tameness condition on a class of monoids that entails a natural
characterization of the regular languages recognizable by programs over monoids
from the class. Second, we prove that the class known as $\mathbf{DA}$
satisfies tameness and hence that the regular languages recognized by programs
over monoids in $\mathbf{DA}$ are precisely those recognizable in the classical
sense by morphisms from $\mathbf{QDA}$. Third, we show by contrast that the
well studied class of monoids called $\mathbf{J}$ is not tame. Finally, we
exhibit a program-length-based hierarchy within the class of languages
recognized by programs over monoids from $\mathbf{DA}$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1901.02103v1,On the Dimensionality of Embeddings for Sparse Features and Data,"In this note we discuss a common misconception, namely that embeddings are
always used to reduce the dimensionality of the item space. We show that when
we measure dimensionality in terms of information entropy then the embedding of
sparse probability distributions, that can be used to represent sparse features
or data, may or not reduce the dimensionality of the item space. However, the
embeddings do provide a different and often more meaningful representation of
the items for a particular task at hand. Also, we give upper bounds and more
precise guidelines for choosing the embedding dimension.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0810.4934v1,Exponential-Time Approximation of Hard Problems,"We study optimization problems that are neither approximable in polynomial
time (at least with a constant factor) nor fixed parameter tractable, under
widely believed complexity assumptions. Specifically, we focus on Maximum
Independent Set, Vertex Coloring, Set Cover, and Bandwidth.
  In recent years, many researchers design exact exponential-time algorithms
for these and other hard problems. The goal is getting the time complexity
still of order $O(c^n)$, but with the constant $c$ as small as possible. In
this work we extend this line of research and we investigate whether the
constant $c$ can be made even smaller when one allows constant factor
approximation. In fact, we describe a kind of approximation schemes --
trade-offs between approximation factor and the time complexity.
  We study two natural approaches. The first approach consists of designing a
backtracking algorithm with a small search tree. We present one result of that
kind: a $(4r-1)$-approximation of Bandwidth in time $O^*(2^{n/r})$, for any
positive integer $r$.
  The second approach uses general transformations from exponential-time exact
algorithms to approximations that are faster but still exponential-time. For
example, we show that for any reduction rate $r$, one can transform any
$O^*(c^n)$-time algorithm for Set Cover into a $(1+\ln r)$-approximation
algorithm running in time $O^*(c^{n/r})$. We believe that results of that kind
extend the applicability of exact algorithms for NP-hard problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1511.03609v1,"Automated Dynamic Firmware Analysis at Scale: A Case Study on Embedded
  Web Interfaces","Embedded devices are becoming more widespread, interconnected, and
web-enabled than ever. However, recent studies showed that these devices are
far from being secure. Moreover, many embedded systems rely on web interfaces
for user interaction or administration. Unfortunately, web security is known to
be difficult, and therefore the web interfaces of embedded systems represent a
considerable attack surface.
  In this paper, we present the first fully automated framework that applies
dynamic firmware analysis techniques to achieve, in a scalable manner,
automated vulnerability discovery within embedded firmware images. We apply our
framework to study the security of embedded web interfaces running in
Commercial Off-The-Shelf (COTS) embedded devices, such as routers, DSL/cable
modems, VoIP phones, IP/CCTV cameras. We introduce a methodology and implement
a scalable framework for discovery of vulnerabilities in embedded web
interfaces regardless of the vendor, device, or architecture. To achieve this
goal, our framework performs full system emulation to achieve the execution of
firmware images in a software-only environment, i.e., without involving any
physical embedded devices. Then, we analyze the web interfaces within the
firmware using both static and dynamic tools. We also present some interesting
case-studies, and discuss the main challenges associated with the dynamic
analysis of firmware images and their web interfaces and network services. The
observations we make in this paper shed light on an important aspect of
embedded devices which was not previously studied at a large scale.
  We validate our framework by testing it on 1925 firmware images from 54
different vendors. We discover important vulnerabilities in 185 firmware
images, affecting nearly a quarter of vendors in our dataset. These
experimental results demonstrate the effectiveness of our approach.",0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0
http://arxiv.org/abs/2007.02246v1,Blind Inverse Gamma Correction with Maximized Differential Entropy,"Unwanted nonlinear gamma distortion frequently occurs in a great diversity of
images during the procedures of image acquisition, processing, and/or display.
And the gamma distortion often varies with capture setup change and luminance
variation. Blind inverse gamma correction, which automatically determines a
proper restoration gamma value from a given image, is of paramount importance
to attenuate the distortion. For blind inverse gamma correction, an adaptive
gamma transformation method (AGT-ME) is proposed directly from a maximized
differential entropy model. And the corresponding optimization has a
mathematical concise closed-form solution, resulting in efficient
implementation and accurate gamma restoration of AGT-ME. Considering the human
eye has a non-linear perception sensitivity, a modified version AGT-ME-VISUAL
is also proposed to achieve better visual performance. Tested on variable
datasets, AGT-ME could obtain an accurate estimation of a large range of gamma
distortion (0.1 to 3.0), outperforming the state-of-the-art methods. Besides,
the proposed AGT-ME and AGT-ME-VISUAL were applied to three typical
applications, including automatic gamma adjustment, natural/medical image
contrast enhancement, and fringe projection profilometry image restoration.
Furthermore, the AGT-ME/ AGT-ME-VISUAL is general and can be seamlessly
extended to the masked image, multi-channel (color or spectrum) image, or
multi-frame video, and free of the arbitrary tuning parameter. Besides, the
corresponding Python code (https://github.com/yongleex/AGT-ME) is also provided
for interested users.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9811004v4,Does Meaning Evolve?,"A common method of making a theory more understandable, is by comparing it to
another theory which has been better developed. Radical interpretation is a
theory which attempts to explain how communication has meaning. Radical
interpretation is treated as another time-dependent theory and compared to the
time dependent theory of biological evolution. The main reason for doing this
is to find the nature of the time dependence; producing analogs between the two
theories is a necessary prerequisite to this and brings up many problems. Once
the nature of the time dependence is better known it might allow the underlying
mechanism to be uncovered. Several similarities and differences are uncovered,
there appear to be more differences than similarities.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1607.00315v1,"A multilevel framework for sparse optimization with application to
  inverse covariance estimation and logistic regression","Solving l1 regularized optimization problems is common in the fields of
computational biology, signal processing and machine learning. Such l1
regularization is utilized to find sparse minimizers of convex functions. A
well-known example is the LASSO problem, where the l1 norm regularizes a
quadratic function. A multilevel framework is presented for solving such l1
regularized sparse optimization problems efficiently. We take advantage of the
expected sparseness of the solution, and create a hierarchy of problems of
similar type, which is traversed in order to accelerate the optimization
process. This framework is applied for solving two problems: (1) the sparse
inverse covariance estimation problem, and (2) l1-regularized logistic
regression. In the first problem, the inverse of an unknown covariance matrix
of a multivariate normal distribution is estimated, under the assumption that
it is sparse. To this end, an l1 regularized log-determinant optimization
problem needs to be solved. This task is challenging especially for large-scale
datasets, due to time and memory limitations. In the second problem, the
l1-regularization is added to the logistic regression classification objective
to reduce overfitting to the data and obtain a sparse model. Numerical
experiments demonstrate the efficiency of the multilevel framework in
accelerating existing iterative solvers for both of these problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0803.0726v9,A quadratic algorithm for road coloring,"The Road Coloring Theorem states that every aperiodic directed graph with
constant out-degree has a synchronized coloring. This theorem had been
conjectured during many years as the Road Coloring Problem before being settled
by A. Trahtman. Trahtman's proof leads to an algorithm that finds a
synchronized labeling with a cubic worst-case time complexity. We show a
variant of his construction with a worst-case complexity which is quadratic in
time and linear in space. We also extend the Road Coloring Theorem to the
periodic case.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0411044v1,Routing Algorithms for Wireless Sensor Networks,"Our contribution in this paper is e3D, a diffusion based routing protocol
that prolongs the system lifetime, evenly distributes the power dissipation
throughout the network, and incurs minimal overhead for synchronizing
communication. We compare e3D with other algorithms in terms of system
lifetime, power dissipation distribution, cost of synchronization, and
simplicity of the algorithm.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1003.3047v2,On the Relative Strength of Pebbling and Resolution,"The last decade has seen a revival of interest in pebble games in the context
of proof complexity. Pebbling has proven a useful tool for studying
resolution-based proof systems when comparing the strength of different
subsystems, showing bounds on proof space, and establishing size-space
trade-offs. The typical approach has been to encode the pebble game played on a
graph as a CNF formula and then argue that proofs of this formula must inherit
(various aspects of) the pebbling properties of the underlying graph.
Unfortunately, the reductions used here are not tight. To simulate resolution
proofs by pebblings, the full strength of nondeterministic black-white pebbling
is needed, whereas resolution is only known to be able to simulate
deterministic black pebbling. To obtain strong results, one therefore needs to
find specific graph families which either have essentially the same properties
for black and black-white pebbling (not at all true in general) or which admit
simulations of black-white pebblings in resolution. This paper contributes to
both these approaches. First, we design a restricted form of black-white
pebbling that can be simulated in resolution and show that there are graph
families for which such restricted pebblings can be asymptotically better than
black pebblings. This proves that, perhaps somewhat unexpectedly, resolution
can strictly beat black-only pebbling, and in particular that the space lower
bounds on pebbling formulas in [Ben-Sasson and Nordstrom 2008] are tight.
Second, we present a versatile parametrized graph family with essentially the
same properties for black and black-white pebbling, which gives sharp
simultaneous trade-offs for black and black-white pebbling for various
parameter settings. Both of our contributions have been instrumental in
obtaining the time-space trade-off results for resolution-based proof systems
in [Ben-Sasson and Nordstrom 2009].",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1110.5395v4,Effectiveness and detection of denial of service attacks in Tor,"Tor is currently one of the more popular systems for anonymizing near
real-time communications on the Internet. Recently, Borisov et al. proposed a
denial of service based attack on Tor (and related systems) that significantly
increases the probability of compromising the anonymity provided. In this
paper, we analyze the effectiveness of the attack using both an analytic model
and simulation. We also describe two algorithms for detecting such attacks, one
deterministic and proved correct, the other probabilistic and verified in
simulation.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1612.06232v3,"A Knowledge-Assisted Visual Malware Analysis System: Design, Validation,
  and Reflection of KAMAS","IT-security experts engage in behavior-based malware analysis in order to
learn about previously unknown samples of malicious software (malware) or
malware families. For this, they need to find and categorize suspicious
patterns from large collections of execution traces. Currently available
systems do not meet the analysts' needs described as: visual access suitable
for complex data structures, visual representations appropriate for IT-security
experts, provide work flow-specific interaction techniques, and the ability to
externalize knowledge in the form of rules to ease analysis and for sharing
with colleagues. To close this gap, we designed and developed KAMAS, a
knowledge-assisted visualization system for behavior-based malware analysis.
KAMAS supports malware analysts with visual analytics and knowledge
externalization methods for the analysis process. The paper at hand is a design
study that describes the design, implementation, and evaluation of the
prototype. We report on the validation of KAMAS by expert reviews, a user study
with domain experts, and focus group meetings with analysts from industry.
Additionally, we reflect the gained insights of the design study and discuss
the advantages and disadvantages of the applied visualization methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1805.03090v1,Deception in Optimal Control,"In this paper, we consider an adversarial scenario where one agent seeks to
achieve an objective and its adversary seeks to learn the agent's intentions
and prevent the agent from achieving its objective. The agent has an incentive
to try to deceive the adversary about its intentions, while at the same time
working to achieve its objective. The primary contribution of this paper is to
introduce a mathematically rigorous framework for the notion of deception
within the context of optimal control. The central notion introduced in the
paper is that of a belief-induced reward: a reward dependent not only on the
agent's state and action, but also adversary's beliefs. Design of an optimal
deceptive strategy then becomes a question of optimal control design on the
product of the agent's state space and the adversary's belief space. The
proposed framework allows for deception to be defined in an arbitrary control
system endowed with a reward function, as well as with additional
specifications limiting the agent's control policy. In addition to defining
deception, we discuss design of optimally deceptive strategies under
uncertainties in agent's knowledge about the adversary's learning process. In
the latter part of the paper, we focus on a setting where the agent's behavior
is governed by a Markov decision process, and show that the design of optimally
deceptive strategies under lack of knowledge about the adversary naturally
reduces to previously discussed problems in control design on partially
observable or uncertain Markov decision processes. Finally, we present two
examples of deceptive strategies: a ""cops and robbers"" scenario and an example
where an agent may use camouflage while moving. We show that optimally
deceptive strategies in such examples follow the intuitive idea of how to
deceive an adversary in the above settings.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0805.2785v3,"Proof Search Specifications of Bisimulation and Modal Logics for the
  pi-Calculus","We specify the operational semantics and bisimulation relations for the
finite pi-calculus within a logic that contains the nabla quantifier for
encoding generic judgments and definitions for encoding fixed points. Since we
restrict to the finite case, the ability of the logic to unfold fixed points
allows this logic to be complete for both the inductive nature of operational
semantics and the coinductive nature of bisimulation. The nabla quantifier
helps with the delicate issues surrounding the scope of variables within
pi-calculus expressions and their executions (proofs). We illustrate several
merits of the logical specifications permitted by this logic: they are natural
and declarative; they contain no side-conditions concerning names of variables
while maintaining a completely formal treatment of such variables; differences
between late and open bisimulation relations arise from familar logic
distinctions; the interplay between the three quantifiers (for all, exists, and
nabla) and their scopes can explain the differences between early and late
bisimulation and between various modal operators based on bound input and
output actions; and proof search involving the application of inference rules,
unification, and backtracking can provide complete proof systems for one-step
transitions, bisimulation, and satisfaction in modal logic. We also illustrate
how one can encode the pi-calculus with replications, in an extended logic with
induction and co-induction.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1401.5690v2,On the Complexity of Computing with Planar Algebraic Curves,"In this paper, we give improved bounds for the computational complexity of
computing with planar algebraic curves. More specifically, for arbitrary
coprime polynomials $f$, $g \in \mathbb{Z}[x,y]$ and an arbitrary polynomial $h
\in \mathbb{Z}[x,y]$, each of total degree less than $n$ and with integer
coefficients of absolute value less than $2^\tau$, we show that each of the
following problems can be solved in a deterministic way with a number of bit
operations bounded by $\tilde{O}(n^6+n^5\tau)$, where we ignore polylogarithmic
factors in $n$ and $\tau$:
  (1) The computation of isolating regions in $\mathbb{C}^2$ for all complex
solutions of the system $f = g = 0$,
  (2) the computation of a separating form for the solutions of $f = g = 0$,
  (3) the computation of the sign of $h$ at all real valued solutions of $f = g
= 0$, and
  (4) the computation of the topology of the planar algebraic curve
$\mathcal{C}$ defined as the real valued vanishing set of the polynomial $f$.
  Our bound improves upon the best currently known bounds for the first three
problems by a factor of $n^2$ or more and closes the gap to the
state-of-the-art randomized complexity for the last problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1410.7815v1,Energy-Aware Lease Scheduling in Virtualized Data Centers,"Energy efficiency has become an important measurement of scheduling
algorithms in virtualized data centers. One of the challenges of
energy-efficient scheduling algorithms, however, is the trade-off between
minimizing energy consumption and satisfying quality of service (e.g.
performance, resource availability on time for reservation requests). We
consider resource needs in the context of virtualized data centers of a private
cloud system, which provides resource leases in terms of virtual machines (VMs)
for user applications. In this paper, we propose heuristics for scheduling VMs
that address the above challenge. On performance evaluation, simulated results
have shown a significant reduction on total energy consumption of our proposed
algorithms compared with an existing First-Come-First-Serve (FCFS) scheduling
algorithm with the same fulfillment of performance requirements. We also
discuss the improvement of energy saving when additionally using migration
policies to the above mentioned algorithms.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0910.5920v1,Evaluating Trust in Grid Certificates,"Digital certificates are used to secure international computation and data
storage grids used for e-Science projects, like the Worldwide Large Hadron
Collider Computing Grid. The International Grid Trust Federation has defined
the Grid Certificate Profile: a set of guidelines for digital certificates used
for grid authentication. We have designed and implemented a program and related
test suites for checking X.509 certificates against the certificate profiles
and policies relevant for use on the Grid. The result is a practical tool that
assists implementers and users of public key infrastructures to reach
appropriate trust decisions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2112.15589v1,"3-D Material Style Transfer for Reconstructing Unknown Appearance in
  Complex Natural Materials","We propose a 3-D material style transfer framework for reconstructing
invisible (or faded) appearance properties in complex natural materials. Our
algorithm addresses the technical challenge of transferring appearance
properties from one object to another of the same material when both objects
have intricate, noncorresponding color patterns. Eggshells, exoskeletons, and
minerals, for example, have patterns composed of highly randomized layers of
organic and inorganic compounds. These materials pose a challenge as the
distribution of compounds that determine surface color changes from object to
object and within local pattern regions. Our solution adapts appearance
observations from a material property distribution in an exemplar to the
material property distribution of a target object to reconstruct its unknown
appearance. We use measured reflectance in 3-D bispectral textures to record
changing material property distributions. Our novel implementation of spherical
harmonics uses principles from chemistry and biology to learn relationships
between color (hue and saturation) and material composition and concentration
in an exemplar. The encoded relationships are transformed to the property
distribution of a target for color recovery and material assignment.
Quantitative and qualitative evaluation methods show that we replicate color
patterns more accurately than methods that only rely on shape correspondences
and coarse-level perceptual differences. We demonstrate applications of our
work for reconstructing color in extinct fossils, restoring faded artifacts and
generating synthetic textures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.00570v1,"Proposed new dynamic power insertion method for stabilized power
  generation based on battery energy storage system","The solar energy is clean and future energy for electricity generation. Its
energy has enormous potential but do not optimal to utilized caused by
intermittent energy. The intermittent radiance and ambient temperature
influence the energy produced fluctuates and unstable. These power fluctuations
affect system stability and frequency. To overcome this problem, several
methods for PV power stabilization have been developed. One of them is the
Stabilized Power Generation where PV output power is to a certain power value.
The result of SPG method is reducing in PV power fluctuations but still
unstable. For reaching the stable condition of PV Power output, the Dynamic
Power Insertion method is proposed which is a modification of the SPG method
with energy storage batteries. DPI improve the SPG method and make PV power
stable with active power management with charge and discharge action. The
battery is using as energy storage when PV power is larger than Power limit. On
the other hand battery as an energy source for active power insertion at PV
power is smaller than Power limit. Thus the output power in each condition can
be maintained as Power PV equals to P limit. For test this method, DPI modules
are built on ARM lpc1768 NXP and monitored with the Thing speak webserver
(simulated with Simulink MatLab Software). The experimental results of DPI can
stabilize PV power fluctuation at its setting power with an error of 5 percent.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1406.7685v1,"Efficiency Analysis of Materialized views in DataWarehouse Using
  selfmaintenance","A data warehouse is a large data repository for the purpose of analysis and
decision making in organizations. To improve the query performance and to get
fast access to the data, data is stored as materialized views (MV) in the data
warehouse. When data at source gets updated, the materialized views also need
to be updated. In this paper, we focus on the problem of maintenance of these
materialized views and address the issue of finding such auxiliary views (AV)
that together with the materialized views make the data self-maintainable and
take minimal space. We propose an algorithm that uses key and referential
constraints which reduces the total number of tuples in auxiliary views and
uses idea of information sharing between these auxiliary views to further
reduce number of auxiliary views.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.03707v1,Learning from Experience for Rapid Generation of Local Car Maneuvers,"Being able to rapidly respond to the changing scenes and traffic situations
by generating feasible local paths is of pivotal importance for car autonomy.
We propose to train a deep neural network (DNN) to plan feasible and
nearly-optimal paths for kinematically constrained vehicles in small constant
time. Our DNN model is trained using a novel weakly supervised approach and a
gradient-based policy search. On real and simulated scenes and a large set of
local planning problems, we demonstrate that our approach outperforms the
existing planners with respect to the number of successfully completed tasks.
While the path generation time is about 40 ms, the generated paths are smooth
and comparable to those obtained from conventional path planners.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0107001v2,Analysis of Network Traffic in Switched Ethernet Systems,"A 100 Mbps Ethernet link between a college campus and the outside world was
monitored with a dedicated PC and the measured data analysed for its
statistical properties. Similar measurements were taken at an internal node of
the network. The networks in both cases are a full-duplex switched Ethernet.
Inter-event interval histograms and power spectra of the throughput aggregated
for 10ms bins were used to analyse the measured traffic. For most investigated
cases both methods reveal that the traffic behaves according to a power law.
The results will be used in later studies to parameterise models for network
traffic.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0508038v1,Quantum Algorithm Processor For Finding Exact Divisors,"Wiring diagrams are given for a quantum algorithm processor in CMOS to
compute, in parallel, all divisors of an n-bit integer. Lines required in a
wiring diagram are proportional to n. Execution time is proportional to the
square of n.",0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0802.1306v1,Network as a computer: ranking paths to find flows,"We explore a simple mathematical model of network computation, based on
Markov chains. Similar models apply to a broad range of computational
phenomena, arising in networks of computers, as well as in genetic, and neural
nets, in social networks, and so on. The main problem of interaction with such
spontaneously evolving computational systems is that the data are not uniformly
structured. An interesting approach is to try to extract the semantical content
of the data from their distribution among the nodes. A concept is then
identified by finding the community of nodes that share it. The task of data
structuring is thus reduced to the task of finding the network communities, as
groups of nodes that together perform some non-local data processing. Towards
this goal, we extend the ranking methods from nodes to paths. This allows us to
extract some information about the likely flow biases from the available static
information about the network.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0803.1120v3,"The Rate Loss of Single-Letter Characterization: The ""Dirty"" Multiple
  Access Channel","For general memoryless systems, the typical information theoretic solution -
when exists - has a ""single-letter"" form. This reflects the fact that optimum
performance can be approached by a random code (or a random binning scheme),
generated using independent and identically distributed copies of some
single-letter distribution. Is that the form of the solution of any
(information theoretic) problem? In fact, some counter examples are known. The
most famous is the ""two help one"" problem: Korner and Marton showed that if we
want to decode the modulo-two sum of two binary sources from their independent
encodings, then linear coding is better than random coding. In this paper we
provide another counter example, the ""doubly-dirty"" multiple access channel
(MAC). Like the Korner-Marton problem, this is a multi-terminal scenario where
side information is distributed among several terminals; each transmitter knows
part of the channel interference but the receiver is not aware of any part of
it. We give an explicit solution for the capacity region of a binary version of
the doubly-dirty MAC, demonstrate how the capacity region can be approached
using a linear coding scheme, and prove that the ""best known single-letter
region"" is strictly contained in it. We also state a conjecture regarding a
similar rate loss of single letter characterization in the Gaussian case.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1708.09234v1,Fighting with the Sparsity of Synonymy Dictionaries,"Graph-based synset induction methods, such as MaxMax and Watset, induce
synsets by performing a global clustering of a synonymy graph. However, such
methods are sensitive to the structure of the input synonymy graph: sparseness
of the input dictionary can substantially reduce the quality of the extracted
synsets. In this paper, we propose two different approaches designed to
alleviate the incompleteness of the input dictionaries. The first one performs
a pre-processing of the graph by adding missing edges, while the second one
performs a post-processing by merging similar synset clusters. We evaluate
these approaches on two datasets for the Russian language and discuss their
impact on the performance of synset induction methods. Finally, we perform an
extensive error analysis of each approach and discuss prominent alternative
methods for coping with the problem of the sparsity of the synonymy
dictionaries.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1002.4668v1,Accelerating sequential programs using FastFlow and self-offloading,"FastFlow is a programming environment specifically targeting cache-coherent
shared-memory multi-cores. FastFlow is implemented as a stack of C++ template
libraries built on top of lock-free (fence-free) synchronization mechanisms. In
this paper we present a further evolution of FastFlow enabling programmers to
offload part of their workload on a dynamically created software accelerator
running on unused CPUs. The offloaded function can be easily derived from
pre-existing sequential code. We emphasize in particular the effective
trade-off between human productivity and execution efficiency of the approach.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9909004v1,Convex Tours of Bounded Curvature,"We consider the motion planning problem for a point constrained to move along
a smooth closed convex path of bounded curvature. The workspace of the moving
point is bounded by a convex polygon with m vertices, containing an obstacle in
a form of a simple polygon with $n$ vertices. We present an O(m+n) time
algorithm finding the path, going around the obstacle, whose curvature is the
smallest possible.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1310.5043v3,One Quantifier Alternation in First-Order Logic with Modular Predicates,"Adding modular predicates yields a generalization of first-order logic FO
over words. The expressive power of FO[<,MOD] with order comparison $x<y$ and
predicates for $x \equiv i \mod n$ has been investigated by Barrington,
Compton, Straubing and Therien. The study of FO[<,MOD]-fragments was initiated
by Chaubard, Pin and Straubing. More recently, Dartois and Paperman showed that
definability in the two-variable fragment FO2[<,MOD] is decidable. In this
paper we continue this line of work.
  We give an effective algebraic characterization of the word languages in
Sigma2[<,MOD]. The fragment Sigma2 consists of first-order formulas in prenex
normal form with two blocks of quantifiers starting with an existential block.
In addition we show that Delta2[<,MOD], the largest subclass of Sigma2[<,MOD]
which is closed under negation, has the same expressive power as two-variable
logic FO2[<,MOD]. This generalizes the result FO2[<] = Delta2[<] of Therien and
Wilke to modular predicates. As a byproduct, we obtain another decidable
characterization of FO2[<,MOD].",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1212.5959v1,A Large-Scale Study of Online Shopping Behavior,"The continuous growth of electronic commerce has stimulated great interest in
studying online consumer behavior. Given the significant growth in online
shopping, better understanding of customers allows better marketing strategies
to be designed. While studies of online shopping attitude are widespread in the
literature, studies of browsing habits differences in relation to online
shopping are scarce.
  This research performs a large scale study of the relationship between
Internet browsing habits of users and their online shopping behavior. Towards
this end, we analyze data of 88,637 users who have bought more in total half a
milion products from the retailer sites Amazon and Walmart. Our results
indicate that even coarse-grained Internet browsing behavior has predictive
power in terms of what users will buy online. Furthermore, we discover both
surprising (e.g., ""expensive products do not come with more effort in terms of
purchase"") and expected (e.g., ""the more loyal a user is to an online shop, the
less effort they spend shopping"") facts.
  Given the lack of large-scale studies linking online browsing and online
shopping behavior, we believe that this work is of general interest to people
working in related areas.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1801.08003v3,Threadable Curves,"We define a plane curve to be threadable if it can rigidly pass through a
point-hole in a line L without otherwise touching L. Threadable curves are in a
sense generalizations of monotone curves. We have two main results. The first
is a linear-time algorithm for deciding whether a polygonal curve is
threadable---O(n) for a curve of n vertices---and if threadable, finding a
sequence of rigid motions to thread it through a hole. We also sketch an
argument that shows that the threadability of algebraic curves can be decided
in time polynomial in the degree of the curve. The second main result is an O(n
polylog n)-time algorithm for deciding whether a 3D polygonal curve can thread
through hole in a plane in R^3, and if so, providing a description of the rigid
motions that achieve the threading.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.00538v1,'Entanglement' -- A new dynamic metric to measure team flow,"We introduce ""entanglement"", a novel metric to measure how synchronized
communication between team members is. This measure calculates the Euclidean
distance among team members' social network metrics timeseries. We validate the
metric with four case studies. The first case study uses entanglement of 11
medical innovation teams to predict team performance and learning behavior. The
second case looks at the e-mail communication of 113 senior executives of an
international services firm, predicting employee turnover through lack of
entanglement of an employee. The third case analyzes the individual employee
performance of 81 managers. The fourth case study predicts performance of 13
customer-dedicated teams at a big international company by comparing
entanglement in the e-mail interactions with satisfaction of their customers
measured through Net Promoter Score (NPS). While we can only speculate about
what is causing the entanglement effect, we find that it is a new and versatile
indicator for the analysis of employees' communication, analyzing the hitherto
underused temporal dimension of online social networks which could be used as a
powerful predictor of employee and team performance, employee turnover, and
customer satisfaction.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1401.7528v3,"A Monte-Carlo Approach to Lifespan Failure Performance Analysis of the
  Network Fabric in Modular Data Centers","Data centers have been evolved from a passive element of compute
infrastructure to become an active, core part of any ICT solution. In
particular, modular data centers (MDCs), which are a promising design approach
to improve resiliency of data centers, can play a key role in deploying ICT
infrastructure in remote and inhospitable environments in order to take
advantage of low temperatures and hydro- and wind-electric capabilities. This
is because of capability of the modular data centers to survive even in lack of
continuous on-site maintenance and support. The most critical part of a data
center is its network fabric that could impede the whole system even if all
other components are fully functional, assuming that other analyses has been
already performed to ensure the reliability of the underlying infrastructure
and support systems. In this work, a complete failure analysis of modular data
centers using failure models of various components including servers, switches,
and links is performed using a proposed Monte-Carlo approach. The proposed
Monte-Carlo approach, which is based on the concept of snapshots, allows us to
effectively calculate the performance of a design along its lifespan even up to
the terminal stages. To show the capabilities of the proposed approach, various
network topologies, such as FatTree, BCube, MDCube, and their modifications are
considered. The performance and also the lifespan of each topology design in
presence of failures of their components are studied against the topology
parameters.",0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1812.01243v9,Efficient Attention: Attention with Linear Complexities,"Dot-product attention has wide applications in computer vision and natural
language processing. However, its memory and computational costs grow
quadratically with the input size. Such growth prohibits its application on
high-resolution inputs. To remedy this drawback, this paper proposes a novel
efficient attention mechanism equivalent to dot-product attention but with
substantially less memory and computational costs. Its resource efficiency
allows more widespread and flexible integration of attention modules into a
network, which leads to better accuracies. Empirical evaluations demonstrated
the effectiveness of its advantages. Efficient attention modules brought
significant performance boosts to object detectors and instance segmenters on
MS-COCO 2017. Further, the resource efficiency democratizes attention to
complex models, where high costs prohibit the use of dot-product attention. As
an exemplar, a model with efficient attention achieved state-of-the-art
accuracies for stereo depth estimation on the Scene Flow dataset. Code is
available at https://github.com/cmsflash/efficient-attention.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0703011v1,Can we Compute the Similarity Between Surfaces?,"A suitable measure for the similarity of shapes represented by parameterized
curves or surfaces is the Fr\'echet distance. Whereas efficient algorithms are
known for computing the Fr\'echet distance of polygonal curves, the same
problem for triangulated surfaces is NP-hard. Furthermore, it remained open
whether it is computable at all. Here, using a discrete approximation we show
that it is {\em upper semi-computable}, i.e., there is a non-halting Turing
machine which produces a monotone decreasing sequence of rationals converging
to the result. It follows that the decision problem, whether the Fr\'echet
distance of two given surfaces lies below some specified value, is recursively
enumerable.
  Furthermore, we show that a relaxed version of the problem, the computation
of the {\em weak Fr\'echet distance} can be solved in polynomial time. For
this, we give a computable characterization of the weak Fr\'echet distance in a
geometric data structure called the {\em free space diagram}.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1109.0345v1,Planar and Poly-Arc Lombardi Drawings,"In Lombardi drawings of graphs, edges are represented as circular arcs, and
the edges incident on vertices have perfect angular resolution. However, not
every graph has a Lombardi drawing, and not every planar graph has a planar
Lombardi drawing. We introduce k-Lombardi drawings, in which each edge may be
drawn with k circular arcs, noting that every graph has a smooth 2-Lombardi
drawing. We show that every planar graph has a smooth planar 3-Lombardi drawing
and further investigate topics connecting planarity and Lombardi drawings.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.06759v3,"Automatic Social Distance Estimation From Images: Performance
  Evaluation, Test Benchmark, and Algorithm","The COVID-19 virus has caused a global pandemic since March 2020. The World
Health Organization (WHO) has provided guidelines on how to reduce the spread
of the virus and one of the most important measures is social distancing.
Maintaining a minimum of one meter distance from other people is strongly
suggested to reduce the risk of infection. This has created a strong interest
in monitoring the social distances either as a safety measure or to study how
the measures have affected human behavior and country-wise differences in this.
The need for automatic social distance estimation algorithms is evident, but
there is no suitable test benchmark for such algorithms. Collecting images with
measured ground-truth pair-wise distances between all the people using
different camera settings is cumbersome. Furthermore, performance evaluation
for social distance estimation algorithms is not straightforward and there is
no widely accepted evaluation protocol. In this paper, we provide a dataset of
varying images with measured pair-wise social distances under different camera
positionings and focal length values. We suggest a performance evaluation
protocol and provide a benchmark to easily evaluate social distance estimation
algorithms. We also propose a method for automatic social distance estimation.
Our method takes advantage of object detection and human pose estimation. It
can be applied on any single image as long as focal length and sensor size
information are known. The results on our benchmark are encouraging with 92%
human detection rate and only 28.9% average error in distance estimation among
the detected people.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1603.02478v2,An Introduction to Mechanized Reasoning,"Mechanized reasoning uses computers to verify proofs and to help discover new
theorems. Computer scientists have applied mechanized reasoning to economic
problems but -- to date -- this work has not yet been properly presented in
economics journals. We introduce mechanized reasoning to economists in three
ways. First, we introduce mechanized reasoning in general, describing both the
techniques and their successful applications. Second, we explain how mechanized
reasoning has been applied to economic problems, concentrating on the two
domains that have attracted the most attention: social choice theory and
auction theory. Finally, we present a detailed example of mechanized reasoning
in practice by means of a proof of Vickrey's familiar theorem on second-price
auctions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.09677v1,"Adaptive Low-Rank Regularization with Damping Sequences to Restrict Lazy
  Weights in Deep Networks","Overfitting is one of the critical problems in deep neural networks. Many
regularization schemes try to prevent overfitting blindly. However, they
decrease the convergence speed of training algorithms. Adaptive regularization
schemes can solve overfitting more intelligently. They usually do not affect
the entire network weights. This paper detects a subset of the weighting layers
that cause overfitting. The overfitting recognizes by matrix and tensor
condition numbers. An adaptive regularization scheme entitled Adaptive Low-Rank
(ALR) is proposed that converges a subset of the weighting layers to their
Low-Rank Factorization (LRF). It happens by minimizing a new Tikhonov-based
loss function. ALR also encourages lazy weights to contribute to the
regularization when epochs grow up. It uses a damping sequence to increment
layer selection likelihood in the last generations. Thus before falling the
training accuracy, ALR reduces the lazy weights and regularizes the network
substantially. The experimental results show that ALR regularizes the deep
networks well with high training speed and low resource usage.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/2012.05965v1,Analog Computation and Representation,"Relative to digital computation, analog computation has been neglected in the
philosophical literature. To the extent that attention has been paid to analog
computation, it has been misunderstood. The received view -- that analog
computation has to do essentially with continuity -- is simply wrong, as shown
by careful attention to historical examples of discontinuous, discrete analog
computers. Instead of the received view, I develop an account of analog
computation in terms of a particular type of analog representation that allows
for discontinuity. This account thus characterizes all types of analog
computation, whether continuous or discrete. Furthermore, the structure of this
account can be generalized to other types of computation: analog computation
essentially involves analog representation, whereas digital computation
essentially involves digital representation. Besides being a necessary
component of a complete philosophical understanding of computation in general,
understanding analog computation is important for computational explanation in
contemporary neuroscience and cognitive science.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
http://arxiv.org/abs/1002.3175v4,Solutions to the GSM Security Weaknesses,"Recently, the mobile industry has experienced an extreme increment in number
of its users. The GSM network with the greatest worldwide number of users
succumbs to several security vulnerabilities. Although some of its security
problems are addressed in its upper generations, there are still many operators
using 2G systems. This paper briefly presents the most important security flaws
of the GSM network and its transport channels. It also provides some practical
solutions to improve the security of currently available 2G systems.",0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1210.6118v1,Proceedings First Workshop on GRAPH Inspection and Traversal Engineering,"These are the proceedings of the First Workshop on GRAPH Inspection and
Traversal Engineering (GRAPHITE 2012), which took place on April 1, 2012 in
Tallinn, Estonia, as a satellite event of the 15th European Joint Conferences
on Theory and Practice of Software (ETAPS 2012).
  The topic of the GRAPHITE workshop is graph search in all its forms in
computer science. Graph search algorithms tend to have common characteristics,
such as duplicate state detection, independent of their application domain.
Over the past few years, it has been shown that the scalability of such
algorithms can be dramatically improved by using, e.g., external memory, by
exploiting parallel architectures, such as clusters, multi-core CPUs, and
graphics processing units, and by using heuristics to guide the search. The
goal of this event is to gather scientists from different communities, such as
model checking, artificial intelligence planning, game playing, and algorithm
engineering, who do research on graph search algorithms, such that awareness of
each others' work is increased.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0609141v1,Polygon Convexity: A Minimal O(n) Test,"An O(n) test for polygon convexity is stated and proved. It is also proved
that the test is minimal in a certain exact sense.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.04735v3,Validating Static Warnings via Testing Code Fragments,"Static analysis is an important approach for finding bugs and vulnerabilities
in software. However, inspecting and confirming static warnings are challenging
and time-consuming. In this paper, we present a novel solution that
automatically generates test cases based on static warnings to validate true
and false positives. We designed a syntactic patching algorithm that can
generate syntactically valid, semantic preserving executable code fragments
from static warnings. We developed a build and testing system to automatically
test code fragments using fuzzers, KLEE and Valgrind. We evaluated our
techniques using 12 real-world C projects and 1955 warnings from two commercial
static analysis tools. We successfully built 68.5% code fragments and generated
1003 test cases. Through automatic testing, we identified 48 true positives and
27 false positives, and 205 likely false positives. We matched 4 CVE and
real-world bugs using Helium, and they are only triggered by our tool but not
other baseline tools. We found that testing code fragments is scalable and
useful; it can trigger bugs that testing entire programs or testing procedures
failed to trigger.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1712.02463v2,"CURE-TSR: Challenging Unreal and Real Environments for Traffic Sign
  Recognition","In this paper, we investigate the robustness of traffic sign recognition
algorithms under challenging conditions. Existing datasets are limited in terms
of their size and challenging condition coverage, which motivated us to
generate the Challenging Unreal and Real Environments for Traffic Sign
Recognition (CURE-TSR) dataset. It includes more than two million traffic sign
images that are based on real-world and simulator data. We benchmark the
performance of existing solutions in real-world scenarios and analyze the
performance variation with respect to challenging conditions. We show that
challenging conditions can decrease the performance of baseline methods
significantly, especially if these challenging conditions result in loss or
misplacement of spatial information. We also investigate the effect of data
augmentation and show that utilization of simulator data along with real-world
data enhance the average recognition performance in real-world scenarios. The
dataset is publicly available at https://ghassanalregib.com/cure-tsr/.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.12239v1,"Policy Search using Dynamic Mirror Descent MPC for Model Free Off Policy
  RL","Recent works in Reinforcement Learning (RL) combine model-free (Mf)-RL
algorithms with model-based (Mb)-RL approaches to get the best from both:
asymptotic performance of Mf-RL and high sample-efficiency of Mb-RL. Inspired
by these works, we propose a hierarchical framework that integrates online
learning for the Mb-trajectory optimization with off-policy methods for the
Mf-RL. In particular, two loops are proposed, where the Dynamic Mirror Descent
based Model Predictive Control (DMD-MPC) is used as the inner loop to obtain an
optimal sequence of actions. These actions are in turn used to significantly
accelerate the outer loop Mf-RL. We show that our formulation is generic for a
broad class of MPC based policies and objectives, and includes some of the
well-known Mb-Mf approaches. Based on the framework we define two algorithms to
increase sample efficiency of Off Policy RL and to guide end to end RL
algorithms for online adaption respectively. Thus we finally introduce two
novel algorithms: Dynamic-Mirror Descent Model Predictive RL(DeMoRL), which
uses the method of elite fractions for the inner loop and Soft Actor-Critic
(SAC) as the off-policy RL for the outer loop and Dynamic-Mirror Descent Model
Predictive Layer(DeMo Layer), a special case of the hierarchical framework
which guides linear policies trained using Augmented Random Search(ARS). Our
experiments show faster convergence of the proposed DeMo RL, and better or
equal performance compared to other Mf-Mb approaches on benchmark MuJoCo
control tasks. The DeMo Layer was tested on classical Cartpole and custom-built
Quadruped trained using Linear Policy.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.02389v1,Migrating Client Code without Change Examples,"API developers evolve software libraries to fix bugs, add new features, or
refactor code. To benefit from such library evolution, the programmers of
client projects have to repetitively upgrade their library usages and adapt
their codebases to any library API breaking changes (e.g., API renaming). Such
adaptive changes can be tedious and error-prone. Existing tools provide limited
support to help programmers migrate client projects from old library versions
to new ones. For instance, some tools extract API mappings be-tween library
versions and only suggest simple adaptive changes (i.e., statement updates);
other tools suggest or automate more complicated edits (e.g., statement
insertions) based on user-provided exemplar code migrations. However, when new
library versions are available, it is usually cumbersome and time-consuming for
users to provide sufficient human-crafted samples in order to guide automatic
migration. In this paper, we propose a novel approach, AutoUpdate, to further
improve the state of the art. Instead of learning from change examples, we
designed AutoUpdate to automate migration in a compiler-directed way. Namely,
given a compilation error triggered by upgrading libraries, AutoUpdate exploits
13 migration opera-tors to generate candidate edits, and tentatively applies
each edit until the error is resolved or all edits are explored. We conducted
two experiments. The first experiment involves migrating 371 tutorial examples
between versions of 5 popular libraries. AutoUpdate reduced migration-related
compilation errors for 92.7% of tasks. It eliminated such errors for 32.4% of
tasks, and 33.9% of the tasks have identical edits to manual migrations. In the
second experiment, we applied AutoUpdate to migrate two real client projects of
lucene. AutoUpdate successfully migrated both projects, and the migrated code
passed all tests.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.04838v1,"Improving the Robustness of Trading Strategy Backtesting with Boltzmann
  Machines and Generative Adversarial Networks","This article explores the use of machine learning models to build a market
generator. The underlying idea is to simulate artificial multi-dimensional
financial time series, whose statistical properties are the same as those
observed in the financial markets. In particular, these synthetic data must
preserve the probability distribution of asset returns, the stochastic
dependence between the different assets and the autocorrelation across time.
The article proposes then a new approach for estimating the probability
distribution of backtest statistics. The final objective is to develop a
framework for improving the risk management of quantitative investment
strategies, in particular in the space of smart beta, factor investing and
alternative risk premia.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1711.07593v1,"An Enhanced Middleware for Collaborative Privacy in IPTV Recommender
  Services","One of the concerns users have to confronted when using IPTV system is the
information overload that makes it difficult for them to find a suitable
content according to their personal preferences. Recommendation service is one
of the most widely adopted technologies for alleviating this problem, these
services intend to provide people with referrals of items they will appreciate
based on their preferences. IPTV users must ensure their sensitive preferences
collected by any recommendation service are properly secured. In this work, we
introduce a framework for private recommender service based on Enhanced
Middleware for Collaborative Privacy (EMCP). EMCP executes a two-stage
concealment process that gives the user a complete control on the privacy level
of his/her profile. We utilize trust mechanism to augment the accuracy and
privacy of the recommendations. Trust heuristic spot users who are trustworthy
with respect to the user requesting the recommendation. Later, the neighborhood
formation is calculated using proximity metrics based on these trustworthy
users. Finally, Users submit their profiles in an obfuscated form without
revealing any information about their data, and the computation of
recommendations proceeds over the obfuscated data using secure multiparty
computation protocol. We expand the obfuscation scope from single obfuscation
level for all users to arbitrary obfuscation levels based on trustworthy
between users. In other words, we correlate the obfuscation level with
different trust levels, so the more trusted a target user is the less
obfuscation copy of profile he can access. We also provide an IPTV network
scenario and experimentation results. Our results and analysis show that our
two-stage concealment process not only protects the privacy of users but also
can maintain the recommendations accuracy.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2103.07279v2,"Patient-specific virtual spine straightening and vertebra inpainting: An
  automatic framework for osteoplasty planning","Symptomatic spinal vertebral compression fractures (VCFs) often require
osteoplasty treatment. A cement-like material is injected into the bone to
stabilize the fracture, restore the vertebral body height and alleviate pain.
Leakage is a common complication and may occur due to too much cement being
injected. In this work, we propose an automated patient-specific framework that
can allow physicians to calculate an upper bound of cement for the injection
and estimate the optimal outcome of osteoplasty. The framework uses the patient
CT scan and the fractured vertebra label to build a virtual healthy spine using
a high-level approach. Firstly, the fractured spine is segmented with a
three-step Convolution Neural Network (CNN) architecture. Next, a per-vertebra
rigid registration to a healthy spine atlas restores its curvature. Finally, a
GAN-based inpainting approach replaces the fractured vertebra with an
estimation of its original shape. Based on this outcome, we then estimate the
maximum amount of bone cement for injection. We evaluate our framework by
comparing the virtual vertebrae volumes of ten patients to their healthy
equivalent and report an average error of 3.88$\pm$7.63\%. The presented
pipeline offers a first approach to a personalized automatic high-level
framework for planning osteoplasty procedures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1602.04339v1,Mathematical Theory Exploration in Theorema: Reduction Rings,"In this paper we present the first-ever computer formalization of the theory
of Gr\""obner bases in reduction rings, which is an important theory in
computational commutative algebra, in Theorema. Not only the formalization, but
also the formal verification of all results has already been fully completed by
now; this, in particular, includes the generic implementation and correctness
proof of Buchberger's algorithm in reduction rings. Thanks to the seamless
integration of proving and computing in Theorema, this implementation can now
be used to compute Gr\""obner bases in various different domains directly within
the system. Moreover, a substantial part of our formalization is made up solely
by ""elementary theories"" such as sets, numbers and tuples that are themselves
independent of reduction rings and may therefore be used as the foundations of
future theory explorations in Theorema.
  In addition, we also report on two general-purpose Theorema tools we
developed for an efficient and convenient exploration of mathematical theories:
an interactive proving strategy and a ""theory analyzer"" that already proved
extremely useful when creating large structured knowledge bases.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1701.02394v3,Domains and Event Structures for Fusions,"Stable event structures, and their duality with prime algebraic domains
arising as partial orders of configurations, are a landmark of concurrency
theory, providing a clear characterisation of causality in computations. They
have been used for defining a concurrent semantics of several formalisms, from
Petri nets to (linear) graph rewriting systems, which in turn lay at the basis
of many visual modelling frameworks. Stability however is restrictive when
dealing with formalisms with ""fusion"", i.e., where a computational step can not
only consume and produce but also merge parts of the state. This happens, e.g.,
for graph rewriting systems with non-linear rules, which are needed to cover
some relevant applications (such as the graphical encoding of calculi with name
passing). Guided by the need of capturing the semantics of formalisms with
fusion we leave aside stability and we characterise, as a natural
generalisation of prime algebraic domains, a class of domains, referred to as
weak prime domains. We then identify a corresponding class of event structures,
that we call connected event structures, via a duality result formalised as an
equivalence of categories. We show that connected event structures are exactly
the class of event structures that arise as the semantics of non-linear graph
rewriting systems. Interestingly, the category of general unstable event
structures coreflects into our category of weak prime domains, so that our
result provides a characterisation of the partial orders of configurations of
such event structures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1002.0414v1,"Feature Level Fusion of Biometrics Cues: Human Identification with
  Doddingtons Caricature","This paper presents a multimodal biometric system of fingerprint and ear
biometrics. Scale Invariant Feature Transform (SIFT) descriptor based feature
sets extracted from fingerprint and ear are fused. The fused set is encoded by
K-medoids partitioning approach with less number of feature points in the set.
K-medoids partition the whole dataset into clusters to minimize the error
between data points belonging to the clusters and its center. Reduced feature
set is used to match between two biometric sets. Matching scores are generated
using wolf-lamb user-dependent feature weighting scheme introduced by
Doddington. The technique is tested to exhibit its robust performance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.06253v1,"Precise estimation on the order of local testability of deterministic
  finite automaton","A locally testable language L is a language with the property that for some
non negative integer k, called the order or the level of local testable,
whether or not a word u in the language L depends on (1) the prefix and the
suffix of the word u of length k-1 and (2) the set of intermediate partial
strings of length k of the word u. For given k the language is called
k-testable. We give necessary and sufficient conditions for the language of an
automaton to be k-testable in the terms of the length of paths of a related
graph. Some estimations of the upper and of the lower bound of testable order
follow from these results. We improve the upper bound on the testable order of
locally testable deterministic finite automaton with n states to n(n-2)+1 This
bound is the best possible. We give an answer on the following conjecture of
Kim, McNaughton and Mac-CLoskey for deterministic finite locally testable
automaton with n states: \Is the local testable order of no greater than n in
power 1.5 when the alphabet size is two?"" Our answer is negative. In the case
of size two the situation is the same as in general case.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.07162v1,Needed Computations Shortcutting Needed Steps,"We define a compilation scheme for a constructor-based, strongly-sequential,
graph rewriting system which shortcuts some needed steps. The object code is
another constructor-based graph rewriting system. This system is normalizing
for the original system when using an innermost strategy. Consequently, the
object code can be easily implemented by eager functions in a variety of
programming languages. We modify this object code in a way that avoids total or
partial construction of the contracta of some needed steps of a computation.
When computing normal forms in this way, both memory consumption and execution
time are reduced compared to ordinary rewriting computations in the original
system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.13815v1,"As long as you talk about me: The importance of family firm brands and
  the contingent role of family-firm identity","This study explores the role of external audiences in determining the
importance of family firm brands and the relationship with firm performance.
Drawing on text mining and social network analysis techniques, and considering
the brand prevalence, diversity, and connectivity dimensions, we use the
semantic brand score to measure the importance the media give to family firm
brands. The analysis of a sample of 52,555 news articles published in 2017
about 63 Italian entrepreneurial families reveals that brand importance is
positively associated with family firm revenues, and this relationship is
stronger when there is identity match between the family and the firm. This
study advances current literature by offering a rich and multifaceted
perspective on how external audiences perceptions of the brand shape family
firm performance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1211.6166v1,Tracking and Quantifying Censorship on a Chinese Microblogging Site,"We present measurements and analysis of censorship on Weibo, a popular
microblogging site in China. Since we were limited in the rate at which we
could download posts, we identified users likely to participate in sensitive
topics and recursively followed their social contacts. We also leveraged new
natural language processing techniques to pick out trending topics despite the
use of neologisms, named entities, and informal language usage in Chinese
social media. We found that Weibo dynamically adapts to the changing interests
of its users through multiple layers of filtering. The filtering includes both
retroactively searching posts by keyword or repost links to delete them, and
rejecting posts as they are posted. The trend of sensitive topics is
short-lived, suggesting that the censorship is effective in stopping the
""viral"" spread of sensitive issues. We also give evidence that sensitive topics
in Weibo only scarcely propagate beyond a core of sensitive posters.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1101.5446v1,Dialectica Interpretation with Marked Counterexamples,"Goedel's functional ""Dialectica"" interpretation can be used to extract
functional programs from non-constructive proofs in arithmetic by employing two
sorts of higher-order witnessing terms: positive realisers and negative
counterexamples. In the original interpretation decidability of atoms is
required to compute the correct counterexample from a set of candidates. When
combined with recursion, this choice needs to be made for every step in the
extracted program, however, in some special cases the decision on negative
witnesses can be calculated only once. We present a variant of the
interpretation in which the time complexity of extracted programs can be
improved by marking the chosen witness and thus avoiding recomputation. The
achieved effect is similar to using an abortive control operator to interpret
computational content of non-constructive principles.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1712.02547v3,"Persuasive Technology in Reducing Prolonged Sedentary Behavior at Work:
  A Systematic Review","Prolonged sedentary behavior is prevalent among office workers and has been
found to be detrimental to health. Preventing and reducing prolonged sedentary
behavior require interventions, and persuasive technology is expected to make a
contribution in this domain. In this paper, we use the framework of persuasive
system design (PSD) principles to investigate the utilization and effectiveness
of persuasive technology in intervention studies at reducing sedentary behavior
at work. This systematic review reveals that reminders are the most frequently
used PSD principle. The analysis on reminders shows that hourly PC reminders
alone have no significant effect on reducing sedentary behavior at work, while
coupling with education or other informative session seems to be promising.
Details of deployed persuasive technology with behavioral theories and user
experience evaluation are expected to be reported explicitly in the future
intervention studies.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2007.00541v1,"Benchmarking for Metaheuristic Black-Box Optimization: Perspectives and
  Open Challenges","Research on new optimization algorithms is often funded based on the
motivation that such algorithms might improve the capabilities to deal with
real-world and industrially relevant optimization challenges. Besides a huge
variety of different evolutionary and metaheuristic optimization algorithms,
also a large number of test problems and benchmark suites have been developed
and used for comparative assessments of algorithms, in the context of global,
continuous, and black-box optimization. For many of the commonly used synthetic
benchmark problems or artificial fitness landscapes, there are however, no
methods available, to relate the resulting algorithm performance assessments to
technologically relevant real-world optimization problems, or vice versa. Also,
from a theoretical perspective, many of the commonly used benchmark problems
and approaches have little to no generalization value. Based on a mini-review
of publications with critical comments, advice, and new approaches, this
communication aims to give a constructive perspective on several open
challenges and prospective research directions related to systematic and
generalizable benchmarking for black-box optimization.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.05647v1,Analysis and Prediction of NLP Models Via Task Embeddings,"Task embeddings are low-dimensional representations that are trained to
capture task properties. In this paper, we propose MetaEval, a collection of
$101$ NLP tasks. We fit a single transformer to all MetaEval tasks jointly
while conditioning it on learned embeddings. The resulting task embeddings
enable a novel analysis of the space of tasks. We then show that task aspects
can be mapped to task embeddings for new tasks without using any annotated
examples.
  Predicted embeddings can modulate the encoder for zero-shot inference and
outperform a zero-shot baseline on GLUE tasks. The provided multitask setup can
function as a benchmark for future transfer learning research.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1507.06228v2,Training Very Deep Networks,"Theoretical and empirical evidence indicates that the depth of neural
networks is crucial for their success. However, training becomes more difficult
as depth increases, and training of very deep networks remains an open problem.
Here we introduce a new architecture designed to overcome this. Our so-called
highway networks allow unimpeded information flow across many layers on
information highways. They are inspired by Long Short-Term Memory recurrent
networks and use adaptive gating units to regulate the information flow. Even
with hundreds of layers, highway networks can be trained directly through
simple gradient descent. This enables the study of extremely deep and efficient
architectures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0712.3116v1,"Proceedings of the 17th Workshop on Logic-based methods in Programming
  Environments (WLPE 2007)","This volume contains the papers presented at WLPE 2007: the 17th Workshop on
Logic-based Methods in Programming Environments on 13th September, 2007 in
Porto, Portugal. It was held as a satellite workshop of ICLP 2007, the 23th
International Conference on Logic Programming.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1009.6171v1,Cut Elimination for a Logic with Induction and Co-induction,"Proof search has been used to specify a wide range of computation systems. In
order to build a framework for reasoning about such specifications, we make use
of a sequent calculus involving induction and co-induction. These proof
principles are based on a proof theoretic (rather than set-theoretic) notion of
definition. Definitions are akin to logic programs, where the left and right
rules for defined atoms allow one to view theories as ""closed"" or defining
fixed points. The use of definitions and free equality makes it possible to
reason intentionally about syntax. We add in a consistent way rules for pre and
post fixed points, thus allowing the user to reason inductively and
co-inductively about properties of computational system making full use of
higher-order abstract syntax. Consistency is guaranteed via cut-elimination,
where we give the first, to our knowledge, cut-elimination procedure in the
presence of general inductive and co-inductive definitions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1808.05890v2,"A High Order Method for Pricing of Financial Derivatives using Radial
  Basis Function generated Finite Differences","In this paper, we consider the numerical pricing of financial derivatives
using Radial Basis Function generated Finite Differences in space. Such
discretization methods have the advantage of not requiring Cartesian grids.
Instead, the nodes can be placed with higher density in areas where there is a
need for higher accuracy. Still, the discretization matrix is fairly sparse. As
a model problem, we consider the pricing of European options in 2D. Since such
options have a discontinuity in the first derivative of the payoff function
which prohibits high order convergence, we smooth this function using an
established technique for Cartesian grids. Numerical experiments show that we
acquire a fourth order scheme in space, both for the uniform and the nonuniform
node layouts that we use. The high order method with the nonuniform node layout
achieves very high accuracy with relatively few nodes. This renders the
potential for solving pricing problems in higher spatial dimensions since the
computational memory and time demand become much smaller with this method
compared to standard techniques.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1304.3145v1,Exact Algorithms for Weighted and Unweighted Borda Manipulation Problems,"Both weighted and unweighted Borda manipulation problems have been proved
$\mathcal{NP}$-hard. However, there is no exact combinatorial algorithm known
for these problems. In this paper, we initiate the study of exact combinatorial
algorithms for both weighted and unweighted Borda manipulation problems. More
precisely, we propose $O^*((m\cdot 2^m)^{t+1})$time and
$O^*(t^{2m})$time\footnote{$O^*()$ is the $O()$ notation with suppressed
factors polynomial in the size of the input.} combinatorial algorithms for
weighted and unweighted Borda manipulation problems, respectively, where $t$ is
the number of manipulators and $m$ is the number of candidates. Thus, for $t=2$
we solve one of the open problems posted by Betzler et al. [IJCAI 2011]. As a
byproduct of our results, we show that the {{unweighted Borda manipulation}}
problem admits an algorithm of running time $O^*(2^{9m^2\log{m}})$, based on an
integer linear programming technique. Finally, we study the {{unweighted Borda
manipulation}} problem under single-peaked elections and present
polynomial-time algorithms for the problem in the case of two manipulators, in
contrast to the $\mathcal{NP}$-hardness of this case in general settings.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.06907v1,Multiple Source Dual Fault Tolerant BFS Trees,"Let $G=(V,E)$ be a graph with $n$ vertices and $m$ edges, with a designated
set of $\sigma$ sources $S\subseteq V$. The fault tolerant subgraph for any
graph problem maintains a sparse subgraph $H$ of $G$, such that for any set $F$
of $k$ failures, the solution for the graph problem on $G\setminus F$ is
maintained in $H\setminus F$. We address the problem of maintaining a fault
tolerant subgraph for Breath First Search tree (BFS) of the graph from a single
source $s\in V$ (referred as $k$ FT-BFS) or multiple sources $S\subseteq V$
(referred as $k$ FT-MBFS).
  The problem of $k$ FT-BFS was first studied by Parter and Peleg [ESA13]. They
designed an algorithm to compute FT-BFS subgraph of size $O(n^{3/2})$. Further,
they showed how their algorithm can be easily extended to FT-MBFS requiring
$O(\sigma^{1/2}n^{3/2})$ space. They also presented matching lower bounds for
these results. The result was later extended to solve dual FT-BFS by Parter
[PODC15] requiring $O(n^{5/3})$ space, again with matching lower bounds.
However, their result was limited to only edge failures in undirected graphs
and involved very complex analysis. Moreover, their solution doesn't seems to
be directly extendible for dual FT-MBFS problem.
  We present a similar algorithm to solve dual FT-BFS problem with a much
simpler analysis. Moreover, our algorithm also works for vertex failures and
directed graphs, and can be easily extended to handle dual FT-MBFS problem,
matching the lower bound of $O(\sigma^{1/3}n^{5/3})$ space described by Parter
[PODC15].The key difference in our approach is a much simpler classification of
path interactions which formed the basis of the analysis by Parter [PODC15].
Our dual FT-MBFS structure also seamlessly gives a dual fault tolerant spanner
with additive stretch of +2 having size $O(n^{7/8})$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1211.6724v2,On Approximating Graph Bipartization via Node Deletion,"Although the results are correct, it was pointed out that the results follow
from some previously known results. Accordingly, this version of the paper is
withdrawn by the authors.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0312057v1,Abduction in Well-Founded Semantics and Generalized Stable Models,"Abductive logic programming offers a formalism to declaratively express and
solve problems in areas such as diagnosis, planning, belief revision and
hypothetical reasoning. Tabled logic programming offers a computational
mechanism that provides a level of declarativity superior to that of Prolog,
and which has supported successful applications in fields such as parsing,
program analysis, and model checking. In this paper we show how to use tabled
logic programming to evaluate queries to abductive frameworks with integrity
constraints when these frameworks contain both default and explicit negation.
The result is the ability to compute abduction over well-founded semantics with
explicit negation and answer sets. Our approach consists of a transformation
and an evaluation method. The transformation adjoins to each objective literal
$O$ in a program, an objective literal $not(O)$ along with rules that ensure
that $not(O)$ will be true if and only if $O$ is false. We call the resulting
program a {\em dual} program. The evaluation method, \wfsmeth, then operates on
the dual program. \wfsmeth{} is sound and complete for evaluating queries to
abductive frameworks whose entailment method is based on either the
well-founded semantics with explicit negation, or on answer sets. Further,
\wfsmeth{} is asymptotically as efficient as any known method for either class
of problems. In addition, when abduction is not desired, \wfsmeth{} operating
on a dual program provides a novel tabling method for evaluating queries to
ground extended programs whose complexity and termination properties are
similar to those of the best tabling methods for the well-founded semantics. A
publicly available meta-interpreter has been developed for \wfsmeth{} using the
XSB system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1809.08538v2,Strategic Attack & Defense in Security Diffusion Games,"Security games model the confrontation between a defender protecting a set of
targets and an attacker who tries to capture them. A variant of these games
assumes security interdependence between targets, facilitating contagion of an
attack. So far only stochastic spread of an attack has been considered. In this
work, we introduce a version of security games, where the attacker
strategically drives the entire spread of attack and where interconnections
between nodes affect their susceptibility to be captured.
  We find that the strategies effective in the settings without contagion or
with stochastic contagion are no longer feasible when spread of attack is
strategic. While in the former settings it was possible to efficiently find
optimal strategies of the attacker, doing so in the latter setting turns out to
be an NP-complete problem for an arbitrary network. However, for some simpler
network structures, such as cliques, stars, and trees, we show that it is
possible to efficiently find optimal strategies of both players. For arbitrary
networks, we study and compare the efficiency of various heuristic strategies.
As opposed to previous works with no or stochastic contagion, we find that
centrality-based defense is often effective when spread of attack is strategic,
particularly for centrality measures based on the Shapley value.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0307015v2,"Architecture of an Open-Sourced, Extensible Data Warehouse Builder:
  InterBase 6 Data Warehouse Builder (IB-DWB)","We report the development of an open-sourced data warehouse builder,
InterBase Data Warehouse Builder (IB-DWB), based on Borland InterBase 6 Open
Edition Database Server. InterBase 6 is used for its low maintenance and small
footprint. IB-DWB is designed modularly and consists of 5 main components, Data
Plug Platform, Discoverer Platform, Multi-Dimensional Cube Builder, and Query
Supporter, bounded together by a Kernel. It is also an extensible system, made
possible by the Data Plug Platform and the Discoverer Platform. Currently,
extensions are only possible via dynamic linked-libraries (DLLs).
Multi-Dimensional Cube Builder represents a basal mean of data aggregation. The
architectural philosophy of IB-DWB centers around providing a base platform
that is extensible, which is functionally supported by expansion modules.
IB-DWB is currently being hosted by sourceforge.net (Project Unix Name:
ib-dwb), licensed under GNU General Public License, Version 2.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1211.2384v2,Strong Bounds for Evolution in Undirected Graphs,"This work studies the generalized Moran process, as introduced by Lieberman
et al. [Nature, 433:312-316, 2005]. We introduce the parameterized notions of
selective amplifiers and selective suppressors of evolution, i.e. of networks
(graphs) with many ""strong starts"" and many ""weak starts"" for the mutant,
respectively. We first prove the existence of strong selective amplifiers and
of (quite) strong selective suppressors. Furthermore we provide strong upper
bounds and almost tight lower bounds (by proving the ""Thermal Theorem"") for the
traditional notion of fixation probability of Lieberman et al., i.e. assuming a
random initial placement of the mutant.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1903.07594v3,Combining Model and Parameter Uncertainty in Bayesian Neural Networks,"Bayesian neural networks (BNNs) have recently regained a significant amount
of attention in the deep learning community due to the development of scalable
approximate Bayesian inference techniques. There are several advantages of
using Bayesian approach: Parameter and prediction uncertainty become easily
available, facilitating rigid statistical analysis. Furthermore, prior
knowledge can be incorporated. However so far there have been no scalable
techniques capable of combining both model (structural) and parameter
uncertainty. In this paper we introduce the concept of model uncertainty in
BNNs and hence make inference in the joint space of models and parameters.
Moreover, we suggest an adaptation of a scalable variational inference approach
with reparametrization of marginal inclusion probabilities to incorporate the
model space constraints. Finally, we show that incorporating model uncertainty
via Bayesian model averaging and Bayesian model selection allows to drastically
sparsify the structure of BNNs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0703064v2,Automatic Structures: Richness and Limitations,"We study the existence of automatic presentations for various algebraic
structures. An automatic presentation of a structure is a description of the
universe of the structure by a regular set of words, and the interpretation of
the relations by synchronised automata. Our first topic concerns characterising
classes of automatic structures. We supply a characterisation of the automatic
Boolean algebras, and it is proven that the free Abelian group of infinite
rank, as well as certain Fraisse limits, do not have automatic presentations.
In particular, the countably infinite random graph and the random partial order
do not have automatic presentations. Furthermore, no infinite integral domain
is automatic. Our second topic is the isomorphism problem. We prove that the
complexity of the isomorphism problem for the class of all automatic structures
is \Sigma_1^1-complete.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.03374v1,"Pilot-Edge: Distributed Resource Management Along the Edge-to-Cloud
  Continuum","Many science and industry IoT applications necessitate data processing across
the edge-to-cloud continuum to meet performance, security, cost, and privacy
requirements. However, diverse abstractions and infrastructures for managing
resources and tasks across the edge-to-cloud scenario are required. We propose
Pilot-Edge as a common abstraction for resource management across the
edge-to-cloud continuum. Pilot-Edge is based on the pilot abstraction, which
decouples resource and workload management, and provides a
Function-as-a-Service (FaaS) interface for application-level tasks. The
abstraction allows applications to encapsulate common functions in high-level
tasks that can then be configured and deployed across the continuum. We
characterize Pilot-Edge on geographically distributed infrastructures using
machine learning workloads (e.g., k-means and auto-encoders). Our experiments
demonstrate how Pilot-Edge manages distributed resources and allows
applications to evaluate task placement based on multiple factors (e.g., model
complexities, throughput, and latency).",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9907007v2,Cross-Language Information Retrieval for Technical Documents,"This paper proposes a Japanese/English cross-language information retrieval
(CLIR) system targeting technical documents. Our system first translates a
given query containing technical terms into the target language, and then
retrieves documents relevant to the translated query. The translation of
technical terms is still problematic in that technical terms are often compound
words, and thus new terms can be progressively created simply by combining
existing base words. In addition, Japanese often represents loanwords based on
its phonogram. Consequently, existing dictionaries find it difficult to achieve
sufficient coverage. To counter the first problem, we use a compound word
translation method, which uses a bilingual dictionary for base words and
collocational statistics to resolve translation ambiguity. For the second
problem, we propose a transliteration method, which identifies phonetic
equivalents in the target language. We also show the effectiveness of our
system using a test collection for CLIR.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0106024v1,Objects and their computational framework,"Most of the object notions are embedded into a logical domain, especially
when dealing with a database theory. Thus, their properties within a
computational domain are not yet studied properly. The main topic of this paper
is to analyze different concepts of the distinct computational primitive frames
to extract the useful object properties and their possible advantages. Some
important metaoperators are used to unify the approaches and to establish their
possible correspondences.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.6765v1,"Performance comparison between Java and JNI for optimal implementation
  of computational micro-kernels","General purpose CPUs used in high performance computing (HPC) support a
vector instruction set and an out-of-order engine dedicated to increase the
instruction level parallelism. Hence, related optimizations are currently
critical to improve the performance of applications requiring numerical
computation. Moreover, the use of a Java run-time environment such as the
HotSpot Java Virtual Machine (JVM) in high performance computing is a promising
alternative. It benefits from its programming flexibility, productivity and the
performance is ensured by the Just-In-Time (JIT) compiler. Though, the JIT
compiler suffers from two main drawbacks. First, the JIT is a black box for
developers. We have no control over the generated code nor any feedback from
its optimization phases like vectorization. Secondly, the time constraint
narrows down the degree of optimization compared to static compilers like GCC
or LLVM. So, it is compelling to use statically compiled code since it benefits
from additional optimization reducing performance bottlenecks. Java enables to
call native code from dynamic libraries through the Java Native Interface
(JNI). Nevertheless, JNI methods are not inlined and require an additional cost
to be invoked compared to Java ones. Therefore, to benefit from better static
optimization, this call overhead must be leveraged by the amount of computation
performed at each JNI invocation. In this paper we tackle this problem and we
propose to do this analysis for a set of micro-kernels. Our goal is to select
the most efficient implementation considering the amount of computation defined
by the calling context. We also investigate the impact on performance of
several different optimization schemes which are vectorization, out-of-order
optimization, data alignment, method inlining and the use of native memory for
JNI methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.09291v3,Deciding FO2 Alternation for Automata over Finite and Infinite Words,"We consider two-variable first-order logic $\text{FO}^2$ and its quantifier
alternation hierarchies over both finite and infinite words. Our main results
are forbidden patterns for deterministic automata (finite words) and for
Carton-Michel automata (infinite words). In order to give concise patterns, we
allow the use of subwords on paths in finite graphs. This concept is formalized
as subword-patterns. For certain types of subword-patterns there exists a
non-deterministic logspace algorithm to decide their presence or absence in a
given automaton. In particular, this leads to $\mathbf{NL}$ algorithms for
deciding the levels of the $\text{FO}^2$ quantifier alternation hierarchies.
This applies to both full and half levels, each over finite and infinite words.
Moreover, we show that these problems are $\mathbf{NL}$-hard and, hence,
$\mathbf{NL}$-complete.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.05780v2,"Emotional Avatars: The Interplay between Affect and Ownership of a
  Virtual Body","Human bodies influence the owners' affect through posture, facial
expressions, and movement. It remains unclear whether similar links between
virtual bodies and affect exist. Such links could present design opportunities
for virtual environments and advance our understanding of fundamental concepts
of embodied VR.
  An initial outside-the-lab between-subjects study using commodity equipment
presented 207 participants with seven avatar manipulations, related to posture,
facial expression, and speed. We conducted a lab-based between-subjects study
using high-end VR equipment with 41 subjects to clarify affect's impact on body
ownership.
  The results show that some avatar manipulations can subtly influence affect.
Study I found that facial manipulations emerged as most effective in this
regard, particularly for positive affect. Also, body ownership showed a
moderating influence on affect: in Study I body ownership varied with valence
but not with arousal, and Study II showed body ownership to vary with positive
but not with negative affect.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1510.08628v2,"WarpLDA: a Cache Efficient O(1) Algorithm for Latent Dirichlet
  Allocation","Developing efficient and scalable algorithms for Latent Dirichlet Allocation
(LDA) is of wide interest for many applications. Previous work has developed an
O(1) Metropolis-Hastings sampling method for each token. However, the
performance is far from being optimal due to random accesses to the parameter
matrices and frequent cache misses.
  In this paper, we first carefully analyze the memory access efficiency of
existing algorithms for LDA by the scope of random access, which is the size of
the memory region in which random accesses fall, within a short period of time.
We then develop WarpLDA, an LDA sampler which achieves both the best O(1) time
complexity per token and the best O(K) scope of random access. Our empirical
results in a wide range of testing conditions demonstrate that WarpLDA is
consistently 5-15x faster than the state-of-the-art Metropolis-Hastings based
LightLDA, and is comparable or faster than the sparsity aware F+LDA. With
WarpLDA, users can learn up to one million topics from hundreds of millions of
documents in a few hours, at an unprecedentedly throughput of 11G tokens per
second.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.01492v2,Explainable Empirical Risk Minimization,"The successful application of machine learning (ML) methods becomes
increasingly dependent on their interpretability or explainability. Designing
explainable ML systems is instrumental to ensuring transparency of automated
decision-making that targets humans. The explainability of ML methods is also
an essential ingredient for trustworthy artificial intelligence. A key
challenge in ensuring explainability is its dependence on the specific human
user (""explainee""). The users of machine learning methods might have vastly
different background knowledge about machine learning principles. One user
might have a university degree in machine learning or related fields, while
another user might have never received formal training in high-school
mathematics. This paper applies information-theoretic concepts to develop a
novel measure for the subjective explainability of the predictions delivered by
a ML method. We construct this measure via the conditional entropy of
predictions, given a user signal. This user signal might be obtained from user
surveys or biophysical measurements. Our main contribution is the explainable
empirical risk minimization (EERM) principle of learning a hypothesis that
optimally balances between the subjective explainability and risk. The EERM
principle is flexible and can be combined with arbitrary machine learning
models. We present several practical implementations of EERM for linear models
and decision trees. Numerical experiments demonstrate the application of EERM
to detecting the use of inappropriate language on social media.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1706.04692v1,"Bias and high-dimensional adjustment in observational studies of peer
  effects","Peer effects, in which the behavior of an individual is affected by the
behavior of their peers, are posited by multiple theories in the social
sciences. Other processes can also produce behaviors that are correlated in
networks and groups, thereby generating debate about the credibility of
observational (i.e. nonexperimental) studies of peer effects. Randomized field
experiments that identify peer effects, however, are often expensive or
infeasible. Thus, many studies of peer effects use observational data, and
prior evaluations of causal inference methods for adjusting observational data
to estimate peer effects have lacked an experimental ""gold standard"" for
comparison. Here we show, in the context of information and media diffusion on
Facebook, that high-dimensional adjustment of a nonexperimental control group
(677 million observations) using propensity score models produces estimates of
peer effects statistically indistinguishable from those from using a large
randomized experiment (220 million observations). Naive observational
estimators overstate peer effects by 320% and commonly used variables (e.g.,
demographics) offer little bias reduction, but adjusting for a measure of prior
behaviors closely related to the focal behavior reduces bias by 91%.
High-dimensional models adjusting for over 3,700 past behaviors provide
additional bias reduction, such that the full model reduces bias by over 97%.
This experimental evaluation demonstrates that detailed records of individuals'
past behavior can improve studies of social influence, information diffusion,
and imitation; these results are encouraging for the credibility of some
studies but also cautionary for studies of rare or new behaviors. More
generally, these results show how large, high-dimensional data sets and
statistical learning techniques can be used to improve causal inference in the
behavioral sciences.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1403.0636v1,"The path most travelled: Mining road usage patterns from massive call
  data","Rapid urbanization places increasing stress on already burdened
transportation systems, resulting in delays and poor levels of service.
Billions of spatiotemporal call detail records (CDRs) collected from mobile
devices create new opportunities to quantify and solve these problems. However,
there is a need for tools to map new data onto existing transportation
infrastructure. In this work, we propose a system that leverages this data to
identify patterns in road usage. First, we develop an algorithm to mine
billions of calls and learn location transition probabilities of callers. These
transition probabilities are then upscaled with demographic data to estimate
origin-destination (OD) flows of residents between any two intersections of a
city. Next, we implement a distributed incremental traffic assignment algorithm
to route these flows on road networks and estimate congestion and level of
service for each roadway. From this assignment, we construct a bipartite usage
network by connecting census tracts to the roads used by their inhabitants.
Comparing the topologies of the physical road network and bipartite usage
network allows us to classify each road's role in a city's transportation
network and detect causes of local bottlenecks. Finally, we demonstrate an
interactive, web-based visualization platform that allows researchers,
policymakers, and drivers to explore road congestion and usage in a new
dimension. To demonstrate the flexibility of this system, we perform these
analyses in multiple cities across the globe with diverse geographical and
sociodemographic qualities. This platform provides a foundation to build
congestion mitigation solutions and generate new insights into urban mobility.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0712.3203v2,"Solving Medium-Density Subset Sum Problems in Expected Polynomial Time:
  An Enumeration Approach","The subset sum problem (SSP) can be briefly stated as: given a target integer
$E$ and a set $A$ containing $n$ positive integer $a_j$, find a subset of $A$
summing to $E$. The \textit{density} $d$ of an SSP instance is defined by the
ratio of $n$ to $m$, where $m$ is the logarithm of the largest integer within
$A$. Based on the structural and statistical properties of subset sums, we
present an improved enumeration scheme for SSP, and implement it as a complete
and exact algorithm (EnumPlus). The algorithm always equivalently reduces an
instance to be low-density, and then solve it by enumeration. Through this
approach, we show the possibility to design a sole algorithm that can
efficiently solve arbitrary density instance in a uniform way. Furthermore, our
algorithm has considerable performance advantage over previous algorithms.
Firstly, it extends the density scope, in which SSP can be solved in expected
polynomial time. Specifically, It solves SSP in expected $O(n\log{n})$ time
when density $d \geq c\cdot \sqrt{n}/\log{n}$, while the previously best
density scope is $d \geq c\cdot n/(\log{n})^{2}$. In addition, the overall
expected time and space requirement in the average case are proven to be
$O(n^5\log n)$ and $O(n^5)$ respectively. Secondly, in the worst case, it
slightly improves the previously best time complexity of exact algorithms for
SSP. Specifically, the worst-case time complexity of our algorithm is proved to
be $O((n-6)2^{n/2}+n)$, while the previously best result is $O(n2^{n/2})$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0809.0949v3,"Efficient Implementation of the Generalized Tunstall Code Generation
  Algorithm","A method is presented for constructing a Tunstall code that is linear time in
the number of output items. This is an improvement on the state of the art for
non-Bernoulli sources, including Markov sources, which require a (suboptimal)
generalization of Tunstall's algorithm proposed by Savari and analytically
examined by Tabus and Rissanen. In general, if n is the total number of output
leaves across all Tunstall trees, s is the number of trees (states), and D is
the number of leaves of each internal node, then this method takes O((1+(log
s)/D) n) time and O(n) space.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1503.08134v1,"Context-Aware Wireless Small Cell Networks: How to Exploit User
  Information for Resource Allocation","In this paper, a novel context-aware approach for resource allocation in
two-tier wireless small cell networks~(SCNs) is proposed. In particular, the
SCN's users are divided into two types: frequent users, who are regular users
of certain small cells, and occasional users, who are one-time or infrequent
users of a particular small cell. Given such \emph{context} information, each
small cell base station (SCBS) aims to maximize the overall performance
provided to its frequent users, while ensuring that occasional users are also
well serviced. We formulate the problem as a noncooperative game in which the
SCBSs are the players. The strategy of each SCBS is to choose a proper power
allocation so as to optimize a utility function that captures the tradeoff
between the users' quality-of-service gains and the costs in terms of resource
expenditures. We provide a sufficient condition for the existence and
uniqueness of a pure strategy Nash equilibrium for the game, and we show that
this condition is independent of the number of users in the network. Simulation
results show that the proposed context-aware resource allocation game yields
significant performance gains, in terms of the average utility per SCBS,
compared to conventional techniques such as proportional fair allocation and
sum-rate maximization.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0712.0109v1,Recommended Practices for Spreadsheet Testing,"This paper presents the authors recommended practices for spreadsheet
testing. Documented spreadsheet error rates are unacceptable in corporations
today. Although improvements are needed throughout the systems development life
cycle, credible improvement programs must include comprehensive testing.
Several forms of testing are possible, but logic inspection is recommended for
module testing. Logic inspection appears to be feasible for spreadsheet
developers to do, and logic inspection appears to be safe and effective.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1
http://arxiv.org/abs/cs/0205059v2,A Connection-Centric Survey of Recommender Systems Research,"Recommender systems attempt to reduce information overload and retain
customers by selecting a subset of items from a universal set based on user
preferences. While research in recommender systems grew out of information
retrieval and filtering, the topic has steadily advanced into a legitimate and
challenging research area of its own. Recommender systems have traditionally
been studied from a content-based filtering vs. collaborative design
perspective. Recommendations, however, are not delivered within a vacuum, but
rather cast within an informal community of users and social context.
Therefore, ultimately all recommender systems make connections among people and
thus should be surveyed from such a perspective. This viewpoint is
under-emphasized in the recommender systems literature. We therefore take a
connection-oriented viewpoint toward recommender systems research. We posit
that recommendation has an inherently social element and is ultimately intended
to connect people either directly as a result of explicit user modeling or
indirectly through the discovery of relationships implicit in extant data.
Thus, recommender systems are characterized by how they model users to bring
people together: explicitly or implicitly. Finally, user modeling and the
connection-centric viewpoint raise broadening and social issues--such as
evaluation, targeting, and privacy and trust--which we also briefly address.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1504.05009v1,Non-Uniform Robust Network Design in Planar Graphs,"Robust optimization is concerned with constructing solutions that remain
feasible also when a limited number of resources is removed from the solution.
Most studies of robust combinatorial optimization to date made the assumption
that every resource is equally vulnerable, and that the set of scenarios is
implicitly given by a single budget constraint. This paper studies a robustness
model of a different kind. We focus on \textbf{bulk-robustness}, a model
recently introduced~\cite{bulk} for addressing the need to model non-uniform
failure patterns in systems.
  We significantly extend the techniques used in~\cite{bulk} to design
approximation algorithm for bulk-robust network design problems in planar
graphs. Our techniques use an augmentation framework, combined with linear
programming (LP) rounding that depends on a planar embedding of the input
graph. A connection to cut covering problems and the dominating set problem in
circle graphs is established. Our methods use few of the specifics of
bulk-robust optimization, hence it is conceivable that they can be adapted to
solve other robust network design problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.09685v1,Mirror Ritual: An Affective Interface for Emotional Self-Reflection,"This paper introduces a new form of real-time affective interface that
engages the user in a process of conceptualisation of their emotional state.
Inspired by Barrett's Theory of Constructed Emotion, `Mirror Ritual' aims to
expand upon the user's accessible emotion concepts, and to ultimately provoke
emotional reflection and regulation. The interface uses classified emotions --
obtained through facial expression recognition -- as a basis for dynamically
generating poetry. The perceived emotion is used to seed a poetry generation
system based on OpenAI's GPT-2 model, fine-tuned on a specially curated corpus.
We evaluate the device's ability to foster a personalised, meaningful
experience for individual users over a sustained period. A qualitative analysis
revealed that participants were able to affectively engage with the mirror,
with each participant developing a unique interpretation of its poetry in the
context of their own emotional landscape.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.04884v1,"cltorch: a Hardware-Agnostic Backend for the Torch Deep Neural Network
  Library, Based on OpenCL","This paper presents cltorch, a hardware-agnostic backend for the Torch neural
network framework. cltorch enables training of deep neural networks on GPUs
from diverse hardware vendors, including AMD, NVIDIA, and Intel. cltorch
contains sufficient implementation to run models such as AlexNet, VGG,
Overfeat, and GoogleNet. It is written using the OpenCL language, a portable
compute language, governed by the Khronos Group. cltorch is the top-ranked
hardware-agnostic machine learning framework on Chintala's convnet-benchmarks
page.
  This paper presents the technical challenges encountered whilst creating the
cltorch backend for Torch, and looks in detail at the challenges related to
obtaining a fast hardware-agnostic implementation.
  The convolutional layers are identified as the key area of focus for
accelerating hardware-agnostic frameworks. Possible approaches to accelerating
the convolutional implementation are identified including: implementation of
the convolutions using the implicitgemm or winograd algorithm, using a GEMM
implementation adapted to the geometries associated with the convolutional
algorithm, or using a pluggable hardware-specific convolutional implementation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.09419v1,"The role of IT ambidexterity, digital dynamic capability and knowledge
  processes as enablers of patient agility: an empirical study","There is a limited understanding of IT's role as a crucial enabler of patient
agility and the department's ability to respond to patient's needs and wishes
adequately. This study's objective is to contribute to the insights of the
validity of the hypothesized relationship between IT resources, practices and
capabilities, and hospital departments' knowledge processes and the
department's ability to adequately sense and respond to patient needs and
wishes, i.e., patient agility. This study conveniently sampled data from 107
clinical hospital departments in the Netherlands and uses structural equation
modeling for model assessment. IT ambidexterity positively enhances the
development of a digital dynamic capability. Likewise, IT ambidexterity also
positively impacts the hospital department's knowledge processes. Both digital
dynamic capability and knowledge processes positively influence patient
agility. IT ambidexterity promotes taking advantage of IT resources and
experiments to reshape patient services and enhance patient agility.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0
http://arxiv.org/abs/1212.4129v2,"Graph Products Revisited: Tight Approximation Hardness of Induced
  Matching, Poset Dimension and More","Graph product is a fundamental tool with rich applications in both graph
theory and theoretical computer science. It is usually studied in the form
$f(G*H)$ where $G$ and $H$ are graphs, * is a graph product and $f$ is a graph
property. For example, if $f$ is the independence number and * is the
disjunctive product, then the product is known to be multiplicative:
$f(G*H)=f(G)f(H)$.
  In this paper, we study graph products in the following non-standard form:
$f((G\oplus H)*J)$ where $G$, $H$ and $J$ are graphs, $\oplus$ and * are two
different graph products and $f$ is a graph property. We show that if $f$ is
the induced and semi-induced matching number, then for some products $\oplus$
and *, it is subadditive in the sense that $f((G\oplus H)*J)\leq
f(G*J)+f(H*J)$. Moreover, when $f$ is the poset dimension number, it is almost
subadditive.
  As applications of this result (we only need $J=K_2$ here), we obtain tight
hardness of approximation for various problems in discrete mathematics and
computer science: bipartite induced and semi-induced matching (a.k.a. maximum
expanding sequences), poset dimension, maximum feasible subsystem with 0/1
coefficients, unit-demand min-buying and single-minded pricing, donation center
location, boxicity, cubicity, threshold dimension and independent packing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0609119v2,"Verification, Validation and Integrity of Distributed and Interchanged
  Rule Based Policies and Contracts in the Semantic Web","Rule-based policy and contract systems have rarely been studied in terms of
their software engineering properties. This is a serious omission, because in
rule-based policy or contract representation languages rules are being used as
a declarative programming language to formalize real-world decision logic and
create IS production systems upon. This paper adopts an SE methodology from
extreme programming, namely test driven development, and discusses how it can
be adapted to verification, validation and integrity testing (V&V&I) of policy
and contract specifications. Since, the test-driven approach focuses on the
behavioral aspects and the drawn conclusions instead of the structure of the
rule base and the causes of faults, it is independent of the complexity of the
rule language and the system under test and thus much easier to use and
understand for the rule engineer and the user.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1503.06842v3,"Apuntes sobre teora del comportamiento corrupto: nociones
  cibernticas e informticas para una actualizacin de la ecuacin de
  Klitgaard","This essay presents an exploration of elements from information theory and
cibernetics on the struggle against corruption behavior in public sector and
beyond; the existence of an exemplary or corrupt ethical equilibriums are
explored by updating Klitgaard corruption formula along with the presence of
information pressure, entropy and cibernetics servomechanisms in digital
societies, including alternatives and sistemics approaches for further
anti-corruption policies implementation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1406.7768v1,"From a Competition for Self-Driving Miniature Cars to a Standardized
  Experimental Platform: Concept, Models, Architecture, and Evaluation","Context: Competitions for self-driving cars facilitated the development and
research in the domain of autonomous vehicles towards potential solutions for
the future mobility.
  Objective: Miniature vehicles can bridge the gap between simulation-based
evaluations of algorithms relying on simplified models, and those
time-consuming vehicle tests on real-scale proving grounds.
  Method: This article combines findings from a systematic literature review,
an in-depth analysis of results and technical concepts from contestants in a
competition for self-driving miniature cars, and experiences of participating
in the 2013 competition for self-driving cars.
  Results: A simulation-based development platform for real-scale vehicles has
been adapted to support the development of a self-driving miniature car.
Furthermore, a standardized platform was designed and realized to enable
research and experiments in the context of future mobility solutions.
  Conclusion: A clear separation between algorithm conceptualization and
validation in a model-based simulation environment enabled efficient and
riskless experiments and validation. The design of a reusable, low-cost, and
energy-efficient hardware architecture utilizing a standardized
software/hardware interface enables experiments, which would otherwise require
resources like a large real-scale test track.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0006016v1,The X-Files: Investigating Alien Performance in a Thin-client World,"Many scientific applications use the X11 window environment; an open source
windows GUI standard employing a client/server architecture. X11 promotes:
distributed computing, thin-client functionality, cheap desktop displays,
compatibility with heterogeneous servers, remote services and administration,
and greater maturity than newer web technologies. This paper details the
author's investigations into close encounters with alien performance in
X11-based seismic applications running on a 200-node cluster, backed by 2 TB of
mass storage. End-users cited two significant UFOs (Unidentified Faulty
Operations) i) long application launch times and ii) poor interactive response
times. The paper is divided into three major sections describing Close
Encounters of the 1st Kind: citings of UFO experiences, the 2nd Kind: recording
evidence of a UFO, and the 3rd Kind: contact and analysis. UFOs do exist and
this investigation presents a real case study for evaluating workload analysis
and other diagnostic tools.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1911.02408v2,On the Average Complexity of the $k$-Level,"Let ${\cal L}$ be an arrangement of $n$ lines in the Euclidean plane. The
\emph{$k$-level} of ${\cal L}$ consists of all vertices $v$ of the arrangement
which have exactly $k$ lines of ${\cal L}$ passing below $v$. The complexity
(the maximum size) of the $k$-level in a line arrangement has been widely
studied. In 1998 Dey proved an upper bound of $O(n\cdot (k+1)^{1/3})$. Due to
the correspondence between lines in the plane and great-circles on the sphere,
the asymptotic bounds carry over to arrangements of great-circles on the
sphere, where the $k$-level denotes the vertices at distance at most $k$ to a
marked cell, the \emph{south pole}.
  We prove an upper bound of $O((k+1)^2)$ on the expected complexity of the
$k$-level in great-circle arrangements if the south pole is chosen uniformly at
random among all cells.
  We also consider arrangements of great $(d-1)$-spheres on the sphere
$\mathbb{S}^d$ which are orthogonal to a set of random points on
$\mathbb{S}^d$. In this model, we prove that the expected complexity of the
$k$-level is of order $\Theta((k+1)^{d-1})$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1604.00536v1,Improving SAT Solvers via Blocked Clause Decomposition,"The decision variable selection policy used by the most competitive CDCL
(Conflict-Driven Clause Learning) SAT solvers is either VSIDS (Variable State
Independent Decaying Sum) or its variants such as exponential version EVSIDS.
The common characteristic of VSIDS and its variants is to make use of
statistical information in the solving process, but ignore structure
information of the problem. For this reason, this paper modifies the decision
variable selection policy, and presents a SAT solving technique based on BCD
(Blocked Clause Decomposition). Its basic idea is that a part of decision
variables are selected by VSIDS heuristic, while another part of decision
variables are selected by blocked sets that are obtained by BCD. Compared with
the existing BCD-based technique, our technique is simple, and need not to
reencode CNF formulas. SAT solvers for certified UNSAT track can apply also our
BCD-based technique. Our experiments on application benchmarks demonstrate that
the new variables selection policy based on BCD can increase the performance of
SAT solvers such as abcdSAT. The solver with BCD solved an instance from the
SAT Race 2015 that was not solved by any solver so far. This shows that in some
cases, the heuristic based on structure information is more efficient than that
based on statistical information.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.05236v1,"Design and fabrication of solar powered remote controlled all terrain
  sprayer and mower robot","Manual spraying of pesticides and herbicides to crops and weed inhibitors
onto the field are quite laborious work to humans. Manual trimming of selected
unwanted plants or harvested crops from the field is also difficult. Our
project proposes a multipurpose solar powered, flexible, Remote Controlled,
semi-automated spraying robot with 4 Degrees of Freedom (DoF) in spatial
movement, with an additional plant mowing equipment. The robot is designed to
spray pesticide/insecticide directly onto individual lesions minimizing wastage
or excess chemical spraying, hence making the system cost effective and also
environment friendly. It is designed to cut down undesired plants selectively
by remotely controlling the start and stop of the mowing system. Alternatively,
it also serves the purpose of maintaining lawns and sports field made of grass.
The same system can be used for water spraying and mowing the grass to desired
levels, leading to proper maintenance of the field. The robot is designed to
move at 1.4m/s, with an effective spraying area of 0.98 sq. m. by the nozzle
and an effective cutting area of 0.3 sq. m. by the mower, when stationary. The
prototype has a battery back-up of 7.2hrs under minimum load conditions.",0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0
http://arxiv.org/abs/1805.00407v1,Cooperative system of emission source localization based on SDF,"Efficient and precise location of emission sources in an urbanized
environment is very important in electronic warfare. Therefore, unmanned aerial
vehicles (UAVs) are increasingly used for such tasks. In this paper, we present
the cooperation of several UAVs creating a wireless sensor network (WSN) that
locates the emission source. In the proposed WSN, the location is based on
spectrum sensing and the signal Doppler frequency method. The paper presents
the concept of the system. Simulation studies are used to assess the efficiency
of the cooperative WSN. In this case, the location effectiveness for the WSN is
compared to the single UAV.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.07347v1,"Object Detection and Pose Estimation from RGB and Depth Data for
  Real-time, Adaptive Robotic Grasping","In recent times, object detection and pose estimation have gained significant
attention in the context of robotic vision applications. Both the
identification of objects of interest as well as the estimation of their pose
remain important capabilities in order for robots to provide effective
assistance for numerous robotic applications ranging from household tasks to
industrial manipulation. This problem is particularly challenging because of
the heterogeneity of objects having different and potentially complex shapes,
and the difficulties arising due to background clutter and partial occlusions
between objects. As the main contribution of this work, we propose a system
that performs real-time object detection and pose estimation, for the purpose
of dynamic robot grasping. The robot has been pre-trained to perform a small
set of canonical grasps from a few fixed poses for each object. When presented
with an unknown object in an arbitrary pose, the proposed approach allows the
robot to detect the object identity and its actual pose, and then adapt a
canonical grasp in order to be used with the new pose. For training, the system
defines a canonical grasp by capturing the relative pose of an object with
respect to the gripper attached to the robot's wrist. During testing, once a
new pose is detected, a canonical grasp for the object is identified and then
dynamically adapted by adjusting the robot arm's joint angles, so that the
gripper can grasp the object in its new pose. We conducted experiments using a
humanoid PR2 robot and showed that the proposed framework can detect
well-textured objects, and provide accurate pose estimation in the presence of
tolerable amounts of out-of-plane rotation. The performance is also illustrated
by the robot successfully grasping objects from a wide range of arbitrary
poses.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1307.7096v1,"Toward Recovering Complete SRS for Softbody Simulation System and a
  Sample Application -- a Team 9a SOEN6481 W13 Project Report","This document aims at specifying the requirements and capturing the needs of
users for building a softbody simulation system. This system has different
applications ranging from computer games to surgery training which facilitates
the creation and visualization of a certain softbody object. It also allows
users to interact with created object at real time. A softbody or deformable
object is an object whose shape changes due to an external force. Deformation
type varies depending on the amount of object deformation. Each object can have
multiple layers and each layer can have its own properties. So layers can be
different in pressure, density and motion.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2105.04607v1,Efficient Self-Supervised Data Collection for Offline Robot Learning,"A practical approach to robot reinforcement learning is to first collect a
large batch of real or simulated robot interaction data, using some data
collection policy, and then learn from this data to perform various tasks,
using offline learning algorithms. Previous work focused on manually designing
the data collection policy, and on tasks where suitable policies can easily be
designed, such as random picking policies for collecting data about object
grasping. For more complex tasks, however, it may be difficult to find a data
collection policy that explores the environment effectively, and produces data
that is diverse enough for the downstream task. In this work, we propose that
data collection policies should actively explore the environment to collect
diverse data. In particular, we develop a simple-yet-effective goal-conditioned
reinforcement-learning method that actively focuses data collection on novel
observations, thereby collecting a diverse data-set. We evaluate our method on
simulated robot manipulation tasks with visual inputs and show that the
improved diversity of active data collection leads to significant improvements
in the downstream learning tasks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1305.2694v3,Further Refinements of Miller Algorithm on Edwards curves,"Recently, Edwards curves have received a lot of attention in the
cryptographic community due to their fast scalar multiplication algorithms.
Then, many works on the application of these curves to pairing-based
cryptography have been introduced. Xu and Lin (CT-RSA, 2010) presented
refinements to improve the Miller algorithm that is central role compute
pairings on Edwards curves. In this paper, we study further refinements to
Miller algorithm. Our approach is generic, hence it allow to compute both Weil
and Tate pairings on pairing-friendly Edwards curves of any embedding degree.
We analyze and show that our algorithm is faster than the original Miller
algorithm and the Xu-Lin's refinements.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1102.3493v2,"Scalable constructions of fractional repetition codes in distributed
  storage systems","In distributed storage systems built using commodity hardware, it is
necessary to have data redundancy in order to ensure system reliability. In
such systems, it is also often desirable to be able to quickly repair storage
nodes that fail. We consider a scheme--introduced by El Rouayheb and
Ramchandran--which uses combinatorial block design in order to design storage
systems that enable efficient (and exact) node repair. In this work, we
investigate systems where node sizes may be much larger than replication
degrees, and explicitly provide algorithms for constructing these storage
designs. Our designs, which are related to projective geometries, are based on
the construction of bipartite cage graphs (with girth 6) and the concept of
mutually-orthogonal Latin squares. Via these constructions, we can guarantee
that the resulting designs require the fewest number of storage nodes for the
given parameters, and can further show that these systems can be easily
expanded without need for frequent reconfiguration.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9812023v1,Virtual Kathakali : Gesture Driven Metamorphosis,"Training in motor skills such as athletics, dance, or gymnastics is not
possible today except in the direct presence of the coach/instructor. This
paper describes a computer vision based gesture recognition system which is
used to metamorphose the user into a Virtual person, e.g. as a Kathakali
dancer, which is graphically recreated at a near or diatant location. Thus this
can be seen by an off-site coach using low-bandwidth joint-motion data which
permits real time animation. The metamorphosis involves altering the appearance
and identity of the user and also creating a specific environment possibly in
interaction with other virtual creatures.
  A robust vision module is used to identify the user, based on very simple
binary image processing in real time which also manages to resolve
self-occlusion, correct for clothing/colour and other variations among users.
Gestures are identified by locating key points at the shoulder, elbow and wrist
joint, which are then recreated in an articulated humanoid model, which in this
instance, representes a Kathakali dancer in elaborate traditional dress. Unlike
glove based or other and movement tracking systems, this application requires
the user to wear no hardwire devices and is aimed at making gesture tracking
simpler, cheaper, and more user friendly.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1609.00322v2,Open Call-by-Value (Extended Version),"The elegant theory of the call-by-value lambda-calculus relies on weak
evaluation and closed terms, that are natural hypotheses in the study of
programming languages. To model proof assistants, however, strong evaluation
and open terms are required, and it is well known that the operational
semantics of call-by-value becomes problematic in this case. Here we study the
intermediate setting -- that we call Open Call-by-Value -- of weak evaluation
with open terms, on top of which Gr\'egoire and Leroy designed the abstract
machine of Coq. Various calculi for Open Call-by-Value already exist, each one
with its pros and cons. This paper presents a detailed comparative study of the
operational semantics of four of them, coming from different areas such as the
study of abstract machines, denotational semantics, linear logic proof nets,
and sequent calculus. We show that these calculi are all equivalent from a
termination point of view, justifying the slogan Open Call-by-Value.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.03242v3,PolyAdd: Polynomial Formal Verification of Adder Circuits,"Only by formal verification approaches functional correctness can be ensured.
While for many circuits fast verification is possible, in other cases the
approaches fail. In general no efficient algorithms can be given, since the
underlying verification problem is NP-complete. In this paper we prove that for
different types of adder circuits polynomial verification can be ensured based
on BDDs. While it is known that the output functions for addition are
polynomially bounded, we show in the following that the entire construction
process can be carried out in polynomial time. This is shown for the simple
Ripple Carry Adder, but also for fast adders like the Conditional Sum Adder and
the Carry Look Ahead Adder. Properties about the adder function are proven and
the core principle of polynomial verification is described that can also be
extended to other classes of functions and circuit realizations.",0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1604.01275v1,On the importance and feasibility of forecasting data in sensors,"The first generation of wireless sensor nodes have constrained energy
resources and computational power, which discourages applications to process
any task other than measuring and transmitting towards a central server.
However, nowadays, sensor networks tend to be incorporated into the Internet of
Things and the hardware evolution may change the old strategy of avoiding data
computation in the sensor nodes. In this paper, we show the importance of
reducing the number of transmissions in sensor networks and present the use of
forecasting methods as a way of doing it. Experiments using real sensor data
show that state-of-the-art forecasting methods can be successfully implemented
in the sensor nodes to keep the quality of their measurements and reduce up to
30% of their transmissions, lowering the channel utilization. We conclude that
there is an old paradigm that is no longer the most beneficial, which is the
strategy of always transmitting a measurement when it differs by more than a
threshold from the last one transmitted. Adopting more complex forecasting
methods in the sensor nodes is the alternative to significantly reduce the
number of transmissions without compromising the quality of their measurements,
and therefore support the exponential growth of the Internet of Things.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9809123v1,A role of constraint in self-organization,"In this paper we introduce a neural network model of self-organization. This
model uses a variation of Hebb rule for updating its synaptic weights, and
surely converges to the equilibrium status. The key point of the convergence is
the update rule that constrains the total synaptic weight and this seems to
make the model stable. We investigate the role of the constraint and show that
it is the constraint that makes the model stable. For analyzing this setting,
we propose a simple probabilistic game that models the neural network and the
self-organization process. Then, we investigate the characteristics of this
game, namely, the probability that the game becomes stable and the number of
the steps it takes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1005.0518v1,On Decidable Growth-Rate Properties of Imperative Programs,"In 2008, Ben-Amram, Jones and Kristiansen showed that for a simple ""core""
programming language - an imperative language with bounded loops, and
arithmetics limited to addition and multiplication - it was possible to decide
precisely whether a program had certain growth-rate properties, namely
polynomial (or linear) bounds on computed values, or on the running time.
  This work emphasized the role of the core language in mitigating the
notorious undecidability of program properties, so that one deals with
decidable problems.
  A natural and intriguing problem was whether more elements can be added to
the core language, improving its utility, while keeping the growth-rate
properties decidable. In particular, the method presented could not handle a
command that resets a variable to zero. This paper shows how to handle resets.
The analysis is given in a logical style (proof rules), and its complexity is
shown to be PSPACE-complete (in contrast, without resets, the problem was
PTIME). The analysis algorithm evolved from the previous solution in an
interesting way: focus was shifted from proving a bound to disproving it, and
the algorithm works top-down rather than bottom-up.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.01076v2,The complexity of approximating the complex-valued Potts model,"We study the complexity of approximating the partition function of the
$q$-state Potts model and the closely related Tutte polynomial for complex
values of the underlying parameters. Apart from the classical connections with
quantum computing and phase transitions in statistical physics, recent work in
approximate counting has shown that the behaviour in the complex plane, and
more precisely the location of zeros, is strongly connected with the complexity
of the approximation problem, even for positive real-valued parameters.
Previous work in the complex plane by Goldberg and Guo focused on $q=2$, which
corresponds to the case of the Ising model; for $q>2$, the behaviour in the
complex plane is not as well understood and most work applies only to the
real-valued Tutte plane.
  Our main result is a complete classification of the complexity of the
approximation problems for all non-real values of the parameters, by
establishing \#P-hardness results that apply even when restricted to planar
graphs. Our techniques apply to all $q\geq 2$ and further complement/refine
previous results both for the Ising model and the Tutte plane, answering in
particular a question raised by Bordewich, Freedman, Lov\'{a}sz and Welsh in
the context of quantum computations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0611029v3,Linear Encodings of Bounded LTL Model Checking,"We consider the problem of bounded model checking (BMC) for linear temporal
logic (LTL). We present several efficient encodings that have size linear in
the bound. Furthermore, we show how the encodings can be extended to LTL with
past operators (PLTL). The generalised encoding is still of linear size, but
cannot detect minimal length counterexamples. By using the virtual unrolling
technique minimal length counterexamples can be captured, however, the size of
the encoding is quadratic in the specification. We also extend virtual
unrolling to Buchi automata, enabling them to accept minimal length
counterexamples.
  Our BMC encodings can be made incremental in order to benefit from
incremental SAT technology. With fairly small modifications the incremental
encoding can be further enhanced with a termination check, allowing us to prove
properties with BMC. Experiments clearly show that our new encodings improve
performance of BMC considerably, particularly in the case of the incremental
encoding, and that they are very competitive for finding bugs. An analysis of
the liveness-to-safety transformation reveals many similarities to the BMC
encodings in this paper. Using the liveness-to-safety translation with
BDD-based invariant checking results in an efficient method to find shortest
counterexamples that complements the BMC-based approach.",0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.06965v2,GPU System Calls,"GPUs are becoming first-class compute citizens and are being tasked to
perform increasingly complex work. Modern GPUs increasingly support
programmability- enhancing features such as shared virtual memory and hardware
cache coherence, enabling them to run a wider variety of programs. But a key
aspect of general-purpose programming where GPUs are still found lacking is the
ability to invoke system calls. We explore how to directly invoke generic
system calls in GPU programs. We examine how system calls should be meshed with
prevailing GPGPU programming models where thousands of threads are organized in
a hierarchy of execution groups: Should a system call be invoked at the
individual GPU task, or at different execution group levels? What are
reasonable ordering semantics for GPU system calls across these hierarchy of
execution groups? To study these questions, we implemented GENESYS -- a
mechanism to allow GPU pro- grams to invoke system calls in the Linux operating
system. Numerous subtle changes to Linux were necessary, as the existing kernel
assumes that only CPUs invoke system calls. We analyze the performance of
GENESYS using micro-benchmarks and three applications that exercise the
filesystem, networking, and memory allocation subsystems of the kernel. We
conclude by analyzing the suitability of all of Linux's system calls for the
GPU.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0704.0062v1,On-line Viterbi Algorithm and Its Relationship to Random Walks,"In this paper, we introduce the on-line Viterbi algorithm for decoding hidden
Markov models (HMMs) in much smaller than linear space. Our analysis on
two-state HMMs suggests that the expected maximum memory used to decode
sequence of length $n$ with $m$-state HMM can be as low as $\Theta(m\log n)$,
without a significant slow-down compared to the classical Viterbi algorithm.
Classical Viterbi algorithm requires $O(mn)$ space, which is impractical for
analysis of long DNA sequences (such as complete human genome chromosomes) and
for continuous data streams. We also experimentally demonstrate the performance
of the on-line Viterbi algorithm on a simple HMM for gene finding on both
simulated and real DNA sequences.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9906016v1,Automatically Selecting Useful Phrases for Dialogue Act Tagging,"We present an empirical investigation of various ways to automatically
identify phrases in a tagged corpus that are useful for dialogue act tagging.
We found that a new method (which measures a phrase's deviation from an
optimally-predictive phrase), enhanced with a lexical filtering mechanism,
produces significantly better cues than manually-selected cue phrases, the
exhaustive set of phrases in a training corpus, and phrases chosen by
traditional metrics, like mutual information and information gain.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.06268v1,"Smoothed Bernstein Online Aggregation for Day-Ahead Electricity Demand
  Forecasting","We present a winning method of the IEEE DataPort Competition on Day-Ahead
Electricity Demand Forecasting: Post-COVID Paradigm. The day-ahead load
forecasting approach is based on online forecast combination of multiple point
prediction models. It contains four steps: i) data cleaning and preprocessing,
ii) a holiday adjustment procedure, iii) training of individual forecasting
models, iv) forecast combination by smoothed Bernstein Online Aggregation
(BOA). The approach is flexible and can quickly adopt to new energy system
situations as they occurred during and after COVID-19 shutdowns. The pool of
individual prediction models ranges from rather simple time series models to
sophisticated models like generalized additive models (GAMs) and
high-dimensional linear models estimated by lasso. They incorporate
autoregressive, calendar and weather effects efficiently. All steps contain
novel concepts that contribute to the excellent forecasting performance of the
proposed method. This holds particularly for the holiday adjustment procedure
and the fully adaptive smoothed BOA approach.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9906030v1,SCR3: towards usability of formal methods,"This paper gives an overview of SCR3 -- a toolset designed to increase the
usability of formal methods for software development. Formal requirements are
specified in SCR3 in an easy to use and review format, and then used in
checking requirements for correctness and in verifying consistency between
annotated code and requirements.
  In this paper we discuss motivations behind this work, describe several tools
which are part of SCR3, and illustrate their operation on an example of a
Cruise Control system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1909.03054v1,"Calibrating Wayfinding Decisions in Pedestrian Simulation Models: The
  Entropy Map","This paper presents entropy maps, an approach to describing and visualising
uncertainty among alternative potential movement intentions in pedestrian
simulation models. In particular, entropy maps show the instantaneous level of
randomness in decisions of a pedestrian agent situated in a specific point of
the simulated environment with an heatmap approach. Experimental results
highlighting the relevance of this tool supporting modelers are provided and
discussed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.15525v3,Self-Learning Threshold-Based Load Balancing,"We consider a large-scale service system where incoming tasks have to be
instantaneously dispatched to one out of many parallel server pools. The
user-perceived performance degrades with the number of concurrent tasks and the
dispatcher aims at maximizing the overall quality-of-service by balancing the
load through a simple threshold policy. We demonstrate that such a policy is
optimal on the fluid and diffusion scales, while only involving a small
communication overhead, which is crucial for large-scale deployments. In order
to set the threshold optimally, it is important, however, to learn the load of
the system, which may be unknown. For that purpose, we design a control rule
for tuning the threshold in an online manner. We derive conditions which
guarantee that this adaptive threshold settles at the optimal value, along with
estimates for the time until this happens. In addition, we provide numerical
experiments which support the theoretical results and further indicate that our
policy copes effectively with time-varying demand patterns.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9912017v1,"Mixed-Level Knowledge Representation and Variable-Depth Inference in
  Natural Language Processing","A system is described that uses a mixed-level knowledge representation based
on standard Horn Clause Logic to represent (part of) the meaning of natural
language documents. A variable-depth search strategy is outlined that
distinguishes between the different levels of abstraction in the knowledge
representation to locate specific passages in the documents. A detailed
description of the linguistic aspects of the system is given. Mixed-level
representations as well as variable-depth search strategies are applicable in
fields outside that of NLP.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1303.4533v2,Two Variable vs. Linear Temporal Logic in Model Checking and Games,"Model checking linear-time properties expressed in first-order logic has
non-elementary complexity, and thus various restricted logical languages are
employed. In this paper we consider two such restricted specification logics,
linear temporal logic (LTL) and two-variable first-order logic (FO2). LTL is
more expressive but FO2 can be more succinct, and hence it is not clear which
should be easier to verify. We take a comprehensive look at the issue, giving a
comparison of verification problems for FO2, LTL, and various sublogics thereof
across a wide range of models. In particular, we look at unary temporal logic
(UTL), a subset of LTL that is expressively equivalent to FO2; we also consider
the stutter-free fragment of FO2, obtained by omitting the successor relation,
and the expressively equivalent fragment of UTL, obtained by omitting the next
and previous connectives. We give three logic-to-automata translations which
can be used to give upper bounds for FO2 and UTL and various sublogics. We
apply these to get new bounds for both non-deterministic systems (hierarchical
and recursive state machines, games) and for probabilistic systems (Markov
chains, recursive Markov chains, and Markov decision processes). We couple
these with matching lower-bound arguments. Next, we look at combining FO2
verification techniques with those for LTL. We present here a language that
subsumes both FO2 and LTL, and inherits the model checking properties of both
languages. Our results give both a unified approach to understanding the
behaviour of FO2 and LTL, along with a nearly comprehensive picture of the
complexity of verification for these logics and their sublogics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0803.0248v1,Networks become navigable as nodes move and forget,"We propose a dynamical process for network evolution, aiming at explaining
the emergence of the small world phenomenon, i.e., the statistical observation
that any pair of individuals are linked by a short chain of acquaintances
computable by a simple decentralized routing algorithm, known as greedy
routing. Previously proposed dynamical processes enabled to demonstrate
experimentally (by simulations) that the small world phenomenon can emerge from
local dynamics. However, the analysis of greedy routing using the probability
distributions arising from these dynamics is quite complex because of mutual
dependencies. In contrast, our process enables complete formal analysis. It is
based on the combination of two simple processes: a random walk process, and an
harmonic forgetting process. Both processes reflect natural behaviors of the
individuals, viewed as nodes in the network of inter-individual acquaintances.
We prove that, in k-dimensional lattices, the combination of these two
processes generates long-range links mutually independently distributed as a
k-harmonic distribution. We analyze the performances of greedy routing at the
stationary regime of our process, and prove that the expected number of steps
for routing from any source to any target in any multidimensional lattice is a
polylogarithmic function of the distance between the two nodes in the lattice.
Up to our knowledge, these results are the first formal proof that navigability
in small worlds can emerge from a dynamical process for network evolution. Our
dynamical process can find practical applications to the design of spatial
gossip and resource location protocols.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1604.03515v5,"Horn Fragments of the Halpern-Shoham Interval Temporal Logic (Technical
  Report)","We investigate the satisfiability problem for Horn fragments of the
Halpern-Shoham interval temporal logic depending on the type (box or diamond)
of the interval modal operators, the type of the underlying linear order
(discrete or dense), and the type of semantics for the interval relations
(reflexive or irreflexive). For example, we show that satisfiability of Horn
formulas with diamonds is undecidable for any type of linear orders and
semantics. On the contrary, satisfiability of Horn formulas with boxes is
tractable over both discrete and dense orders under the reflexive semantics and
over dense orders under the irreflexive semantics, but becomes undecidable over
discrete orders under the irreflexive semantics. Satisfiability of binary Horn
formulas with both boxes and diamonds is always undecidable under the
irreflexive semantics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1001.0440v1,Tutoring System for Dance Learning,"Recent advances in hardware sophistication related to graphics display, audio
and video devices made available a large number of multimedia and hypermedia
applications. These multimedia applications need to store and retrieve the
different forms of media like text, hypertext, graphics, still images,
animations, audio and video. Dance is one of the important cultural forms of a
nation and dance video is one such multimedia types. Archiving and retrieving
the required semantics from these dance media collections is a crucial and
demanding multimedia application. This paper summarizes the difference dance
video archival techniques and systems. Keywords: Multimedia, Culture Media,
Metadata archival and retrieval systems, MPEG-7, XML.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1401.8175v3,Equilibrium Points of an AND-OR Tree: under Constraints on Probability,"We study a probability distribution d on the truth assignments to a uniform
binary AND-OR tree. Liu and Tanaka [2007, Inform. Process. Lett.] showed the
following: If d achieves the equilibrium among independent distributions (ID)
then d is an independent identical distribution (IID). We show a stronger form
of the above result. Given a real number r such that 0 < r < 1, we consider a
constraint that the probability of the root node having the value 0 is r. Our
main result is the following: When we restrict ourselves to IDs satisfying this
constraint, the above result of Liu and Tanaka still holds. The proof employs
clever tricks of induction. In particular, we show two fundamental
relationships between expected cost and probability in an IID on an OR-AND
tree: (1) The ratio of the cost to the probability (of the root having the
value 0) is a decreasing function of the probability x of the leaf. (2) The
ratio of derivative of the cost to the derivative of the probability is a
decreasing function of x, too.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.06617v1,(R)SE challenges in HPC,"We discuss some specific software engineering challenges in the field of
high-performance computing, and argue that the slow adoption of SE tools and
techniques is at least in part caused by the fact that these do not address the
HPC challenges `out-of-the-box'. By giving some examples of solutions for
designing, testing and benchmarking HPC software, we intend to bring software
engineering and HPC closer together.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1810.00635v4,Comparing Type Systems for Deadlock Freedom,"Message-passing software systems exhibit non-trivial forms of concurrency and
distribution; they are expected to follow intended protocols among
communicating services, but also to never ""get stuck"". This intuitive
requirement has been expressed by liveness properties such as progress or
(dead)lock freedom and various type systems ensure these properties for
concurrent processes. Unfortunately, very little is known about the precise
relationship between these type systems and the classes of typed processes they
induce.
  This paper puts forward the first comparative study of different type systems
for message-passing processes that guarantee deadlock freedom. We compare two
classes of deadlock-free typed processes, here denoted L and K. The class L
stands out for its canonicity: it results from Curry-Howard interpretations of
linear logic propositions as session types. The class K, obtained by encoding
session types into Kobayashi's linear types with usages, includes processes not
typable in other type systems. We show that L is strictly included in K, and
identify the precise conditions under which they coincide. We also provide two
type-preserving translations of processes in K into processes in L.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1512.05616v1,Deep-Spying: Spying using Smartwatch and Deep Learning,"Wearable technologies are today on the rise, becoming more common and broadly
available to mainstream users. In fact, wristband and armband devices such as
smartwatches and fitness trackers already took an important place in the
consumer electronics market and are becoming ubiquitous. By their very nature
of being wearable, these devices, however, provide a new pervasive attack
surface threatening users privacy, among others.
  In the meantime, advances in machine learning are providing unprecedented
possibilities to process complex data efficiently. Allowing patterns to emerge
from high dimensional unavoidably noisy data.
  The goal of this work is to raise awareness about the potential risks related
to motion sensors built-in wearable devices and to demonstrate abuse
opportunities leveraged by advanced neural network architectures.
  The LSTM-based implementation presented in this research can perform
touchlogging and keylogging on 12-keys keypads with above-average accuracy even
when confronted with raw unprocessed data. Thus demonstrating that deep neural
networks are capable of making keystroke inference attacks based on motion
sensors easier to achieve by removing the need for non-trivial pre-processing
pipelines and carefully engineered feature extraction strategies. Our results
suggest that the complete technological ecosystem of a user can be compromised
when a wearable wristband device is worn.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0
http://arxiv.org/abs/2001.06528v1,"Activism by the AI Community: Analysing Recent Achievements and Future
  Prospects","The artificial intelligence community (AI) has recently engaged in activism
in relation to their employers, other members of the community, and their
governments in order to shape the societal and ethical implications of AI. It
has achieved some notable successes, but prospects for further political
organising and activism are uncertain. We survey activism by the AI community
over the last six years; apply two analytical frameworks drawing upon the
literature on epistemic communities, and worker organising and bargaining; and
explore what they imply for the future prospects of the AI community. Success
thus far has hinged on a coherent shared culture, and high bargaining power due
to the high demand for a limited supply of AI talent. Both are crucial to the
future of AI activism and worthy of sustained attention.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0
http://arxiv.org/abs/1801.07138v1,"Wikipedia in academia as a teaching tool: from averse to proactive
  faculty profiles","This study concerned the active use of Wikipedia as a teaching tool in the
classroom in higher education, trying to identify different usage profiles and
their characterization. A questionnaire survey was administrated to all
full-time and part-time teachers at the Universitat Oberta de Catalunya and the
Universitat Pompeu Fabra, both in Barcelona, Spain. The questionnaire was
designed using the Technology Acceptance Model as a reference, including items
about teachers web 2.0 profile, Wikipedia usage, expertise, perceived
usefulness, easiness of use, visibility and quality, as well as Wikipedia
status among colleagues and incentives to use it more actively. Clustering and
statistical analysis were carried out using the k-medoids algorithm and
differences between clusters were assessed by means of contingency tables and
generalized linear models (logit). The respondents were classified in four
clusters, from less to more likely to adopt and use Wikipedia in the classroom,
namely averse (25.4%), reluctant (17.9%), open (29.5%) and proactive (27.2%).
Proactive faculty are mostly men teaching part-time in STEM fields, mainly
engineering, while averse faculty are mostly women teaching full-time in
non-STEM fields. Nevertheless, questionnaire items related to visibility,
quality, image, usefulness and expertise determine the main differences between
clusters, rather than age, gender or domain. Clusters involving a positive view
of Wikipedia and at least some frequency of use clearly outnumber those with a
strictly negative stance. This goes against the common view that faculty
members are mostly sceptical about Wikipedia. Environmental factors such as
academic culture and colleagues opinion are more important than faculty
personal characteristics, especially with respect to what they think about
Wikipedia quality.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/0802.3974v2,"Syntax diagrams as a formalism for representation of syntactic relations
  of formal languages","The new approach to representation of syntax of formal languages-- a
formalism of syntax diagrams is offered. Syntax diagrams look a convenient
language for the description of syntactic relations in the languages having
nonlinear representation of texts, for example, for representation of syntax
lows of the language of structural chemical formulas. The formalism of
neighbourhood grammar is used to describe the set of correct syntax constructs.
The neighbourhood the grammar consists of a set of families of
""neighbourhoods""-- the diagrams defined for each symbol of the language's
alphabet. The syntax diagram is correct if each symbol is included into this
diagram together with some neighbourhood. In other words, correct diagrams are
needed to be covered by elements of the neighbourhood grammar. Thus, the
grammar of formal language can be represented as system of the covers defined
for each correct syntax diagram.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0609051v1,Multilingual person name recognition and transliteration,"We present an exploratory tool that extracts person names from multilingual
news collections, matches name variants referring to the same person, and
infers relationships between people based on the co-occurrence of their names
in related news. A novel feature is the matching of name variants across
languages and writing systems, including names written with the Greek, Cyrillic
and Arabic writing system. Due to our highly multilingual setting, we use an
internal standard representation for name representation and matching, instead
of adopting the traditional bilingual approach to transliteration. This work is
part of the news analysis system NewsExplorer that clusters an average of
25,000 news articles per day to detect related news within the same and across
different languages.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.10936v3,"Toward Understanding the Influence of Individual Clients in Federated
  Learning","Federated learning allows mobile clients to jointly train a global model
without sending their private data to a central server. Extensive works have
studied the performance guarantee of the global model, however, it is still
unclear how each individual client influences the collaborative training
process. In this work, we defined a new notion, called {\em Fed-Influence}, to
quantify this influence over the model parameters, and proposed an effective
and efficient algorithm to estimate this metric. In particular, our design
satisfies several desirable properties: (1) it requires neither retraining nor
retracing, adding only linear computational overhead to clients and the server;
(2) it strictly maintains the tenets of federated learning, without revealing
any client's local private data; and (3) it works well on both convex and
non-convex loss functions, and does not require the final model to be optimal.
Empirical results on a synthetic dataset and the FEMNIST dataset demonstrate
that our estimation method can approximate Fed-Influence with small bias.
Further, we show an application of Fed-Influence in model debugging.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1004.0351v1,An Oblivious Spanning Tree for Buy-at-Bulk Network Design Problems,"We consider the problem of constructing a single spanning tree for the
single-source buy-at-bulk network design problem for doubling-dimension graphs.
We compute a spanning tree to route a set of demands (or data) along a graph to
or from a designated root node. The demands could be aggregated at (or
symmetrically distributed to) intermediate nodes where the fusion-cost is
specified by a non-negative concave function $f$. We describe a novel approach
for developing an oblivious spanning tree in the sense that it is independent
of the number of data sources (or demands) and cost function at intermediate
nodes. To our knowledge, this is the first paper to propose a single spanning
tree solution to this problem (as opposed to multiple overlay trees). There has
been no prior work where the tree is oblivious to both the fusion cost function
and the set of sources (demands). We present a deterministic, polynomial-time
algorithm for constructing a spanning tree in low doubling graphs that
guarantees $\log^{3}D\cdot\log n$-approximation over the optimal cost, where
$D$ is the diameter of the graph and $n$ the total number of nodes. With
constant fusion-cost function our spanning tree gives a $O(\log^3
D)$-approximation for every Steiner tree to the root.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.12986v1,"Findings of the NLP4IF-2021 Shared Tasks on Fighting the COVID-19
  Infodemic and Censorship Detection","We present the results and the main findings of the NLP4IF-2021 shared tasks.
Task 1 focused on fighting the COVID-19 infodemic in social media, and it was
offered in Arabic, Bulgarian, and English. Given a tweet, it asked to predict
whether that tweet contains a verifiable claim, and if so, whether it is likely
to be false, is of general interest, is likely to be harmful, and is worthy of
manual fact-checking; also, whether it is harmful to society, and whether it
requires the attention of policy makers. Task~2 focused on censorship
detection, and was offered in Chinese. A total of ten teams submitted systems
for task 1, and one team participated in task 2; nine teams also submitted a
system description paper. Here, we present the tasks, analyze the results, and
discuss the system submissions and the methods they used. Most submissions
achieved sizable improvements over several baselines, and the best systems used
pre-trained Transformers and ensembles. The data, the scorers and the
leaderboards for the tasks are available at
http://gitlab.com/NLP4IF/nlp4if-2021.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.11066v1,"A two-step machine learning approach for crop disease detection: an
  application of GAN and UAV technology","Automated plant diagnosis is a technology that promises large increases in
cost-efficiency for agriculture. However, multiple problems reduce the
effectiveness of drones, including the inverse relationship between resolution
and speed and the lack of adequate labeled training data. This paper presents a
two-step machine learning approach that analyzes low-fidelity and high-fidelity
images in sequence, preserving efficiency as well as accuracy. Two
data-generators are also used to minimize class imbalance in the high-fidelity
dataset and to produce low-fidelity data that is representative of UAV images.
The analysis of applications and methods is conducted on a database of
high-fidelity apple tree images which are corrupted with class imbalance. The
application begins by generating high-fidelity data using generative networks
and then uses this novel data alongside the original high-fidelity data to
produce low-fidelity images. A machine-learning identifier identifies plants
and labels them as potentially diseased or not. A machine learning classifier
is then given the potentially diseased plant images and returns actual
diagnoses for these plants. The results show an accuracy of 96.3% for the
high-fidelity system and a 75.5% confidence level for our low-fidelity system.
Our drone technology shows promising results in accuracy when compared to
labor-based methods of diagnosis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1309.7341v1,"OntoMaven: Maven-based Ontology Development and Management of
  Distributed Ontology Repositories","In collaborative agile ontology development projects support for modular
reuse of ontologies from large existing remote repositories, ontology project
life cycle management, and transitive dependency management are important
needs. The Apache Maven approach has proven its success in distributed
collaborative Software Engineering by its widespread adoption. The contribution
of this paper is a new design artifact called OntoMaven. OntoMaven adopts the
Maven-based development methodology and adapts its concepts to knowledge
engineering for Maven-based ontology development and management of ontology
artifacts in distributed ontology repositories.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1506.05715v1,"Simultaneous Embedding: Edge Orderings, Relative Positions, Cutvertices","A simultaneous embedding (with fixed edges) of two graphs $G^1$ and $G^2$
with common graph $G=G^1 \cap G^2$ is a pair of planar drawings of $G^1$ and
$G^2$ that coincide on $G$. It is an open question whether there is a
polynomial-time algorithm that decides whether two graphs admit a simultaneous
embedding (problem SEFE).
  In this paper, we present two results. First, a set of three linear-time
preprocessing algorithms that remove certain substructures from a given SEFE
instance, producing a set of equivalent SEFE instances without such
substructures. The structures we can remove are (1) cutvertices of the union
graph $G^\cup = G^1 \cup G^2$, (2) most separating pairs of $G^\cup$, and (3)
connected components of $G$ that are biconnected but not a cycle.
  Second, we give an $O(n^3)$-time algorithm solving SEFE for instances with
the following restriction. Let $u$ be a pole of a P-node $\mu$ in the SPQR-tree
of a block of $G^1$ or $G^2$. Then at most three virtual edges of $\mu$ may
contain common edges incident to $u$. All algorithms extend to the sunflower
case, i.e., to the case of more than three graphs pairwise intersecting in the
same common graph.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.00551v2,Cybonto: Towards Human Cognitive Digital Twins for Cybersecurity,"Cyber defense is reactive and slow. On average, the time-to-remedy is
hundreds of times larger than the time-to-compromise. In response to the
expanding ever-more-complex threat landscape, Digital Twins (DTs) and
particularly Human Digital Twins (HDTs) offer the capability of running massive
simulations across multiple knowledge domains. Simulated results may offer
insights into adversaries' behaviors and tactics, resulting in better proactive
cyber-defense strategies. For the first time, this paper solidifies the vision
of DTs and HDTs for cybersecurity via the Cybonto conceptual framework
proposal. The paper also contributes the Cybonto ontology, formally documenting
108 constructs and thousands of cognitive-related paths based on 20 time-tested
psychology theories. Finally, the paper applied 20 network centrality
algorithms in analyzing the 108 constructs. The identified top 10 constructs
call for extensions of current digital cognitive architectures in preparation
for the DT future.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1309.5290v1,An introduction to the Europe Media Monitor family of applications,"Most large organizations have dedicated departments that monitor the media to
keep up-to-date with relevant developments and to keep an eye on how they are
represented in the news. Part of this media monitoring work can be automated.
In the European Union with its 23 official languages, it is particularly
important to cover media reports in many languages in order to capture the
complementary news content published in the different countries. It is also
important to be able to access the news content across languages and to merge
the extracted information. We present here the four publicly accessible systems
of the Europe Media Monitor (EMM) family of applications, which cover between
19 and 50 languages (see http://press.jrc.it/overview.html). We give an
overview of their functionality and discuss some of the implications of the
fact that they cover quite so many languages. We discuss design issues
necessary to be able to achieve this high multilinguality, as well as the
benefits of this multilinguality.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0010036v1,Lattice Structure and Convergence of a Game of Cards,"This paper is devoted to the study of the dynamics of a discrete system
related to some self stabilizing protocol on a ring of processors.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1710.02827v2,"Beyond Worst-Case (In)approximability of Nonsubmodular Influence
  Maximization","We consider the problem of maximizing the spread of influence in a social
network by choosing a fixed number of initial seeds, formally referred to as
the influence maximization problem. It admits a $(1-1/e)$-factor approximation
algorithm if the influence function is submodular. Otherwise, in the worst
case, the problem is NP-hard to approximate to within a factor of
$N^{1-\varepsilon}$. This paper studies whether this worst-case hardness result
can be circumvented by making assumptions about either the underlying network
topology or the cascade model. All of our assumptions are motivated by many
real life social network cascades.
  First, we present strong inapproximability results for a very restricted
class of networks called the (stochastic) hierarchical blockmodel, a special
case of the well-studied (stochastic) blockmodel in which relationships between
blocks admit a tree structure. We also provide a dynamic-program based
polynomial time algorithm which optimally computes a directed variant of the
influence maximization problem on hierarchical blockmodel networks. Our
algorithm indicates that the inapproximability result is due to the
bidirectionality of influence between agent-blocks.
  Second, we present strong inapproximability results for a class of influence
functions that are ""almost"" submodular, called 2-quasi-submodular. Our
inapproximability results hold even for any 2-quasi-submodular $f$ fixed in
advance. This result also indicates that the ""threshold"" between submodularity
and nonsubmodularity is sharp, regarding the approximability of influence
maximization.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1403.1891v2,"Counterfactual Estimation and Optimization of Click Metrics for Search
  Engines","Optimizing an interactive system against a predefined online metric is
particularly challenging, when the metric is computed from user feedback such
as clicks and payments. The key challenge is the counterfactual nature: in the
case of Web search, any change to a component of the search engine may result
in a different search result page for the same query, but we normally cannot
infer reliably from search log how users would react to the new result page.
Consequently, it appears impossible to accurately estimate online metrics that
depend on user feedback, unless the new engine is run to serve users and
compared with a baseline in an A/B test. This approach, while valid and
successful, is unfortunately expensive and time-consuming. In this paper, we
propose to address this problem using causal inference techniques, under the
contextual-bandit framework. This approach effectively allows one to run
(potentially infinitely) many A/B tests offline from search log, making it
possible to estimate and optimize online metrics quickly and inexpensively.
Focusing on an important component in a commercial search engine, we show how
these ideas can be instantiated and applied, and obtain very promising results
that suggest the wide applicability of these techniques.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.05564v1,"A novel algorithm for clearing financial obligations between companies
  -- an application within the Romanian Ministry of Economy","The concept of clearing or netting, as defined in the glossaries of European
Central Bank, has a great impact on the economy of a country influencing the
exchanges and the interactions between companies. On short, netting refers to
an alternative to the usual way in which the companies make the payments to
each other: it is an agreement in which each party sets off amounts it owes
against amounts owed to it. Based on the amounts two or more parties owe
between them, the payment is substituted by a direct settlement. In this paper
we introduce a set of graph algorithms which provide optimal netting solutions
for the scale of a country economy. The set of algorithms computes results in
an efficient time and is tested on invoice data provided by the Romanian
Ministry of Economy. Our results show that classical graph algorithms are still
capable of solving very important modern problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1912.10041v6,Probabilistic process algebra and strategic interleaving,"We first present a probabilistic version of ACP that rests on the principle
that probabilistic choices are always resolved before choices involved in
alternative composition and parallel composition are resolved and then extend
this probabilistic version of ACP with a form of interleaving in which parallel
processes are interleaved according to what is known as a process-scheduling
policy in the field of operating systems. We use the term strategic
interleaving for this more constrained form of interleaving. The extension
covers probabilistic process-scheduling policies.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.02311v1,"Sonic Sculpture: Activating Engagement with Head-Mounted Augmented
  Reality","This work examines how head-mounted AR can be used to build an interactive
sonic landscape to engage with a public sculpture. We describe a sonic artwork,
""Listening To Listening"", that has been designed to accompany a real-world
sculpture with two prototype interaction schemes. Our artwork is created for
the HoloLens platform so that users can have an individual experience in a
mixed reality context. Personal head-mounted AR systems have recently become
available and practical for integration into public art projects, however
research into sonic sculpture works has yet to account for the affordances of
current portable and mainstream AR systems. In this work, we take advantage of
the HoloLens' spatial awareness to build sonic spaces that have a precise
spatial relationship to a given sculpture and where the sculpture itself is
modelled in the augmented scene as an ""invisible hologram"". We describe the
artistic rationale for our artwork, the design of the two interaction schemes,
and the technical and usability feedback that we have obtained from
demonstrations during iterative development.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0511074v5,Every Sequence is Decompressible from a Random One,"Kucera and Gacs independently showed that every infinite sequence is Turing
reducible to a Martin-Lof random sequence. This result is extended by showing
that every infinite sequence S is Turing reducible to a Martin-Lof random
sequence R such that the asymptotic number of bits of R needed to compute n
bits of S, divided by n, is precisely the constructive dimension of S. It is
shown that this is the optimal ratio of query bits to computed bits achievable
with Turing reductions. As an application of this result, a new
characterization of constructive dimension is given in terms of Turing
reduction compression ratios.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9810010v2,C++ Templates as Partial Evaluation,"This paper explores the relationship between C++ templates and partial
evaluation. Templates were designed to support generic programming, but
unintentionally provided the ability to perform compile-time computations and
code generation. These features are completely accidental, and as a result
their syntax is awkward. By recasting these features in terms of partial
evaluation, a much simpler syntax can be achieved. C++ may be regarded as a
two-level language in which types are first-class values. Template
instantiation resembles an offline partial evaluator. This paper describes
preliminary work toward a single mechanism based on Partial Evaluation which
unifies generic programming, compile-time computation and code generation. The
language Catat is introduced to illustrate these ideas.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1507.07264v2,Everything old is new again: Quoted Domain Specific Languages,"We describe a new approach to domain specific languages (DSLs), called Quoted
DSLs (QDSLs), that resurrects two old ideas: quotation, from McCarthy's Lisp of
1960, and the subformula property, from Gentzen's natural deduction of 1935.
Quoted terms allow the DSL to share the syntax and type system of the host
language. Normalising quoted terms ensures the subformula property, which
guarantees that one can use higher-order types in the source while guaranteeing
first-order types in the target, and enables using types to guide fusion. We
test our ideas by re-implementing Feldspar, which was originally implemented as
an Embedded DSL (EDSL), as a QDSL; and we compare the QDSL and EDSL variants.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0809.2696v1,An Unified Definition of Data Mining,"Since many years, theoretical concepts of Data Mining have been developed and
improved. Data Mining has become applied to many academic and industrial
situations, and recently, soundings of public opinion about privacy have been
carried out. However, a consistent and standardized definition is still
missing, and the initial explanation given by Frawley et al. has pragmatically
often changed over the years. Furthermore, alternative terms like Knowledge
Discovery have been conjured and forged, and a necessity of a Data Warehouse
has been endeavoured to persuade the users. In this work, we pick up current
definitions and introduce an unified definition that covers existing attempted
explanations. For this, we appeal to the natural original of chemical states of
aggregation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2102.04231v1,"Neurogenetic Programming Framework for Explainable Reinforcement
  Learning","Automatic programming, the task of generating computer programs compliant
with a specification without a human developer, is usually tackled either via
genetic programming methods based on mutation and recombination of programs, or
via neural language models. We propose a novel method that combines both
approaches using a concept of a virtual neuro-genetic programmer: using
evolutionary methods as an alternative to gradient descent for neural network
training}, or scrum team. We demonstrate its ability to provide performant and
explainable solutions for various OpenAI Gym tasks, as well as inject expert
knowledge into the otherwise data-driven search for solutions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1906.06997v1,"Productivity equation and the m distributions of information processing
  in workflows","This research investigates an equation of productivity for workflows
regarding its robustness towards the definition of workflows as probabilistic
distributions. The equation was formulated across its derivations through a
theoretical framework about information theory, probabilities and complex
adaptive systems. By defining the productivity equation for organism-object
interactions, workflows mathematical derivations can be predicted and monitored
without strict empirical methods and allows workflow flexibility for
organism-object environments.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1306.2484v3,"Generalization of Boole-Shannon expansion, consistency of Boolean
  equations and elimination by orthonormal expansion","The well known Boole-Shannon expansion of Boolean functions in several
variables (with co-efficients in a Boolean algebra $B$) is also known in more
general form in terms of expansion in a set $\Phi$ of orthonormal functions.
However, unlike the one variable step of this expansion an analogous
elimination theorem and consistency is not well known. This article proves such
an elimination theorem for a special class of Boolean functions denoted
$B(\Phi)$. When the orthonormal set $\Phi$ is of polynomial size in number $n$
of variables, the consistency of a Boolean equation $f=0$ can be determined in
polynomial number of $B$-operations. A characterization of $B(\Phi)$ is also
shown and an elimination based procedure for computing consistency of Boolean
equations is proposed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.06701v1,Agile Governance Theory: conceptual development,"Context: Competitiveness is the key to a sustainable development and it
demands agility at the business and organizational levels, which in turn
requires a flexible and customizable IT environment and effective and
responsive governance in order to deliver value to the business. Objective:
This paper describes the conceptual development of a theory for analyze and
describe agile governance in order to increasing the success rate of their
practice, achieving organizational performance and business competitiveness.
Method: We adopt a multi-method research, framing the theory conceptual
development using Dubin's method of theory building. Results: We have developed
a conceptual framework of the theory encompassing its constructs, laws of
interaction, boundaries and system states. Conclusion: This theory can provide
a better understanding of the nature of agile governance, by mapping of its
constructs, mediators, moderators and disturbing factors, in order to help
organizations reach better results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1208.2761v1,"A Simple Optimum-Time FSSP Algorithm for Multi-Dimensional Cellular
  Automata","The firing squad synchronization problem (FSSP) on cellular automata has been
studied extensively for more than forty years, and a rich variety of
synchronization algorithms have been proposed for not only one-dimensional
arrays but two-dimensional arrays. In the present paper, we propose a simple
recursive-halving based optimum-time synchronization algorithm that can
synchronize any rectangle arrays of size m*n with a general at one corner in
m+n+max(m, n)-3 steps. The algorithm is a natural expansion of the well-known
FSSP algorithm proposed by Balzer [1967], Gerken [1987], and Waksman [1966] and
it can be easily expanded to three-dimensional arrays, even to
multi-dimensional arrays with a general at any position of the array.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0906.2995v2,Fragments of first-order logic over infinite words,"We give topological and algebraic characterizations as well as language
theoretic descriptions of the following subclasses of first-order logic FO[<]
for omega-languages: Sigma_2, FO^2, the intersection of FO^2 and Sigma_2, and
Delta_2 (and by duality Pi_2 and the intersection of FO^2 and Pi_2). These
descriptions extend the respective results for finite words. In particular, we
relate the above fragments to language classes of certain (unambiguous)
polynomials. An immediate consequence is the decidability of the membership
problem of these classes, but this was shown before by Wilke and Bojanczyk and
is therefore not our main focus. The paper is about the interplay of algebraic,
topological, and language theoretic properties.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0912.0807v1,Practical Algorithmic Techniques for Several String Processing Problems,"The domains of data mining and knowledge discovery make use of large amounts
of textual data, which need to be handled efficiently. Specific problems, like
finding the maximum weight ordered common subset of a set of ordered sets or
searching for specific patterns within texts, occur frequently in this context.
In this paper we present several novel and practical algorithmic techniques for
processing textual data (strings) in order to efficiently solve multiple
problems. Our techniques make use of efficient string algorithms and data
structures, like KMP, suffix arrays, tries and deterministic finite automata.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0008002v1,Structure of some sand pile model,"SPM (Sand Pile Model) is a simple discrete dynamical system used in physics
to represent granular objects. It is deeply related to integer partitions, and
many other combinatorics problems, such as tilings or rewriting systems. The
evolution of the system started with n stacked grains generates a lattice,
denoted by SPM(n). We study here the structure of this lattice. We first
explain how it can be constructed, by showing its strong self-similarity
property. Then, we define SPM(infini), a natural extension of SPM when one
starts with an infinite number of grains. Again, we give an efficient
construction algorithm and a coding of this lattice using a self-similar tree.
The two approaches give different recursive formulae for the cardinal of
SPM(n), where no closed formula have ever been found.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.03390v1,GANtron: Emotional Speech Synthesis with Generative Adversarial Networks,"Speech synthesis is used in a wide variety of industries. Nonetheless, it
always sounds flat or robotic. The state of the art methods that allow for
prosody control are very cumbersome to use and do not allow easy tuning. To
tackle some of these drawbacks, in this work we target the implementation of a
text-to-speech model where the inferred speech can be tuned with the desired
emotions. To do so, we use Generative Adversarial Networks (GANs) together with
a sequence-to-sequence model using an attention mechanism. We evaluate four
different configurations considering different inputs and training strategies,
study them and prove how our best model can generate speech files that lie in
the same distribution as the initial training dataset. Additionally, a new
strategy to boost the training convergence by applying a guided attention loss
is proposed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0007025v1,A Moment of Perfect Clarity I: The Parallel Census Technique,"We discuss the history and uses of the parallel census technique---an elegant
tool in the study of certain computational objects having polynomially bounded
census functions. A sequel will discuss advances (including Cai, Naik, and
Sivakumar [CNS95] and Glasser [Gla00]), some related to the parallel census
technique and some due to other approaches, in the complexity-class collapses
that follow if NP has sparse hard sets under reductions weaker than (full)
truth-table reductions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.10451v1,Neural Networks for Semantic Gaze Analysis in XR Settings,"Virtual-reality (VR) and augmented-reality (AR) technology is increasingly
combined with eye-tracking. This combination broadens both fields and opens up
new areas of application, in which visual perception and related cognitive
processes can be studied in interactive but still well controlled settings.
However, performing a semantic gaze analysis of eye-tracking data from
interactive three-dimensional scenes is a resource-intense task, which so far
has been an obstacle to economic use. In this paper we present a novel approach
which minimizes time and information necessary to annotate volumes of interest
(VOIs) by using techniques from object recognition. To do so, we train
convolutional neural networks (CNNs) on synthetic data sets derived from
virtual models using image augmentation techniques. We evaluate our method in
real and virtual environments, showing that the method can compete with
state-of-the-art approaches, while not relying on additional markers or
preexisting databases but instead offering cross-platform use.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.08278v1,A Note on Occur-Check,"Most known results on avoiding the occur-check are based on the notion of
""not subject to occur-check"" (NSTO). It means that unification is performed
only on such pairs of atoms for which the occur-check never succeeds in any run
of a nondeterministic unification algorithm. Here we show that this requirement
is too strong. We show how to weaken it, and present some related sufficient
conditions under which the occur-check may be safely omitted. We show examples
for which the proposed approach provides more general results than the
approaches based on well-moded and nicely moded programs (this includes cases
to which the latter approaches are inapplicable).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1005.2907v1,Interactive Realizers and Monads,"We propose a realizability interpretation of a system for quantifier free
arithmetic which is equivalent to the fragment of classical arithmetic without
""nested"" quantifiers, called here EM1-arithmetic. We interpret classical proofs
as interactive learning strategies, namely as processes going through several
stages of knowledge and learning by interacting with the ""environment"" and with
each other. We give a categorical presentation of the interpretation through
the construction of two suitable monads.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0611074v2,"On ""P = NP: Linear Programming Formulation of the Traveling Salesman
  Problem"": A reply to Hofman's Claim of a ""Counter-Example""","We show that Hofman's claim of a ""counter-example"" to Diaby's LP formulation
of the TSP is invalid.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1507.00655v1,Reasoning about embedded dependencies using inclusion dependencies,"The implication problem for the class of embedded dependencies is
undecidable. However, this does not imply lackness of a proof procedure as
exemplified by the chase algorithm. In this paper we present a complete
axiomatization of embedded dependencies that is based on the chase and uses
inclusion dependencies and implicit existential quantification in the
intermediate steps of deductions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0211013v1,"Algorithmic scalability in globally constrained conservative parallel
  discrete event simulations of asynchronous systems","We consider parallel simulations for asynchronous systems employing L
processing elements that are arranged on a ring. Processors communicate only
among the nearest neighbors and advance their local simulated time only if it
is guaranteed that this does not violate causality. In simulations with no
constraints, in the infinite L-limit the utilization scales (Korniss et al, PRL
84, 2000); but, the width of the virtual time horizon diverges (i.e., the
measurement phase of the algorithm does not scale). In this work, we introduce
a moving window global constraint, which modifies the algorithm so that the
measurement phase scales as well. We present results of systematic studies in
which the system size (i.e., L and the volume load per processor) as well as
the constraint are varied. The constraint eliminates the extreme fluctuations
in the virtual time horizon, provides a bound on its width, and controls the
average progress rate. The width of the window constraint can serve as a tuning
parameter that, for a given volume load per processor, could be adjusted to
optimize the utilization so as to maximize the efficiency. This result may find
numerous applications in modeling the evolution of general spatially extended
short-range interacting systems with asynchronous dynamics, including dynamic
Monte Carlo studies.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9811013v1,The Asilomar Report on Database Research,"The database research community is rightly proud of success in basic
research, and its remarkable record of technology transfer. Now the field needs
to radically broaden its research focus to attack the issues of capturing,
storing, analyzing, and presenting the vast array of online data. The database
research community should embrace a broader research agenda -- broadening the
definition of database management to embrace all the content of the Web and
other online data stores, and rethinking our fundamental assumptions in light
of technology shifts. To accelerate this transition, we recommend changing the
way research results are evaluated and presented. In particular, we advocate
encouraging more speculative and long-range work, moving conferences to a
poster format, and publishing all research literature on the Web.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9904004v1,Mixing Metaphors,"Mixed metaphors have been neglected in recent metaphor research. This paper
suggests that such neglect is short-sighted. Though mixing is a more complex
phenomenon than straight metaphors, the same kinds of reasoning and knowledge
structures are required. This paper provides an analysis of both parallel and
serial mixed metaphors within the framework of an AI system which is already
capable of reasoning about straight metaphorical manifestations and argues that
the processes underlying mixing are central to metaphorical meaning. Therefore,
any theory of metaphors must be able to account for mixing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.12927v1,"How to restart? An agent-based simulation model towards the definition
  of strategies for COVID-19 ""second phase"" in public buildings","Restarting public buildings activities in the ""second phase"" of COVID-19
emergency should be supported by operational measures to avoid a second virus
spreading. Buildings hosting the continuous presence of the same users and
significant overcrowd conditions over space/time (e.g. large offices,
universities) are critical scenarios due to the prolonged contact with
infectors. Beside individual's risk-mitigation strategies performed (facial
masks), stakeholders should promote additional strategies, i.e. occupants' load
limitation (towards ""social distancing"") and access control. Simulators could
support the measures effectiveness evaluation. This work provides an
Agent-Based Model to estimate the virus spreading in the closed built
environment. The model adopts a probabilistic approach to jointly simulate
occupants' movement and virus transmission according to proximity-based and
exposure-time-based rules proposed by international health organizations.
Scenarios can be defined in terms of building occupancy, mitigation strategies
and virus-related aspects. The model is calibrated on experimental data
(""Diamond Princess"" cruise) and then applied to a relevant case-study (a part
of a university campus). Results demonstrate the model capabilities. Concerning
the case-study, adopting facial masks seems to be a paramount strategy to
reduce virus spreading in each initial condition, by maintaining an acceptable
infected people's number. The building capacity limitation could support such
measure by potentially moving from FFPk masks to surgical masks use by
occupants (thus improving users' comfort issues). A preliminary model to
combine acceptable mask filters-occupants' density combination is proposed. The
model could be modified to consider other recurring scenarios in other public
buildings (e.g. tourist facilities, cultural buildings).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1903.06119v2,"Correct Approximation of IEEE 754 Floating-Point Arithmetic for Program
  Verification","Verification of programs using floating-point arithmetic is challenging on
several accounts. One of the difficulties of reasoning about such programs is
due to the peculiarities of floating-point arithmetic: rounding errors,
infinities, non-numeric objects (NaNs), signed zeroes, denormal numbers,
different rounding modes, etc. One possibility to reason about floating-point
arithmetic is to model a program computation path by means of a set of ternary
constraints of the form z = x op y and use constraint propagation techniques to
infer new information on the variables' possible values. In this setting, we
define and prove the correctness of algorithms to precisely bound the value of
one of the variables x, y or z, starting from the bounds known for the other
two. We do this for each of the operations and for each rounding mode defined
by the IEEE 754 binary floating-point standard, even in the case the rounding
mode in effect is only partially known. This is the first time that such
so-called filtering algorithms are defined and their correctness is formally
proved. This is an important slab for paving the way to formal verification of
programs that use floating-point arithmetics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.05946v1,Reconstructing a Polyhedron between Polygons in Parallel Slices,"Given two $n$-vertex polygons, $P=(p_1, \ldots, p_n)$ lying in the $xy$-plane
at $z=0$, and $P'=(p'_1, \ldots, p'_n)$ lying in the $xy$-plane at $z=1$, a
banded surface is a triangulated surface homeomorphic to an annulus connecting
$P$ and $P'$ such that the triangulation's edge set contains vertex disjoint
paths $\pi_i$ connecting $p_i$ to $p'_i$ for all $i =1, \ldots, n$. The surface
then consists of bands, where the $i$th band goes between $\pi_i$ and
$\pi_{i+1}$. We give a polynomial-time algorithm to find a banded surface
without Steiner points if one exists. We explore connections between banded
surfaces and linear morphs, where time in the morph corresponds to the $z$
direction. In particular, we show that if $P$ and $P'$ are convex and the
linear morph from $P$ to $P'$ (which moves the $i$th vertex on a straight line
from $p_i$ to $p'_i$) remains planar at all times, then there is a banded
surface without Steiner points.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.12567v1,"Wind speed prediction using multidimensional convolutional neural
  networks","Accurate wind speed forecasting is of great importance for many economic,
business and management sectors. This paper introduces a new model based on
convolutional neural networks (CNNs) for wind speed prediction tasks. In
particular, we show that compared to classical CNN-based models, the proposed
model is able to better characterise the spatio-temporal evolution of the wind
data by learning the underlying complex input-output relationships from
multiple dimensions (views) of the input data. The proposed model exploits the
spatio-temporal multivariate multidimensional historical weather data for
learning new representations used for wind forecasting. We conduct experiments
on two real-life weather datasets. The datasets are measurements from cities in
Denmark and in the Netherlands. The proposed model is compared with traditional
2- and 3-dimensional CNN models, a 2D-CNN model with an attention layer and a
2D-CNN model equipped with upscaling and depthwise separable convolutions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.05753v2,"Understanding Task Design Trade-offs in Crowdsourced Paraphrase
  Collection","Linguistically diverse datasets are critical for training and evaluating
robust machine learning systems, but data collection is a costly process that
often requires experts. Crowdsourcing the process of paraphrase generation is
an effective means of expanding natural language datasets, but there has been
limited analysis of the trade-offs that arise when designing tasks. In this
paper, we present the first systematic study of the key factors in
crowdsourcing paraphrase collection. We consider variations in instructions,
incentives, data domains, and workflows. We manually analyzed paraphrases for
correctness, grammaticality, and linguistic diversity. Our observations provide
new insight into the trade-offs between accuracy and diversity in crowd
responses that arise as a result of task design, providing guidance for future
paraphrase generation procedures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1510.06486v1,"Big Data Analytics-Enhanced Cloud Computing: Challenges, Architectural
  Elements, and Future Directions","The emergence of cloud computing has made dynamic provisioning of elastic
capacity to applications on-demand. Cloud data centers contain thousands of
physical servers hosting orders of magnitude more virtual machines that can be
allocated on demand to users in a pay-as-you-go model. However, not all systems
are able to scale up by just adding more virtual machines. Therefore, it is
essential, even for scalable systems, to project workloads in advance rather
than using a purely reactive approach. Given the scale of modern cloud
infrastructures generating real time monitoring information, along with all the
information generated by operating systems and applications, this data poses
the issues of volume, velocity, and variety that are addressed by Big Data
approaches. In this paper, we investigate how utilization of Big Data analytics
helps in enhancing the operation of cloud computing environments. We discuss
diverse applications of Big Data analytics in clouds, open issues for enhancing
cloud operations via Big Data analytics, and architecture for anomaly detection
and prevention in clouds along with future research directions.",0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.14957v1,"A Riemannian Newton Trust-Region Method for Fitting Gaussian Mixture
  Models","Gaussian Mixture Models are a powerful tool in Data Science and Statistics
that are mainly used for clustering and density approximation. The task of
estimating the model parameters is in practice often solved by the Expectation
Maximization (EM) algorithm which has its benefits in its simplicity and low
per-iteration costs. However, the EM converges slowly if there is a large share
of hidden information or overlapping clusters. Recent advances in Manifold
Optimization for Gaussian Mixture Models have gained increasing interest. We
introduce a formula for the Riemannian Hessian for Gaussian Mixture Models. On
top, we propose a new Riemannian Newton Trust-Region method which outperforms
current approaches both in terms of runtime and number of iterations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1507.03325v2,Scientific Computing Meets Big Data Technology: An Astronomy Use Case,"Scientific analyses commonly compose multiple single-process programs into a
dataflow. An end-to-end dataflow of single-process programs is known as a
many-task application. Typically, tools from the HPC software stack are used to
parallelize these analyses. In this work, we investigate an alternate approach
that uses Apache Spark -- a modern big data platform -- to parallelize
many-task applications. We present Kira, a flexible and distributed astronomy
image processing toolkit using Apache Spark. We then use the Kira toolkit to
implement a Source Extractor application for astronomy images, called Kira SE.
With Kira SE as the use case, we study the programming flexibility, dataflow
richness, scheduling capacity and performance of Apache Spark running on the
EC2 cloud. By exploiting data locality, Kira SE achieves a 2.5x speedup over an
equivalent C program when analyzing a 1TB dataset using 512 cores on the Amazon
EC2 cloud. Furthermore, we show that by leveraging software originally designed
for big data infrastructure, Kira SE achieves competitive performance to the C
implementation running on the NERSC Edison supercomputer. Our experience with
Kira indicates that emerging Big Data platforms such as Apache Spark are a
performant alternative for many-task scientific applications.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0802.3492v2,The RDF Virtual Machine,"The Resource Description Framework (RDF) is a semantic network data model
that is used to create machine-understandable descriptions of the world and is
the basis of the Semantic Web. This article discusses the application of RDF to
the representation of computer software and virtual computing machines. The
Semantic Web is posited as not only a web of data, but also as a web of
programs and processes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.11616v1,"Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video
  Super-Resolution","In this paper, we explore the space-time video super-resolution task, which
aims to generate a high-resolution (HR) slow-motion video from a low frame rate
(LFR), low-resolution (LR) video. A simple solution is to split it into two
sub-tasks: video frame interpolation (VFI) and video super-resolution (VSR).
However, temporal interpolation and spatial super-resolution are intra-related
in this task. Two-stage methods cannot fully take advantage of the natural
property. In addition, state-of-the-art VFI or VSR networks require a large
frame-synthesis or reconstruction module for predicting high-quality video
frames, which makes the two-stage methods have large model sizes and thus be
time-consuming. To overcome the problems, we propose a one-stage space-time
video super-resolution framework, which directly synthesizes an HR slow-motion
video from an LFR, LR video. Rather than synthesizing missing LR video frames
as VFI networks do, we firstly temporally interpolate LR frame features in
missing LR video frames capturing local temporal contexts by the proposed
feature temporal interpolation network. Then, we propose a deformable ConvLSTM
to align and aggregate temporal information simultaneously for better
leveraging global temporal contexts. Finally, a deep reconstruction network is
adopted to predict HR slow-motion video frames. Extensive experiments on
benchmark datasets demonstrate that the proposed method not only achieves
better quantitative and qualitative performance but also is more than three
times faster than recent two-stage state-of-the-art methods, e.g., DAIN+EDVR
and DAIN+RBPN.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1703.05475v3,A quest to unravel the metric structure behind perturbed networks,"Graphs and network data are ubiquitous across a wide spectrum of scientific
and application domains. Often in practice, an input graph can be considered as
an observed snapshot of a (potentially continuous) hidden domain or process.
Subsequent analysis, processing, and inferences are then performed on this
observed graph. In this paper we advocate the perspective that an observed
graph is often a noisy version of some discretized 1-skeleton of a hidden
domain, and specifically we will consider the following natural network model:
We assume that there is a true graph ${G^*}$ which is a certain proximity graph
for points sampled from a hidden domain $\mathcal{X}$; while the observed graph
$G$ is an Erd$\""{o}$s-R$\'{e}$nyi type perturbed version of ${G^*}$.
  Our network model is related to, and slightly generalizes, the
much-celebrated small-world network model originally proposed by Watts and
Strogatz. However, the main question we aim to answer is orthogonal to the
usual studies of network models (which often focuses on characterizing /
predicting behaviors and properties of real-world networks). Specifically, we
aim to recover the metric structure of ${G^*}$ (which reflects that of the
hidden space $\mathcal{X}$ as we will show) from the observed graph $G$. Our
main result is that a simple filtering process based on the \emph{Jaccard
index} can recover this metric within a multiplicative factor of $2$ under our
network model. Our work makes one step towards the general question of
inferring structure of a hidden space from its observed noisy graph
representation. In addition, our results also provide a theoretical
understanding for Jaccard-Index-based denoising approaches.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0312008v1,"Embedding Web-based Statistical Translation Models in Cross-Language
  Information Retrieval","Although more and more language pairs are covered by machine translation
services, there are still many pairs that lack translation resources.
Cross-language information retrieval (CLIR) is an application which needs
translation functionality of a relatively low level of sophistication since
current models for information retrieval (IR) are still based on a
bag-of-words. The Web provides a vast resource for the automatic construction
of parallel corpora which can be used to train statistical translation models
automatically. The resulting translation models can be embedded in several ways
in a retrieval model. In this paper, we will investigate the problem of
automatically mining parallel texts from the Web and different ways of
integrating the translation models within the retrieval process. Our
experiments on standard test collections for CLIR show that the Web-based
translation models can surpass commercial MT systems in CLIR tasks. These
results open the perspective of constructing a fully automatic query
translation device for CLIR at a very low cost.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1905.09221v1,A Note on Reasoning on $\textit{DL-Lite}_{\cal R}$ with Defeasibility,"Representation of defeasible information is of interest in description
logics, as it is related to the need of accommodating exceptional instances in
knowledge bases. In this direction, in our previous works we presented a
datalog translation for reasoning on (contextualized) OWL RL knowledge bases
with a notion of justified exceptions on defeasible axioms. While it covers a
relevant fragment of OWL, the resulting reasoning process needs a complex
encoding in order to capture reasoning on negative information. In this paper,
we consider the case of knowledge bases in $\textit{DL-Lite}_{\cal R}$, i.e.
the language underlying OWL QL. We provide a definition for
$\textit{DL-Lite}_{\cal R}$ knowledge bases with defeasible axioms and study
their properties. The limited form of $\textit{DL-Lite}_{\cal R}$ axioms allows
us to formulate a simpler encoding into datalog (under answer set semantics)
with direct rules for reasoning on negative information. The resulting
materialization method gives rise to a complete reasoning procedure for
instance checking in $\textit{DL-Lite}_{\cal R}$ with defeasible axioms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0708.4284v1,Optimal Per-Edge Processing Times in the Semi-Streaming Model,"We present semi-streaming algorithms for basic graph problems that have
optimal per-edge processing times and therefore surpass all previous
semi-streaming algorithms for these tasks. The semi-streaming model, which is
appropriate when dealing with massive graphs, forbids random access to the
input and restricts the memory to O(n*polylog n) bits.
  Particularly, the formerly best per-edge processing times for finding the
connected components and a bipartition are O(alpha(n)), for determining
k-vertex and k-edge connectivity O(k^2n) and O(n*log n) respectively for any
constant k and for computing a minimum spanning forest O(log n). All these time
bounds we reduce to O(1).
  Every presented algorithm determines a solution asymptotically as fast as the
best corresponding algorithm up to date in the classical RAM model, which
therefore cannot convert the advantage of unlimited memory and random access
into superior computing times for these problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1209.5567v2,"Closed-set lattice of regular sets based on a serial and transitive
  relation through matroids","Rough sets are efficient for data pre-processing in data mining. Matroids are
based on linear algebra and graph theory, and have a variety of applications in
many fields. Both rough sets and matroids are closely related to lattices. For
a serial and transitive relation on a universe, the collection of all the
regular sets of the generalized rough set is a lattice. In this paper, we use
the lattice to construct a matroid and then study relationships between the
lattice and the closed-set lattice of the matroid. First, the collection of all
the regular sets based on a serial and transitive relation is proved to be a
semimodular lattice. Then, a matroid is constructed through the height function
of the semimodular lattice. Finally, we propose an approach to obtain all the
closed sets of the matroid from the semimodular lattice. Borrowing from
matroids, results show that lattice theory provides an interesting view to
investigate rough sets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1212.3362v1,Material Targets for Scaling All Spin Logic,"All-spin logic devices are promising candidates to augment and complement
beyond-CMOS integrated circuit computing due to non-volatility, ultra-low
operating voltages, higher logical efficiency, and high density integration.
However, the path to reach lower energy-delay product performance compared to
CMOS transistors currently is not clear. We show that scaling and engineering
the nanoscale magnetic materials and interfaces is the key to realizing spin
logic devices that can surpass energy-delay performance of CMOS transistors.
With validated stochastic nano-magnetic and vector spin transport numerical
models, we derive the target material and interface properties for the
nanomagnets and channels. We identified promising new directions for material
engineering/discovery focusing on systematic scaling of magnetic anisotropy
(Hk) with saturation magnetization (Ms), use of perpendicular magnetic
anisotropy, and interface spin mixing conductance of ferromagnet/spin channel
interface (Gmix). We provide systematic targets for scaling spin logic
energy-delay product toward a 2 aJ.ns energy-delay product, comprehending the
stochastic noise for nanomagnets.",0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.4356v2,Lessons from the Congested Clique Applied to MapReduce,"The main results of this paper are (I) a simulation algorithm which, under
quite general constraints, transforms algorithms running on the Congested
Clique into algorithms running in the MapReduce model, and (II) a distributed
$O(\Delta)$-coloring algorithm running on the Congested Clique which has an
expected running time of (i) $O(1)$ rounds, if $\Delta \geq \Theta(\log^4 n)$;
and (ii) $O(\log \log n)$ rounds otherwise. Applying the simulation theorem to
the Congested-Clique $O(\Delta)$-coloring algorithm yields an $O(1)$-round
$O(\Delta)$-coloring algorithm in the MapReduce model.
  Our simulation algorithm illustrates a natural correspondence between
per-node bandwidth in the Congested Clique model and memory per machine in the
MapReduce model. In the Congested Clique (and more generally, any network in
the $\mathcal{CONGEST}$ model), the major impediment to constructing fast
algorithms is the $O(\log n)$ restriction on message sizes. Similarly, in the
MapReduce model, the combined restrictions on memory per machine and total
system memory have a dominant effect on algorithm design. In showing a fairly
general simulation algorithm, we highlight the similarities and differences
between these models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1408.5380v1,"Evolving Modular Genetic Regulatory Networks with a Recursive, Top-Down
  Approach","Being able to design genetic regulatory networks (GRNs) to achieve a desired
cellular function is one of the main goals of synthetic biology. However,
determining minimal GRNs that produce desired time-series behaviors is
non-trivial. In this paper, we propose a 'top-down' approach to evolving small
GRNs and then use these to recursively boot-strap the identification of larger,
more complex, modular GRNs. We start with relatively dense GRNs and then use
differential evolution (DE) to evolve interaction coefficients. When the target
dynamical behavior is found embedded in a dense GRN, we narrow the focus of the
search and begin aggressively pruning out excess interactions at the end of
each generation. We first show that the method can quickly rediscover known
small GRNs for a toggle switch and an oscillatory circuit. Next we include
these GRNs as non-evolvable subnetworks in the subsequent evolution of more
complex, modular GRNs. Successful solutions found in canonical DE where we
truncated small interactions to zero, with or without an interaction penalty
term, invariably contained many excess interactions. In contrast, by
incorporating aggressive pruning and the penalty term, the DE was able to find
minimal or nearly minimal GRNs in all test problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1210.7858v1,Solving Linear System of Equations Via A Convex Hull Algorithm,"We present new iterative algorithms for solving a square linear system $Ax=b$
in dimension $n$ by employing the {\it Triangle Algorithm} \cite{kal12}, a
fully polynomial-time approximation scheme for testing if the convex hull of a
finite set of points in a Euclidean space contains a given point. By converting
$Ax=b$ into a convex hull problem and solving via the Triangle Algorithm,
together with a {\it sensitivity theorem}, we compute in $O(n^2\epsilon^{-2})$
arithmetic operations an approximate solution satisfying $\Vert Ax_\epsilon - b
\Vert \leq \epsilon \rho$, where $\rho= \max \{\Vert a_1 \Vert,..., \Vert a_n
\Vert, \Vert b \Vert \}$, and $a_i$ is the $i$-th column of $A$. In another
approach we apply the Triangle Algorithm incrementally, solving a sequence of
convex hull problems while repeatedly employing a {\it distance duality}. The
simplicity and theoretical complexity bounds of the proposed algorithms,
requiring no structural restrictions on the matrix $A$, suggest their potential
practicality, offering alternatives to the existing exact and iterative
methods, especially for large scale linear systems. The assessment of
computational performance however is the subject of future experimentations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1603.04588v1,Classification with Repulsion Tensors: A Case Study on Face Recognition,"We consider dimensionality reduction methods for face recognition in a
supervised setting, using an image-as-matrix representation. A common procedure
is to project image matrices into a smaller space in which the recognition is
performed. These methods are often called ""two-dimensional"" in the literature
and there exist counterparts that use an image-as-vector representation. When
two face images are close to each other in the input space they may remain
close after projection - but this is not desirable in the situation when these
two images are from different classes, and this often affects the recognition
performance. We extend a previously developed `repulsion Laplacean' technique
based on adding terms to the objective function with the goal or creation a
repulsion energy between such images in the projected space. This scheme, which
relies on a repulsion graph, is generic and can be incorporated into various
two-dimensional methods. It can be regarded as a multilinear generalization of
the repulsion strategy by Kokiopoulou and Saad [Pattern Recog., 42 (2009), pp.
2392--2402]. Experimental results demonstrate that the proposed methodology
offers significant recognition improvement relative to the underlying
two-dimensional methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0403030v1,"Performance Evaluation of Packet-to-Cell Segmentation Schemes in Input
  Buffered Packet Switches","Most input buffered packet switches internally segment variable-length
packets into fixed-length cells. The last cell in a segmented packet will
contain overhead bytes if the packet length is not evenly divisible by the cell
length. Switch speed-up is used to compensate for this overhead. In this paper,
we develop an analytical model of a single-server queue where an input stream
of packets is segmented into cells for service. Analytical models are developed
for M/M/1, M/H2/1, and M/E2/1 queues with a discretized (or quantized) service
time. These models and simulation using real packet traces are used to evaluate
the effect of speed-up on mean queue length. We propose and evaluate a new
method of segmenting a packet trailer and subsequent packet header into a
single cell. This cell merging method reduces the required speed-up. No changes
to switch-matrix scheduling algorithms are needed. Simulation with a packet
trace shows a reduction in the needed speed-up for an iSLIP scheduled input
buffered switch.",0,0,0,0,0,0,1,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1801.07664v4,Internal Universes in Models of Homotopy Type Theory,"We begin by recalling the essentially global character of universes in
various models of homotopy type theory, which prevents a straightforward
axiomatization of their properties using the internal language of the presheaf
toposes from which these model are constructed. We get around this problem by
extending the internal language with a modal operator for expressing properties
of global elements. In this setting we show how to construct a universe that
classifies the Cohen-Coquand-Huber-M\""ortberg (CCHM) notion of fibration from
their cubical sets model, starting from the assumption that the interval is
tiny - a property that the interval in cubical sets does indeed have. This
leads to an elementary axiomatization of that and related models of homotopy
type theory within what we call crisp type theory.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0705.2876v1,"An online algorithm for generating fractal hash chains applied to
  digital chains of custody","This paper gives an online algorithm for generating Jakobsson's fractal hash
chains. Our new algorithm compliments Jakobsson's fractal hash chain algorithm
for preimage traversal since his algorithm assumes the entire hash chain is
precomputed and a particular list of Ceiling(log n) hash elements or pebbles
are saved. Our online algorithm for hash chain traversal incrementally
generates a hash chain of n hash elements without knowledge of n before it
starts. For any n, our algorithm stores only the Ceiling(log n) pebbles which
are precisely the inputs for Jakobsson's amortized hash chain preimage
traversal algorithm. This compact representation is useful to generate,
traverse, and store a number of large digital hash chains on a small and
constrained device. We also give an application using both Jakobsson's and our
new algorithm applied to digital chains of custody for validating dynamically
changing forensics data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/0810.3708v2,Characterising Testing Preorders for Finite Probabilistic Processes,"In 1992 Wang & Larsen extended the may- and must preorders of De Nicola and
Hennessy to processes featuring probabilistic as well as nondeterministic
choice. They concluded with two problems that have remained open throughout the
years, namely to find complete axiomatisations and alternative
characterisations for these preorders. This paper solves both problems for
finite processes with silent moves. It characterises the may preorder in terms
of simulation, and the must preorder in terms of failure simulation. It also
gives a characterisation of both preorders using a modal logic. Finally it
axiomatises both preorders over a probabilistic version of CSP.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.05539v4,BioLCNet: Reward-modulated Locally Connected Spiking Neural Networks,"Brain-inspired computation and information processing alongside compatibility
with neuromorphic hardware have made spiking neural networks (SNN) a promising
method for solving learning tasks in machine learning (ML). However, the mere
use of spiking neurons is not sufficient for building models that mimic the
learning mechanisms in the biological brain; network architecture and learning
rules are important factors to consider when developing such artificial agents.
In this work, inspired by the human visual pathway and the role of dopamine in
learning, we propose a reward-modulated locally connected spiking neural
network, BioLCNet, for visual learning tasks. To extract visual features from
Poisson-distributed spike trains, we used local filters that are more analogous
to the biological visual system compared to convolutional filters with weight
sharing. In the decoding layer, we applied a spike population-based voting
scheme to determine the decision of the network. We employed
Spike-timing-dependent plasticity (STDP) for learning the visual features, and
its reward-modulated variant (R-STDP) for training the decoder based on the
reward or punishment feedback signal. For evaluation, we first assessed the
robustness of our rewarding mechanism to varying target responses in a
classical conditioning experiment. Afterwards, we evaluated the performance of
our network on image classification tasks of MNIST and XOR MNIST datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1304.3169v2,The Computational Complexity of Random Serial Dictatorship,"In social choice settings with linear preferences, random dictatorship is
known to be the only social decision scheme satisfying strategyproofness and ex
post efficiency. When also allowing indifferences, random serial dictatorship
(RSD) is a well-known generalization of random dictatorship that retains both
properties. RSD has been particularly successful in the special domain of
random assignment where indifferences are unavoidable. While executing RSD is
obviously feasible, we show that computing the resulting probabilities is
#P-complete and thus intractable, both in the context of voting and assignment.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1811.03337v2,Distributed Exact Weighted All-Pairs Shortest Paths in Near-Linear Time,"In the {\em distributed all-pairs shortest paths} problem (APSP), every node
in the weighted undirected distributed network (the CONGEST model) needs to
know the distance from every other node using least number of communication
rounds (typically called {\em time complexity}). The problem admits
$(1+o(1))$-approximation $\tilde\Theta(n)$-time algorithm and a nearly-tight
$\tilde \Omega(n)$ lower bound [Nanongkai, STOC'14; Lenzen and Patt-Shamir
PODC'15]\footnote{$\tilde \Theta$, $\tilde O$ and $\tilde \Omega$ hide
polylogarithmic factors. Note that the lower bounds also hold even in the
unweighted case and in the weighted case with polynomial approximation
ratios~\cite{LenzenP_podc13,HolzerW12,PelegRT12,Nanongkai-STOC14}.}. For the
exact case, Elkin [STOC'17] presented an $O(n^{5/3} \log^{2/3} n)$ time bound,
which was later improved to $\tilde O(n^{5/4})$ [Huang, Nanongkai, Saranurak
FOCS'17]. It was shown that any super-linear lower bound (in $n$) requires a
new technique [Censor-Hillel, Khoury, Paz, DISC'17], but otherwise it remained
widely open whether there exists a $\tilde O(n)$-time algorithm for the exact
case, which would match the best possible approximation algorithm.
  This paper resolves this question positively: we present a randomized (Las
Vegas) $\tilde O(n)$-time algorithm, matching the lower bound up to
polylogarithmic factors. Like the previous $\tilde O(n^{5/4})$ bound, our
result works for directed graphs with zero (and even negative) edge weights. In
addition to the improved running time, our algorithm works in a more general
setting than that required by the previous $\tilde O(n^{5/4})$ bound; in our
setting (i) the communication is only along edge directions (as opposed to
bidirectional), and (ii) edge weights are arbitrary (as opposed to integers in
{1, 2, ... poly(n)}). ...",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1608.04579v1,Endpoint-transparent Multipath Transport with Software-defined Networks,"Multipath forwarding consists of using multiple paths simultaneously to
transport data over the network. While most such techniques require endpoint
modifications, we investigate how multipath forwarding can be done inside the
network, transparently to endpoint hosts. With such a network-centric approach,
packet reordering becomes a critical issue as it may cause critical performance
degradation.
  We present a Software Defined Network architecture which automatically sets
up multipath forwarding, including solutions for reordering and performance
improvement, both at the sending side through multipath scheduling algorithms,
and the receiver side, by resequencing out-of-order packets in a dedicated
in-network buffer.
  We implemented a prototype with commonly available technology and evaluated
it in both emulated and real networks. Our results show consistent throughput
improvements, thanks to the use of aggregated path capacity. We give
comparisons to Multipath TCP, where we show our approach can achieve a similar
performance while offering the advantage of endpoint transparency.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9809110v1,Similarity-Based Models of Word Cooccurrence Probabilities,"In many applications of natural language processing (NLP) it is necessary to
determine the likelihood of a given word combination. For example, a speech
recognizer may need to determine which of the two word combinations ``eat a
peach'' and ``eat a beach'' is more likely. Statistical NLP methods determine
the likelihood of a word combination from its frequency in a training corpus.
However, the nature of language is such that many word combinations are
infrequent and do not occur in any given corpus. In this work we propose a
method for estimating the probability of such previously unseen word
combinations using available information on ``most similar'' words.
  We describe probabilistic word association models based on distributional
word similarity, and apply them to two tasks, language modeling and pseudo-word
disambiguation. In the language modeling task, a similarity-based model is used
to improve probability estimates for unseen bigrams in a back-off language
model. The similarity-based method yields a 20% perplexity improvement in the
prediction of unseen bigrams and statistically significant reductions in
speech-recognition error.
  We also compare four similarity-based estimation methods against back-off and
maximum-likelihood estimation methods on a pseudo-word sense disambiguation
task in which we controlled for both unigram and bigram frequency to avoid
giving too much weight to easy-to-disambiguate high-frequency configurations.
The similarity-based methods perform up to 40% better on this particular task.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1202.1098v3,Causal graph dynamics,"We extend the theory of Cellular Automata to arbitrary, time-varying graphs.
In other words we formalize, and prove theorems about, the intuitive idea of a
labelled graph which evolves in time - but under the natural constraint that
information can only ever be transmitted at a bounded speed, with respect to
the distance given by the graph. The notion of translation-invariance is also
generalized. The definition we provide for these ""causal graph dynamics"" is
simple and axiomatic. The theorems we provide also show that it is robust. For
instance, causal graph dynamics are stable under composition and under
restriction to radius one. In the finite case some fundamental facts of
Cellular Automata theory carry through: causal graph dynamics admit a
characterization as continuous functions, and they are stable under inversion.
The provided examples suggest a wide range of applications of this mathematical
object, from complex systems science to theoretical physics. KEYWORDS:
Dynamical networks, Boolean networks, Generative networks automata, Cayley
cellular automata, Graph Automata, Graph rewriting automata, Parallel graph
transformations, Amalgamated graph transformations, Time-varying graphs, Regge
calculus, Local, No-signalling.",0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.00576v1,Exposing Previously Undetectable Faults in Deep Neural Networks,"Existing methods for testing DNNs solve the oracle problem by constraining
the raw features (e.g. image pixel values) to be within a small distance of a
dataset example for which the desired DNN output is known. But this limits the
kinds of faults these approaches are able to detect. In this paper, we
introduce a novel DNN testing method that is able to find faults in DNNs that
other methods cannot. The crux is that, by leveraging generative machine
learning, we can generate fresh test inputs that vary in their high-level
features (for images, these include object shape, location, texture, and
colour). We demonstrate that our approach is capable of detecting deliberately
injected faults as well as new faults in state-of-the-art DNNs, and that in
both cases, existing methods are unable to find these faults.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1008.3788v2,"Doubly Exponential Solution for Randomized Load Balancing Models with
  General Service Times","In this paper, we provide a novel and simple approach to study the
supermarket model with general service times. This approach is based on the
supplementary variable method used in analyzing stochastic models extensively.
We organize an infinite-size system of integral-differential equations by means
of the density dependent jump Markov process, and obtain a close-form solution:
doubly exponential structure, for the fixed point satisfying the system of
nonlinear equations, which is always a key in the study of supermarket models.
The fixed point is decomposited into two groups of information under a product
form: the arrival information and the service information. based on this, we
indicate two important observations: the fixed point for the supermarket model
is different from the tail of stationary queue length distribution for the
ordinary M/G/1 queue, and the doubly exponential solution to the fixed point
can extensively exist even if the service time distribution is heavy-tailed.
Furthermore, we analyze the exponential convergence of the current location of
the supermarket model to its fixed point, and study the Lipschitz condition in
the Kurtz Theorem under general service times. Based on these analysis, one can
gain a new understanding how workload probing can help in load balancing jobs
with general service times such as heavy-tailed service.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1407.4832v1,Collaborative Filtering Ensemble for Personalized Name Recommendation,"Out of thousands of names to choose from, picking the right one for your
child is a daunting task. In this work, our objective is to help parents making
an informed decision while choosing a name for their baby. We follow a
recommender system approach and combine, in an ensemble, the individual
rankings produced by simple collaborative filtering algorithms in order to
produce a personalized list of names that meets the individual parents' taste.
Our experiments were conducted using real-world data collected from the query
logs of 'nameling' (nameling.net), an online portal for searching and exploring
names, which corresponds to the dataset released in the context of the ECML
PKDD Discover Challenge 2013. Our approach is intuitive, easy to implement, and
features fast training and prediction steps.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0308031v1,Artificial Neural Networks for Beginners,"The scope of this teaching package is to make a brief induction to Artificial
Neural Networks (ANNs) for people who have no previous knowledge of them. We
first make a brief introduction to models of networks, for then describing in
general terms ANNs. As an application, we explain the backpropagation
algorithm, since it is widely used and many other algorithms are derived from
it. The user should know algebra and the handling of functions and vectors.
Differential calculus is recommendable, but not necessary. The contents of this
package should be understood by people with high school education. It would be
useful for people who are just curious about what are ANNs, or for people who
want to become familiar with them, so when they study them more fully, they
will already have clear notions of ANNs. Also, people who only want to apply
the backpropagation algorithm without a detailed and formal explanation of it
will find this material useful. This work should not be seen as ""Nets for
dummies"", but of course it is not a treatise. Much of the formality is skipped
for the sake of simplicity. Detailed explanations and demonstrations can be
found in the referred readings. The included exercises complement the
understanding of the theory. The on-line resources are highly recommended for
extending this brief induction.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1812.07782v1,"Decentralized Periodic Approach for Adaptive Fault Diagnosis in
  Distributed Systems","In this paper, Decentralized Periodic Approach for Adaptive Fault Diagnosis
(DP-AFD) algorithm is proposed for fault diagnosis in distributed systems with
arbitrary topology. Faulty nodes may be either unresponsive, may have either
software or hardware faults. The proposed algorithm detects the faulty nodes
situated in geographically distributed locations. This algorithm does not
depend on a single node or leader to detect the faults in the system. However,
it empowers more than one node to detect the fault-free and faulty nodes in the
system. Thus, at the end of each test cycle, every fault-free node acts as a
leader to diagnose faults in the system. This feature of the algorithm makes it
applicable to any arbitrary network. After every test cycle of the algorithm,
all the nodes have knowledge about faulty nodes and each node is tested only
once. With this knowledge, there can be redistribution of load, which was
earlier assigned to the faulty nodes. Also, the algorithm permits repaired node
re-entry and new node entry. In a system of n nodes, the maximum number of
faulty nodes can be (n-1) which is detected by DP-AFD algorithm. DP-AFD is
periodic in nature which executes test cycles after regular intervals to detect
the faulty nodes in the given distributed system.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.10315v2,"Kernel Methods for Unobserved Confounding: Negative Controls, Proxies,
  and Instruments","Negative control is a strategy for learning the causal relationship between
treatment and outcome in the presence of unmeasured confounding. The treatment
effect can nonetheless be identified if two auxiliary variables are available:
a negative control treatment (which has no effect on the actual outcome), and a
negative control outcome (which is not affected by the actual treatment). These
auxiliary variables can also be viewed as proxies for a traditional set of
control variables, and they bear resemblance to instrumental variables. I
propose a family of algorithms based on kernel ridge regression for learning
nonparametric treatment effects with negative controls. Examples include dose
response curves, dose response curves with distribution shift, and
heterogeneous treatment effects. Data may be discrete or continuous, and low,
high, or infinite dimensional. I prove uniform consistency and provide finite
sample rates of convergence. I estimate the dose response curve of cigarette
smoking on infant birth weight adjusting for unobserved confounding due to
household income, using a data set of singleton births in the state of
Pennsylvania between 1989 and 1991.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0405100v1,Typing constraint logic programs,"We present a prescriptive type system with parametric polymorphism and
subtyping for constraint logic programs. The aim of this type system is to
detect programming errors statically. It introduces a type discipline for
constraint logic programs and modules, while maintaining the capabilities of
performing the usual coercions between constraint domains, and of typing
meta-programming predicates, thanks to the flexibility of subtyping. The
property of subject reduction expresses the consistency of a prescriptive type
system w.r.t. the execution model: if a program is ""well-typed"", then all
derivations starting from a ""well-typed"" goal are again ""well-typed"". That
property is proved w.r.t. the abstract execution model of constraint
programming which proceeds by accumulation of constraints only, and w.r.t. an
enriched execution model with type constraints for substitutions. We describe
our implementation of the system for type checking and type inference. We
report our experimental results on type checking ISO-Prolog, the (constraint)
libraries of Sicstus Prolog and other Prolog programs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.08566v1,"Controlled invariant sets: implicit closed-form representations and
  applications","In this paper we revisit the problem of computing robust controlled invariant
sets for discrete-time linear systems. The key idea is that by considering
controllers that exhibit eventually periodic behavior, we obtain a closed-form
expression for an implicit representation of a robust controlled invariant set
in the space of states and finite input sequences. Due to the derived
closed-form expression, our method is suitable for high dimensional systems.
Optionally, one obtains an explicit robust controlled invariant set by
projecting the implicit representation to the original state space. The
proposed method is complete in the absence of disturbances, with a weak
completeness result established when disturbances are present. Moreover, we
show that a specific controller choice yields a hierarchy of robust controlled
invariant sets. To validate the proposed method, we present thorough case
studies illustrating that in safety-critical scenarios the implicit
representation suffices in place of the explicit invariant set.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.03493v1,Signal reconstruction via operator guiding,"Signal reconstruction from a sample using an orthogonal projector onto a
guiding subspace is theoretically well justified, but may be difficult to
practically implement. We propose more general guiding operators, which
increase signal components in the guiding subspace relative to those in a
complementary subspace, e.g., iterative low-pass edge-preserving filters for
super-resolution of images. Two examples of super-resolution illustrate our
technology: a no-flash RGB photo guided using a high resolution flash RGB
photo, and a depth image guided using a high resolution RGB photo.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.14813v1,Stable Anderson Acceleration for Deep Learning,"Anderson acceleration (AA) is an extrapolation technique designed to speed-up
fixed-point iterations like those arising from the iterative training of DL
models. Training DL models requires large datasets processed in randomly
sampled batches that tend to introduce in the fixed-point iteration stochastic
oscillations of amplitude roughly inversely proportional to the size of the
batch. These oscillations reduce and occasionally eliminate the positive effect
of AA. To restore AA's advantage, we combine it with an adaptive moving average
procedure that smoothes the oscillations and results in a more regular sequence
of gradient descent updates. By monitoring the relative standard deviation
between consecutive iterations, we also introduce a criterion to automatically
assess whether the moving average is needed. We applied the method to the
following DL instantiations: (i) multi-layer perceptrons (MLPs) trained on the
open-source graduate admissions dataset for regression, (ii) physics informed
neural networks (PINNs) trained on source data to solve 2d and 100d Burgers'
partial differential equations (PDEs), and (iii) ResNet50 trained on the
open-source ImageNet1k dataset for image classification. Numerical results
obtained using up to 1,536 NVIDIA V100 GPUs on the OLCF supercomputer Summit
showed the stabilizing effect of the moving average on AA for all the problems
above.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1808.03370v1,Fast Flexible Function Dispatch in Julia,"Technical computing is a challenging application area for programming
languages to address. This is evinced by the unusually large number of
specialized languages in the area (e.g. MATLAB, R), and the complexity of
common software stacks, often involving multiple languages and custom code
generators. We believe this is ultimately due to key characteristics of the
domain: highly complex operators, a need for extensive code specialization for
performance, and a desire for permissive high-level programming styles allowing
productive experimentation. The Julia language attempts to provide a more
effective structure for this kind of programming by allowing programmers to
express complex polymorphic behaviors using dynamic multiple dispatch over
parametric types. The forms of extension and reuse permitted by this paradigm
have proven valuable for technical computing. We report on how this approach
has allowed domain experts to express useful abstractions while simultaneously
providing a natural path to better performance for high-level technical code.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1307.1252v1,"The Complexity of Fully Proportional Representation for Single-Crossing
  Electorates","We study the complexity of winner determination in single-crossing elections
under two classic fully proportional representation
rules---Chamberlin--Courant's rule and Monroe's rule. Winner determination for
these rules is known to be NP-hard for unrestricted preferences. We show that
for single-crossing preferences this problem admits a polynomial-time algorithm
for Chamberlin--Courant's rule, but remains NP-hard for Monroe's rule. Our
algorithm for Chamberlin--Courant's rule can be modified to work for elections
with bounded single-crossing width. To circumvent the hardness result for
Monroe's rule, we consider single-crossing elections that satisfy an additional
constraint, namely, ones where each candidate is ranked first by at least one
voter (such elections are called narcissistic). For single-crossing
narcissistic elections, we provide an efficient algorithm for the egalitarian
version of Monroe's rule.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1506.07810v1,Canonizing Graphs of Bounded Tree Width in Logspace,"Graph canonization is the problem of computing a unique representative, a
canon, from the isomorphism class of a given graph. This implies that two
graphs are isomorphic exactly if their canons are equal. We show that graphs of
bounded tree width can be canonized by logarithmic-space (logspace) algorithms.
This implies that the isomorphism problem for graphs of bounded tree width can
be decided in logspace. In the light of isomorphism for trees being hard for
the complexity class logspace, this makes the ubiquitous class of graphs of
bounded tree width one of the few classes of graphs for which the complexity of
the isomorphism problem has been exactly determined.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.14457v1,Verification of a Smart Contract for a Simple Casino,"We describe the verification of an existing smart contract for a simple
casino application, using the Whiley specification and programming language,
with a fully automated verification engine based on Boogie and Z3. After
finding and fixing several specification and code issues in the smart contract,
we are able to verify all the operations of the smart contract.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.4027v1,Three-Way Joins on MapReduce: An Experimental Study,"We study three-way joins on MapReduce. Joins are very useful in a multitude
of applications from data integration and traversing social networks, to mining
graphs and automata-based constructions. However, joins are expensive, even for
moderate data sets; we need efficient algorithms to perform distributed
computation of joins using clusters of many machines. MapReduce has become an
increasingly popular distributed computing system and programming paradigm. We
consider a state-of-the-art MapReduce multi-way join algorithm by Afrati and
Ullman and show when it is appropriate for use on very large data sets. By
providing a detailed experimental study, we demonstrate that this algorithm
scales much better than what is suggested by the original paper. However, if
the join result needs to be summarized or aggregated, as opposed to being only
enumerated, then the aggregation step can be integrated into a cascade of
two-way joins, making it more efficient than the other algorithm, and thus
becomes the preferred solution.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1811.11853v2,Safe Deferred Memory Reclamation with Types,"Memory management in lock-free data structures remains a major challenge in
concurrent programming. Design techniques including read-copy-update (RCU) and
hazard pointers provide workable solutions, and are widely used to great
effect. These techniques rely on the concept of a grace period: nodes that
should be freed are not deallocated immediately, and all threads obey a
protocol to ensure that the deallocating thread can detect when all possible
readers have completed their use of the object. This provides an approach to
safe deallocation, but only when these subtle protocols are implemented
correctly.
  We present a static type system to ensure the correct use of RCU memory
management: that nodes removed from a data structure are always scheduled for
subsequent deallocation, and that nodes are scheduled for deallocation at most
once. As part of our soundness proof, we give an abstract semantics for RCU
memory management primitives which captures the fundamental properties of RCU.
Our type system allows us to give the first proofs of memory safety for RCU
linked list and binary search tree implementations without requiring full
verification.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1612.05675v1,Targeting Infeasibility Questions on Obfuscated Codes,"Software deobfuscation is a crucial activity in security analysis and
especially, in malware analysis. While standard static and dynamic approaches
suffer from well-known shortcomings, Dynamic Symbolic Execution (DSE) has
recently been proposed has an interesting alternative, more robust than static
analysis and more complete than dynamic analysis. Yet, DSE addresses certain
kinds of questions encountered by a reverser namely feasibility questions. Many
issues arising during reverse, e.g. detecting protection schemes such as opaque
predicates fall into the category of infeasibility questions. In this article,
we present the Backward-Bounded DSE, a generic, precise, efficient and robust
method for solving infeasibility questions. We demonstrate the benefit of the
method for opaque predicates and call stack tampering, and give some insight
for its usage for some other protection schemes. Especially, the technique has
successfully been used on state-of-the-art packers as well as on the
government-grade X-Tunnel malware -- allowing its entire deobfuscation.
Backward-Bounded DSE does not supersede existing DSE approaches, but rather
complements them by addressing infeasibility questions in a scalable and
precise manner. Following this line, we propose sparse disassembly, a
combination of Backward-Bounded DSE and static disassembly able to enlarge
dynamic disassembly in a guaranteed way, hence getting the best of dynamic and
static disassembly. This work paves the way for robust, efficient and precise
disassembly tools for heavily-obfuscated binaries.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/cs/0610006v2,"A Typed Hybrid Description Logic Programming Language with Polymorphic
  Order-Sorted DL-Typed Unification for Semantic Web Type Systems","In this paper we elaborate on a specific application in the context of hybrid
description logic programs (hybrid DLPs), namely description logic Semantic Web
type systems (DL-types) which are used for term typing of LP rules based on a
polymorphic, order-sorted, hybrid DL-typed unification as procedural semantics
of hybrid DLPs. Using Semantic Web ontologies as type systems facilitates
interchange of domain-independent rules over domain boundaries via dynamically
typing and mapping of explicitly defined type ontologies.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1506.00555v1,Writing and Publishing Scientific Articles in Computer Science,"Over 15 years of teaching, advising students and coordinating scientific
research activities and projects in computer science, we have observed the
difficulties of students to write scientific papers to present the results of
their research practices. In addition, they repeatedly have doubts about the
publishing process. In this article we propose a conceptual framework to
support the writing and publishing of scientific papers in computer science,
providing a kind of guide for computer science students to effectively present
the results of their research practices, particularly for experimental
research.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/2010.06156v1,"High Area/Energy Efficiency RRAM CNN Accelerator with Kernel-Reordering
  Weight Mapping Scheme Based on Pattern Pruning","Resistive Random Access Memory (RRAM) is an emerging device for
processing-in-memory (PIM) architecture to accelerate convolutional neural
network (CNN). However, due to the highly coupled crossbar structure in the
RRAM array, it is difficult to exploit the sparsity of the network in
RRAM-based CNN accelerator. To optimize the weight mapping of sparse network in
the RRAM array and achieve high area and energy efficiency, we propose a novel
weight mapping scheme and corresponding RRAM-based CNN accelerator architecture
based on pattern pruning and Operation Unit(OU) mechanism. Experimental results
show that our work can achieve 4.16x-5.20x crossbar area efficiency,
1.98x-2.15x energy efficiency, and 1.15x-1.35x performance speedup in
comparison with the traditional weight mapping method.",0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.05064v1,"Effects of Voice-Based Synthetic Assistant on Performance of Emergency
  Care Provider in Training","As part of a perennial project, our team is actively engaged in developing
new synthetic assistant (SA) technologies to assist in training combat medics
and medical first responders. It is critical that medical first responders are
well trained to deal with emergencies more effectively. This would require
real-time monitoring and feedback for each trainee. Therefore, we introduced a
voice-based SA to augment the training process of medical first responders and
enhance their performance in the field. The potential benefits of SAs include a
reduction in training costs and enhanced monitoring mechanisms. Despite the
increased usage of voice-based personal assistants (PAs) in day-to-day life,
the associated effects are commonly neglected for a study of human factors.
Therefore, this paper focuses on performance analysis of the developed
voice-based SA in emergency care provider training for a selected emergency
treatment scenario. The research discussed in this paper follows design science
in developing proposed technology; at length, we discussed architecture and
development and presented working results of voice-based SA. The empirical
testing was conducted on two groups as user studies using statistical analysis
tools, one trained with conventional methods and the other with the help of SA.
The statistical results demonstrated the amplification in training efficacy and
performance of medical responders powered by SA. Furthermore, the paper also
discusses the accuracy and time of task execution (t) and concludes with the
guidelines for resolving the identified problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1008.5166v1,"Network Archaeology: Uncovering Ancient Networks from Present-day
  Interactions","Often questions arise about old or extinct networks. What proteins interacted
in a long-extinct ancestor species of yeast? Who were the central players in
the Last.fm social network 3 years ago? Our ability to answer such questions
has been limited by the unavailability of past versions of networks. To
overcome these limitations, we propose several algorithms for reconstructing a
network's history of growth given only the network as it exists today and a
generative model by which the network is believed to have evolved. Our
likelihood-based method finds a probable previous state of the network by
reversing the forward growth model. This approach retains node identities so
that the history of individual nodes can be tracked. We apply these algorithms
to uncover older, non-extant biological and social networks believed to have
grown via several models, including duplication-mutation with complementarity,
forest fire, and preferential attachment. Through experiments on both synthetic
and real-world data, we find that our algorithms can estimate node arrival
times, identify anchor nodes from which new nodes copy links, and can reveal
significant features of networks that have long since disappeared.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.13180v2,"Variational Gibbs inference for statistical model estimation from
  incomplete data","Statistical models are central to machine learning with broad applicability
across a range of downstream tasks. The models are controlled by free
parameters that are typically estimated from data by maximum-likelihood
estimation or approximations thereof. However, when faced with real-world
datasets many of the models run into a critical issue: they are formulated in
terms of fully-observed data, whereas in practice the datasets are plagued with
missing data. The theory of statistical model estimation from incomplete data
is conceptually similar to the estimation of latent-variable models, where
powerful tools such as variational inference (VI) exist. However, in contrast
to standard latent-variable models, parameter estimation with incomplete data
often requires estimating exponentially-many conditional distributions of the
missing variables, hence making standard VI methods intractable. We address
this gap by introducing variational Gibbs inference (VGI), a new
general-purpose method to estimate the parameters of statistical models from
incomplete data. We validate VGI on a set of synthetic and real-world
estimation tasks, estimating important machine learning models such as VAEs and
normalising flows from incomplete data. The proposed method, whilst
general-purpose, achieves competitive or better performance than existing
model-specific estimation methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1801.10312v1,"A Deep Ranking Model for Spatio-Temporal Highlight Detection from a 360
  Video","We address the problem of highlight detection from a 360 degree video by
summarizing it both spatially and temporally. Given a long 360 degree video, we
spatially select pleasantly-looking normal field-of-view (NFOV) segments from
unlimited field of views (FOV) of the 360 degree video, and temporally
summarize it into a concise and informative highlight as a selected subset of
subshots. We propose a novel deep ranking model named as Composition View Score
(CVS) model, which produces a spherical score map of composition per video
segment, and determines which view is suitable for highlight via a sliding
window kernel at inference. To evaluate the proposed framework, we perform
experiments on the Pano2Vid benchmark dataset and our newly collected 360
degree video highlight dataset from YouTube and Vimeo. Through evaluation using
both quantitative summarization metrics and user studies via Amazon Mechanical
Turk, we demonstrate that our approach outperforms several state-of-the-art
highlight detection methods. We also show that our model is 16 times faster at
inference than AutoCam, which is one of the first summarization algorithms of
360 degree videos",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1809.05275v1,Learning to Fingerprint the Latent Structure in Question Articulation,"Abstract Machine understanding of questions is tightly related to recognition
of articulation in the context of the computational capabilities of an
underlying processing algorithm. In this paper a mathematical model to capture
and distinguish the latent structure in the articulation of questions is
presented. We propose an objective-driven approach to represent this latent
structure and show that such an approach is beneficial when examples of
complementary objectives are not available. We show that the latent structure
can be represented as a system that maximizes a cost function related to the
underlying objective. Further, we show that the optimization formulation can be
approximated to building a memory of patterns represented as a trained neural
auto-encoder. Experimental evaluation using many clusters of questions, each
related to an objective, shows 80% recognition accuracy and negligible false
positive across these clusters of questions. We then extend the same memory to
a related task where the goal is to iteratively refine a dataset of questions
based on the latent articulation. We also demonstrate a refinement scheme
called K-fingerprints, that achieves nearly 100% recognition with negligible
false positive across the different clusters of questions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1411.6829v3,Approximately counting locally-optimal structures,"A locally-optimal structure is a combinatorial structure such as a maximal
independent set that cannot be improved by certain (greedy) local moves, even
though it may not be globally optimal. It is trivial to construct an
independent set in a graph. It is easy to (greedily) construct a maximal
independent set. However, it is NP-hard to construct a globally-optimal
(maximum) independent set. In general, constructing a locally-optimal structure
is somewhat more difficult than constructing an arbitrary structure, and
constructing a globally-optimal structure is more difficult than constructing a
locally-optimal structure. The same situation arises with listing. The
differences between the problems become obscured when we move from listing to
counting because nearly everything is #P-complete. However, we highlight an
interesting phenomenon that arises in approximate counting, where the situation
is apparently reversed. Specifically, we show that counting maximal independent
sets is complete for #P with respect to approximation-preserving reductions,
whereas counting all independent sets, or counting maximum independent sets is
complete for an apparently smaller class, $\mathrm{\#RH}\Pi_1$ which has a
prominent role in the complexity of approximate counting. Motivated by the
difficulty of approximately counting maximal independent sets in bipartite
graphs, we also study the problem of approximately counting other
locally-optimal structures that arise in algorithmic applications, particularly
problems involving minimal separators and minimal edge separators. Minimal
separators have applications via fixed-parameter-tractable algorithms for
constructing triangulations and phylogenetic trees. Although exact
(exponential-time) algorithms exist for listing these structures, we show that
the counting problems are #P-complete with respect to both exact and
approximation-preserving reductions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1005.3224v1,Cellular Automata in Stream Ciphers,"A wide family of nonlinear sequence generators, the so-called
clock-controlled shrinking generators, has been analyzed and identified with a
subset of linear cellular automata. The algorithm that converts the given
generator into a linear model based on automata is very simple and can be
applied in a range of practical interest. Due to the linearity of these
automata as well as the characteristics of this class of generators, a
cryptanalytic approach can be proposed. Linear cellular structures easily model
keystream generators with application in stream cipher cryptography.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1001.0383v2,Restricted Space Algorithms for Isomorphism on Bounded Treewidth Graphs,"The Graph Isomorphism problem restricted to graphs of bounded treewidth or
bounded tree distance width are known to be solvable in polynomial time
[Bod90],[YBFT99]. We give restricted space algorithms for these problems
proving the following results: - Isomorphism for bounded tree distance width
graphs is in L and thus complete for the class. We also show that for this kind
of graphs a canon can be computed within logspace. - For bounded treewidth
graphs, when both input graphs are given together with a tree decomposition,
the problem of whether there is an isomorphism which respects the
decompositions (i.e. considering only isomorphisms mapping bags in one
decomposition blockwise onto bags in the other decomposition) is in L. - For
bounded treewidth graphs, when one of the input graphs is given with a tree
decomposition the isomorphism problem is in LogCFL. - As a corollary the
isomorphism problem for bounded treewidth graphs is in LogCFL. This improves
the known TC1 upper bound for the problem given by Grohe and Verbitsky
[GroVer06].",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.05234v3,"Identifying Linear Models in Multi-Resolution Population Data using
  Minimum Description Length Principle to Predict Household Income","One shirt size cannot fit everybody, while we cannot make a unique shirt that
fits perfectly for everyone because of resource limitation. This analogy is
true for the policy making. Policy makers cannot establish a single policy to
solve all problems for all regions because each region has its own unique
issue. In the other extreme, policy makers also cannot create a policy for each
small village due to the resource limitation. Would it be better if we can find
a set of largest regions such that the population of each region within this
set has common issues and we can establish a single policy for them? In this
work, we propose a framework using regression analysis and minimum description
length (MDL) to find a set of largest areas that have common indicators, which
can be used to predict household incomes efficiently. Given a set of household
features, and a multi-resolution partition that represents administrative
divisions, our framework reports a set C* of largest subdivisions that have a
common model for population-income prediction. We formalize a problem of
finding C* and propose the algorithm as a solution. We use both simulation
datasets as well as a real-world dataset of Thailand's population household
information to demonstrate our framework performance and application. The
results show that our framework performance is better than the baseline
methods. We show the results of our method can be used to find indicators of
income prediction for many areas in Thailand. By increasing these indicator
values, we expect people in these areas to gain more incomes. Hence, the policy
makers can plan to establish the policies by using these indicators in our
results as a guideline to solve low-income issues. Our framework can be used to
support policy makers to establish policies regarding any other dependent
variable beyond incomes in order to combat poverty and other issues.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2007.15617v1,A Core Calculus for Static Latency Tracking with Placement Types,"Developing efficient geo-distributed applications is challenging as
programmers can easily introduce computations that entail high latency
communication. We propose a language design which makes latency explicit and
extracts type-level bounds for a computation's runtime latency. We present our
initial steps with a core calculus that enables extracting provably correct
latency bounds and outline future work.",0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1103.5957v1,Multi-path Routing Metrics for Reliable Wireless Mesh Routing Topologies,"Several emerging classes of applications that run over wireless networks have
a need for mathematical models and tools to systematically characterize the
reliability of the network. We propose two metrics for measuring the
reliability of wireless mesh routing topologies, one for flooding and one for
unicast routing. The Flooding Path Probability (FPP) metric measures the
end-to-end packet delivery probability when each node broadcasts a packet after
hearing from all its upstream neighbors. The Unicast Retransmission Flow (URF)
metric measures the end-to-end packet delivery probability when a relay node
retransmits a unicast packet on its outgoing links until it receives an
acknowledgement or it tries all the links. Both metrics rely on specific packet
forwarding models, rather than heuristics, to derive explicit expressions of
the end-to-end packet delivery probability from individual link probabilities
and the underlying connectivity graph.
  We also propose a distributed, greedy algorithm that uses the URF metric to
construct a reliable routing topology. This algorithm constructs a Directed
Acyclic Graph (DAG) from a weighted, undirected connectivity graph, where each
link is weighted by its success probability. The algorithm uses a vector of
decreasing reliability thresholds to coordinate when nodes can join the routing
topology. Simulations demonstrate that, on average, this algorithm constructs a
more reliable topology than the usual minimum hop DAG.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1512.02179v1,Faster Information Gathering in Ad-Hoc Radio Tree Networks,"We study information gathering in ad-hoc radio networks. Initially, each node
of the network has a piece of information called a rumor, and the overall
objective is to gather all these rumors in the designated target node. The
ad-hoc property refers to the fact that the topology of the network is unknown
when the computation starts. Aggregation of rumors is not allowed, which means
that each node may transmit at most one rumor in one step.
  We focus on networks with tree topologies, that is we assume that the network
is a tree with all edges directed towards the root, but, being ad-hoc, its
actual topology is not known. We provide two deterministic algorithms for this
problem. For the model that does not assume any collision detection nor
acknowledgement mechanisms, we give an $O(n\log\log n)$-time algorithm,
improving the previous upper bound of $O(n\log n)$. We also show that this
running time can be further reduced to $O(n)$ if the model allows for
acknowledgements of successful transmissions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.12052v2,Few-shot 3D Point Cloud Semantic Segmentation,"Many existing approaches for 3D point cloud semantic segmentation are fully
supervised. These fully supervised approaches heavily rely on large amounts of
labeled training data that are difficult to obtain and cannot segment new
classes after training. To mitigate these limitations, we propose a novel
attention-aware multi-prototype transductive few-shot point cloud semantic
segmentation method to segment new classes given a few labeled examples.
Specifically, each class is represented by multiple prototypes to model the
complex data distribution of labeled points. Subsequently, we employ a
transductive label propagation method to exploit the affinities between labeled
multi-prototypes and unlabeled points, and among the unlabeled points.
Furthermore, we design an attention-aware multi-level feature learning network
to learn the discriminative features that capture the geometric dependencies
and semantic correlations between points. Our proposed method shows significant
and consistent improvements compared to baselines in different few-shot point
cloud semantic segmentation settings (i.e., 2/3-way 1/5-shot) on two benchmark
datasets. Our code is available at https://github.com/Na-Z/attMPTI.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0204036v1,Semantic Component Composition,"Building complex software systems necessitates the use of component-based
architectures. In theory, of the set of components needed for a design, only
some small portion of them are ""custom""; the rest are reused or refactored
existing pieces of software. Unfortunately, this is an idealized situation.
Just because two components should work together does not mean that they will
work together.
  The ""glue"" that holds components together is not just technology. The
contracts that bind complex systems together implicitly define more than their
explicit type. These ""conceptual contracts"" describe essential aspects of
extra-system semantics: e.g., object models, type systems, data representation,
interface action semantics, legal and contractual obligations, and more.
  Designers and developers spend inordinate amounts of time technologically
duct-taping systems to fulfill these conceptual contracts because system-wide
semantics have not been rigorously characterized or codified. This paper
describes a formal characterization of the problem and discusses an initial
implementation of the resulting theoretical system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.01968v6,Positive first-order logic on words,"We study FO+, a fragment of first-order logic on finite words, where monadic
predicates can only appear positively. We show that there is a FO-definable
language that is monotone in monadic predicates but not definable in FO+. This
provides a simple proof that Lyndon's preservation theorem fails on finite
structures. We additionally show that given a regular language, it is
undecidable whether it is definable in FO+.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0509099v3,State-Based Control of Fuzzy Discrete Event Systems,"To effectively represent possibility arising from states and dynamics of a
system, fuzzy discrete event systems as a generalization of conventional
discrete event systems have been introduced recently. Supervisory control
theory based on event feedback has been well established for such systems.
Noting that the system state description, from the viewpoint of specification,
seems more convenient, we investigate the state-based control of fuzzy discrete
event systems in this paper. We first present an approach to finding all fuzzy
states that are reachable by controlling the system. After introducing the
notion of controllability for fuzzy states, we then provide a necessary and
sufficient condition for a set of fuzzy states to be controllable. We also find
that event-based control and state-based control are not equivalent and further
discuss the relationship between them. Finally, we examine the possibility of
driving a fuzzy discrete event system under control from a given initial state
to a prescribed set of fuzzy states and then keeping it there indefinitely.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.02554v3,Simultaneously Achieving Ex-ante and Ex-post Fairness,"We present a polynomial-time algorithm that computes an ex-ante envy-free
lottery over envy-free up to one item (EF1) deterministic allocations. It has
the following advantages over a recently proposed algorithm: it does not rely
on the linear programming machinery including separation oracles; it is
SD-efficient (both ex-ante and ex-post); and the ex-ante outcome is equivalent
to the outcome returned by the well-known probabilistic serial rule. As a
result, we answer a question raised by Freeman, Shah, and Vaish (2020) whether
the outcome of the probabilistic serial rule can be implemented by ex-post EF1
allocations. In the light of a couple of impossibility results that we prove,
our algorithm can be viewed as satisfying a maximal set of properties. Under
binary utilities, our algorithm is also ex-ante group-strategyproof and ex-ante
Pareto optimal. Finally, we also show that checking whether a given random
allocation can be implemented by a lottery over EF1 and Pareto optimal
allocations is NP-hard.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0904.0027v1,"Faith in the Algorithm, Part 2: Computational Eudaemonics","Eudaemonics is the study of the nature, causes, and conditions of human
well-being. According to the ethical theory of eudaemonia, reaping satisfaction
and fulfillment from life is not only a desirable end, but a moral
responsibility. However, in modern society, many individuals struggle to meet
this responsibility. Computational mechanisms could better enable individuals
to achieve eudaemonia by yielding practical real-world systems that embody
algorithms that promote human flourishing. This article presents eudaemonic
systems as the evolutionary goal of the present day recommender system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2003.14171v2,Recognizing Characters in Art History Using Deep Learning,"In the field of Art History, images of artworks and their contexts are core
to understanding the underlying semantic information. However, the highly
complex and sophisticated representation of these artworks makes it difficult,
even for the experts, to analyze the scene. From the computer vision
perspective, the task of analyzing such artworks can be divided into
sub-problems by taking a bottom-up approach. In this paper, we focus on the
problem of recognizing the characters in Art History. From the iconography of
$Annunciation$ $of$ $the$ $Lord$ (Figure 1), we consider the representation of
the main protagonists, $Mary$ and $Gabriel$, across different artworks and
styles. We investigate and present the findings of training a character
classifier on features extracted from their face images. The limitations of
this method, and the inherent ambiguity in the representation of $Gabriel$,
motivated us to consider their bodies (a bigger context) to analyze in order to
recognize the characters. Convolutional Neural Networks (CNN) trained on the
bodies of $Mary$ and $Gabriel$ are able to learn person related features and
ultimately improve the performance of character recognition. We introduce a new
technique that generates more data with similar styles, effectively creating
data in the similar domain. We present experiments and analysis on three
different models and show that the model trained on domain related data gives
the best performance for recognizing character. Additionally, we analyze the
localized image regions for the network predictions. Code is open-sourced and
available at
https://github.com/prathmeshrmadhu/recognize_characters_art_history and the
link to the published peer-reviewed article is
https://dl.acm.org/citation.cfm?id=3357242.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.13492v1,Self-Supervised Encoder for Fault Prediction in Electrochemical Cells,"Predicting faults before they occur helps to avoid potential safety hazards.
Furthermore, planning the required maintenance actions in advance reduces
operation costs. In this article, the focus is on electrochemical cells. In
order to predict a cell's fault, the typical approach is to estimate the
expected voltage that a healthy cell would present and compare it with the
cell's measured voltage in real-time. This approach is possible because, when a
fault is about to happen, the cell's measured voltage differs from the one
expected for the same operating conditions. However, estimating the expected
voltage is challenging, as the voltage of a healthy cell is also affected by
its degradation -- an unknown parameter. Expert-defined parametric models are
currently used for this estimation task. Instead, we propose the use of a
neural network model based on an encoder-decoder architecture. The network
receives the operating conditions as input. The encoder's task is to find a
faithful representation of the cell's degradation and to pass it to the
decoder, which in turn predicts the expected cell's voltage. As no labeled
degradation data is given to the network, we consider our approach to be a
self-supervised encoder. Results show that we were able to predict the voltage
of multiple cells while diminishing the prediction error that was obtained by
the parametric models by 53%. This improvement enabled our network to predict a
fault 31 hours before it happened, a 64% increase in reaction time compared to
the parametric model. Moreover, the output of the encoder can be plotted,
adding interpretability to the neural network model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1402.3809v2,Toward Resilient Algorithms and Applications,"Over the past decade, the high performance computing community has become
increasingly concerned that preserving the reliable, digital machine model will
become too costly or infeasible. In this paper we discuss four approaches for
developing new algorithms that are resilient to hard and soft failures.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1507.05946v3,"Buzz: An Extensible Programming Language for Self-Organizing
  Heterogeneous Robot Swarms","We present Buzz, a novel programming language for heterogeneous robot swarms.
Buzz advocates a compositional approach, offering primitives to define swarm
behaviors both from the perspective of the single robot and of the overall
swarm. Single-robot primitives include robot-specific instructions and
manipulation of neighborhood data. Swarm-based primitives allow for the dynamic
management of robot teams, and for sharing information globally across the
swarm. Self-organization stems from the completely decentralized mechanisms
upon which the Buzz run-time platform is based. The language can be extended to
add new primitives (thus supporting heterogeneous robot swarms), and its
run-time platform is designed to be laid on top of other frameworks, such as
Robot Operating System. We showcase the capabilities of Buzz by providing code
examples, and analyze scalability and robustness of the run-time platform
through realistic simulated experiments with representative swarm algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.09028v1,"Probabilistically Sampled and Spectrally Clustered Plant Genotypes using
  Phenotypic Characteristics","Clustering genotypes based upon their phenotypic characteristics is used to
obtain diverse sets of parents that are useful in their breeding programs. The
Hierarchical Clustering (HC) algorithm is the current standard in clustering of
phenotypic data. This algorithm suffers from low accuracy and high
computational complexity issues. To address the accuracy challenge, we propose
the use of Spectral Clustering (SC) algorithm. To make the algorithm
computationally cheap, we propose using sampling, specifically, Pivotal
Sampling that is probability based. Since application of samplings to
phenotypic data has not been explored much, for effective comparison, another
sampling technique called Vector Quantization (VQ) is adapted for this data as
well. VQ has recently given promising results for genome data.
  The novelty of our SC with Pivotal Sampling algorithm is in constructing the
crucial similarity matrix for the clustering algorithm and defining
probabilities for the sampling technique. Although our algorithm can be applied
to any plant genotypes, we test it on the phenotypic data obtained from about
2400 Soybean genotypes. SC with Pivotal Sampling achieves substantially more
accuracy (in terms of Silhouette Values) than all the other proposed
competitive clustering with sampling algorithms (i.e. SC with VQ, HC with
Pivotal Sampling, and HC with VQ). The complexities of our SC with Pivotal
Sampling algorithm and these three variants are almost same because of the
involved sampling. In addition to this, SC with Pivotal Sampling outperforms
the standard HC algorithm in both accuracy and computational complexity. We
experimentally show that we are up to 45% more accurate than HC in terms of
clustering accuracy. The computational complexity of our algorithm is more than
a magnitude lesser than HC.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0811.0037v2,A complexity dichotomy for hypergraph partition functions,"We consider the complexity of counting homomorphisms from an $r$-uniform
hypergraph $G$ to a symmetric $r$-ary relation $H$. We give a dichotomy theorem
for $r>2$, showing for which $H$ this problem is in FP and for which $H$ it is
#P-complete. This generalises a theorem of Dyer and Greenhill (2000) for the
case $r=2$, which corresponds to counting graph homomorphisms. Our dichotomy
theorem extends to the case in which the relation $H$ is weighted, and the goal
is to compute the \emph{partition function}, which is the sum of weights of the
homomorphisms. This problem is motivated by statistical physics, where it
arises as computing the partition function for particle models in which certain
combinations of $r$ sites interact symmetrically. In the weighted case, our
dichotomy theorem generalises a result of Bulatov and Grohe (2005) for graphs,
where $r=2$. When $r=2$, the polynomial time cases of the dichotomy correspond
simply to rank-1 weights. Surprisingly, for all $r>2$ the polynomial time cases
of the dichotomy have rather more structure. It turns out that the weights must
be superimposed on a combinatorial structure defined by solutions of an
equation over an Abelian group. Our result also gives a dichotomy for a closely
related constraint satisfaction problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.06748v1,Designing Perfect Simulation Algorithms using Local Correctness,"Consider a randomized algorithm that draws samples exactly from a
distribution using recursion. Such an algorithm is called a perfect simulation,
and here a variety of methods for building this type of algorithm are shown to
derive from the same result: the Fundamental Theorem of Perfect Simulation
(FTPS). The FTPS gives two necessary and sufficient conditions for the output
of a recursive probabilistic algorithm to come exactly from the desired
distribution. First, the algorithm must terminate with probability 1. Second,
the algorithm must be locally correct, which means that if the recursive calls
in the original algorithm are replaced by oracles that draw from the desired
distribution, then this new algorithm can be proven to be correct. While it is
usually straightforward to verify these conditions, they are surprisingly
powerful, giving the correctness of Acceptance/Rejection, Coupling from the
Past, the Randomness Recycler, Read-once CFTP, Partial Rejection Sampling,
Partially Recursive Acceptance Rejection, and various Bernoulli Factories. We
illustrate the use of this algorithm by building a new Bernoulli Factory for
linear functions that is 41\% faster than the previous method.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1310.3423v5,"Sublinear Column-wise Actions of the Matrix Exponential on Social
  Networks","We consider stochastic transition matrices from large social and information
networks. For these matrices, we describe and evaluate three fast methods to
estimate one column of the matrix exponential. The methods are designed to
exploit the properties inherent in social networks, such as a power-law degree
distribution. Using only this property, we prove that one of our algorithms has
a sublinear runtime. We present further experimental evidence showing that all
of them run quickly on social networks with billions of edges and accurately
identify the largest elements of the column.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0610117v1,"Quantifier elimination for the reals with a predicate for the powers of
  two","In 1985, van den Dries showed that the theory of the reals with a predicate
for the integer powers of two admits quantifier elimination in an expanded
language, and is hence decidable. He gave a model-theoretic argument, which
provides no apparent bounds on the complexity of a decision procedure. We
provide a syntactic argument that yields a procedure that is primitive
recursive, although not elementary. In particular, we show that it is possible
to eliminate a single block of existential quantifiers in time $2^0_{O(n)}$,
where $n$ is the length of the input formula and $2_k^x$ denotes $k$-fold
iterated exponentiation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1303.1008v1,Testing Java implementations of algebraic specifications,"In this paper we focus on exploiting a specification and the structures that
satisfy it, to obtain a means of comparing implemented and expected behaviours
and find the origin of faults in implementations. We present an approach to the
creation of tests that are based on those specification-compliant structures,
and to the interpretation of those tests' results leading to the discovery of
the method responsible for an eventual test failure. Results of comparative
experiments with a tool implementing this approach are presented.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1309.1043v1,"Proceedings Machines, Computations and Universality 2013","This volume contains the papers presented at the 6th conference on Machines,
Computations and Universality (MCU 2013). MCU 2013 was held in Zurich,
Switzerland, September 9-11, 2013. The MCU series began in Paris in 1995 and
has since been concerned with gaining a deeper understanding of computation
through the study of models of general purpose computation. This volume
continues in this tradition and includes new simple universal models of
computation, and other results that clarify the relationships between models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.06418v1,A Note on Unbounded Polyhedra Derived from Convex Caps,"The construction of an unbounded polyhedron from a ""jagged'' convex cap is
described, and several of its properties discussed, including its relation to
Alexandrov's ""limit angle.""",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1910.04703v1,"Predictive Simulation: Using Regression and Artificial Neural Networks
  to Negate Latency in Networked Interactive Virtual Reality","Current virtual reality systems are typically limited by performance/cost,
usability (size), or a combination of both. By using a networked client/server
environment, we have solved these limitations for the client. However, in doing
so we have introduced a new problem, namely increased latency. Interactive
networked virtual environments such as games and simulations have existed for
nearly as long as the Internet and have consistently faced latency issues. We
propose a solution for negating the effects of latency for interactive
networked virtual environments with lightweight clients, with respect to the
server being used. The proposed method extrapolates future client states to be
incorporated in the server's updates, which helps to synchronize actions on the
client-side and the results coming from the server. We refer to this approach
as predictive simulation. In addition to describing our method, in this paper,
we look at extrapolation methods because the success of our predictive
simulation method is dependent on strong predictions. We focus on regression
methods and briefly examine the use of artificial neural networks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.08701v1,"Measuring Total Transverse Reference-free Displacements of Railroad
  Bridges using 2 Degrees of Freedom (2DOF): Experimental Validation","Railroad bridge engineers are interested in the displacement of railroad
bridges when the train is crossing the bridge for engineering decision making
of their assets. Measuring displacements under train crossing events is
difficult. If simplified reference-free methods would be accurate and
validated, owners would conduct objective performance assessment of their
bridge inventories under trains. Researchers have developed new sensing
technologies (reference-free) to overcome the limitations of reference
point-based displacement sensors. Reference-free methods use accelerometers to
estimate displacements, by decomposing the total displacement in two parts: a
high-frequency dynamic displacement component, and a low-frequency
pseudo-static displacement component. In the past, researchers have used the
Euler-Bernoulli beam theory formula to estimate the pseudo-static displacement
assuming railroad bridge piles and columns can be simplified as cantilever
beams. However, according to railroad bridge managers, railroad bridges have a
different degree of fixity for each pile of each bent. Displacements can be
estimated assuming a similar degree of fixity for deep foundations, but
inherent errors will affect the accuracy of displacement estimation. This paper
solves this problem expanding the 1 Degree of Freedom (1DOF) solution to a new
2 Degrees of Freedom (2DOF), to collect displacements under trains and enable
cost-effective condition-based information related to bridge safety.
Researchers developed a simplified beam to demonstrate the total displacement
estimation using 2DOF and further conducted experimental results in the
laboratory. The estimated displacement of the 2DOF model is more accurate than
that of the 1DOF model for ten train crossing events. With only one sensor
added to the ground of the pile, this method provides owners with approximately
40% more accurate displacements.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0406053v2,"Approximation Algorithms for Minimum PCR Primer Set Selection with
  Amplification Length and Uniqueness Constraints","A critical problem in the emerging high-throughput genotyping protocols is to
minimize the number of polymerase chain reaction (PCR) primers required to
amplify the single nucleotide polymorphism loci of interest. In this paper we
study PCR primer set selection with amplification length and uniqueness
constraints from both theoretical and practical perspectives. We give a greedy
algorithm that achieves a logarithmic approximation factor for the problem of
minimizing the number of primers subject to a given upperbound on the length of
PCR amplification products. We also give, using randomized rounding, the first
non-trivial approximation algorithm for a version of the problem that requires
unique amplification of each amplification target. Empirical results on
randomly generated testcases as well as testcases extracted from the from the
National Center for Biotechnology Information's genomic databases show that our
algorithms are highly scalable and produce better results compared to previous
heuristics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1304.0829v4,Regular graphs and the spectra of two-variable logic with counting,"The {\em spectrum} of a first-order logic sentence is the set of natural
numbers that are cardinalities of its finite models. In this paper we show that
when restricted to using only two variables, but allowing counting quantifiers,
the spectra of first-order logic sentences are semilinear and hence, closed
under complement. At the heart of our proof are semilinear characterisations
for the existence of regular and biregular graphs, the class of graphs in which
there are a priori bounds on the degrees of the vertices.
  Our proof also provides a simple characterisation of models of two-variable
logic with counting -- that is, up to renaming and extending the relation
names, they are simply a collection of regular and biregular graphs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0305053v1,Developing Open Data Models for Linguistic Field Data,"The UQ Flint Archive houses the field notes and elicitation recordings made
by Elwyn Flint in the 1950's and 1960's during extensive linguistic survey work
across Queensland, Australia.
  The process of digitizing the contents of the UQ Flint Archive provides a
number of interesting challenges in the context of EMELD. Firstly, all of the
linguistic data is for languages which are either endangered or extinct, and as
such forms a valuable ethnographic repository. Secondly, the physical format of
the data is itself in danger of decline, and as such digitization is an
important preservation task in the short to medium term. Thirdly, the adoption
of open standards for the encoding and presentation of text and audio data for
linguistic field data, whilst enabling preservation, represents a new field of
research in itself where best practice has yet to be formalised. Fourthly, the
provision of this linguistic data online as a new data source for future
research introduces concerns of data portability and longevity.
  This paper will outline the origins of the data model, the content creation
components, presentation forms based on the data model, data capture tools and
media conversion components. It will also address some of the larger questions
regarding the digitization and annotation of linguistic field work based on
experience gained through work with the Flint Archive contents.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1111.4766v5,On Strong Graph Partitions and Universal Steiner Trees,"We study the problem of constructing universal Steiner trees for undirected
graphs. Given a graph $G$ and a root node $r$, we seek a single spanning tree
$T$ of minimum {\em stretch}, where the stretch of $T$ is defined to be the
maximum ratio, over all terminal sets $X$, of the cost of the minimal sub-tree
$T_X$ of $T$ that connects $X$ to $r$ to the cost of an optimal Steiner tree
connecting $X$ to $r$ in $G$. Universal Steiner trees (USTs) are important for
data aggregation problems where computing the Steiner tree from scratch for
every input instance of terminals is costly, as for example in low energy
sensor network applications.
  We provide a polynomial time \ust\ construction for general graphs with
$2^{O(\sqrt{\log n})}$-stretch. We also give a polynomial time
$\polylog(n)$-stretch construction for minor-free graphs. One basic building
block of our algorithms is a hierarchy of graph partitions, each of which
guarantees small strong diameter for each cluster and bounded neighbourhood
intersections for each node. We show close connections between the problems of
constructing USTs and building such graph partitions. Our construction of
partition hierarchies for general graphs is based on an iterative cluster
merging procedure, while the one for minor-free graphs is based on a separator
theorem for such graphs and the solution to a cluster aggregation problem that
may be of independent interest even for general graphs. To our knowledge, this
is the first subpolynomial-stretch ($o(n^\epsilon)$ for any $\epsilon > 0$) UST
construction for general graphs, and the first polylogarithmic-stretch UST
construction for minor-free graphs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1711.07872v1,Revisiting Connected Vertex Cover: FPT Algorithms and Lossy Kernels,"The CONNECTED VERTEX COVER problem asks for a vertex cover in a graph that
induces a connected subgraph. The problem is known to be fixed-parameter
tractable (FPT), and is unlikely to have a polynomial sized kernel (under
complexity theoretic assumptions) when parameterized by the solution size. In a
recent paper, Lokshtanov et al.[STOC 2017], have shown an $\alpha$-approximate
kernel for the problem for every $\alpha > 1$, in the framework of approximate
or lossy kernelization. In this work, we exhibit lossy kernels and FPT
algorithms for CONNECTED VERTEX COVER for parameters that are more natural and
functions of the input, and in some cases, smaller than the solution size. The
parameters we consider are the sizes of a split deletion set, clique deletion
set, clique cover, cluster deletion set and chordal deletion set.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1810.13263v1,Exploring Parallel-in-Time Approaches for Eddy Current Problems,"We consider the usage of parallel-in-time algorithms of the Parareal and
multigrid-reduction-in-time (MGRIT) methodologies for the parallel-in-time
solution of the eddy current problem. Via application of these methods to a
two-dimensional model problem for a coaxial cable model, we show that a
significant speedup can be achieved in comparison to sequential time stepping.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.08262v1,"Verification of a lazy cache coherence protocol against a weak memory
  model","In this paper we verify a modern lazy cache coherence protocol, TSO-CC,
against the memory consistency model it was designed for, TSO. We achieve this
by first showing a weak simulation relation between TSO-CC (with a fixed number
of processors) and a novel finite-state operational model which exhibits the
laziness of TSO-CC and satisfies TSO. We then extend this by an existing
parameterisation technique, allowing verification for an unlimited number of
processors. The approach is executed entirely within a model checker, no
external tool is required and very little in-depth knowledge of formal
verification methods is required of the verifier.",0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0803.1555v1,"Privacy Preserving ID3 over Horizontally, Vertically and Grid
  Partitioned Data","We consider privacy preserving decision tree induction via ID3 in the case
where the training data is horizontally or vertically distributed. Furthermore,
we consider the same problem in the case where the data is both horizontally
and vertically distributed, a situation we refer to as grid partitioned data.
We give an algorithm for privacy preserving ID3 over horizontally partitioned
data involving more than two parties. For grid partitioned data, we discuss two
different evaluation methods for preserving privacy ID3, namely, first merging
horizontally and developing vertically or first merging vertically and next
developing horizontally. Next to introducing privacy preserving data mining
over grid-partitioned data, the main contribution of this paper is that we
show, by means of a complexity analysis that the former evaluation method is
the more efficient.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1103.0464v1,"A Weakest Chain Approach to Assessing the Overall Effectiveness of the
  802.11 Wireless Network Security","This study aims to assess wireless network security holistically and attempts
to determine the weakest link among the parts that comprise the 'secure' aspect
of the wireless networks: security protocols, wireless technologies and user
habits. The assessment of security protocols is done by determining the time
taken to break a specific protocol's encryption key, or to pass an access
control by using brute force attack techniques. Passphrase strengths as well as
encryption key strengths ranging from 40 to 256 bits are evaluated. Different
scenarios are planned and created for passphrase generation, using different
character sets and different number of characters. Then each scenario is
evaluated based on the time taken to break that passphrase. At the end of the
study, it is determined that the choice of the passphrase is the weakest part
of the entire 802.11 wireless security system.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.11729v1,"FakeNewsLab: Experimental Study on Biases and Pitfalls Preventing us
  from Distinguishing True from False News","Misinformation posting and spreading in Social Media is ignited by personal
decisions on the truthfulness of news that may cause wide and deep cascades at
a large scale in a fraction of minutes. When individuals are exposed to
information, they usually take a few seconds to decide if the content (or the
source) is reliable, and eventually to share it. Although the opportunity to
verify the rumour is often just one click away, many users fail to make a
correct evaluation. We studied this phenomenon by implementing a web-based
questionnaire that was compiled by 7,298 different volunteers. Participants
were asked to mark 20 news as true or false. Interestingly, false news is
correctly identified more frequently than true news, but showing the full
article instead of just the title, surprisingly, does not increase general
accuracy. Also, displaying the original source of the news may contribute to
mislead the user in some cases, while the wisdom of the crowd can positively
assist individuals' ability to classify correctly. Furthermore, participants
that autonomously opened an additional browser tab while compiling the survey
show higher accuracy than users that did not. This suggests a parallel
fact-checking activity that support users' decisions. Finally, users that
declare themselves as young adults are also those who open a new tab more
often, suggesting more familiarity with the Web.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2106.12085v1,A Graph-based Method for Session-based Recommendations,"We present a graph-based approach for the data management tasks and the
efficient operation of a system for session-based next-item recommendations.
The proposed method can collect data continuously and incrementally from an
ecommerce web site, thus seemingly prepare the necessary data infrastructure
for the recommendation algorithm to operate without any excessive training
phase. Our work aims at developing a recommender method that represents a
balance between data processing and management efficiency requirements and the
effectiveness of the recommendations produced. We use the Neo4j graph database
to implement a prototype of such a system. Furthermore, we use an industry
dataset corresponding to a typical e-commerce session-based scenario, and we
report on experiments using our graph-based approach and other state-of-the-art
machine learning and deep learning methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.07769v1,High-speed KATAN Ciphers on-a-Chip,"Security in embedded systems has become a main requirement in modern
electronic devices. The demand for low-cost and highly secure cryptographic
algorithms is increasingly growing in fields such as mobile telecommunications,
handheld devices, etc. In this paper, we analyze and evaluate the development
of cheap and relatively fast hardware implementations of the KATAN family of
block ciphers. KATAN is a family of six hardware oriented block ciphers. All
KATAN ciphers share an 80-bit key and have 32, 48, or 64-bit blocks. We use
VHDL under Altera Quartus in conjunction with ModelSim to implement and analyze
our hardware designs. The developed designs are mapped onto high-performance
Field Programmable Gate Arrays. We compare our findings with similar hardware
implementations and C software versions of the algorithms. The performance
analysis of the C implementations is done using Intel Vtune Amplifier running
on Dell precision T7500 with its dual quad-core Xeon processor and 24 GB of
RAM. The obtained results show better performance when compared with existing
hardware and software implementations.",0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.00100v1,Automated Neuron Shape Analysis from Electron Microscopy,"Morphology based analysis of cell types has been an area of great interest to
the neuroscience community for several decades. Recently, high resolution
electron microscopy (EM) datasets of the mouse brain have opened up
opportunities for data analysis at a level of detail that was previously
impossible. These datasets are very large in nature and thus, manual analysis
is not a practical solution. Of particular interest are details to the level of
post synaptic structures. This paper proposes a fully automated framework for
analysis of post-synaptic structure based neuron analysis from EM data. The
processing framework involves shape extraction, representation with an
autoencoder, and whole cell modeling and analysis based on shape distributions.
We apply our novel framework on a dataset of 1031 neurons obtained from imaging
a 1mm x 1mm x 40 micrometer volume of the mouse visual cortex and show the
strength of our method in clustering and classification of neuronal shapes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1909.03469v1,Accurate Computation of the Log-Sum-Exp and Softmax Functions,"Evaluating the log-sum-exp function or the softmax function is a key step in
many modern data science algorithms, notably in inference and classification.
Because of the exponentials that these functions contain, the evaluation is
prone to overflow and underflow, especially in low precision arithmetic.
Software implementations commonly use alternative formulas that avoid overflow
and reduce the chance of harmful underflow, employing a shift or another
rewriting. Although mathematically equivalent, these variants behave
differently in floating-point arithmetic. We give rounding error analyses of
different evaluation algorithms and interpret the error bounds using condition
numbers for the functions. We conclude, based on the analysis and numerical
experiments, that the shifted formulas are of similar accuracy to the unshifted
ones and that the shifted softmax formula is typically more accurate than a
division-free variant.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1805.05126v1,"Functional Baby Talk: Analysis of Code Fragments from Novice Haskell
  Programmers","What kinds of mistakes are made by novice Haskell developers, as they learn
about functional programming? Is it possible to analyze these errors in order
to improve the pedagogy of Haskell? In 2016, we delivered a massive open online
course which featured an interactive code evaluation environment. We captured
and analyzed 161K interactions from learners. We report typical novice
developer behavior; for instance, the mean time spent on an interactive
tutorial is around eight minutes. Although our environment was restricted, we
gain some understanding of Haskell novice errors. Parenthesis mismatches,
lexical scoping errors and do block misunderstandings are common. Finally, we
make recommendations about how such beginner code evaluation environments might
be enhanced.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1602.06043v3,"De Morgan Dual Nominal Quantifiers Modelling Private Names in
  Non-Commutative Logic","This paper explores the proof theory necessary for recommending an expressive
but decidable first-order system, named MAV1, featuring a de Morgan dual pair
of nominal quantifiers. These nominal quantifiers called `new' and `wen' are
distinct from the self-dual Gabbay-Pitts and Miller-Tiu nominal quantifiers.
The novelty of these nominal quantifiers is they are polarised in the sense
that `new' distributes over positive operators while `wen' distributes over
negative operators. This greater control of bookkeeping enables private names
to be modelled in processes embedded as formulae in MAV1. The technical
challenge is to establish a cut elimination result, from which essential
properties including the transitivity of implication follow. Since the system
is defined using the calculus of structures, a generalisation of the sequent
calculus, novel techniques are employed. The proof relies on an intricately
designed multiset-based measure of the size of a proof, which is used to guide
a normalisation technique called splitting. The presence of equivariance, which
swaps successive quantifiers, induces complex inter-dependencies between
nominal quantifiers, additive conjunction and multiplicative operators in the
proof of splitting. Every rule is justified by an example demonstrating why the
rule is necessary for soundly embedding processes and ensuring that cut
elimination holds.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.06480v1,"Network-regularized Sparse Logistic Regression Models for Clinical Risk
  Prediction and Biomarker Discovery","Molecular profiling data (e.g., gene expression) has been used for clinical
risk prediction and biomarker discovery. However, it is necessary to integrate
other prior knowledge like biological pathways or gene interaction networks to
improve the predictive ability and biological interpretability of biomarkers.
Here, we first introduce a general regularized Logistic Regression (LR)
framework with regularized term $\lambda \|\bm{w}\|_1 +
\eta\bm{w}^T\bm{M}\bm{w}$, which can reduce to different penalties, including
Lasso, elastic net, and network-regularized terms with different $\bm{M}$. This
framework can be easily solved in a unified manner by a cyclic coordinate
descent algorithm which can avoid inverse matrix operation and accelerate the
computing speed. However, if those estimated $\bm{w}_i$ and $\bm{w}_j$ have
opposite signs, then the traditional network-regularized penalty may not
perform well. To address it, we introduce a novel network-regularized sparse LR
model with a new penalty $\lambda \|\bm{w}\|_1 + \eta|\bm{w}|^T\bm{M}|\bm{w}|$
to consider the difference between the absolute values of the coefficients. And
we develop two efficient algorithms to solve it. Finally, we test our methods
and compare them with the related ones using simulated and real data to show
their efficiency.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.01422v2,The Prolog Debugger and Declarative Programming. Examples,"This paper contains examples for a companion paper ""The Prolog Debugger and
Declarative Programming"", which discusses (in)adequacy of the Prolog debugger
for declarative programming.
  Logic programming is a declarative programming paradigm. Programming language
Prolog makes logic programming possible, at least to a substantial extent.
However the Prolog debugger works solely in terms of the operational semantics.
So it is incompatible with declarative programming. The companion paper tries
to find methods of using it from the declarative point of view. Here we provide
examples of applying them.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.07897v1,Learning Graph Partitions,"Given a partition of a graph into connected components, the membership oracle
asserts whether any two vertices of the graph lie in the same component or not.
We prove that for $n\ge k\ge 2$, learning the components of an $n$-vertex
hidden graph with $k$ components requires at least $\frac{1}{2}(n-k)(k-1)$
membership queries. This proves the optimality of the $O(nk)$ algorithm
proposed by Reyzin and Srivastava (2007) for this problem, improving on the
best known information-theoretic bound of $\Omega(n\log k)$ queries. Further,
we construct an oracle that can learn the number of components of $G$ in
asymptotically fewer queries than learning the full partition, thus answering
another question posed by the same authors. Lastly, we introduce a more
applicable version of this oracle, and prove asymptotically tight bounds of
$\widetilde\Theta(m)$ queries for both learning and verifying an $m$-edge
hidden graph $G$ using this oracle.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.12313v2,Are scene graphs good enough to improve Image Captioning?,"Many top-performing image captioning models rely solely on object features
computed with an object detection model to generate image descriptions.
However, recent studies propose to directly use scene graphs to introduce
information about object relations into captioning, hoping to better describe
interactions between objects. In this work, we thoroughly investigate the use
of scene graphs in image captioning. We empirically study whether using
additional scene graph encoders can lead to better image descriptions and
propose a conditional graph attention network (C-GAT), where the image
captioning decoder state is used to condition the graph updates. Finally, we
determine to what extent noise in the predicted scene graphs influence caption
quality. Overall, we find no significant difference between models that use
scene graph features and models that only use object detection features across
different captioning metrics, which suggests that existing scene graph
generation models are still too noisy to be useful in image captioning.
Moreover, although the quality of predicted scene graphs is very low in
general, when using high quality scene graphs we obtain gains of up to 3.3
CIDEr compared to a strong Bottom-Up Top-Down baseline. We open source code to
reproduce all our experiments in
https://github.com/iacercalixto/butd-image-captioning.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1206.3666v1,Unsupervised adaptation of brain machine interface decoders,"The performance of neural decoders can degrade over time due to
nonstationarities in the relationship between neuronal activity and behavior.
In this case, brain-machine interfaces (BMI) require adaptation of their
decoders to maintain high performance across time. One way to achieve this is
by use of periodical calibration phases, during which the BMI system (or an
external human demonstrator) instructs the user to perform certain movements or
behaviors. This approach has two disadvantages: (i) calibration phases
interrupt the autonomous operation of the BMI and (ii) between two calibration
phases the BMI performance might not be stable but continuously decrease. A
better alternative would be that the BMI decoder is able to continuously adapt
in an unsupervised manner during autonomous BMI operation, i.e. without knowing
the movement intentions of the user.
  In the present article, we present an efficient method for such unsupervised
training of BMI systems for continuous movement control. The proposed method
utilizes a cost function derived from neuronal recordings, which guides a
learning algorithm to evaluate the decoding parameters. We verify the
performance of our adaptive method by simulating a BMI user with an optimal
feedback control model and its interaction with our adaptive BMI decoder. The
simulation results show that the cost function and the algorithm yield fast and
precise trajectories towards targets at random orientations on a 2-dimensional
computer screen. For initially unknown and non-stationary tuning parameters,
our unsupervised method is still able to generate precise trajectories and to
keep its performance stable in the long term. The algorithm can optionally work
also with neuronal error signals instead or in conjunction with the proposed
unsupervised adaptation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.07521v2,"Multi-objective Clustering: A Data-driven Analysis of MOCLE, MOCK and
  $$-MOCK","We present a data-driven analysis of MOCK, $\Delta$-MOCK, and MOCLE. These
are three closely related approaches that use multi-objective optimization for
crisp clustering. More specifically, based on a collection of 12 datasets
presenting different proprieties, we investigate the performance of MOCLE and
MOCK compared to the recently proposed $\Delta$-MOCK. Besides performing a
quantitative analysis identifying which method presents a good/poor performance
with respect to another, we also conduct a more detailed analysis on why such a
behavior happened. Indeed, the results of our analysis provide useful insights
into the strengths and weaknesses of the methods investigated.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.8543v1,QPEL: Quantum Program and Effect Language,"We present the syntax and rules of deduction of QPEL (Quantum Program and
Effect Language), a language for describing both quantum programs, and
properties of quantum programs - effects on the appropriate Hilbert space. We
show how semantics may be given in terms of state-and-effect triangles, a
categorical setting that allows semantics in terms of Hilbert spaces,
C*-algebras, and other categories. We prove soundness and completeness results
that show the derivable judgements are exactly those provable in all
state-and-effect triangles.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.11241v1,RoboKit-MV: an Educational Initiative,"In this paper, we present a robot model and code base for affordable
education in the field of humanoid robotics. We give an overview of the
software and hardware of a robot that won several competitions with the team
RoboKit in 2019-2021, provide analysis of the contemporary market of education
in robotics, and highlight the reasoning beyond certain design solutions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1302.4544v1,Distributed Random Walks,"Performing random walks in networks is a fundamental primitive that has found
applications in many areas of computer science, including distributed
computing. In this paper, we focus on the problem of sampling random walks
efficiently in a distributed network and its applications. Given bandwidth
constraints, the goal is to minimize the number of rounds required to obtain
random walk samples.
  All previous algorithms that compute a random walk sample of length $\ell$ as
a subroutine always do so naively, i.e., in $O(\ell)$ rounds. The main
contribution of this paper is a fast distributed algorithm for performing
random walks. We present a sublinear time distributed algorithm for performing
random walks whose time complexity is sublinear in the length of the walk. Our
algorithm performs a random walk of length $\ell$ in $\tilde{O}(\sqrt{\ell D})$
rounds ($\tilde{O}$ hides $\polylog{n}$ factors where $n$ is the number of
nodes in the network) with high probability on an undirected network, where $D$
is the diameter of the network. For small diameter graphs, this is a
significant improvement over the naive $O(\ell)$ bound. Furthermore, our
algorithm is optimal within a poly-logarithmic factor as there exists a
matching lower bound [Nanongkai et al. PODC 2011]. We further extend our
algorithms to efficiently perform $k$ independent random walks in
$\tilde{O}(\sqrt{k\ell D} + k)$ rounds. We also show that our algorithm can be
applied to speedup the more general Metropolis-Hastings sampling.
  Our random walk algorithms can be used to speed up distributed algorithms in
applications that use random walks as a subroutine, such as computing a random
spanning tree and estimating mixing time and related parameters. Our algorithm
is fully decentralized and can serve as a building block in the design of
topologically-aware networks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.08022v1,Zero-shot Node Classification with Decomposed Graph Prototype Network,"Node classification is a central task in graph data analysis. Scarce or even
no labeled data of emerging classes is a big challenge for existing methods. A
natural question arises: can we classify the nodes from those classes that have
never been seen? In this paper, we study this zero-shot node classification
(ZNC) problem which has a two-stage nature: (1) acquiring high-quality class
semantic descriptions (CSDs) for knowledge transfer, and (2) designing a well
generalized graph-based learning model. For the first stage, we give a novel
quantitative CSDs evaluation strategy based on estimating the real class
relationships, so as to get the ""best"" CSDs in a completely automatic way. For
the second stage, we propose a novel Decomposed Graph Prototype Network (DGPN)
method, following the principles of locality and compositionality for zero-shot
model generalization. Finally, we conduct extensive experiments to demonstrate
the effectiveness of our solutions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1404.4246v1,An Approach to Assertion-based Debugging of Higher-Order (C)LP Programs,"Higher-order constructs extend the expressiveness of first-order (Constraint)
Logic Programming ((C)LP) both syntactically and semantically. At the same time
assertions have been in use for some time in (C)LP systems helping programmers
detect errors and validate programs. However, these assertion-based extensions
to (C)LP have not been integrated well with higher-order to date. This paper
contributes to filling this gap by extending the assertion-based approach to
error detection and program validation to the higher-order context within
(C)LP. We propose an extension of properties and assertions as used in (C)LP in
order to be able to fully describe arguments that are predicates. The extension
makes the full power of the assertion language available when describing
higher-order arguments. We provide syntax and semantics for (higher-order)
properties and assertions, as well as for programs which contain such
assertions, including the notions of error and partial correctness and provide
some formal results. We also discuss several alternatives for performing
run-time checking of such programs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1508.03868v3,"Visual Affect Around the World: A Large-scale Multilingual Visual
  Sentiment Ontology","Every culture and language is unique. Our work expressly focuses on the
uniqueness of culture and language in relation to human affect, specifically
sentiment and emotion semantics, and how they manifest in social multimedia. We
develop sets of sentiment- and emotion-polarized visual concepts by adapting
semantic structures called adjective-noun pairs, originally introduced by Borth
et al. (2013), but in a multilingual context. We propose a new
language-dependent method for automatic discovery of these adjective-noun
constructs. We show how this pipeline can be applied on a social multimedia
platform for the creation of a large-scale multilingual visual sentiment
concept ontology (MVSO). Unlike the flat structure in Borth et al. (2013), our
unified ontology is organized hierarchically by multilingual clusters of
visually detectable nouns and subclusters of emotionally biased versions of
these nouns. In addition, we present an image-based prediction task to show how
generalizable language-specific models are in a multilingual context. A new,
publicly available dataset of >15.6K sentiment-biased visual concepts across 12
languages with language-specific detector banks, >7.36M images and their
metadata is also released.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0707.0762v1,PhantomOS: A Next Generation Grid Operating System,"Grid Computing has made substantial advances in the past decade; these are
primarily due to the adoption of standardized Grid middleware. However Grid
computing has not yet become pervasive because of some barriers that we believe
have been caused by the adoption of middleware centric approaches. These
barriers include: scant support for major types of applications such as
interactive applications; lack of flexible, autonomic and scalable Grid
architectures; lack of plug-and-play Grid computing and, most importantly, no
straightforward way to setup and administer Grids. PhantomOS is a project which
aims to address many of these barriers. Its goal is the creation of a user
friendly pervasive Grid computing platform that facilitates the rapid
deployment and easy maintenance of Grids whilst providing support for major
types of applications on Grids of almost any topology. In this paper we present
the detailed system architecture and an overview of its implementation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0412086v1,"Artificial Ant Colonies in Digital Image Habitats - A Mass Behaviour
  Effect Study on Pattern Recognition","Some recent studies have pointed that, the self-organization of neurons into
brain-like structures, and the self-organization of ants into a swarm are
similar in many respects. If possible to implement, these features could lead
to important developments in pattern recognition systems, where perceptive
capabilities can emerge and evolve from the interaction of many simple local
rules. The principle of the method is inspired by the work of Chialvo and
Millonas who developed the first numerical simulation in which swarm cognitive
map formation could be explained. From this point, an extended model is
presented in order to deal with digital image habitats, in which artificial
ants could be able to react to the environment and perceive it. Evolution of
pheromone fields point that artificial ant colonies could react and adapt
appropriately to any type of digital habitat. KEYWORDS: Swarm Intelligence,
Self-Organization, Stigmergy, Artificial Ant Systems, Pattern Recognition and
Perception, Image Segmentation, Gestalt Perception Theory, Distributed
Computation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1103.4071v1,Efficient Resource Oblivious Algorithms for Multicores,"We consider the design of efficient algorithms for a multicore computing
environment with a global shared memory and p cores, each having a cache of
size M, and with data organized in blocks of size B. We characterize the class
of `Hierarchical Balanced Parallel (HBP)' multithreaded computations for
multicores. HBP computations are similar to the hierarchical divide & conquer
algorithms considered in recent work, but have some additional features that
guarantee good performance even when accounting for the cache misses due to
false sharing. Most of our HBP algorithms are derived from known
cache-oblivious algorithms with high parallelism, however we incorporate new
techniques that reduce the effect of false-sharing.
  Our approach to addressing false sharing costs (or more generally, block
misses) is to ensure that any task that can be stolen shares O(1) blocks with
other tasks. We use a gapping technique for computations that have larger than
O(1) block sharing. We also incorporate the property of limited access writes
analyzed in a companion paper, and we bound the cost of accessing shared blocks
on the execution stacks of tasks.
  We present the Priority Work Stealing (PWS) scheduler, and we establish that,
given a sufficiently `tall' cache, PWS deterministically schedules several
highly parallel HBP algorithms, including those for scans, matrix computations
and FFT, with cache misses bounded by the sequential complexity, when
accounting for both traditional cache misses and for false sharing. We also
present a list ranking algorithm with almost optimal bounds. PWS schedules
without using cache or block size information, and uses knowledge of processors
only to the extent of determining the available locations from which tasks may
be stolen; thus it schedules resource-obliviously.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.02690v2,Theorem Proving and Algebra,"This book can be seen either as a text on theorem proving that uses
techniques from general algebra, or else as a text on general algebra
illustrated and made concrete by practical exercises in theorem proving. The
book considers several different logical systems, including first-order logic,
Horn clause logic, equational logic, and first-order logic with equality.
Similarly, several different proof paradigms are considered. However, we do
emphasize equational logic, and for simplicity we use only the OBJ3 software
system, though it is used in a rather flexible manner. We do not pursue the
lofty goal of mechanizing proofs like those of which mathematicians are justly
so proud; instead, we seek to take steps towards providing mechanical
assistance for proofs that are useful for computer scientists in developing
software and hardware. This more modest goal has the advantage of both being
achievable and having practical benefits.
  The following topics are covered: many-sorted signature, algebra and
homomorphism; term algebra and substitution; equation and satisfaction;
conditional equations; equational deduction and its completeness; deduction for
conditional equations; the theorem of constants; interpretation and equivalence
of theories; term rewriting, termination, confluence and normal form; abstract
rewrite systems; standard models, abstract data types, initiality, and
induction; rewriting and deduction modulo equations; first-order logic, models,
and proof planning; second-order algebra; order-sorted algebra and rewriting;
modules; unification and completion; and hidden algebra. In parallel with these
are a gradual introduction to OBJ3, applications to group theory, various
abstract data types (such as number systems, lists, and stacks), propositional
calculus, hardware verification, the {\lambda}-calculus, correctness of
functional programs, and other topics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0809.0360v2,The Complexity of Enriched Mu-Calculi,"The fully enriched &mu;-calculus is the extension of the propositional
&mu;-calculus with inverse programs, graded modalities, and nominals. While
satisfiability in several expressive fragments of the fully enriched
&mu;-calculus is known to be decidable and ExpTime-complete, it has recently
been proved that the full calculus is undecidable. In this paper, we study the
fragments of the fully enriched &mu;-calculus that are obtained by dropping at
least one of the additional constructs. We show that, in all fragments obtained
in this way, satisfiability is decidable and ExpTime-complete. Thus, we
identify a family of decidable logics that are maximal (and incomparable) in
expressive power. Our results are obtained by introducing two new automata
models, showing that their emptiness problems are ExpTime-complete, and then
reducing satisfiability in the relevant logics to these problems. The automata
models we introduce are two-way graded alternating parity automata over
infinite trees (2GAPTs) and fully enriched automata (FEAs) over infinite
forests. The former are a common generalization of two incomparable automata
models from the literature. The latter extend alternating automata in a similar
way as the fully enriched &mu;-calculus extends the standard &mu;-calculus.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.13041v3,"Intermittent Inverse-Square Lvy Walks are Optimal for Finding Targets
  of All Sizes","L\'evy walks are random walk processes whose step-lengths follow a
long-tailed power-law distribution. Due to their abundance as movement patterns
of biological organisms, significant theoretical efforts have been devoted to
identifying the foraging circumstances that would make such patterns
advantageous. However, despite extensive research, there is currently no
mathematical proof indicating that L\'evy walks are, in any manner, preferable
strategies in higher dimensions than one. Here we prove that in finite
two-dimensional terrains, the inverse-square L\'evy walk strategy is extremely
efficient at finding sparse targets of arbitrary size and shape. Moreover, this
holds even under the weak model of intermittent detection. Conversely, any
other intermittent L\'evy walk fails to efficiently find either large targets
or small ones. Our results shed new light on the \emph{L\'evy foraging
hypothesis}, and are thus expected to impact future experiments on animals
performing L\'evy walks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.10248v1,LP2PB: Translating Answer Set Programs into Pseudo-Boolean Theories,"Answer set programming (ASP) is a well-established knowledge representation
formalism. Most ASP solvers are based on (extensions of) technology from
Boolean satisfiability solving. While these solvers have shown to be very
successful in many practical applications, their strength is limited by their
underlying proof system, resolution. In this paper, we present a new tool LP2PB
that translates ASP programs into pseudo-Boolean theories, for which solvers
based on the (stronger) cutting plane proof system exist. We evaluate our tool,
and the potential of cutting-plane-based solving for ASP on traditional ASP
benchmarks as well as benchmarks from pseudo-Boolean solving. Our results are
mixed: overall, traditional ASP solvers still outperform our translational
approach, but several benchmark families are identified where the balance
shifts the other way, thereby suggesting that further investigation into a
stronger proof system for ASP is valuable.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1104.4978v2,"Approximating the Termination Value of One-Counter MDPs and Stochastic
  Games","One-counter MDPs (OC-MDPs) and one-counter simple stochastic games (OC-SSGs)
are 1-player, and 2-player turn-based zero-sum, stochastic games played on the
transition graph of classic one-counter automata (equivalently, pushdown
automata with a 1-letter stack alphabet). A key objective for the analysis and
verification of these games is the termination objective, where the players aim
to maximize (minimize, respectively) the probability of hitting counter value
0, starting at a given control state and given counter value. Recently, we
studied qualitative decision problems (""is the optimal termination value = 1?"")
for OC-MDPs (and OC-SSGs) and showed them to be decidable in P-time (in NP and
coNP, respectively). However, quantitative decision and approximation problems
(""is the optimal termination value ? p"", or ""approximate the termination value
within epsilon"") are far more challenging. This is so in part because optimal
strategies may not exist, and because even when they do exist they can have a
highly non-trivial structure. It thus remained open even whether any of these
quantitative termination problems are computable. In this paper we show that
all quantitative approximation problems for the termination value for OC-MDPs
and OC-SSGs are computable. Specifically, given a OC-SSG, and given epsilon >
0, we can compute a value v that approximates the value of the OC-SSG
termination game within additive error epsilon, and furthermore we can compute
epsilon-optimal strategies for both players in the game. A key ingredient in
our proofs is a subtle martingale, derived from solving certain LPs that we can
associate with a maximizing OC-MDP. An application of Azuma's inequality on
these martingales yields a computable bound for the ""wealth"" at which a ""rich
person's strategy"" becomes epsilon-optimal for OC-MDPs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1501.03992v1,PSPACE-Completeness of Majority Automata Networks,"We study the dynamics of majority automata networks when the vertices are
updated according to a block sequential updating scheme. In particular, we show
that the complexity of the problem of predicting an eventual state change in
some vertex, given an initial configuration, is PSPACE-complete.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.08077v2,The Problem with XSD Binary Floating Point Datatypes in RDF,"The XSD binary floating point datatypes are regularly used for precise
numeric values in RDF. However, the use of these datatypes for knowledge
representation can systematically impair the quality of data and, compared to
the XSD decimal datatype, increases the probability of data processing
producing false results. We argue why in most cases the XSD decimal datatype is
better suited to represent numeric values in RDF. A survey of the actual usage
of datatypes on the relevant subset of the December 2020 Web Data Commons
dataset, containing 19453060341 literals from real web data, substantiates the
practical relevancy of the described problem: 29 %-68 % of binary floating
point values are distorted due to the datatype.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1509.06139v1,"On the number of lambda terms with prescribed size of their De Bruijn
  representation","John Tromp introduced the so-called 'binary lambda calculus' as a way to
encode lambda terms in terms of binary words. Later, Grygiel and Lescanne
conjectured that the number of binary lambda terms with $m$ free indices and of
size $n$ (encoded as binary words of length $n$) is $o(n^{-3/2} \tau^{-n})$ for
$\tau \approx 1.963448\ldots$. We generalize the proposed notion of size and
show that for several classes of lambda terms, including binary lambda terms
with $m$ free indices, the number of terms of size $n$ is $\Theta(n^{-3/2}
\rho^{-n})$ with some class dependent constant $\rho$, which in particular
disproves the above mentioned conjecture. A way to obtain lower and upper
bounds for the constant near the leading term is presented and numerical
results for a few previously introduced classes of lambda terms are given.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1802.05925v1,Energy Optimization of Robotic Cells,"This study focuses on the energy optimization of industrial robotic cells,
which is essential for sustainable production in the long term. A holistic
approach that considers a robotic cell as a whole toward minimizing energy
consumption is proposed. The mathematical model, which takes into account
various robot speeds, positions, power-saving modes, and alternative orders of
operations, can be transformed into a mixed-integer linear programming
formulation that is, however, suitable only for small instances. To optimize
complex robotic cells, a hybrid heuristic accelerated by using multicore
processors and the Gurobi simplex method for piecewise linear convex functions
is implemented. The experimental results showed that the heuristic solved 93 %
of instances with a solution quality close to a proven lower bound. Moreover,
compared with the existing works, which typically address problems with three
to four robots, this study solved real-size problem instances with up to 12
robots and considered more optimization aspects. The proposed algorithms were
also applied on an existing robotic cell in \v{S}koda Auto. The outcomes, based
on simulations and measurements, indicate that, compared with the previous
state (at maximal robot speeds and without deeper power-saving modes), the
energy consumption can be reduced by about 20 % merely by optimizing the robot
speeds and applying power-saving modes. All the software and generated datasets
used in this research are publicly available.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0309018v1,Using Propagation for Solving Complex Arithmetic Constraints,"Solving a system of nonlinear inequalities is an important problem for which
conventional numerical analysis has no satisfactory method. With a
box-consistency algorithm one can compute a cover for the solution set to
arbitrarily close approximation. Because of difficulties in the use of
propagation for complex arithmetic expressions, box consistency is computed
with interval arithmetic. In this paper we present theorems that support a
simple modification of propagation that allows complex arithmetic expressions
to be handled efficiently. The version of box consistency that is obtained in
this way is stronger than when interval arithmetic is used.",0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0710.4499v2,"Remarks on Jurdzinski and Lorys' proof that palindromes are not a
  Church-Rosser language","In 2002 Jurdzinski and Lorys settled a long-standing conjecture that
palindromes are not a Church-Rosser language. Their proof required a
sophisticated theory about computation graphs of 2-stack automata. We present
their proof in terms of 1-tape Turing machines.We also provide an alternative
proof of Buntrock and Otto's result that the set of non-square bitstrings,
which is context-free, is not Church-Rosser.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1706.03357v1,"Generic Axiomatization of Families of Noncrossing Graphs in Dependency
  Parsing","We present a simple encoding for unlabeled noncrossing graphs and show how
its latent counterpart helps us to represent several families of directed and
undirected graphs used in syntactic and semantic parsing of natural language as
context-free languages. The families are separated purely on the basis of
forbidden patterns in latent encoding, eliminating the need to differentiate
the families of non-crossing graphs in inference algorithms: one algorithm
works for all when the search space can be controlled in parser input.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.11380v2,"Combining Weighted Total Variation and Deep Image Prior for natural and
  medical image restoration via ADMM","In the last decades, unsupervised deep learning based methods have caught
researchers attention, since in many real applications, such as medical
imaging, collecting a great amount of training examples is not always feasible.
Moreover, the construction of a good training set is time consuming and hard
because the selected data have to be enough representative for the task. In
this paper, we focus on the Deep Image Prior (DIP) framework and we propose to
combine it with a space-variant Total Variation regularizer with an automatic
estimation of the local regularization parameters. Differently from other
existing approaches, we solve the arising minimization problem via the flexible
Alternating Direction Method of Multipliers (ADMM). Furthermore, we provide a
specific implementation also for the standard isotropic Total Variation. The
promising performances of the proposed approach, in terms of PSNR and SSIM
values, are addressed through several experiments on simulated as well as real
natural and medical corrupted images.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1012.1529v2,"On the approximability and exact algorithms for vector domination and
  related problems in graphs","We consider two graph optimization problems called vector domination and
total vector domination. In vector domination one seeks a small subset S of
vertices of a graph such that any vertex outside S has a prescribed number of
neighbors in S. In total vector domination, the requirement is extended to all
vertices of the graph. We prove that these problems (and several variants
thereof) cannot be approximated to within a factor of clnn, where c is a
suitable constant and n is the number of the vertices, unless P = NP. We also
show that two natural greedy strategies have approximation factors ln D+O(1),
where D is the maximum degree of the input graph. We also provide exact
polynomial time algorithms for several classes of graphs. Our results extend,
improve, and unify several results previously known in the literature.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.00706v2,BRepNet: A topological message passing system for solid models,"Boundary representation (B-rep) models are the standard way 3D shapes are
described in Computer-Aided Design (CAD) applications. They combine lightweight
parametric curves and surfaces with topological information which connects the
geometric entities to describe manifolds. In this paper we introduce BRepNet, a
neural network architecture designed to operate directly on B-rep data
structures, avoiding the need to approximate the model as meshes or point
clouds. BRepNet defines convolutional kernels with respect to oriented coedges
in the data structure. In the neighborhood of each coedge, a small collection
of faces, edges and coedges can be identified and patterns in the feature
vectors from these entities detected by specific learnable parameters. In
addition, to encourage further deep learning research with B-reps, we publish
the Fusion 360 Gallery segmentation dataset. A collection of over 35,000 B-rep
models annotated with information about the modeling operations which created
each face. We demonstrate that BRepNet can segment these models with higher
accuracy than methods working on meshes, and point clouds.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0510034v1,COMODI: On the Graphical User Interface,"We propose a series of features for the graphical user interface (GUI) of the
COmputational MOdule Integrator (COMODI) \cite{Synasc05a}\cite{COMODI}. In view
of the special requirements that a COMODI type of framework for scientific
computing imposes and inspiring from existing solutions that provide advanced
graphical visual programming environments, we identify those elements and
associated behaviors that will have to find their way into the first release of
COMODI.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0310041v1,A Dynamic Programming Algorithm for the Segmentation of Greek Texts,"In this paper we introduce a dynamic programming algorithm to perform linear
text segmentation by global minimization of a segmentation cost function which
consists of: (a) within-segment word similarity and (b) prior information about
segment length. The evaluation of the segmentation accuracy of the algorithm on
a text collection consisting of Greek texts showed that the algorithm achieves
high segmentation accuracy and appears to be very innovating and promissing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0912.3429v2,"Decidability of the interval temporal logic ABBar over the natural
  numbers","In this paper, we focus our attention on the interval temporal logic of the
Allen's relations ""meets"", ""begins"", and ""begun by"" (ABBar for short),
interpreted over natural numbers. We first introduce the logic and we show that
it is expressive enough to model distinctive interval properties,such as
accomplishment conditions, to capture basic modalities of point-based temporal
logic, such as the until operator, and to encode relevant metric constraints.
Then, we prove that the satisfiability problem for ABBar over natural numbers
is decidable by providing a small model theorem based on an original
contraction method. Finally, we prove the EXPSPACE-completeness of the problem",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1902.00340v1,"Decentralized Stochastic Optimization and Gossip Algorithms with
  Compressed Communication","We consider decentralized stochastic optimization with the objective function
(e.g. data samples for machine learning task) being distributed over $n$
machines that can only communicate to their neighbors on a fixed communication
graph. To reduce the communication bottleneck, the nodes compress (e.g.
quantize or sparsify) their model updates. We cover both unbiased and biased
compression operators with quality denoted by $\omega \leq 1$ ($\omega=1$
meaning no compression). We (i) propose a novel gossip-based stochastic
gradient descent algorithm, CHOCO-SGD, that converges at rate
$\mathcal{O}\left(1/(nT) + 1/(T \delta^2 \omega)^2\right)$ for strongly convex
objectives, where $T$ denotes the number of iterations and $\delta$ the
eigengap of the connectivity matrix. Despite compression quality and network
connectivity affecting the higher order terms, the first term in the rate,
$\mathcal{O}(1/(nT))$, is the same as for the centralized baseline with exact
communication. We (ii) present a novel gossip algorithm, CHOCO-GOSSIP, for the
average consensus problem that converges in time
$\mathcal{O}(1/(\delta^2\omega) \log (1/\epsilon))$ for accuracy $\epsilon >
0$. This is (up to our knowledge) the first gossip algorithm that supports
arbitrary compressed messages for $\omega > 0$ and still exhibits linear
convergence. We (iii) show in experiments that both of our algorithms do
outperform the respective state-of-the-art baselines and CHOCO-SGD can reduce
communication by at least two orders of magnitudes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1802.07702v1,ARRIVAL: Next Stop in CLS,"We study the computational complexity of ARRIVAL, a zero-player game on
$n$-vertex switch graphs introduced by Dohrau, G\""{a}rtner, Kohler,
Matou\v{s}ek, and Welzl. They showed that the problem of deciding termination
of this game is contained in $\text{NP} \cap \text{coNP}$. Karthik C. S.
recently introduced a search variant of ARRIVAL and showed that it is in the
complexity class PLS. In this work, we significantly improve the known upper
bounds for both the decision and the search variants of ARRIVAL.
  First, we resolve a question suggested by Dohrau et al. and show that the
decision variant of ARRIVAL is in $\text{UP} \cap \text{coUP}$. Second, we
prove that the search variant of ARRIVAL is contained in CLS. Third, we give a
randomized $\mathcal{O}(1.4143^n)$-time algorithm to solve both variants.
  Our main technical contributions are (a) an efficiently verifiable
characterization of the unique witness for termination of the ARRIVAL game, and
(b) an efficient way of sampling from the state space of the game. We show that
the problem of finding the unique witness is contained in CLS, whereas it was
previously conjectured to be FPSPACE-complete. The efficient sampling procedure
yields the first algorithm for the problem that has expected runtime
$\mathcal{O}(c^n)$ with $c<2$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0908.0482v5,"REESSE1+ . Reward . Proof by Experiment . A New Approach to Proof of P
  != NP","The authors discuss what is provable security in cryptography. Think that
provable security is asymptotic, relative, and dynamic, and only a supplement
to but not a replacement of exact security analysis. Because the conjecture P
!= NP has not been proven yet, and it is possible in terms of the two
incompleteness theorems of Kurt Godel that there is some cryptosystem of which
the security cannot or only ideally be proven in the random oracle model, the
security of a cryptosystem is between provability and unprovability, and any
academic conclusion must be checked and verified with practices or experiments
as much as possible. Extra, a new approach to proof of P != NP is pointed out.
Lastly, a reward is offered for the subexponential time solutions to the three
REESSE1+ problems: MPP, ASPP, and TLP with n >= 80 and lg M >= 80, which may be
regarded as a type of security proof by experiment.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1304.3663v4,"Cooperative localization by dual foot-mounted inertial sensors and
  inter-agent ranging","The implementation challenges of cooperative localization by dual
foot-mounted inertial sensors and inter-agent ranging are discussed and work on
the subject is reviewed. System architecture and sensor fusion are identified
as key challenges. A partially decentralized system architecture based on
step-wise inertial navigation and step-wise dead reckoning is presented. This
architecture is argued to reduce the computational cost and required
communication bandwidth by around two orders of magnitude while only giving
negligible information loss in comparison with a naive centralized
implementation. This makes a joint global state estimation feasible for up to a
platoon-sized group of agents. Furthermore, robust and low-cost sensor fusion
for the considered setup, based on state space transformation and
marginalization, is presented. The transformation and marginalization are used
to give the necessary flexibility for presented sampling based updates for the
inter-agent ranging and ranging free fusion of the two feet of an individual
agent. Finally, characteristics of the suggested implementation are
demonstrated with simulations and a real-time system implementation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.01419v2,Fast Differentially Private Matrix Factorization,"Differentially private collaborative filtering is a challenging task, both in
terms of accuracy and speed. We present a simple algorithm that is provably
differentially private, while offering good performance, using a novel
connection of differential privacy to Bayesian posterior sampling via
Stochastic Gradient Langevin Dynamics. Due to its simplicity the algorithm
lends itself to efficient implementation. By careful systems design and by
exploiting the power law behavior of the data to maximize CPU cache bandwidth
we are able to generate 1024 dimensional models at a rate of 8.5 million
recommendations per second on a single PC.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1906.08148v7,"Approximate multiplication of nearly sparse matrices with decay in a
  fully recursive distributed task-based parallel framework","In this paper we consider parallel implementations of approximate
multiplication of large matrices with exponential decay of elements. Such
matrices arise in computations related to electronic structure calculations and
some other fields of computational science. Commonly, sparsity is introduced by
dropping out small entries (truncation) of input matrices. Another approach,
the sparse approximate multiplication algorithm [M. Challacombe and N. Bock,
arXiv preprint 1011.3534, 2010] performs truncation of sub-matrix products. We
consider these two methods and their combination, i.e. truncation of both input
matrices and sub-matrix products. Implementations done using the Chunks and
Tasks programming model and library [E. H. Rubensson and E. Rudberg, Parallel
Comput., 40:328-343, 2014] are presented and discussed. We show that the
absolute error in the Frobenius norm behaves as $O\left(n^{1/2} \right), n
\longrightarrow \infty $ and $O\left(\tau^{p/2} \right), \tau \longrightarrow
0,\,\, \forall p < 2$ for all three methods, where $n$ is the matrix size and
$\tau$ is the truncation threshold. We compare the methods on a model problem
and show that the combined method outperforms the original two. The methods are
also applied to matrices coming from large chemical systems with $\sim 10^6$
atoms. We show that the combination of the two methods achieves better weak
scaling by reducing the amount of communication by a factor of $\approx 2$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1306.5279v2,"Affect Control Processes: Intelligent Affective Interaction using a
  Partially Observable Markov Decision Process","This paper describes a novel method for building affectively intelligent
human-interactive agents. The method is based on a key sociological insight
that has been developed and extensively verified over the last twenty years,
but has yet to make an impact in artificial intelligence. The insight is that
resource bounded humans will, by default, act to maintain affective
consistency. Humans have culturally shared fundamental affective sentiments
about identities, behaviours, and objects, and they act so that the transient
affective sentiments created during interactions confirm the fundamental
sentiments. Humans seek and create situations that confirm or are consistent
with, and avoid and supress situations that disconfirm or are inconsistent
with, their culturally shared affective sentiments. This ""affect control
principle"" has been shown to be a powerful predictor of human behaviour. In
this paper, we present a probabilistic and decision-theoretic generalisation of
this principle, and we demonstrate how it can be leveraged to build affectively
intelligent artificial agents. The new model, called BayesAct, can maintain
multiple hypotheses about sentiments simultaneously as a probability
distribution, and can make use of an explicit utility function to make
value-directed action choices. This allows the model to generate affectively
intelligent interactions with people by learning about their identity,
predicting their behaviours using the affect control principle, and taking
actions that are simultaneously goal-directed and affect-sensitive. We
demonstrate this generalisation with a set of simulations. We then show how our
model can be used as an emotional ""plug-in"" for artificially intelligent
systems that interact with humans in two different settings: an exam practice
assistant (tutor) and an assistive device for persons with a cognitive
disability.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.09174v1,A Formalisation of Abstract Argumentation in Higher-Order Logic,"We present an approach for representing abstract argumentation frameworks
based on an encoding into classical higher-order logic. This provides a uniform
framework for computer-assisted assessment of abstract argumentation frameworks
using interactive and automated reasoning tools. This enables the formal
analysis and verification of meta-theoretical properties as well as the
flexible generation of extensions and labellings with respect to well-known
argumentation semantics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1402.0851v2,Interval scheduling and colorful independent sets,"Numerous applications in scheduling, such as resource allocation or steel
manufacturing, can be modeled using the NP-hard Independent Set problem (given
an undirected graph and an integer k, find a set of at least k pairwise
non-adjacent vertices). Here, one encounters special graph classes like 2-union
graphs (edge-wise unions of two interval graphs) and strip graphs (edge-wise
unions of an interval graph and a cluster graph), on which Independent Set
remains NP-hard but admits constant-ratio approximations in polynomial time. We
study the parameterized complexity of Independent Set on 2-union graphs and on
subclasses like strip graphs. Our investigations significantly benefit from a
new structural ""compactness"" parameter of interval graphs and novel problem
formulations using vertex-colored interval graphs. Our main contributions are:
  1. We show a complexity dichotomy: restricted to graph classes closed under
induced subgraphs and disjoint unions, Independent Set is polynomial-time
solvable if both input interval graphs are cluster graphs, and is NP-hard
otherwise.
  2. We chart the possibilities and limits of effective polynomial-time
preprocessing (also known as kernelization).
  3. We extend Halld\'orsson and Karlsson (2006)'s fixed-parameter algorithm
for Independent Set on strip graphs parameterized by the structural parameter
""maximum number of live jobs"" to show that the problem (also known as Job
Interval Selection) is fixed-parameter tractable with respect to the parameter
k and generalize their algorithm from strip graphs to 2-union graphs.
Preliminary experiments with random data indicate that Job Interval Selection
with up to fifteen jobs and 5*10^5 intervals can be solved optimally in less
than five minutes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1805.10833v4,Bayesian Learning with Wasserstein Barycenters,"Based on recent developments in optimal transport theory, we propose a novel
model-selection strategy for Bayesian learning. More precisely, the goal of
this paper is to introduce the Wasserstein barycenter of the posterior law on
models, as a Bayesian predictive posterior, alternative to classical choices
such as the maximum a posteriori and the model average Bayesian estimators.
After formulating the general problem of Bayesian model selection in a common,
parameter-free framework, we exhibit conditions granting the existence and
statistical consistency of this estimator, discuss some of its general and
specific properties, and provide insight into its theoretical advantages.
Furthermore, we illustrate how it can be computed using the theoretical
stochastic gradient descent (SGD) algorithm in Wasserstein space introduced in
a companion paper arXiv:2201.04232v2 [math.OC] , and provide a numerical
example for experimental validation of the proposed method.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.11678v2,"Automating Test Case Identification in Java Open Source Projects on
  GitHub","Software testing is one of the very important Quality Assurance (QA)
components. A lot of researchers deal with the testing process in terms of
tester motivation and how tests should or should not be written. However, it is
not known from the recommendations how the tests are written in real projects.
In this paper, the following was investigated: (i) the denotation of the word
""test"" in different natural languages; (ii) whether the number of occurrences
of the word ""test"" correlates with the number of test cases; and (iii) what
testing frameworks are mostly used. The analysis was performed on 38 GitHub
open source repositories thoroughly selected from the set of 4.3M GitHub
projects. We analyzed 20,340 test cases in 803 classes manually and 170k
classes using an automated approach. The results show that: (i) there exists a
weak correlation (r = 0.655) between the number of occurrences of the word
""test"" and the number of test cases in a class; (ii) the proposed algorithm
using static file analysis correctly detected 97% of test cases; (iii) 15% of
the analyzed classes used main() function whose represent regular Java programs
that test the production code without using any third-party framework. The
identification of such tests is very complex due to implementation diversity.
The results may be leveraged to more quickly identify and locate test cases in
a repository, to understand practices in customized testing solutions, and to
mine tests to improve program comprehension in the future.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1710.05732v5,Generating Reflectance Curves from sRGB Triplets,"The color sensation evoked by an object depends on both the spectral power
distribution of the illumination and the reflectance properties of the object
being illuminated. The color sensation can be characterized by three
color-space values, such as XYZ, RGB, HSV, L*a*b*, etc. It is straightforward
to compute the three values given the illuminant and reflectance curves. The
converse process of computing a reflectance curve given the color-space values
and the illuminant is complicated by the fact that an infinite number of
different reflectance curves can give rise to a single set of color-space
values (metamerism). This paper presents five algorithms for generating a
reflectance curve from a specified sRGB triplet, written for a general
audience. The algorithms are designed to generate reflectance curves that are
similar to those found with naturally occurring colored objects. The computed
reflectance curves are compared to a database of thousands of reflectance
curves measured from paints and pigments available both commercially and in
nature, and the similarity is quantified. One particularly useful application
of these algorithms is in the field of computer graphics, where modeling color
transformations sometimes requires wavelength-specific information, such as
when modeling subtractive color mixture.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1810.00132v1,A Framework to Support the Trust Process in News and Social Media,"Current society is heavily influenced by the spread of online information,
containing all sorts of claims, commonly found in news stories, tweets, and
social media postings. Depending on the user, they may be considered ""true"" or
""false"", according to the agent's trust on the claim. In this paper, we discuss
the concept of content trust and trust process, and propose a framework to
describe the trust process, which can support various possible models of
content trust.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2012.02959v2,Finite element modelling of in-stent restenosis,"From the perspective of coronary heart disease, the development of stents has
come significantly far in reducing the associated mortality rate, drug-eluting
stents being the epitome of innovative and effective solutions. Within this
work, the intricate process of in-stent restenosis is modelled considering one
of the significant growth factors and its effect on constituents of the
arterial wall. A multiphysical modelling approach is adopted in this regard.
Experimental investigations from the literature have been used to hypothesize
the governing equations and the corresponding parameters. A staggered solution
strategy is utilised to capture the transport phenomena as well as the growth
and remodeling that follows stent implantation. The model herein developed
serves as a tool to predict in-stent restenosis depending on the endothelial
injury sustained and the protuberance of stents into the lumen of the arteries.
Keywords: in-stent restenosis, smooth mucsle cells, platelet-derived growth
factor, extracellular matrix, growth",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.06071v2,aiSTROM -- A roadmap for developing a successful AI strategy,"A total of 34% of AI research and development projects fails or are
abandoned, according to a recent survey by Rackspace Technology of 1,870
companies. We propose a new strategic framework, aiSTROM, that empowers
managers to create a successful AI strategy based on a thorough literature
review. This provides a unique and integrated approach that guides managers and
lead developers through the various challenges in the implementation process.
In the aiSTROM framework, we start by identifying the top n potential projects
(typically 3-5). For each of those, seven areas of focus are thoroughly
analysed. These areas include creating a data strategy that takes into account
unique cross-departmental machine learning data requirements, security, and
legal requirements. aiSTROM then guides managers to think about how to put
together an interdisciplinary artificial intelligence (AI) implementation team
given the scarcity of AI talent. Once an AI team strategy has been established,
it needs to be positioned within the organization, either cross-departmental or
as a separate division. Other considerations include AI as a service (AIaas),
or outsourcing development. Looking at new technologies, we have to consider
challenges such as bias, legality of black-box-models, and keeping humans in
the loop. Next, like any project, we need value-based key performance
indicators (KPIs) to track and validate the progress. Depending on the
company's risk-strategy, a SWOT analysis (strengths, weaknesses, opportunities,
and threats) can help further classify the shortlisted projects. Finally, we
should make sure that our strategy includes continuous education of employees
to enable a culture of adoption. This unique and comprehensive framework offers
a valuable, literature supported, tool for managers and lead developers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0
http://arxiv.org/abs/1403.3996v1,"JSAI: Designing a Sound, Configurable, and Efficient Static Analyzer for
  JavaScript","We describe JSAI, an abstract interpreter for JavaScript. JSAI uses novel
abstract domains to compute a reduced product of type inference, pointer
analysis, string analysis, integer and boolean constant propagation, and
control-flow analysis. In addition, JSAI allows for analysis control-flow
sensitivity (i.e., context-, path-, and heap-sensitivity) to be modularly
configured without requiring any changes to the analysis implementation. JSAI
is designed to be provably sound with respect to a specific concrete semantics
for JavaScript, which has been extensively tested against existing
production-quality JavaScript implementations.
  We provide a comprehensive evaluation of JSAI's performance and precision
using an extensive benchmark suite. This benchmark suite includes real-world
JavaScript applications, machine-generated JavaScript code via Emscripten, and
browser addons. We use JSAI's configurability to evaluate a large number of
analysis sensitivities (some well-known, some novel) and observe some
surprising results. We believe that JSAI's configurability and its formal
specifications position it as a useful research platform to experiment on novel
sensitivities, abstract domains, and client analyses for JavaScript.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1108.5567v1,"Parsing Combinatory Categorial Grammar with Answer Set Programming:
  Preliminary Report","Combinatory categorial grammar (CCG) is a grammar formalism used for natural
language parsing. CCG assigns structured lexical categories to words and uses a
small set of combinatory rules to combine these categories to parse a sentence.
In this work we propose and implement a new approach to CCG parsing that relies
on a prominent knowledge representation formalism, answer set programming (ASP)
- a declarative programming paradigm. We formulate the task of CCG parsing as a
planning problem and use an ASP computational tool to compute solutions that
correspond to valid parses. Compared to other approaches, there is no need to
implement a specific parsing algorithm using such a declarative method. Our
approach aims at producing all semantically distinct parse trees for a given
sentence. From this goal, normalization and efficiency issues arise, and we
deal with them by combining and extending existing strategies. We have
implemented a CCG parsing tool kit - AspCcgTk - that uses ASP as its main
computational means. The C&C supertagger can be used as a preprocessor within
AspCcgTk, which allows us to achieve wide-coverage natural language parsing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1802.08157v1,"High order time integrators for the simulation of charged particle
  motion in magnetic quadrupoles","Magnetic quadrupoles are essential components of particle accelerators like
the Large Hadron Collider. In order to study numerically the stability of the
particle beam crossing a quadrupole, a large number of particle revolutions in
the accelerator must be simulated, thus leading to the necessity to preserve
numerically invariants of motion over a long time interval and to a substantial
computational cost, mostly related to the repeated evaluation of the magnetic
vector potential. In this paper, in order to reduce this cost, we first
consider a specific gauge transformation that allows to reduce significantly
the number of vector potential evaluations. We then analyze the sensitivity of
the numerical solution to the interpolation procedure required to compute
magnetic vector potential data from gridded precomputed values at the locations
required by high order time integration methods. Finally, we compare several
high order integration techniques, in order to assess their accuracy and
efficiency for these long term simulations. Explicit high order Lie methods are
considered, along with implicit high order symplectic integrators and
conventional explicit Runge Kutta methods. Among symplectic methods, high order
Lie integrators yield optimal results in terms of cost/accuracy ratios, but non
symplectic Runge Kutta methods perform remarkably well even in very long term
simulations. Furthermore, the accuracy of the field reconstruction and
interpolation techniques are shown to be limiting factors for the accuracy of
the particle tracking procedures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1508.04851v1,Analysis of Petri Nets and Transition Systems,"This paper describes a stand-alone, no-frills tool supporting the analysis of
(labelled) place/transition Petri nets and the synthesis of labelled transition
systems into Petri nets. It is implemented as a collection of independent,
dedicated algorithms which have been designed to operate modularly, portably,
extensibly, and efficiently.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0908.4016v1,On Relating Edges in Graphs without Cycles of Length 4,"An edge xy is relating in the graph G if there is an independent set S,
containing neither x nor y, such that S_{x} and S_{y} are both maximal
independent sets in G. It is an NP-complete problem to decide whether an edge
is relating (Brown, Nowakowski, Zverovich, 2007). We show that the problem
remains NP-complete even for graphs without cycles of length 4 and 5. On the
other hand, for graphs without cycles of length 4 and 6, the problem can be
solved in polynomial time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.00169v1,Quantifying the Invisible Labor in Crowd Work,"Crowdsourcing markets provide workers with a centralized place to find paid
work. What may not be obvious at first glance is that, in addition to the work
they do for pay, crowd workers also have to shoulder a variety of unpaid
invisible labor in these markets, which ultimately reduces workers' hourly
wages. Invisible labor includes finding good tasks, messaging requesters, or
managing payments. However, we currently know little about how much time crowd
workers actually spend on invisible labor or how much it costs them
economically. To ensure a fair and equitable future for crowd work, we need to
be certain that workers are being paid fairly for all of the work they do. In
this paper, we conduct a field study to quantify the invisible labor in crowd
work. We build a plugin to record the amount of time that 100 workers on Amazon
Mechanical Turk dedicate to invisible labor while completing 40,903 tasks. If
we ignore the time workers spent on invisible labor, workers' median hourly
wage was $3.76. But, we estimated that crowd workers in our study spent 33% of
their time daily on invisible labor, dropping their median hourly wage to
$2.83. We found that the invisible labor differentially impacts workers
depending on their skill level and workers' demographics. The invisible labor
category that took the most time and that was also the most common revolved
around workers having to manage their payments. The second most time-consuming
invisible labor category involved hyper-vigilance, where workers vigilantly
watched over requesters' profiles for newly posted work or vigilantly searched
for labor. We hope that through our paper, the invisible labor in crowdsourcing
becomes more visible, and our results help to reveal the larger implications of
the continuing invisibility of labor in crowdsourcing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1203.1468v2,Crowdsourcing for Usability Testing,"While usability evaluation is critical to designing usable websites,
traditional usability testing can be both expensive and time consuming. The
advent of crowdsourcing platforms such as Amazon Mechanical Turk and
CrowdFlower offer an intriguing new avenue for performing remote usability
testing with potentially many users, quick turn-around, and significant cost
savings. To investigate the potential of such crowdsourced usability testing,
we conducted two similar (though not completely parallel) usability studies
which evaluated a graduate school's website: one via a traditional usability
lab setting, and the other using crowdsourcing. While we find crowdsourcing
exhibits some notable limitations in comparison to the traditional lab
environment, its applicability and value for usability testing is clearly
evidenced. We discuss both methodological differences for crowdsourced
usability testing, as well as empirical contrasts to results from more
traditional, face-to-face usability testing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0809.0874v1,"Between the Information Economy and Student Recruitment: Present
  Conjuncture and Future Prospects","In university programs and curricula, in general we react to the need to meet
market needs. We respond to market stimulus, or at least try to do so. Consider
now an inverted view. Consider our data and perspectives in university programs
as reflecting and indeed presaging economic trends. In this article I pursue
this line of thinking. I show how various past events fit very well into this
new view. I provide explanation for why some technology trends happened as they
did, and why some current developments are important now.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,0
http://arxiv.org/abs/1809.08044v2,Non-Line-of-Sight Reconstruction using Efficient Transient Rendering,"Being able to see beyond the direct line of sight is an intriguing
prospective and could benefit a wide variety of important applications. Recent
work has demonstrated that time-resolved measurements of indirect diffuse light
contain valuable information for reconstructing shape and reflectance
properties of objects located around a corner. In this paper, we introduce a
novel reconstruction scheme that, by design, produces solutions that are
consistent with state-of-the-art physically-based rendering. Our method
combines an efficient forward model (a custom renderer for time-resolved
three-bounce indirect light transport) with an optimization framework to
reconstruct object geometry in an analysis-by-synthesis sense. We evaluate our
algorithm on a variety of synthetic and experimental input data, and show that
it gracefully handles uncooperative scenes with high levels of noise or
non-diffuse material reflectance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.04113v1,Data Minimization for GDPR Compliance in Machine Learning Models,"The EU General Data Protection Regulation (GDPR) mandates the principle of
data minimization, which requires that only data necessary to fulfill a certain
purpose be collected. However, it can often be difficult to determine the
minimal amount of data required, especially in complex machine learning models
such as neural networks. We present a first-of-a-kind method to reduce the
amount of personal data needed to perform predictions with a machine learning
model, by removing or generalizing some of the input features. Our method makes
use of the knowledge encoded within the model to produce a generalization that
has little to no impact on its accuracy. This enables the creators and users of
machine learning models to acheive data minimization, in a provable manner.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/cs/0402019v1,The Munich Rent Advisor: A Success for Logic Programming on the Internet,"Most cities in Germany regularly publish a booklet called the {\em
Mietspiegel}. It basically contains a verbal description of an expert system.
It allows the calculation of the estimated fair rent for a flat. By hand, one
may need a weekend to do so. With our computerized version, the {\em Munich
Rent Advisor}, the user just fills in a form in a few minutes and the rent is
calculated immediately. We also extended the functionality and applicability of
the {\em Mietspiegel} so that the user need not answer all questions on the
form. The key to computing with partial information using high-level
programming was to use constraint logic programming. We rely on the internet,
and more specifically the World Wide Web, to provide this service to a broad
user group. More than ten thousand people have used our service in the last
three years. This article describes the experiences in implementing and using
the {\em Munich Rent Advisor}. Our results suggests that logic programming with
constraints can be an important ingredient in intelligent internet systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0801.2498v2,"An Application of the Feferman-Vaught Theorem to Automata and Logics
  for<br> Words over an Infinite Alphabet","We show that a special case of the Feferman-Vaught composition theorem gives
rise to a natural notion of automata for finite words over an infinite
alphabet, with good closure and decidability properties, as well as several
logical characterizations. We also consider a slight extension of the
Feferman-Vaught formalism which allows to express more relations between
component values (such as equality), and prove related decidability results.
  From this result we get new classes of decidable logics for words over an
infinite alphabet.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0602004v1,Conjunctive Queries over Trees,"We study the complexity and expressive power of conjunctive queries over
unranked labeled trees represented using a variety of structure relations such
as ``child'', ``descendant'', and ``following'' as well as unary relations for
node labels. We establish a framework for characterizing structures
representing trees for which conjunctive queries can be evaluated efficiently.
Then we completely chart the tractability frontier of the problem and establish
a dichotomy theorem for our axis relations, i.e., we find all subset-maximal
sets of axes for which query evaluation is in polynomial time and show that for
all other cases, query evaluation is NP-complete. All polynomial-time results
are obtained immediately using the proof techniques from our framework.
Finally, we study the expressiveness of conjunctive queries over trees and show
that for each conjunctive query, there is an equivalent acyclic positive query
(i.e., a set of acyclic conjunctive queries), but that in general this query is
not of polynomial size.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0912.1262v1,"Open Access, Intellectual Property, and How Biotechnology Becomes a New
  Software Science","Innovation is slowing greatly in the pharmaceutical sector. It is considered
here how part of the problem is due to overly limiting intellectual property
relations in the sector. On the other hand, computing and software in
particular are characterized by great richness of intellectual property
frameworks. Could the intellectual property ecosystem of computing come to the
aid of the biosciences and life sciences? We look at how the answer might well
be yes, by looking at (i) the extent to which a drug mirrors a software
program, and (ii) what is to be gleaned from trends in research publishing in
the life and biosciences.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0
http://arxiv.org/abs/2011.02615v3,Competitive Data-Structure Dynamization,"Data-structure dynamization is a general approach for making static data
structures dynamic. It is used extensively in geometric settings and in the
guise of so-called merge (or compaction) policies in big-data databases such as
Google Bigtable and LevelDB (our focus). Previous theoretical work is based on
worst-case analyses for uniform inputs -- insertions of one item at a time and
constant read rate. In practice, merge policies must not only handle batch
insertions and varying read/write ratios, they can take advantage of such
non-uniformity to reduce cost on a per-input basis.
  To model this, we initiate the study of data-structure dynamization through
the lens of competitive analysis, via two new online set-cover problems. For
each, the input is a sequence of disjoint sets of weighted items. The sets are
revealed one at a time. The algorithm must respond to each with a set cover
that covers all items revealed so far. It obtains the cover incrementally from
the previous cover by adding one or more sets and optionally removing existing
sets. For each new set the algorithm incurs build cost equal to the weight of
the items in the set. In the first problem the objective is to minimize total
build cost plus total query cost, where the algorithm incurs a query cost at
each time $t$ equal to the current cover size. In the second problem, the
objective is to minimize the build cost while keeping the query cost from
exceeding $k$ (a given parameter) at any time. We give deterministic online
algorithms for both variants, with competitive ratios of $\Theta(\log^* n)$ and
$k$, respectively. The latter ratio is optimal for the second variant.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1605.06764v1,3D Face Tracking and Texture Fusion in the Wild,"We present a fully automatic approach to real-time 3D face reconstruction
from monocular in-the-wild videos. With the use of a cascaded-regressor based
face tracking and a 3D Morphable Face Model shape fitting, we obtain a
semi-dense 3D face shape. We further use the texture information from multiple
frames to build a holistic 3D face representation from the video frames. Our
system is able to capture facial expressions and does not require any
person-specific training. We demonstrate the robustness of our approach on the
challenging 300 Videos in the Wild (300-VW) dataset. Our real-time fitting
framework is available as an open source library at http://4dface.org.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1802.02732v2,The Higher-Order Prover Leo-III (Extended Version),"The automated theorem prover Leo-III for classical higher-order logic with
Henkin semantics and choice is presented. Leo-III is based on extensional
higher-order paramodulation and accepts every common TPTP dialect (FOF, TFF,
THF), including their recent extensions to rank-1 polymorphism (TF1, TH1). In
addition, the prover natively supports almost every normal higher-order modal
logic. Leo-III cooperates with first-order reasoning tools using translations
to many-sorted first-order logic and produces verifiable proof certificates.
The prover is evaluated on heterogeneous benchmark sets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1303.4897v1,Maximum Edge-Disjoint Paths in $k$-sums of Graphs,"We consider the approximability of the maximum edge-disjoint paths problem
(MEDP) in undirected graphs, and in particular, the integrality gap of the
natural multicommodity flow based relaxation for it. The integrality gap is
known to be $\Omega(\sqrt{n})$ even for planar graphs due to a simple
topological obstruction and a major focus, following earlier work, has been
understanding the gap if some constant congestion is allowed.
  In this context, it is natural to ask for which classes of graphs does a
constant-factor constant-congestion property hold. It is easy to deduce that
for given constant bounds on the approximation and congestion, the class of
""nice"" graphs is nor-closed. Is the converse true? Does every proper
minor-closed family of graphs exhibit a constant factor, constant congestion
bound relative to the LP relaxation? We conjecture that the answer is yes.
  One stumbling block has been that such bounds were not known for bounded
treewidth graphs (or even treewidth 3). In this paper we give a polytime
algorithm which takes a fractional routing solution in a graph of bounded
treewidth and is able to integrally route a constant fraction of the LP
solution's value. Note that we do not incur any edge congestion. Previously
this was not known even for series parallel graphs which have treewidth 2. The
algorithm is based on a more general argument that applies to $k$-sums of
graphs in some graph family, as long as the graph family has a constant factor,
constant congestion bound. We then use this to show that such bounds hold for
the class of $k$-sums of bounded genus graphs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.16852v2,"Ginkgo: A Modern Linear Operator Algebra Framework for High Performance
  Computing","In this paper, we present Ginkgo, a modern C++ math library for scientific
high performance computing. While classical linear algebra libraries act on
matrix and vector objects, Ginkgo's design principle abstracts all
functionality as ""linear operators"", motivating the notation of a ""linear
operator algebra library"". Ginkgo's current focus is oriented towards providing
sparse linear algebra functionality for high performance GPU architectures, but
given the library design, this focus can be easily extended to accommodate
other algorithms and hardware architectures. We introduce this sophisticated
software architecture that separates core algorithms from architecture-specific
back ends and provide details on extensibility and sustainability measures. We
also demonstrate Ginkgo's usability by providing examples on how to use its
functionality inside the MFEM and deal.ii finite element ecosystems. Finally,
we offer a practical demonstration of Ginkgo's high performance on
state-of-the-art GPU architectures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1512.07319v1,A Process Algebra for Wireless Mesh Networks,"We propose a process algebra for wireless mesh networks that combines novel
treatments of local broadcast, conditional unicast and data structures. In this
framework, we model the Ad-hoc On-Demand Distance Vector (AODV) routing
protocol and (dis)prove crucial properties such as loop freedom and packet
delivery.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.04232v1,De-homogenization using Convolutional Neural Networks,"This paper presents a deep learning-based de-homogenization method for
structural compliance minimization. By using a convolutional neural network to
parameterize the mapping from a set of lamination parameters on a coarse mesh
to a one-scale design on a fine mesh, we avoid solving the least square
problems associated with traditional de-homogenization approaches and save time
correspondingly. To train the neural network, a two-step custom loss function
has been developed which ensures a periodic output field that follows the local
lamination orientations. A key feature of the proposed method is that the
training is carried out without any use of or reference to the underlying
structural optimization problem, which renders the proposed method robust and
insensitive wrt. domain size, boundary conditions, and loading. A
post-processing procedure utilizing a distance transform on the output field
skeleton is used to project the desired lamination widths onto the output field
while ensuring a predefined minimum length-scale and volume fraction. To
demonstrate that the deep learning approach has excellent generalization
properties, numerical examples are shown for several different load and
boundary conditions. For an appropriate choice of parameters, the
de-homogenized designs perform within $7-25\%$ of the homogenization-based
solution at a fraction of the computational cost. With several options for
further improvements, the scheme may provide the basis for future interactive
high-resolution topology optimization.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.07534v2,Hidden Markov Model-Based Encoding for Time-Correlated IoT Sources,"As the use of Internet of Things (IoT) devices for monitoring purposes
becomes ubiquitous, the efficiency of sensor communication is a major issue for
the modern Internet. Channel coding is less efficient for extremely short
packets, and traditional techniques that rely on source compression require
extensive signaling or pre-existing knowledge of the source dynamics. In this
work, we propose an encoding and decoding scheme that learns source dynamics
online using a Hidden Markov Model (HMM), puncturing a short packet code to
outperform existing compression-based approaches. Our approach shows
significant performance improvements for sources that are highly correlated in
time, with no additional complexity on the sender side.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1605.03047v1,"BigFCM: Fast, Precise and Scalable FCM on Hadoop","Clustering plays an important role in mining big data both as a modeling
technique and a preprocessing step in many data mining process implementations.
Fuzzy clustering provides more flexibility than non-fuzzy methods by allowing
each data record to belong to more than one cluster to some degree. However, a
serious challenge in fuzzy clustering is the lack of scalability. Massive
datasets in emerging fields such as geosciences, biology and networking do
require parallel and distributed computations with high performance to solve
real-world problems. Although some clustering methods are already improved to
execute on big data platforms, but their execution time is highly increased for
large datasets. In this paper, a scalable Fuzzy C-Means (FCM) clustering named
BigFCM is proposed and designed for the Hadoop distributed data platform. Based
on the map-reduce programming model, it exploits several mechanisms including
an efficient caching design to achieve several orders of magnitude reduction in
execution time. Extensive evaluation over multi-gigabyte datasets shows that
BigFCM is scalable while it preserves the quality of clustering.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1708.07624v2,SPARQL as a Foreign Language,"In the last years, the Linked Data Cloud has achieved a size of more than 100
billion facts pertaining to a multitude of domains. However, accessing this
information has been significantly challenging for lay users. Approaches to
problems such as Question Answering on Linked Data and Link Discovery have
notably played a role in increasing information access. These approaches are
often based on handcrafted and/or statistical models derived from data
observation. Recently, Deep Learning architectures based on Neural Networks
called seq2seq have shown to achieve state-of-the-art results at translating
sequences into sequences. In this direction, we propose Neural SPARQL Machines,
end-to-end deep architectures to translate any natural language expression into
sentences encoding SPARQL queries. Our preliminary results, restricted on
selected DBpedia classes, show that Neural SPARQL Machines are a promising
approach for Question Answering on Linked Data, as they can deal with known
problems such as vocabulary mismatch and perform graph pattern composition.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0909.2089v3,Indirect jumps improve instruction sequence performance,"Instruction sequences with direct and indirect jump instructions are as
expressive as instruction sequences with direct jump instructions only. We show
that, in the case where the number of instructions is not bounded, we are faced
with increases of the maximal internal delays of instruction sequences on
execution that are not bounded by a linear function if we strive for acceptable
increases of the lengths of instruction sequences on elimination of indirect
jump instructions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.11772v4,State Complexity Bounds for the Commutative Closure of Group Languages,"In this work we construct an automaton for the commutative closure of a given
regular group language. The number of states of the resulting automaton is
bounded by the number of states of the original automaton, raised to the power
of the alphabet size, times the product of the order of the letters, viewed as
permutations of the state set. This gives the asymptotic state bound
$O((n\exp(\sqrt{n\ln n}))^{|\Sigma|})$, if the original regular language is
accepted by an automaton with $n$ states. Depending on the automaton in
question, we label points of $\mathbb N_0^{|\Sigma|}$ by subsets of states and
introduce unary automata which decompose the thus labelled grid. Based on these
constructions, we give a general regularity condition, which is fulfilled for
group languages.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0105010v1,On Assessing the Complexity of Software Architectures,"This paper proposes some new architectural metrics which are appropriate for
evaluating the architectural attributes of a software system. The main feature
of our approach is to assess the complexity of a software architecture by
analyzing various types of architectural dependences in the architecture.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1703.04523v1,"Applicability of Educational Data Mining in Afghanistan: Opportunities
  and Challenges","The author's own experience as a student and later as a lecturer in
Afghanistan has shown that the methods used in the educational system are not
only flawed, but also do not provide the minimum guidance to students to select
proper course of study before they enter the national university entrance
(Kankor) exam. Thus, it often results in high attrition rates and poor
performance in higher education.
  Based on the studies done in other countries, and by the author of this paper
through online questionnaires distributed to university students in Afghanistan
- it was found that proper procedures and specialized studies in high schools
can help students in selecting their major field of study more systematically.
  Additionally, it has come to be known that there are large amounts of data
available for mining purposes, but the methods that the Ministry of Education
and Ministry of Higher Education use to store and produce their data, only
enable them to achieve simple facts and figures. Furthermore, from the results
it can be concluded that there are potential opportunities for educational data
mining application in the domain of Afghanistan's education systems. Finally,
this study will provide the readers with approaches for using Educational Data
Mining to improve the educational business processes. For instance, predict
proper field of study for high school graduates, or, identify first year
university students who are at high risk of attrition.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/0908.1185v1,Side-channel attack on labeling CAPTCHAs,"We propose a new scheme of attack on the Microsoft's ASIRRA CAPTCHA which
represents a significant shortcut to the intended attacking path, as it is not
based in any advance in the state of the art on the field of image recognition.
After studying the ASIRRA Public Corpus, we conclude that the security margin
as stated by their authors seems to be quite optimistic. Then, we analyze which
of the studied parameters for the image files seems to disclose the most
valuable information for helping in correct classification, arriving at a
surprising discovery. This represents a completely new approach to breaking
CAPTCHAs that can be applied to many of the currently proposed image-labeling
algorithms, and to prove this point we show how to use the very same approach
against the HumanAuth CAPTCHA. Lastly, we investigate some measures that could
be used to secure the ASIRRA and HumanAuth schemes, but conclude no easy
solutions are at hand.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0
http://arxiv.org/abs/cs/0305066v2,The CMS Integration Grid Testbed,"The CMS Integration Grid Testbed (IGT) comprises USCMS Tier-1 and Tier-2
hardware at the following sites: the California Institute of Technology, Fermi
National Accelerator Laboratory, the University of California at San Diego, and
the University of Florida at Gainesville. The IGT runs jobs using the Globus
Toolkit with a DAGMan and Condor-G front end. The virtual organization (VO) is
managed using VO management scripts from the European Data Grid (EDG). Gridwide
monitoring is accomplished using local tools such as Ganglia interfaced into
the Globus Metadata Directory Service (MDS) and the agent based Mona Lisa.
Domain specific software is packaged and installed using the Distrib ution
After Release (DAR) tool of CMS, while middleware under the auspices of the
Virtual Data Toolkit (VDT) is distributed using Pacman. During a continuo us
two month span in Fall of 2002, over 1 million official CMS GEANT based Monte
Carlo events were generated and returned to CERN for analysis while being
demonstrated at SC2002. In this paper, we describe the process that led to one
of the world's first continuously available, functioning grids.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1604.00697v3,"A New Learning Method for Inference Accuracy, Core Occupation, and
  Performance Co-optimization on TrueNorth Chip","IBM TrueNorth chip uses digital spikes to perform neuromorphic computing and
achieves ultrahigh execution parallelism and power efficiency. However, in
TrueNorth chip, low quantization resolution of the synaptic weights and spikes
significantly limits the inference (e.g., classification) accuracy of the
deployed neural network model. Existing workaround, i.e., averaging the results
over multiple copies instantiated in spatial and temporal domains, rapidly
exhausts the hardware resources and slows down the computation. In this work,
we propose a novel learning method on TrueNorth platform that constrains the
random variance of each computation copy and reduces the number of needed
copies. Compared to the existing learning method, our method can achieve up to
68.8% reduction of the required neuro-synaptic cores or 6.5X speedup, with even
slightly improved inference accuracy.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1106.0706v2,"Actor-network procedures: Modeling multi-factor authentication, device
  pairing, social interactions","As computation spreads from computers to networks of computers, and migrates
into cyberspace, it ceases to be globally programmable, but it remains
programmable indirectly: network computations cannot be controlled, but they
can be steered by local constraints on network nodes. The tasks of
""programming"" global behaviors through local constraints belong to the area of
security. The ""program particles"" that assure that a system of local
interactions leads towards some desired global goals are called security
protocols. As computation spreads beyond cyberspace, into physical and social
spaces, new security tasks and problems arise. As networks are extended by
physical sensors and controllers, including the humans, and interlaced with
social networks, the engineering concepts and techniques of computer security
blend with the social processes of security. These new connectors for
computational and social software require a new ""discipline of programming"" of
global behaviors through local constraints. Since the new discipline seems to
be emerging from a combination of established models of security protocols with
older methods of procedural programming, we use the name procedures for these
new connectors, that generalize protocols. In the present paper we propose
actor-networks as a formal model of computation in heterogenous networks of
computers, humans and their devices; and we introduce Procedure Derivation
Logic (PDL) as a framework for reasoning about security in actor-networks. On
the way, we survey the guiding ideas of Protocol Derivation Logic (also PDL)
that evolved through our work in security in last 10 years. Both formalisms are
geared towards graphic reasoning and tool support. We illustrate their workings
by analysing a popular form of two-factor authentication, and a multi-channel
device pairing procedure, devised for this occasion.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0
http://arxiv.org/abs/1710.00668v4,"Parameterized Approximation Schemes for Steiner Trees with Small Number
  of Steiner Vertices","We study the Steiner Tree problem, in which a set of terminal vertices needs
to be connected in the cheapest possible way in an edge-weighted graph. This
problem has been extensively studied from the viewpoint of approximation and
also parametrization. In particular, on one hand Steiner Tree is known to be
APX-hard, and W[2]-hard on the other, if parameterized by the number of
non-terminals (Steiner vertices) in the optimum solution. In contrast to this
we give an efficient parameterized approximation scheme (EPAS), which
circumvents both hardness results. Moreover, our methods imply the existence of
a polynomial size approximate kernelization scheme (PSAKS) for the considered
parameter.
  We further study the parameterized approximability of other variants of
Steiner Tree, such as Directed Steiner Tree and Steiner Forest. For neither of
these an EPAS is likely to exist for the studied parameter: for Steiner Forest
an easy observation shows that the problem is APX-hard, even if the input graph
contains no Steiner vertices. For Directed Steiner Tree we prove that
approximating within any function of the studied parameter is W[1]-hard.
Nevertheless, we show that an EPAS exists for Unweighted Directed Steiner Tree,
but a PSAKS does not. We also prove that there is an EPAS and a PSAKS for
Steiner Forest if in addition to the number of Steiner vertices, the number of
connected components of an optimal solution is considered to be a parameter.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0911.3277v1,Automated Predicate Abstraction for Real-Time Models,"We present a technique designed to automatically compute predicate
abstractions for dense real-timed models represented as networks of timed
automata.
  We use the CIPM algorithm in our previous work which computes new invariants
for timed automata control locations and prunes the model, to compute a
predicate abstraction of the model. We do so by taking information regarding
control locations and their newly computed invariants into account.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.00907v1,"Non-linear Associative-Commutative Many-to-One Pattern Matching with
  Sequence Variables","Pattern matching is a powerful tool which is part of many functional
programming languages as well as computer algebra systems such as Mathematica.
Among the existing systems, Mathematica offers the most expressive pattern
matching. Unfortunately, no open source alternative has comparable pattern
matching capabilities. Notably, these features include support for associative
and/or commutative function symbols and sequence variables. While those
features have individually been subject of previous research, their
comprehensive combination has not yet been investigated. Furthermore, in many
applications, a fixed set of patterns is matched repeatedly against different
subjects. This many-to-one matching can be sped up by exploiting similarities
between patterns. Discrimination nets are the state-of-the-art solution for
many-to-one matching. In this thesis, a generalized discrimination net which
supports the full feature set is presented. All algorithms have been
implemented as an open-source library for Python. In experiments on real world
examples, significant speedups of many-to-one over one-to-one matching have
been observed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1502.05774v2,Low-Cost Learning via Active Data Procurement,"We design mechanisms for online procurement of data held by strategic agents
for machine learning tasks. The challenge is to use past data to actively price
future data and give learning guarantees even when an agent's cost for
revealing her data may depend arbitrarily on the data itself. We achieve this
goal by showing how to convert a large class of no-regret algorithms into
online posted-price and learning mechanisms. Our results in a sense parallel
classic sample complexity guarantees, but with the key resource being money
rather than quantity of data: With a budget constraint $B$, we give robust risk
(predictive error) bounds on the order of $1/\sqrt{B}$. Because we use an
active approach, we can often guarantee to do significantly better by
leveraging correlations between costs and data.
  Our algorithms and analysis go through a model of no-regret learning with $T$
arriving pairs (cost, data) and a budget constraint of $B$. Our regret bounds
for this model are on the order of $T/\sqrt{B}$ and we give lower bounds on the
same order.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.12833v4,"Pay Attention to Evolution: Time Series Forecasting with Deep
  Graph-Evolution Learning","Time-series forecasting is one of the most active research topics in
artificial intelligence. Applications in real-world time series should consider
two factors for achieving reliable predictions: modeling dynamic dependencies
among multiple variables and adjusting the model's intrinsic hyperparameters. A
still open gap in that literature is that statistical and ensemble learning
approaches systematically present lower predictive performance than deep
learning methods. They generally disregard the data sequence aspect entangled
with multivariate data represented in more than one time series. Conversely,
this work presents a novel neural network architecture for time-series
forecasting that combines the power of graph evolution with deep recurrent
learning on distinct data distributions; we named our method Recurrent Graph
Evolution Neural Network (ReGENN). The idea is to infer multiple multivariate
relationships between co-occurring time-series by assuming that the temporal
data depends not only on inner variables and intra-temporal relationships
(i.e., observations from itself) but also on outer variables and inter-temporal
relationships (i.e., observations from other-selves). An extensive set of
experiments was conducted comparing ReGENN with dozens of ensemble methods and
classical statistical ones, showing sound improvement of up to 64.87% over the
competing algorithms. Furthermore, we present an analysis of the intermediate
weights arising from ReGENN, showing that by looking at inter and
intra-temporal relationships simultaneously, time-series forecasting is majorly
improved if paying attention to how multiple multivariate data synchronously
evolve.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1610.01861v2,"Efficient Best-Response Computation for Strategic Network Formation
  under Attack","Inspired by real world examples, e.g. the Internet, researchers have
introduced an abundance of strategic games to study natural phenomena in
networks. Unfortunately, almost all of these games have the conceptual drawback
of being computationally intractable, i.e. computing a best response strategy
or checking if an equilibrium is reached is NP-hard. Thus, a main challenge in
the field is to find tractable realistic network formation models.
  We address this challenge by investigating a very recently introduced model
by Goyal et al. [WINE'16] which focuses on robust networks in the presence of a
strong adversary who attacks (and kills) nodes in the network and lets this
attack spread virus-like to neighboring nodes and their neighbors. Our main
result is to establish that this natural model is one of the few exceptions
which are both realistic and computationally tractable. In particular, we
answer an open question of Goyal et al. by providing an efficient algorithm for
computing a best response strategy, which implies that deciding whether the
game has reached a Nash equilibrium can be done efficiently as well. Our
algorithm essentially solves the problem of computing a minimal connection to a
network which maximizes the reachability while hedging against severe attacks
on the network infrastructure and may thus be of independent interest.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.09313v2,"Efficient, arbitrarily high precision hardware logarithmic arithmetic
  for linear algebra","The logarithmic number system (LNS) is arguably not broadly used due to
exponential circuit overheads for summation tables relative to arithmetic
precision. Methods to reduce this overhead have been proposed, yet still yield
designs with high chip area and power requirements. Use remains limited to
lower precision or high multiply/add ratio cases, while much of linear algebra
(near 1:1 multiply/add ratio) does not qualify. We present a dual-base
approximate logarithmic arithmetic comparable to floating point in use, yet
unlike LNS it is easily fully pipelined, extendable to arbitrary precision with
$O(n^2)$ overhead, and energy efficient at a 1:1 multiply/add ratio. Compared
to float32 or float64 vector inner product with FMA, our design is respectively
2.3x and 4.6x more energy efficient in 7 nm CMOS. It depends on exp and log
evaluation 5.4x and 3.2x more energy efficient, at 0.23x and 0.37x the chip
area for equivalent accuracy versus standard hyperbolic CORDIC using
shift-and-add and approximated ODE integration in the style of Revol and
Yakoubsohn. This technique is a novel design alternative for low power, high
precision hardened linear algebra in computer vision, graphics and machine
learning applications.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1511.01223v1,Modular Responsive Web Design using Element Queries,"Responsive Web Design (RWD) enables web applications to adapt to the
characteristics of different devices such as screen size which is important for
mobile browsing. Today, the only W3C standard to support this adaptability is
CSS media queries. However, using media queries it is impossible to create
applications in a modular way, because responsive elements then always depend
on the global context. Hence, responsive elements can only be reused if the
global context is exactly the same, severely limiting their reusability. This
makes it extremely challenging to develop large responsive applications,
because the lack of true modularity makes certain requirement changes either
impossible or expensive to realize.
  In this paper we extend RWD to also include responsive modules, i.e., modules
that adapt their design based on their local context independently of the
global context. We present the ELQ project which implements our approach. ELQ
is a novel implementation of so-called element queries which generalize media
queries. Importantly, our design conforms to existing web specifications,
enabling adoption on a large scale. ELQ is designed to be heavily extensible
using plugins. Experimental results show speed-ups of the core algorithms of up
to 37x compared to previous approaches.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1601.05020v3,"Low Space External Memory Construction of the Succinct Permuted Longest
  Common Prefix Array","The longest common prefix (LCP) array is a versatile auxiliary data structure
in indexed string matching. It can be used to speed up searching using the
suffix array (SA) and provides an implicit representation of the topology of an
underlying suffix tree. The LCP array of a string of length $n$ can be
represented as an array of length $n$ words, or, in the presence of the SA, as
a bit vector of $2n$ bits plus asymptotically negligible support data
structures. External memory construction algorithms for the LCP array have been
proposed, but those proposed so far have a space requirement of $O(n)$ words
(i.e. $O(n \log n)$ bits) in external memory. This space requirement is in some
practical cases prohibitively expensive. We present an external memory
algorithm for constructing the $2n$ bit version of the LCP array which uses
$O(n \log \sigma)$ bits of additional space in external memory when given a
(compressed) BWT with alphabet size $\sigma$ and a sampled inverse suffix array
at sampling rate $O(\log n)$. This is often a significant space gain in
practice where $\sigma$ is usually much smaller than $n$ or even constant. We
also consider the case of computing succinct LCP arrays for circular strings.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.08255v2,A Probabilistic Framework for Imitating Human Race Driver Behavior,"Understanding and modeling human driver behavior is crucial for advanced
vehicle development. However, unique driving styles, inconsistent behavior, and
complex decision processes render it a challenging task, and existing
approaches often lack variability or robustness. To approach this problem, we
propose Probabilistic Modeling of Driver behavior (ProMoD), a modular framework
which splits the task of driver behavior modeling into multiple modules. A
global target trajectory distribution is learned with Probabilistic Movement
Primitives, clothoids are utilized for local path generation, and the
corresponding choice of actions is performed by a neural network. Experiments
in a simulated car racing setting show considerable advantages in imitation
accuracy and robustness compared to other imitation learning algorithms. The
modular architecture of the proposed framework facilitates straightforward
extensibility in driving line adaptation and sequencing of multiple movement
primitives for future research.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0607019v1,Modelling the Probability Density of Markov Sources,"This paper introduces an objective function that seeks to minimise the
average total number of bits required to encode the joint state of all of the
layers of a Markov source. This type of encoder may be applied to the problem
of optimising the bottom-up (recognition model) and top-down (generative model)
connections in a multilayer neural network, and it unifies several previous
results on the optimisation of multilayer neural networks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0506048v1,Enriching a Text by Semantic Disambiguation for Information Extraction,"External linguistic resources have been used for a very long time in
information extraction. These methods enrich a document with data that are
semantically equivalent, in order to improve recall. For instance, some of
these methods use synonym dictionaries. These dictionaries enrich a sentence
with words that have a similar meaning. However, these methods present some
serious drawbacks, since words are usually synonyms only in restricted
contexts. The method we propose here consists of using word sense
disambiguation rules (WSD) to restrict the selection of synonyms to only these
that match a specific syntactico-semantic context. We show how WSD rules are
built and how information extraction techniques can benefit from the
application of these rules.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0912.2047v1,"Efficient Gaussian Elimination on a 2D SIMD Array of Processors without
  Column Broadcasts","This paper presents an efficient method for implementing the Gaussian
elimination technique for an nxm (m>=n) matrix, using a 2D SIMD array of nxm
processors. The described algorithm consists of 2xn-1=O(n) iterations, which
provides an optimal speed-up over the serial version. A particularity of the
algorithm is that it only requires broadcasts on the rows of the processor
matrix and not on its columns. The paper also presents several extensions and
applications of the Gaussian elimination algorithm.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1511.02801v1,Parameterized complexity of length-bounded cuts and multi-cuts,"We show that the Minimal Length-Bounded L-But problem can be computed in
linear time with respect to L and the tree-width of the input graph as
parameters. In this problem the task is to find a set of edges of a graph such
that after removal of this set, the shortest path between two prescribed
vertices is at least L long. We derive an FPT algorithm for a more general
multi-commodity length bounded cut problem when parameterized by the number of
terminals also.
  For the former problem we show a W[1]-hardness result when the
parameterization is done by the path-width only (instead of the tree-width) and
that this problem does not admit polynomial kernel when parameterized by
tree-width and L. We also derive an FPT algorithm for the Minimal
Length-Bounded Cut problem when parameterized by the tree-depth. Thus showing
an interesting paradigm for this problem and parameters tree-depth and
path-width.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1207.7253v1,"Learning a peptide-protein binding affinity predictor with kernel ridge
  regression","We propose a specialized string kernel for small bio-molecules, peptides and
pseudo-sequences of binding interfaces. The kernel incorporates
physico-chemical properties of amino acids and elegantly generalize eight
kernels, such as the Oligo, the Weighted Degree, the Blended Spectrum, and the
Radial Basis Function. We provide a low complexity dynamic programming
algorithm for the exact computation of the kernel and a linear time algorithm
for it's approximation. Combined with kernel ridge regression and SupCK, a
novel binding pocket kernel, the proposed kernel yields biologically relevant
and good prediction accuracy on the PepX database. For the first time, a
machine learning predictor is capable of accurately predicting the binding
affinity of any peptide to any protein. The method was also applied to both
single-target and pan-specific Major Histocompatibility Complex class II
benchmark datasets and three Quantitative Structure Affinity Model benchmark
datasets.
  On all benchmarks, our method significantly (p-value < 0.057) outperforms the
current state-of-the-art methods at predicting peptide-protein binding
affinities. The proposed approach is flexible and can be applied to predict any
quantitative biological activity. The method should be of value to a large
segment of the research community with the potential to accelerate
peptide-based drug and vaccine development.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.08603v1,"Is Curiosity All You Need? On the Utility of Emergent Behaviours from
  Curious Exploration","Curiosity-based reward schemes can present powerful exploration mechanisms
which facilitate the discovery of solutions for complex, sparse or long-horizon
tasks. However, as the agent learns to reach previously unexplored spaces and
the objective adapts to reward new areas, many behaviours emerge only to
disappear due to being overwritten by the constantly shifting objective. We
argue that merely using curiosity for fast environment exploration or as a
bonus reward for a specific task does not harness the full potential of this
technique and misses useful skills. Instead, we propose to shift the focus
towards retaining the behaviours which emerge during curiosity-based learning.
We posit that these self-discovered behaviours serve as valuable skills in an
agent's repertoire to solve related tasks. Our experiments demonstrate the
continuous shift in behaviour throughout training and the benefits of a simple
policy snapshot method to reuse discovered behaviour for transfer tasks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1602.01358v1,Towards Scalable Synthesis of Stochastic Control Systems,"Formal control synthesis approaches over stochastic systems have received
significant attention in the past few years, in view of their ability to
provide provably correct controllers for complex logical specifications in an
automated fashion. Examples of complex specifications of interest include
properties expressed as formulae in linear temporal logic (LTL) or as automata
on infinite strings. A general methodology to synthesize controllers for such
properties resorts to symbolic abstractions of the given stochastic systems.
Symbolic models are discrete abstractions of the given concrete systems with
the property that a controller designed on the abstraction can be refined (or
implemented) into a controller on the original system. Although the recent
development of techniques for the construction of symbolic models has been
quite encouraging, the general goal of formal synthesis over stochastic control
systems is by no means solved. A fundamental issue with the existing techniques
is the known ""curse of dimensionality,"" which is due to the need to discretize
state and input sets and that results in an exponential complexity over the
number of state and input variables in the concrete system. In this work we
propose a novel abstraction technique for incrementally stable stochastic
control systems, which does not require state-space discretization but only
input set discretization, and that can be potentially more efficient (and thus
scalable) than existing approaches. We elucidate the effectiveness of the
proposed approach by synthesizing a schedule for the coordination of two
traffic lights under some safety and fairness requirements for a road traffic
model. Further we argue that this 5-dimensional linear stochastic control
system cannot be studied with existing approaches based on state-space
discretization due to the very large number of generated discrete states.",0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0902.2104v2,"Tableau-based decision procedure for full coalitional multiagent
  temporal-epistemic logic of linear time","We develop a tableau-based decision procedure for the full coalitional
multiagent temporal-epistemic logic of linear time CMATEL(CD+LT). It extends
LTL with operators of common and distributed knowledge for all coalitions of
agents. The tableau procedure runs in exponential time, matching the lower
bound obtained by Halpern and Vardi for a fragment of our logic, thus providing
a complexity-optimal decision procedure for CMATEL(CD+LT).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0405002v2,"Splitting an operator: Algebraic modularity results for logics with
  fixpoint semantics","It is well known that, under certain conditions, it is possible to split
logic programs under stable model semantics, i.e. to divide such a program into
a number of different ""levels"", such that the models of the entire program can
be constructed by incrementally constructing models for each level. Similar
results exist for other non-monotonic formalisms, such as auto-epistemic logic
and default logic. In this work, we present a general, algebraicsplitting
theory for logics with a fixpoint semantics. Together with the framework of
approximation theory, a general fixpoint theory for arbitrary operators, this
gives us a uniform and powerful way of deriving splitting results for each
logic with a fixpoint semantics. We demonstrate the usefulness of these
results, by generalizing existing results for logic programming, auto-epistemic
logic and default logic.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.04069v2,Convolutional neural net face recognition works in non-human-like ways,"Convolutional neural networks (CNNs) give state of the art performance in
many pattern recognition problems but can be fooled by carefully crafted
patterns of noise. We report that CNN face recognition systems also make
surprising ""errors"". We tested six commercial face recognition CNNs and found
that they outperform typical human participants on standard face matching
tasks. However, they also declare matches that humans would not, where one
image from the pair has been transformed to look a different sex or race. This
is not due to poor performance; the best CNNs perform almost perfectly on the
human face matching tasks, but also declare the most matches for faces of a
different apparent race or sex. Although differing on the salience of sex and
race, humans and computer systems are not working in completely different ways.
They tend to find the same pairs of images difficult, suggesting some agreement
about the underlying similarity space.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.13745v1,"HazeDose: Design and Analysis of a Personal Air Pollution Inhaled Dose
  Estimation System using Wearable Sensors","Nowadays air pollution becomes one of the biggest world issues in both
developing and developed countries. Helping individuals understand their air
pollution exposure and health risks, the traditional way is to utilize data
from static monitoring stations and estimate air pollution qualities in a large
area by government agencies. Data from such sensing system is very sparse and
cannot reflect real personal exposure. In recent years, several research groups
have developed participatory air pollution sensing systems which use wearable
or portable units coupled with smartphones to crowd-source urban air pollution
data. These systems have shown remarkable improvement in spatial granularity
over government-operated fixed monitoring systems. In this paper, we extend the
paradigm to HazeDose system, which can personalize the individuals' air
pollution exposure. Specifically, we combine the pollution concentrations
obtained from an air pollution estimation system with the activity data from
the individual's on-body activity monitors to estimate the personal inhalation
dosage of air pollution. Users can visualize their personalized air pollution
exposure information via a mobile application. We show that different
activities, such as walking, cycling, or driving, impact their dosage, and
commuting patterns contribute to a significant proportion of an individual's
daily air pollution dosage. Moreover, we propose a dosage minimization
algorithm, with the trial results showing that up to 14.1% of a biker's daily
exposure can be reduced while using alternative routes the driver can inhale
25.9% less than usual. One heuristic algorithm is also introduced to balance
the execution time and dosage reduction for alternative routes scenarios. The
results show that up to 20.3% dosage reduction can be achieved when the
execution time is almost one seventieth of the original one.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1911.06594v1,"Integrating Threat Modeling and Automated Test Case Generation into
  Industrialized Software Security Testing","Industrial Internet of Things (IIoT) application provide a whole new set of
possibilities to drive efficiency of industrial production forward. However,
with the higher degree of integration among systems, comes a plethora of
newthreats to the latter, as they are not yet designed to be broadly reachable
and interoperable. To mitigate these vast amount of new threats, systematic and
automated test methods are necessary. This comprehensiveness can be achieved by
thorough threat modeling. In order to automate security test, we present an
approach to automate the testing process from threat modeling onward, closing
the gap between threat modeling and automated test case generation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1409.0703v4,On computable abstractions (a conceptual introduction),"This paper introduces abstractions that are meaningful for computers and that
can be built and used according to computers' own criteria, i.e., computable
abstractions. It is analyzed how abstractions can be seen to serve as the
building blocks for the creation of one own's understanding of things, which is
essential in performing intellectual tasks. Thus, abstractional machines are
defined, which following a mechanical process can, based on computable
abstractions, build and use their own understanding of things. Abstractional
machines are illustrated through an example that outlines their application to
the task of natural language processing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.08209v4,Minimising quantifier variance under prior probability shift,"For the binary prevalence quantification problem under prior probability
shift, we determine the asymptotic variance of the maximum likelihood
estimator. We find that it is a function of the Brier score for the regression
of the class label on the features under the test data set distribution. This
observation suggests that optimising the accuracy of a base classifier, as
measured by the Brier score, on the training data set helps to reduce the
variance of the related quantifier on the test data set. Therefore, we also
point out training criteria for the base classifier that imply optimisation of
both of the Brier scores on the training and the test data sets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1804.00108v1,Learning tensors from partial binary measurements,"In this paper we generalize the 1-bit matrix completion problem to higher
order tensors. We prove that when $r=O(1)$ a bounded rank-$r$, order-$d$ tensor
$T$ in $\mathbb{R}^{N} \times \mathbb{R}^{N} \times \cdots \times
\mathbb{R}^{N}$ can be estimated efficiently by only $m=O(Nd)$ binary
measurements by regularizing its max-qnorm and M-norm as surrogates for its
rank. We prove that similar to the matrix case, i.e., when $d=2$, the sample
complexity of recovering a low-rank tensor from 1-bit measurements of a subset
of its entries is the same as recovering it from unquantized measurements.
Moreover, we show the advantage of using 1-bit tensor completion over
matricization both theoretically and numerically. Specifically, we show how the
1-bit measurement model can be used for context-aware recommender systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1104.0752v1,"Modeling network technology deployment rates with different network
  models","To understand the factors that encourage the deployment of a new networking
technology, we must be able to model how such technology gets deployed. We
investigate how network structure influences deployment with a simple
deployment model and different network models through computer simulations. The
results indicate that a realistic model of networking technology deployment
should take network structure into account.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1404.4478v1,Rainbow Colouring of Split Graphs,"A rainbow path in an edge coloured graph is a path in which no two edges are
coloured the same. A rainbow colouring of a connected graph G is a colouring of
the edges of G such that every pair of vertices in G is connected by at least
one rainbow path. The minimum number of colours required to rainbow colour G is
called its rainbow connection number. Between them, Chakraborty et al. [J.
Comb. Optim., 2011] and Ananth et al. [FSTTCS, 2012] have shown that for every
integer k, k \geq 2, it is NP-complete to decide whether a given graph can be
rainbow coloured using k colours.
  A split graph is a graph whose vertex set can be partitioned into a clique
and an independent set. Chandran and Rajendraprasad have shown that the problem
of deciding whether a given split graph G can be rainbow coloured using 3
colours is NP-complete and further have described a linear time algorithm to
rainbow colour any split graph using at most one colour more than the optimum
[COCOON, 2012]. In this article, we settle the computational complexity of the
problem on split graphs and thereby discover an interesting dichotomy.
Specifically, we show that the problem of deciding whether a given split graph
can be rainbow coloured using k colours is NP-complete for k \in {2,3}, but can
be solved in polynomial time for all other values of k.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1201.0874v1,Applicative Bisimulations for Delimited-Control Operators,"We develop a behavioral theory for the untyped call-by-value lambda calculus
extended with the delimited-control operators shift and reset. For this
calculus, we discuss the possible observable behaviors and we define an
applicative bisimilarity that characterizes contextual equivalence. We then
compare the applicative bisimilarity and the CPS equivalence, a relation on
terms often used in studies of control operators. In the process, we illustrate
how bisimilarity can be used to prove equivalence of terms with
delimited-control effects.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1302.6335v1,"Convergence in Infinitary Term Graph Rewriting Systems is Simple
  (Extended Abstract)","In this extended abstract, we present a simple approach to convergence on
term graphs that allows us to unify term graph rewriting and infinitary term
rewriting. This approach is based on a partial order and a metric on term
graphs. These structures arise as straightforward generalisations of the
corresponding structures used in infinitary term rewriting. We compare our
simple approach to a more complicated approach that we developed earlier and
show that this new approach is superior in many ways. The only unfavourable
property that we were able to identify, viz. failure of full correspondence
between weak metric and partial order convergence, is rectified by adopting a
strong convergence discipline.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1212.3034v1,Multi-target tracking algorithms in 3D,"Ladars provide a unique capability for identification of objects and motions
in scenes with fixed 3D field of view (FOV). This paper describes algorithms
for multi-target tracking in 3D scenes including the preprocessing
(mathematical morphology and Parzen windows), labeling of connected components,
sorting of targets by selectable attributes (size, length of track, velocity),
and handling of target states (acquired, coasting, re-acquired and tracked) in
order to assemble the target trajectories. This paper is derived from working
algorithms coded in Matlab, which were tested and reviewed by others, and does
not speculate about usage of general formulas or frameworks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0507030v2,Strictly convex drawings of planar graphs,"Every three-connected planar graph with n vertices has a drawing on an O(n^2)
x O(n^2) grid in which all faces are strictly convex polygons. These drawings
are obtained by perturbing (not strictly) convex drawings on O(n) x O(n) grids.
More generally, a strictly convex drawing exists on a grid of size O(W) x
O(n^4/W), for any choice of a parameter W in the range n<W<n^2. Tighter bounds
are obtained when the faces have fewer sides.
  In the proof, we derive an explicit lower bound on the number of primitive
vectors in a triangle.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1905.03587v1,The multifractal load balancing method,"The load-balancing system, built on the basis of a subsystem load balancer
and subsystem control and monitoring that closely interact with each other was
propose in work. This system is presented as a queuing system with priority
service discipline. In the described queuing system parallel processing flow
applications occur in the multiple serving devices and successive junction of
them into unified stream is done. The method of multifractal load balancing is
submited on the basis of the developed system of load balancing.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1801.05202v1,"Lower bounds for Combinatorial Algorithms for Boolean Matrix
  Multiplication","In this paper we propose models of combinatorial algorithms for the Boolean
Matrix Multiplication (BMM), and prove lower bounds on computing BMM in these
models. First, we give a relatively relaxed combinatorial model which is an
extension of the model by Angluin (1976), and we prove that the time required
by any algorithm for the BMM is at least $\Omega(n^3 / 2^{O( \sqrt{ \log n
})})$. Subsequently, we propose a more general model capable of simulating the
""Four Russians Algorithm"". We prove a lower bound of $\Omega(n^{7/3} /
2^{O(\sqrt{ \log n })})$ for the BMM under this model. We use a special class
of graphs, called $(r,t)$-graphs, originally discovered by Rusza and Szemeredi
(1978), along with randomization, to construct matrices that are hard instances
for our combinatorial models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.11969v1,Rethinking Crowd Sourcing for Semantic Similarity,"Estimation of semantic similarity is crucial for a variety of natural
language processing (NLP) tasks. In the absence of a general theory of semantic
information, many papers rely on human annotators as the source of ground truth
for semantic similarity estimation. This paper investigates the ambiguities
inherent in crowd-sourced semantic labeling. It shows that annotators that
treat semantic similarity as a binary category (two sentences are either
similar or not similar and there is no middle ground) play the most important
role in the labeling. The paper offers heuristics to filter out unreliable
annotators and stimulates further discussions on human perception of semantic
similarity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1808.08652v1,"Unique Solutions of Contractions, CCS, and their HOL Formalisation","The unique solution of contractions is a proof technique for bisimilarity
that overcomes certain syntactic constraints of Milner's ""unique solution of
equations"" technique. The paper presents an overview of a rather comprehensive
formalisation of the core of the theory of CCS in the HOL theorem prover
(HOL4), with a focus towards the theory of unique solutions of contractions.
(The formalisation consists of about 20,000 lines of proof scripts in Standard
ML.) Some refinements of the theory itself are obtained. In particular we
remove the constraints on summation, which must be weakly-guarded, by moving to
rooted contraction, that is, the coarsest precongruence contained in the
contraction preorder.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1506.06671v1,"Beyond Triangles: A Distributed Framework for Estimating 3-profiles of
  Large Graphs","We study the problem of approximating the $3$-profile of a large graph.
$3$-profiles are generalizations of triangle counts that specify the number of
times a small graph appears as an induced subgraph of a large graph. Our
algorithm uses the novel concept of $3$-profile sparsifiers: sparse graphs that
can be used to approximate the full $3$-profile counts for a given large graph.
Further, we study the problem of estimating local and ego $3$-profiles, two
graph quantities that characterize the local neighborhood of each vertex of a
graph.
  Our algorithm is distributed and operates as a vertex program over the
GraphLab PowerGraph framework. We introduce the concept of edge pivoting which
allows us to collect $2$-hop information without maintaining an explicit
$2$-hop neighborhood list at each vertex. This enables the computation of all
the local $3$-profiles in parallel with minimal communication.
  We test out implementation in several experiments scaling up to $640$ cores
on Amazon EC2. We find that our algorithm can estimate the $3$-profile of a
graph in approximately the same time as triangle counting. For the harder
problem of ego $3$-profiles, we introduce an algorithm that can estimate
profiles of hundreds of thousands of vertices in parallel, in the timescale of
minutes.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0601134v4,Combining decision procedures for the reals,"<p>We address the general problem of determining the validity of boolean
combinations of equalities and inequalities between real-valued expressions. In
particular, we consider methods of establishing such assertions using only
restricted forms of distributivity. At the same time, we explore ways in which
""local"" decision or heuristic procedures for fragments of the theory of the
reals can be amalgamated into global ones. </p> <p>Let <em>Tadd[Q]</em> be the
first-order theory of the real numbers in the language of ordered groups, with
negation, a constant <em>1</em>, and function symbols for multiplication by
rational constants. Let <em>Tmult[Q]</em> be the analogous theory for the
multiplicative structure, and let <em>T[Q]</em> be the union of the two. We
show that although <em>T[Q]</em> is undecidable, the universal fragment of
<em>T[Q]</em> is decidable. We also show that terms of <em>T[Q]</em>can
fruitfully be put in a normal form. We prove analogous results for theories in
which <em>Q</em> is replaced, more generally, by suitable subfields <em>F</em>
of the reals. Finally, we consider practical methods of establishing
quantifier-free validities that approximate our (impractical) decidability
results.</p>",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0508007v4,Regularity of Position Sequences,"A person is given a numbered sequence of positions on a sheet of paper. The
person is asked, ""Which will be the next (or the next after that) position?""
Everyone has an opinion as to how he or she would proceed. There are regular
sequences for which there is general agreement on how to continue. However,
there are less regular sequences for which this assessment is less certain.
There are sequences for which every continuation is perceived to be arbitrary.
I would like to present a mathematical model that reflects these opinions and
perceptions with the aid of a valuation function. It is necessary to apply a
rich set of invariant features of position sequences to ensure the quality of
this model. All other properties of the model are arbitrary.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9906007v1,MSO definable string transductions and two-way finite state transducers,"String transductions that are definable in monadic second-order (mso) logic
(without the use of parameters) are exactly those realized by deterministic
two-way finite state transducers. Nondeterministic mso definable string
transductions (i.e., those definable with the use of parameters) correspond to
compositions of two nondeterministic two-way finite state transducers that have
the finite visit property. Both families of mso definable string transductions
are characterized in terms of Hennie machines, i.e., two-way finite state
transducers with the finite visit property that are allowed to rewrite their
input tape.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0803.2212v2,Conditioning Probabilistic Databases,"Past research on probabilistic databases has studied the problem of answering
queries on a static database. Application scenarios of probabilistic databases
however often involve the conditioning of a database using additional
information in the form of new evidence. The conditioning problem is thus to
transform a probabilistic database of priors into a posterior probabilistic
database which is materialized for subsequent query processing or further
refinement. It turns out that the conditioning problem is closely related to
the problem of computing exact tuple confidence values.
  It is known that exact confidence computation is an NP-hard problem. This has
led researchers to consider approximation techniques for confidence
computation. However, neither conditioning nor exact confidence computation can
be solved using such techniques.
  In this paper we present efficient techniques for both problems. We study
several problem decomposition methods and heuristics that are based on the most
successful search techniques from constraint satisfaction, such as the
Davis-Putnam algorithm. We complement this with a thorough experimental
evaluation of the algorithms proposed. Our experiments show that our exact
algorithms scale well to realistic database sizes and can in some scenarios
compete with the most efficient previous approximation algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1910.11232v1,Overview of Logical Foundations of Cyber-Physical Systems,"Cyber-physical systems (CPSs) are important whenever computer technology
interfaces with the physical world as it does in self-driving cars or aircraft
control support systems. Due to their many subtleties, controllers for
cyber-physical systems deserve to be held to the highest correctness standards.
Their correct functioning is crucial, which explains the broad interest in
safety analysis technology for their mathematical models, which are called
hybrid systems because they combine discrete dynamics with continuous dynamics.
Differential dynamic logic (dL) provides logical specification and rigorous
reasoning techniques for hybrid systems. The logic dL is implemented in the
theorem prover KeYmaera X, which has been instrumental in verifying ground
robot controllers, railway systems, and the next-generation airborne collision
avoidance system ACAS X. This chapter provides an informal overview of this
logical approach to CPS safety that is detailed in a recent textbook on Logical
Foundations of Cyber-Physical Systems. It also explains how safety guarantees
obtained in the land of verified models reach the level of CPS execution
unharmed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0802.3479v1,"An Empirical Study of End-User Behaviour in Spreadsheet Error Detection
  & Correction","Very little is known about the process by which end-user developers detect
and correct spreadsheet errors. Any research pertaining to the development of
spreadsheet testing methodologies or auditing tools would benefit from
information on how end-users perform the debugging process in practice.
Thirteen industry-based professionals and thirty-four accounting & finance
students took part in a current ongoing experiment designed to record and
analyse end-user behaviour in spreadsheet error detection and correction.
Professionals significantly outperformed students in correcting certain error
types. Time-based cell activity analysis showed that a strong correlation
exists between the percentage of cells inspected and the number of errors
corrected. The cell activity data was gathered through a purpose written VBA
Excel plug-in that records the time and detail of all cell selection and cell
change actions of individuals.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1
http://arxiv.org/abs/1707.01124v2,Hyperbolic triangulations and discrete random graphs,"The hyperbolic random graph model (HRG) has proven useful in the analysis of
scale-free networks, which are ubiquitous in many fields, from social network
analysis to biology. However, working with this model is algorithmically and
conceptually challenging because of the nature of the distances in the
hyperbolic plane. In this paper we study the algorithmic properties of
regularly generated triangulations in the hyperbolic plane. We propose a
discrete variant of the HRG model where nodes are mapped to the vertices of
such a triangulation; our algorithms allow us to work with this model in a
simple yet efficient way. We present experimental results conducted on real
world networks to evaluate the practical benefits of DHRG in comparison to the
HRG model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0911.3856v1,Delay Bounds for Networks with Heavy-Tailed and Self-Similar Traffic,"We provide upper bounds on the end-to-end backlog and delay in a network with
heavy-tailed and self-similar traffic. The analysis follows a network calculus
approach where traffic is characterized by envelope functions and service is
described by service curves. A key contribution of this paper is the derivation
of a probabilistic sample path bound for heavy-tailed self-similar arrival
processes, which is enabled by a suitable envelope characterization, referred
to as `htss envelope'. We derive a heavy-tailed service curve for an entire
network path when the service at each node on the path is characterized by
heavy-tailed service curves. We obtain backlog and delay bounds for traffic
that is characterized by an htss envelope and receives service given by a
heavy-tailed service curve. The derived performance bounds are non-asymptotic
in that they do not assume a steady-state, large buffer, or many sources
regime. We also explore the scale of growth of delays as a function of the
length of the path. The appendix contains an analysis for self-similar traffic
with a Gaussian tail distribution.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9909007v1,Circular Separability of Polygons,"Two planar sets are circularly separable if there exists a circle enclosing
one of the sets and whose open interior disk does not intersect the other set.
  This paper studies two problems related to circular separability. A
linear-time algorithm is proposed to decide if two polygons are circularly
separable. The algorithm outputs the smallest separating circle. The second
problem asks for the largest circle included in a preprocessed, convex polygon,
under some point and/or line constraints. The resulting circle must contain the
query points and it must lie in the halfplanes delimited by the query lines.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1202.3451v1,"The Future of Search and Discovery in Big Data Analytics: Ultrametric
  Information Spaces","Consider observation data, comprised of n observation vectors with values on
a set of attributes. This gives us n points in attribute space. Having data
structured as a tree, implied by having our observations embedded in an
ultrametric topology, offers great advantage for proximity searching. If we
have preprocessed data through such an embedding, then an observation's nearest
neighbor is found in constant computational time, i.e. O(1) time. A further
powerful approach is discussed in this work: the inducing of a hierarchy, and
hence a tree, in linear computational time, i.e. O(n) time for n observations.
It is with such a basis for proximity search and best match that we can address
the burgeoning problems of processing very large, and possibly also very high
dimensional, data sets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1804.10456v2,"A Reputation Scheme to Discourage Selfish QoS Manipulation in Two-Hop
  Wireless Relay Networks","In wireless networks, stations can improve their received quality of service
(QoS) by handling packets of source flows with higher priority. Additionally,
in cooperative relay networks, the relays can handle transit flows with lower
priority. We use game theory to model a two-hop relay network where each of the
two involved stations can commit such selfish QoS manipulation. We design and
evaluate a reputation-based incentive scheme called RISC2WIN, whereby a trusted
third party (e.g., an access point) can limit selfish behavior and preserve
appropriate QoS for both stations.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1212.2257v1,A Process Calculus with Logical Operators,"In order to combine operational and logical styles of specifications in one
unified framework, the notion of logic labelled transition systems (Logic LTS,
for short) has been presented and explored by L\""{u}ttgen and Vogler in [TCS
373(1-2):19-40; Inform. & Comput. 208:845-867]. In contrast with usual LTS, two
logical constructors $\wedge$ and $\vee$ over Logic LTSs are introduced to
describe logical combinations of specifications. Hitherto such framework has
been dealt with in considerable depth, however, process algebraic style way has
not yet been involved and the axiomatization of constructors over Logic LTSs is
absent. This paper tries to develop L\""{u}ttgen and Vogler's work along this
direction. We will present a process calculus for Logic LTSs (CLL, for short).
The language CLL is explored in detail from two different but equivalent views.
Based on behavioral view, the notion of ready simulation is adopted to
formalize the refinement relation, and the behavioral theory is developed.
Based on proof-theoretic view, a sound and ground-complete axiomatic system for
CLL is provided, which captures operators in CLL through (in)equational laws.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.06498v1,Fit2Form: 3D Generative Model for Robot Gripper Form Design,"The 3D shape of a robot's end-effector plays a critical role in determining
it's functionality and overall performance. Many industrial applications rely
on task-specific gripper designs to ensure the system's robustness and
accuracy. However, the process of manual hardware design is both costly and
time-consuming, and the quality of the resulting design is dependent on the
engineer's experience and domain expertise, which can easily be out-dated or
inaccurate. The goal of this work is to use machine learning algorithms to
automate the design of task-specific gripper fingers. We propose Fit2Form, a 3D
generative design framework that generates pairs of finger shapes to maximize
design objectives (i.e., grasp success, stability, and robustness) for target
grasp objects. We model the design objectives by training a Fitness network to
predict their values for pairs of gripper fingers and their corresponding grasp
objects. This Fitness network then provides supervision to a 3D Generative
network that produces a pair of 3D finger geometries for the target grasp
object. Our experiments demonstrate that the proposed 3D generative design
framework generates parallel jaw gripper finger shapes that achieve more stable
and robust grasps compared to other general-purpose and task-specific gripper
design algorithms. Video can be found at https://youtu.be/utKHP3qb1bg.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1806.04410v1,Is India's Unique Identification Number a legally valid identification?,"A legally valid identification document allows impartial arbitration of the
identification of individuals. It protects individuals from a violation of
their dignity, justice, liberty and equality. It protects the nation from a
destruction of its republic, democratic, sovereign status. In order to test the
ability of an identification document to establish impartial identification of
individuals, it must be evaluated for its ability to establish identity,
undertake identification and build confidence to impartial, reliable and valid
identification. The processes of issuing, using and validating identification
documents alter the ability of the document to establish identity, undertake
identification and build confidence to impartial and valid identification.
These processes alter the ability of the document to serve as proof of
identity, proof of address, proof of being a resident, or even the proof of
existence of a person. We examine the ability of the UID number to serve as an
identification document with the ability to impartially arbitrate the
identification of individuals and serve as proof of identity, address, and
demonstrate existence of a person. We evaluate the implications of the
continued use UID system on our ability to undertake legally valid
identification ensure integrity of the identity and address databases across
the world.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0
http://arxiv.org/abs/cs/0405027v1,Evolution of a Subsumption Architecture Neurocontroller,"An approach to robotics called layered evolution and merging features from
the subsumption architecture into evolutionary robotics is presented, and its
advantages are discussed. This approach is used to construct a layered
controller for a simulated robot that learns which light source to approach in
an environment with obstacles. The evolvability and performance of layered
evolution on this task is compared to (standard) monolithic evolution,
incremental and modularised evolution. To corroborate the hypothesis that a
layered controller performs at least as well as an integrated one, the evolved
layers are merged back into a single network. On the grounds of the test
results, it is argued that layered evolution provides a superior approach for
many tasks, and it is suggested that this approach may be the key to scaling up
evolutionary robotics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1001.0961v3,Computing a Frobenius Coin Problem decision problem in O(n^2),"Expanding on recent results of another an algorithm is presented that
provides solution to the Frobenius Coin Problem in worst case O(n^2) in the
magnitude of the largest denomination.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.11962v1,"A micro-macro Markov chain Monte Carlo method for molecular dynamics
  using reaction coordinate proposals II: indirect reconstruction","We introduce a new micro-macro Markov chain Monte Carlo method (mM-MCMC) with
indirect reconstruction to sample invariant distributions of molecular dynamics
systems that exhibit a time-scale separation between the microscopic (fast)
dynamics, and the macroscopic (slow) dynamics of some low-dimensional set of
reaction coordinates. The algorithm enhances exploration of the state space in
the presence of metastability by allowing larger proposal moves at the
macroscopic level, on which a conditional accept-reject procedure is applied.
Only when the macroscopic proposal is accepted, the full microscopic state is
reconstructed from the newly sampled reaction coordinate value and is subjected
to a second accept/reject procedure. The computational gain stems from the fact
that most proposals are rejected at the macroscopic level, at low computational
cost, while microscopic states, once reconstructed, are almost always accepted.
This paper discusses an indirect method to reconstruct microscopic samples from
macroscopic reaction coordinate values, that can also be applied in cases where
direct reconstruction is cumbersome. The indirect reconstruction method
generates a microscopic sample by performing a biased microscopic simulation,
starting from the previous microscopic sample and driving the microscopic state
towards the proposed reaction coordinate value. We show numerically that the
mM-MCMC scheme with indirect reconstruction can significantly extend the range
of applicability of the mM-MCMC method.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.13727v2,Interval and fuzzy physics-informed neural networks for uncertain fields,"Temporally and spatially dependent uncertain parameters are regularly
encountered in engineering applications. Commonly these uncertainties are
accounted for using random fields and processes, which require knowledge about
the appearing probability distributions functions that is not readily
available. In these cases non-probabilistic approaches such as interval
analysis and fuzzy set theory are helpful uncertainty measures. Partial
differential equations involving fuzzy and interval fields are traditionally
solved using the finite element method where the input fields are sampled using
some basis function expansion methods. This approach however is problematic, as
it is reliant on knowledge about the spatial correlation fields. In this work
we utilize physics-informed neural networks (PINNs) to solve interval and fuzzy
partial differential equations. The resulting network structures termed
interval physics-informed neural networks (iPINNs) and fuzzy physics-informed
neural networks (fPINNs) show promising results for obtaining bounded solutions
of equations involving spatially and/or temporally uncertain parameter fields.
In contrast to finite element approaches, no correlation length specification
of the input fields as well as no Monte-Carlo simulations are necessary. In
fact, information about the input interval fields is obtained directly as a
byproduct of the presented solution scheme. Furthermore, all major advantages
of PINNs are retained, i.e. meshfree nature of the scheme, and ease of inverse
problem set-up.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.7470v1,Loo.py: transformation-based code generation for GPUs and CPUs,"Today's highly heterogeneous computing landscape places a burden on
programmers wanting to achieve high performance on a reasonably broad
cross-section of machines. To do so, computations need to be expressed in many
different but mathematically equivalent ways, with, in the worst case, one
variant per target machine.
  Loo.py, a programming system embedded in Python, meets this challenge by
defining a data model for array-style computations and a library of
transformations that operate on this model. Offering transformations such as
loop tiling, vectorization, storage management, unrolling, instruction-level
parallelism, change of data layout, and many more, it provides a convenient way
to capture, parametrize, and re-unify the growth among code variants. Optional,
deep integration with numpy and PyOpenCL provides a convenient computing
environment where the transition from prototype to high-performance
implementation can occur in a gradual, machine-assisted form.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1510.03994v1,Decidability in the logic of subsequences and supersequences,"We consider first-order logics of sequences ordered by the subsequence
ordering, aka sequence embedding. We show that the \Sigma_2 theory is
undecidable, answering a question left open by Kuske. Regarding fragments with
a bounded number of variables, we show that the FO2 theory is decidable while
the FO3 theory is undecidable.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.06409v1,SyntheticFur dataset for neural rendering,"We introduce a new dataset called SyntheticFur built specifically for machine
learning training. The dataset consists of ray traced synthetic fur renders
with corresponding rasterized input buffers and simulation data files. We
procedurally generated approximately 140,000 images and 15 simulations with
Houdini. The images consist of fur groomed with different skin primitives and
move with various motions in a predefined set of lighting environments. We also
demonstrated how the dataset could be used with neural rendering to
significantly improve fur graphics using inexpensive input buffers by training
a conditional generative adversarial network with perceptual loss. We hope the
availability of such high fidelity fur renders will encourage new advances with
neural rendering for a variety of applications.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0109073v1,"E-Business and SMEs: Preliminary Evidence from Selected Italian
  Districts","The debate on the Information Society shows large agreement on the assumption
that the promised benefits will fully display only if the diffusion of ICTs and
the Internet will involve all the actors of the socio-economic system.
Accordingly, special emphasis is put on the participation of small and medium
enterprises (SMEs), but also on public administrations (PAs) as promoters and
catalysts of private initiatives. As for SMEs, public intervention concerns
both the promotion of fully competitive e-markets and the solution of market
failures. However, effective and efficient intervention requires specific
information on SMEs' approach to e-commerce, often depending upon specific
sector and local condition and in most cases still lacking. In order to
identify the need and the scope for public intervention, the paper focuses on a
peculiar SMEs-intensive productive environment: the manufacturing industrial
district, which traditionally constitutes an examples of winning SMEs' network,
characterised by common industrial culture and intense input-output
interactions. The paper presents empirical evidence from the Italian districts
of Como (textile industry) and Lumezzane (metalwork industry). The research
results show that pro-active entrepreneurs are creatively exploring the
opportunities offered by the Internet to promote their businesses. However, it
is also clear that the transition to the Internet economy still involves a
reduced percentage of potential participants, and that institutional actions
are needed in order to foster a larger participation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1911.11699v2,"Multi-Vehicle Mixed-Reality Reinforcement Learning for Autonomous
  Multi-Lane Driving","Autonomous driving promises to transform road transport. Multi-vehicle and
multi-lane scenarios, however, present unique challenges due to constrained
navigation and unpredictable vehicle interactions. Learning-based
methods---such as deep reinforcement learning---are emerging as a promising
approach to automatically design intelligent driving policies that can cope
with these challenges. Yet, the process of safely learning multi-vehicle
driving behaviours is hard: while collisions---and their near-avoidance---are
essential to the learning process, directly executing immature policies on
autonomous vehicles raises considerable safety concerns. In this article, we
present a safe and efficient framework that enables the learning of driving
policies for autonomous vehicles operating in a shared workspace, where the
absence of collisions cannot be guaranteed. Key to our learning procedure is a
sim2real approach that uses real-world online policy adaptation in a
mixed-reality setup, where other vehicles and static obstacles exist in the
virtual domain. This allows us to perform safe learning by simulating (and
learning from) collisions between the learning agent(s) and other objects in
virtual reality. Our results demonstrate that, after only a few runs in
mixed-reality, collisions are significantly reduced.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1709.06113v1,Crossing Patterns in Nonplanar Road Networks,"We define the crossing graph of a given embedded graph (such as a road
network) to be a graph with a vertex for each edge of the embedding, with two
crossing graph vertices adjacent when the corresponding two edges of the
embedding cross each other. In this paper, we study the sparsity properties of
crossing graphs of real-world road networks. We show that, in large road
networks (the Urban Road Network Dataset), the crossing graphs have connected
components that are primarily trees, and that the remaining non-tree components
are typically sparse (technically, that they have bounded degeneracy). We prove
theoretically that when an embedded graph has a sparse crossing graph, it has
other desirable properties that lead to fast algorithms for shortest paths and
other algorithms important in geographic information systems. Notably, these
graphs have polynomial expansion, meaning that they and all their subgraphs
have small separators.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1903.06949v1,Hand range of motion evaluation for Rheumatoid Arthritis patients,"We introduce a framework for dynamic evaluation of the fingers movements:
flexion, extension, abduction and adduction. This framework estimates angle
measurements from joints computed by a hand pose estimation algorithm using a
depth sensor (Realsense SR300). Given depth maps as input, our framework uses
Pose-REN, which is a state-of-art hand pose estimation method that estimates 3D
hand joint positions using a deep convolutional neural network. The pose
estimation algorithm runs in real-time, allowing users to visualise 3D skeleton
tracking results at the same time as the depth images are acquired. Once 3D
joint poses are obtained, our framework estimates a plane containing the wrist
and MCP joints and measures flexion/extension and abduction/aduction angles by
applying computational geometry operations with respect to this plane. We
analysed flexion and abduction movement patterns using real data, extracting
the movement trajectories. Our preliminary results show that this method allows
an automatic discrimination of hands with Rheumatoid Arthritis (RA) and healthy
patients. The angle between joints can be used as an indicative of current
movement capabilities and function. Although the measurements can be noisy and
less accurate than those obtained statically through goniometry, the
acquisition is much easier, non-invasive and patient-friendly, which shows the
potential of our approach. The system can be used with and without orthosis.
Our framework allows the acquisition of measurements with minimal intervention
and significantly reduces the evaluation time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1807.03566v1,A modelling language for the effective design of Java annotations,"This paper describes a new modelling language for the effective design of
Java annotations. Since their inclusion in the 5th edition of Java, annotations
have grown from a useful tool for the addition of meta-data to play a central
role in many popular software projects. Usually they are conceived as sets with
dependency and integrity constraints within them; however, the native support
provided by Java for expressing this design is very limited. To overcome its
deficiencies and make explicit the rich conceptual model which lies behind a
set of annotations, we propose a domain-specific modelling language. The
proposal has been implemented as an Eclipse plug-in, including an editor and an
integrated code generator that synthesises annotation processors. The language
has been tested using a real set of annotations from the Java Persistence API
(JPA). It has proven to cover a greater scope with respect to other related
work in different shared areas of application.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.08755v1,"Measuring the Novelty of Natural Language Text Using the Conjunctive
  Clauses of a Tsetlin Machine Text Classifier","Most supervised text classification approaches assume a closed world,
counting on all classes being present in the data at training time. This
assumption can lead to unpredictable behaviour during operation, whenever
novel, previously unseen, classes appear. Although deep learning-based methods
have recently been used for novelty detection, they are challenging to
interpret due to their black-box nature. This paper addresses
\emph{interpretable} open-world text classification, where the trained
classifier must deal with novel classes during operation. To this end, we
extend the recently introduced Tsetlin machine (TM) with a novelty scoring
mechanism. The mechanism uses the conjunctive clauses of the TM to measure to
what degree a text matches the classes covered by the training data. We
demonstrate that the clauses provide a succinct interpretable description of
known topics, and that our scoring mechanism makes it possible to discern novel
topics from the known ones. Empirically, our TM-based approach outperforms
seven other novelty detection schemes on three out of five datasets, and
performs second and third best on the remaining, with the added benefit of an
interpretable propositional logic-based representation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.11632v2,"Pores for thought: The use of generative adversarial networks for the
  stochastic reconstruction of 3D multi-phase electrode microstructures with
  periodic boundaries","The generation of multiphase porous electrode microstructures is a critical
step in the optimisation of electrochemical energy storage devices. This work
implements a deep convolutional generative adversarial network (DC-GAN) for
generating realistic n-phase microstructural data. The same network
architecture is successfully applied to two very different three-phase
microstructures: A lithium-ion battery cathode and a solid oxide fuel cell
anode. A comparison between the real and synthetic data is performed in terms
of the morphological properties (volume fraction, specific surface area,
triple-phase boundary) and transport properties (relative diffusivity), as well
as the two-point correlation function. The results show excellent agreement
between for datasets and they are also visually indistinguishable. By modifying
the input to the generator, we show that it is possible to generate
microstructure with periodic boundaries in all three directions. This has the
potential to significantly reduce the simulated volume required to be
considered representative and therefore massively reduce the computational cost
of the electrochemical simulations necessary to predict the performance of a
particular microstructure during optimisation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1709.03036v1,Abductive Matching in Question Answering,"We study question-answering over semi-structured data. We introduce a new way
to apply the technique of semantic parsing by applying machine learning only to
provide annotations that the system infers to be missing; all the other parsing
logic is in the form of manually authored rules. In effect, the machine
learning is used to provide non-syntactic matches, a step that is ill-suited to
manual rules. The advantage of this approach is in its debuggability and in its
transparency to the end-user. We demonstrate the effectiveness of the approach
by achieving state-of-the-art performance of 40.42% accuracy on a standard
benchmark dataset over tables from Wikipedia.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0504027v4,Linear Datalog and Bounded Path Duality of Relational Structures,"In this paper we systematically investigate the connections between logics
with a finite number of variables, structures of bounded pathwidth, and linear
Datalog Programs. We prove that, in the context of Constraint Satisfaction
Problems, all these concepts correspond to different mathematical embodiments
of a unique robust notion that we call bounded path duality. We also study the
computational complexity implications of the notion of bounded path duality. We
show that every constraint satisfaction problem $\csp(\best)$ with bounded path
duality is solvable in NL and that this notion explains in a uniform way all
families of CSPs known to be in NL. Finally, we use the results developed in
the paper to identify new problems in NL.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0308039v1,"A new approach to relevancy in Internet searching - the ""Vox Populi
  Algorithm""","In this paper we will derive a new algorithm for Internet searching. The main
idea of this algorithm is to extend the existing algorithms by a component,
which reflects the interests of the users more than existing methods. The ""Vox
Populi Algorithm"" (VPA) creates a feedback from the users to the content of the
search index. The information derived from the users query analysis is used to
modify the existing crawling algorithms. The VPA controls the distribution of
the resources of the crawler. Finally, we also discuss methods of suppressing
unwanted content (spam).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0806.2007v1,"Experts Fusion and Multilayer Perceptron Based on Belief Learning for
  Sonar Image Classification","The sonar images provide a rapid view of the seabed in order to characterize
it. However, in such as uncertain environment, real seabed is unknown and the
only information we can obtain, is the interpretation of different human
experts, sometimes in conflict. In this paper, we propose to manage this
conflict in order to provide a robust reality for the learning step of
classification algorithms. The classification is conducted by a multilayer
perceptron, taking into account the uncertainty of the reality in the learning
stage. The results of this seabed characterization are presented on real sonar
images.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.06473v2,"Thermoformed Circuit Boards: Fabrication of highly conductive freeform
  3D printed circuit boards with heat bending","Fabricating 3D printed electronics using desktop printers has become more
accessible with recent developments in conductive thermoplastic filaments.
Because of their high resistance and difficulties in printing traces in
vertical directions, most applications are restricted to capacitive sensing. In
this paper, we introduce Thermoformed Circuit Board (TCB), a novel approach
that employs the thermoformability of the 3D printed plastics to construct
various double-sided, rigid and highly conductive freeform circuit boards that
can withstand high current applications through copper electroplating. To
illustrate the capability of the TCB, we showcase a range of examples with
various shapes, electrical characteristics and interaction mechanisms. We also
demonstrate a new design tool extension to an existing CAD environment that
allows users to parametrically draw the substrate and conductive trace, and
export 3D printable files. TCB is an inexpensive and highly accessible
fabrication technique intended to broaden HCI researcher participation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0207086v1,A Model-Theoretic Semantics for Defeasible Logic,"Defeasible logic is an efficient logic for defeasible reasoning. It is
defined through a proof theory and, until now, has had no model theory. In this
paper a model-theoretic semantics is given for defeasible logic. The logic is
sound and complete with respect to the semantics. We also briefly outline how
this approach extends to a wide range of defeasible logics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1802.07465v1,Multiclass Weighted Loss for Instance Segmentation of Cluttered Cells,"We propose a new multiclass weighted loss function for instance segmentation
of cluttered cells. We are primarily motivated by the need of developmental
biologists to quantify and model the behavior of blood T-cells which might help
us in understanding their regulation mechanisms and ultimately help researchers
in their quest for developing an effective immuno-therapy cancer treatment.
Segmenting individual touching cells in cluttered regions is challenging as the
feature distribution on shared borders and cell foreground are similar thus
difficulting discriminating pixels into proper classes. We present two novel
weight maps applied to the weighted cross entropy loss function which take into
account both class imbalance and cell geometry. Binary ground truth training
data is augmented so the learning model can handle not only foreground and
background but also a third touching class. This framework allows training
using U-Net. Experiments with our formulations have shown superior results when
compared to other similar schemes, outperforming binary class models with
significant improvement of boundary adequacy and instance detection. We
validate our results on manually annotated microscope images of T-cells.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.00005v1,A Case Study on Quality Attribute Measurement using MARF and GIPSY,"This literature focuses on doing a comparative analysis between Modular Audio
Recognition Framework (MARF) and the General Intentional Programming System
(GIPSY) with the help of different software metrics. At first, we understand
the general principles, architecture and working of MARF and GIPSY by looking
at their frameworks and running them in the Eclipse environment. Then, we study
some of the important metrics including a few state of the art metrics and rank
them in terms of their usefulness and their influence on the different quality
attributes of a software. The quality attributes are viewed and computed with
the help of the Logiscope and McCabe IQ tools. These tools perform a
comprehensive analysis on the case studies and generate a quality report at the
factor level, criteria level and metrics level. In next step, we identify the
worst code at each of these levels, extract the worst code and provide
recommendations to improve the quality. We implement and test some of the
metrics which are ranked as the most useful metrics with a set of test cases in
JDeodorant. Finally, we perform an analysis on both MARF and GIPSY by doing a
fuzzy code scan using MARFCAT to find the list of weak and vulnerable classes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/0903.1496v1,"How Much Information can One Get from a Wireless Ad Hoc Sensor Network
  over a Correlated Random Field?","New large deviations results that characterize the asymptotic information
rates for general $d$-dimensional ($d$-D) stationary Gaussian fields are
obtained. By applying the general results to sensor nodes on a two-dimensional
(2-D) lattice, the asymptotic behavior of ad hoc sensor networks deployed over
correlated random fields for statistical inference is investigated. Under a 2-D
hidden Gauss-Markov random field model with symmetric first order conditional
autoregression and the assumption of no in-network data fusion, the behavior of
the total obtainable information [nats] and energy efficiency [nats/J] defined
as the ratio of total gathered information to the required energy is obtained
as the coverage area, node density and energy vary. When the sensor node
density is fixed, the energy efficiency decreases to zero with rate
$\Theta({area}^{-1/2})$ and the per-node information under fixed per-node
energy also diminishes to zero with rate $O(N_t^{-1/3})$ as the number $N_t$ of
network nodes increases by increasing the coverage area. As the sensor spacing
$d_n$ increases, the per-node information converges to its limit $D$ with rate
$D-\sqrt{d_n}e^{-\alpha d_n}$ for a given diffusion rate $\alpha$. When the
coverage area is fixed and the node density increases, the per-node information
is inversely proportional to the node density. As the total energy $E_t$
consumed in the network increases, the total information obtainable from the
network is given by $O(\log E_t)$ for the fixed node density and fixed coverage
case and by $\Theta (E_t^{2/3})$ for the fixed per-node sensing energy and
fixed density and increasing coverage case.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.04129v1,"Tensor Completion via Tensor QR Decomposition and $L_{2,1}$-Norm
  Minimization","In this paper, we consider the tensor completion problem, which has many
researchers in the machine learning particularly concerned. Our fast and
precise method is built on extending the $L_{2,1}$-norm minimization and Qatar
Riyal decomposition (LNM-QR) method for matrix completions to tensor
completions, and is different from the popular tensor completion methods using
the tensor singular value decomposition (t-SVD). In terms of shortening the
computing time, t-SVD is replaced with the method computing an approximate
t-SVD based on Qatar Riyal decomposition (CTSVD-QR), which can be used to
compute the largest $r \left(r>0 \right)$ singular values (tubes) and their
associated singular vectors (of tubes) iteratively. We, in addition, use the
tensor $L_{2,1}$-norm instead of the tensor nuclear norm to minimize our model
on account of it is easy to optimize. Then in terms of improving accuracy,
ADMM, a gradient-search-based method, plays a crucial part in our method.
Numerical experimental results show that our method is faster than those
state-of-the-art algorithms and have excellent accuracy.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9906021v1,Reconstructing hv-Convex Polyominoes from Orthogonal Projections,"Tomography is the area of reconstructing objects from projections. Here we
wish to reconstruct a set of cells in a two dimensional grid, given the number
of cells in every row and column. The set is required to be an hv-convex
polyomino, that is all its cells must be connected and the cells in every row
and column must be consecutive. A simple, polynomial algorithm for
reconstructing hv-convex polyominoes is provided, which is several orders of
magnitudes faster than the best previously known algorithm from Barcucci et al.
In addition, the problem of reconstructing a special class of centered
hv-convex polyominoes is addressed. (An object is centered if it contains a row
whose length equals the total width of the object). It is shown that in this
case the reconstruction problem can be solved in linear time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1908.10870v1,"Intergroup Contact in the Wild: Characterizing Language Differences
  between Intergroup and Single-group Members in NBA-related Discussion Forums","Intergroup contact has long been considered as an effective strategy to
reduce prejudice between groups. However, recent studies suggest that exposure
to opposing groups in online platforms can exacerbate polarization. To further
understand the behavior of individuals who actively engage in intergroup
contact in practice, we provide a large-scale observational study of intragroup
behavioral differences between members with and without intergroup contact. We
leverage the existing structure of NBA-related discussion forums on Reddit to
study the context of professional sports. We identify fans of each NBA team as
members of a group and trace whether they have intergroup contact. Our results
show that members with intergroup contact use more negative and abusive
language in their affiliated group than those without such contact, after
controlling for activity levels. We further quantify different levels of
intergroup contact and show that there may exist nonlinear mechanisms regarding
how intergroup contact relates to intragroup behavior. Our findings provide
complementary evidence to experimental studies in a novel context and also shed
light on possible reasons for the different outcomes in prior studies.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1608.01594v2,"Precise Complexity Guarantees for Pointer Analysis via Datalog with
  Extensions","Pointer analysis is a fundamental static program analysis for computing the
set of objects that an expression can refer to. Decades of research has gone
into developing methods of varying precision and efficiency for pointer
analysis for programs that use different language features, but determining
precisely how efficient a particular method is has been a challenge in itself.
  For programs that use different language features, we consider methods for
pointer analysis using Datalog and extensions to Datalog. When the rules are in
Datalog, we present the calculation of precise time complexities from the rules
using a new algorithm for decomposing rules for obtaining the best
complexities. When extensions such as function symbols and universal
quantification are used, we describe algorithms for efficiently implementing
the extensions and the complexities of the algorithms.
  This paper is under consideration for acceptance in TPLP.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.08933v1,"An Objective Laboratory Protocol for Evaluating Cognition of Non-Human
  Systems Against Human Cognition","In this paper I describe and reduce to practice an objective protocol for
evaluating the cognitive capabilities of a non-human system against human
cognition in a laboratory environment. This is important because the existence
of a non-human system with cognitive capabilities comparable to those of humans
might make once-philosophical questions of safety and ethics immediate and
urgent. Past attempts to devise evaluation methods, such as the Turing Test and
many others, have not met this need; most of them either emphasize a single
aspect of human cognition or a single theory of intelligence, fail to capture
the human capacity for generality and novelty, or require success in the
physical world. The protocol is broadly Bayesian, in that its primary output is
a confidence statistic in relation to a claim. Further, it provides insight
into the areas where and to what extent a particular system falls short of
human cognition, which can help to drive further progress or precautions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2111.01042v1,"Logic Rules Meet Deep Learning: A Novel Approach for Ship Type
  Classification","The shipping industry is an important component of the global trade and
economy, however in order to ensure law compliance and safety it needs to be
monitored. In this paper, we present a novel Ship Type classification model
that combines vessel transmitted data from the Automatic Identification System,
with vessel imagery. The main components of our approach are the Faster R-CNN
Deep Neural Network and a Neuro-Fuzzy system with IF-THEN rules. We evaluate
our model using real world data and showcase the advantages of this combination
while also compare it with other methods. Results show that our model can
increase prediction scores by up to 15.4\% when compared with the next best
model we considered, while also maintaining a level of explainability as
opposed to common black box approaches.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.06835v1,"A Review on Edge Analytics: Issues, Challenges, Opportunities, Promises,
  Future Directions, and Applications","Edge technology aims to bring Cloud resources (specifically, the compute,
storage, and network) to the closed proximity of the Edge devices, i.e., smart
devices where the data are produced and consumed. Embedding computing and
application in Edge devices lead to emerging of two new concepts in Edge
technology, namely, Edge computing and Edge analytics. Edge analytics uses some
techniques or algorithms to analyze the data generated by the Edge devices.
With the emerging of Edge analytics, the Edge devices have become a complete
set. Currently, Edge analytics is unable to provide full support for the
execution of the analytic techniques. The Edge devices cannot execute advanced
and sophisticated analytic algorithms following various constraints such as
limited power supply, small memory size, limited resources, etc. This article
aims to provide a detailed discussion on Edge analytics. A clear explanation to
distinguish between the three concepts of Edge technology, namely, Edge
devices, Edge computing, and Edge analytics, along with their issues.
Furthermore, the article discusses the implementation of Edge analytics to
solve many problems in various areas such as retail, agriculture, industry, and
healthcare. In addition, the research papers of the state-of-the-art edge
analytics are rigorously reviewed in this article to explore the existing
issues, emerging challenges, research opportunities and their directions, and
applications.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1508.05567v2,"Dual-Based Approximation Algorithms for Cut-Based Network Connectivity
  Problems","We consider a variety of NP-Complete network connectivity problems. We
introduce a novel dual-based approach to approximating network design problems
with cut-based linear programming relaxations. This approach gives a
$3/2$-approximation to Minimum 2-Edge-Connected Spanning Subgraph that is
equivalent to a previously proposed algorithm. One well-studied branch of
network design models ad hoc networks where each node can either operate at
high or low power. If we allow unidirectional links, we can formalize this into
the problem Dual Power Assignment (DPA). Our dual-based approach gives a
$3/2$-approximation to DPA, improving the previous best approximation known of
$11/7\approx 1.57$.
  Another standard network design problem is Minimum Strongly Connected
Spanning Subgraph (MSCS). We propose a new problem generalizing MSCS and DPA
called Star Strong Connectivity (SSC). Then we show that our dual-based
approach achieves a 1.6-approximation ratio on SSC. As a consequence of our
dual-based approximations, we prove new upper bounds on the integrality gaps of
these problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0201008v1,"Using Tree Automata and Regular Expressions to Manipulate Hierarchically
  Structured Data","Information, stored or transmitted in digital form, is often structured.
Individual data records are usually represented as hierarchies of their
elements. Together, records form larger structures. Information processing
applications have to take account of this structuring, which assigns different
semantics to different data elements or records. Big variety of structural
schemata in use today often requires much flexibility from applications--for
example, to process information coming from different sources. To ensure
application interoperability, translators are needed that can convert one
structure into another. This paper puts forward a formal data model aimed at
supporting hierarchical data processing in a simple and flexible way. The model
is based on and extends results of two classical theories, studying finite
string and tree automata. The concept of finite automata and regular languages
is applied to the case of arbitrarily structured tree-like hierarchical data
records, represented as ""structured strings."" These automata are compared with
classical string and tree automata; the model is shown to be a superset of the
classical models. Regular grammars and expressions over structured strings are
introduced. Regular expression matching and substitution has been widely used
for efficient unstructured text processing; the model described here brings the
power of this proven technique to applications that deal with information
trees. A simple generic alternative is offered to replace today's specialised
ad-hoc approaches. The model unifies structural and content transformations,
providing applications with a single data type. An example scenario of how to
build applications based on this theory is discussed. Further research
directions are outlined.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.02257v1,Visual Question Rewriting for Increasing Response Rate,"When a human asks questions online, or when a conversational virtual agent
asks human questions, questions triggering emotions or with details might more
likely to get responses or answers. we explore how to automatically rewrite
natural language questions to improve the response rate from people. In
particular, a new task of Visual Question Rewriting(VQR) task is introduced to
explore how visual information can be used to improve the new questions. A data
set containing around 4K bland questions, attractive questions and images
triples is collected. We developed some baseline sequence to sequence models
and more advanced transformer based models, which take a bland question and a
related image as input and output a rewritten question that is expected to be
more attractive. Offline experiments and mechanical Turk based evaluations show
that it is possible to rewrite bland questions in a more detailed and
attractive way to increase the response rate, and images can be helpful.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1209.5243v1,"Walking with the Oracle: Efficient Use of Mobile Networks through
  Location-Awareness","Always Best Packet Switching (ABPS) is a novel approach for wireless
communications that enables mobile nodes, equipped with multiple network
interface cards (NICs), to dynamically determine the most appropriate NIC to
use. Using ABPS, a mobile node can seamlessly switch to a different NIC in
order to get better performance, without causing communication interruptions at
the application level. To make this possible, NICs are kept always active and a
software monitor constantly probes the channels for available access points.
While this ensures maximum connection availability, considerable energy may be
wasted when no access points are available for a given NIC. In this paper we
address this issue by investigating the use of an ""oracle"" able to provide
information on network availability. This allows to dynamically switch on/off
NICs based on reported availability, thus reducing the power consumption. We
present a Markov model which allows us to estimate the impact of the oracle on
the ABPS mechanism: results show that significant reduction in energy
consumption can be achieved with minimal impact on connection availability. We
conclude by describing a prototype implementation of the oracle based on Web
services and geolocalization.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1110.3002v1,Are Minds Computable?,"This essay explores the limits of Turing machines concerning the modeling of
minds and suggests alternatives to go beyond those limits.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1712.08342v2,Event-based Failure Prediction in Distributed Business Processes,"Traditionally, research in Business Process Management has put a strong focus
on centralized and intra-organizational processes. However, today's business
processes are increasingly distributed, deviating from a centralized layout,
and therefore calling for novel methodologies of detecting and responding to
unforeseen events, such as errors occurring during process runtime. In this
article, we demonstrate how to employ event-based failure prediction in
business processes. This approach allows to make use of the best of both
traditional Business Process Management Systems and event-based systems. Our
approach employs machine learning techniques and considers various types of
events. We evaluate our solution using two business process data sets,
including one from a real-world event log, and show that we are able to detect
errors and predict failures with high accuracy.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.00735v1,"In search of lost time: Axiomatising parallel composition in process
  algebras","This survey reviews some of the most recent achievements in the saga of the
axiomatisation of parallel composition, along with some classic results. We
focus on the recursion, relabelling and restriction free fragment of CCS and we
discuss the solutions to three problems that were open for many years. The
first problem concerns the status of Bergstra and Klop's auxiliary operators
left merge and communication merge in the finite axiomatisation of parallel
composition modulo bisimiliarity: We argue that, under some natural
assumptions, the addition of a single auxiliary binary operator to CCS does not
yield a finite axiomatisation of bisimilarity. Then we delineate the boundary
between finite and non-finite axiomatisability of the congruences in van
Glabbeek's linear time-branching time spectrum over CCS. Finally, we present a
novel result to the effect that rooted weak bisimilarity has no finite complete
axiomatisation over CCS.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1706.02813v1,Rigorous statistical analysis of HTTPS reachability,"The use of secure connections using HTTPS as the default means, or even the
only means, to connect to web servers is increasing. It is being pushed from
both sides: from the bottom up by client distributions and plugins, and from
the top down by organisations such as Google. However, there are potential
technical hurdles that might lock some clients out of the modern web. This
paper seeks to measure and precisely quantify those hurdles in the wild. More
than three million measurements provide statistically significant evidence of
degradation. We show this through a variety of statistical techniques. Various
factors are shown to influence the problem, ranging from the client's browser,
to the locale from which they connect.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.06379v1,"Answer-Type Modification without Tears: Prompt-Passing Style Translation
  for Typed Delimited-Control Operators","The salient feature of delimited-control operators is their ability to modify
answer types during computation. The feature, answer-type modification (ATM for
short), allows one to express various interesting programs such as typed printf
compactly and nicely, while it makes it difficult to embed these operators in
standard functional languages.
  In this paper, we present a typed translation of delimited-control operators
shift and reset with ATM into a familiar language with multi-prompt shift and
reset without ATM, which lets us use ATM in standard languages without
modifying the type system. Our translation generalizes Kiselyov's direct-style
implementation of typed printf, which uses two prompts to emulate the
modification of answer types, and passes them during computation. We prove that
our translation preserves typing. As the naive prompt-passing style translation
generates and passes many prompts even for pure terms, we show an optimized
translation that generate prompts only when needed, which is also
type-preserving. Finally, we give an implementation in the tagless-final style
which respects typing by construction.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1301.1514v1,Composition Closure of Linear Extended Top-down Tree Transducers,"Linear extended top-down tree transducers (or synchronous tree-substitution
grammars) are popular formal models of tree transformations. The expressive
power of compositions of such transducers with and without regular look-ahead
is investigated. In particular, the restrictions of nondeletion,
epsilon-freeness, and strictness are considered. The composition hierarchy
turns out to be finite for all epsilon-free (all rules consume input) variants
of these transducers except for nondeleting epsilon-free linear extended
top-down tree transducers. The least number of transducers needed for the full
expressive power of arbitrary compositions is presented. In all remaining cases
(including nondeleting epsilon-free linear extended top-down tree transducers)
the composition hierarchy does not collapse.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.02277v2,Counting Small Induced Subgraphs with Hereditary Properties,"We study the computational complexity of the problem $\#\text{IndSub}(\Phi)$
of counting $k$-vertex induced subgraphs of a graph $G$ that satisfy a graph
property $\Phi$. Our main result establishes an exhaustive and explicit
classification for all hereditary properties, including tight conditional lower
bounds under the Exponential Time Hypothesis (ETH):
  - If a hereditary property $\Phi$ is true for all graphs, or if it is true
only for finitely many graphs, then $\#\text{IndSub}(\Phi)$ is solvable in
polynomial time.
  - Otherwise, $\#\text{IndSub}(\Phi)$ is $\#\mathsf{W[1]}$-complete when
parameterised by $k$, and, assuming ETH, it cannot be solved in time $f(k)\cdot
|G|^{o(k)}$ for any function $f$.
  This classification features a wide range of properties for which the
corresponding detection problem (as classified by Khot and Raman [TCS 02]) is
tractable but counting is hard. Moreover, even for properties which are already
intractable in their decision version, our results yield significantly stronger
lower bounds for the counting problem. As additional result, we also present an
exhaustive and explicit parameterised complexity classification for all
properties that are invariant under homomorphic equivalence. By covering one of
the most natural and general notions of closure, namely, closure under
vertex-deletion (hereditary), we generalise some of the earlier results on this
problem. For instance, our results fully subsume and strengthen the existing
classification of $\#\text{IndSub}(\Phi)$ for monotone (subgraph-closed)
properties due to Roth, Schmitt, and Wellnitz [FOCS 20].",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.01292v3,"A General Safety Framework for Learning-Based Control in Uncertain
  Robotic Systems","The proven efficacy of learning-based control schemes strongly motivates
their application to robotic systems operating in the physical world. However,
guaranteeing correct operation during the learning process is currently an
unresolved issue, which is of vital importance in safety-critical systems. We
propose a general safety framework based on Hamilton-Jacobi reachability
methods that can work in conjunction with an arbitrary learning algorithm. The
method exploits approximate knowledge of the system dynamics to guarantee
constraint satisfaction while minimally interfering with the learning process.
We further introduce a Bayesian mechanism that refines the safety analysis as
the system acquires new evidence, reducing initial conservativeness when
appropriate while strengthening guarantees through real-time validation. The
result is a least-restrictive, safety-preserving control law that intervenes
only when (a) the computed safety guarantees require it, or (b) confidence in
the computed guarantees decays in light of new observations. We prove
theoretical safety guarantees combining probabilistic and worst-case analysis
and demonstrate the proposed framework experimentally on a quadrotor vehicle.
Even though safety analysis is based on a simple point-mass model, the
quadrotor successfully arrives at a suitable controller by policy-gradient
reinforcement learning without ever crashing, and safely retracts away from a
strong external disturbance introduced during flight.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.04362v3,"DIPS-Plus: The Enhanced Database of Interacting Protein Structures for
  Interface Prediction","How and where proteins interface with one another can ultimately impact the
proteins' functions along with a range of other biological processes. As such,
precise computational methods for protein interface prediction (PIP) come
highly sought after as they could yield significant advances in drug discovery
and design as well as protein function analysis. However, the traditional
benchmark dataset for this task, Docking Benchmark 5 (DB5), contains only a
modest 230 complexes for training, validating, and testing different machine
learning algorithms. In this work, we expand on a dataset recently introduced
for this task, the Database of Interacting Protein Structures (DIPS), to
present DIPS-Plus, an enhanced, feature-rich dataset of 42,112 complexes for
geometric deep learning of protein interfaces. The previous version of DIPS
contains only the Cartesian coordinates and types of the atoms comprising a
given protein complex, whereas DIPS-Plus now includes a plethora of new
residue-level features including protrusion indices, half-sphere amino acid
compositions, and new profile hidden Markov model (HMM)-based sequence features
for each amino acid, giving researchers a large, well-curated feature bank for
training protein interface prediction methods. We demonstrate through rigorous
benchmarks that training an existing state-of-the-art (SOTA) model for PIP on
DIPS-Plus yields SOTA results, surpassing the performance of all other models
trained on residue-level and atom-level encodings of protein complexes to date.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0203030v2,Source Routing and Scheduling in Packet Networks,"We study {\em routing} and {\em scheduling} in packet-switched networks. We
assume an adversary that controls the injection time, source, and destination
for each packet injected. A set of paths for these packets is {\em admissible}
if no link in the network is overloaded. We present the first on-line routing
algorithm that finds a set of admissible paths whenever this is feasible. Our
algorithm calculates a path for each packet as soon as it is injected at its
source using a simple shortest path computation. The length of a link reflects
its current congestion. We also show how our algorithm can be implemented under
today's Internet routing paradigms.
  When the paths are known (either given by the adversary or computed as above)
our goal is to schedule the packets along the given paths so that the packets
experience small end-to-end delays. The best previous delay bounds for
deterministic and distributed scheduling protocols were exponential in the path
length. In this paper we present the first deterministic and distributed
scheduling protocol that guarantees a polynomial end-to-end delay for every
packet.
  Finally, we discuss the effects of combining routing with scheduling. We
first show that some unstable scheduling protocols remain unstable no matter
how the paths are chosen. However, the freedom to choose paths can make a
difference. For example, we show that a ring with parallel links is stable for
all greedy scheduling protocols if paths are chosen intelligently, whereas this
is not the case if the adversary specifies the paths.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0010011v1,If P \neq NP then Some Strongly Noninvertible Functions are Invertible,"Rabi, Rivest, and Sherman alter the standard notion of noninvertibility to a
new notion they call strong noninvertibility, and show -- via explicit
cryptographic protocols for secret-key agreement ([RS93,RS97] attribute this to
Rivest and Sherman) and digital signatures [RS93,RS97] -- that strongly
noninvertible functions would be very useful components in protocol design.
Their definition of strong noninvertibility has a small twist (``respecting the
argument given'') that is needed to ensure cryptographic usefulness. In this
paper, we show that this small twist has a large, unexpected consequence:
Unless P=NP, some strongly noninvertible functions are invertible.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9910022v1,"Practical experiments with regular approximation of context-free
  languages","Several methods are discussed that construct a finite automaton given a
context-free grammar, including both methods that lead to subsets and those
that lead to supersets of the original context-free language. Some of these
methods of regular approximation are new, and some others are presented here in
a more refined form with respect to existing literature. Practical experiments
with the different methods of regular approximation are performed for
spoken-language input: hypotheses from a speech recognizer are filtered through
a finite automaton.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0204053v1,Qualitative Analysis of Correspondence for Experimental Algorithmics,"Correspondence identifies relationships among objects via similarities among
their components; it is ubiquitous in the analysis of spatial datasets,
including images, weather maps, and computational simulations. This paper
develops a novel multi-level mechanism for qualitative analysis of
correspondence. Operators leverage domain knowledge to establish
correspondence, evaluate implications for model selection, and leverage
identified weaknesses to focus additional data collection. The utility of the
mechanism is demonstrated in two applications from experimental algorithmics --
matrix spectral portrait analysis and graphical assessment of Jordan forms of
matrices. Results show that the mechanism efficiently samples computational
experiments and successfully uncovers high-level problem properties. It
overcomes noise and data sparsity by leveraging domain knowledge to detect
mutually reinforcing interpretations of spatial data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1612.05539v3,Greedy Routing and the Algorithmic Small-World Phenomenom,"The algorithmic small-world phenomenon, empirically established by Milgram's
letter forwarding experiments from the 60s, was theoretically explained by
Kleinberg in 2000. However, from today's perspective his model has several
severe shortcomings that limit the applicability to real-world networks. In
order to give a more convincing explanation of the algorithmic small-world
phenomenon, we study decentralized greedy routing in a more flexible random
graph model (geometric inhomogeneous random graphs) which overcomes all
previous shortcomings. Apart from exhibiting good properties in theory, it has
also been extensively experimentally validated that this model reasonably
captures real-world networks.
  In this model, the greedy routing protocol is purely distributed as each
vertex only needs to know information about its direct neighbors. We prove that
it succeeds with constant probability, and in case of success almost surely
finds an almost shortest path of length {\theta}(loglog n), where our bound is
tight including the leading constant. Moreover, we study natural local patching
methods which augment greedy routing by backtracking and which do not require
any global knowledge. We show that such methods can ensure success probability
1 in an asymptotically tight number of steps.
  These results also address the question of Krioukov et al. whether there are
efficient local routing protocols for the internet graph. There were promising
experimental studies, but the question remained unsolved theoretically. Our
results give for the first time a rigorous and analytical affirmative answer.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.09600v2,Drug Repurposing for COVID-19 via Knowledge Graph Completion,"Objective: To discover candidate drugs to repurpose for COVID-19 using
literature-derived knowledge and knowledge graph completion methods. Methods:
We propose a novel, integrative, and neural network-based literature-based
discovery (LBD) approach to identify drug candidates from both PubMed and
COVID-19-focused research literature. Our approach relies on semantic triples
extracted using SemRep (via SemMedDB). We identified an informative subset of
semantic triples using filtering rules and an accuracy classifier developed on
a BERT variant, and used this subset to construct a knowledge graph. Five SOTA,
neural knowledge graph completion algorithms were used to predict drug
repurposing candidates. The models were trained and assessed using a time
slicing approach and the predicted drugs were compared with a list of drugs
reported in the literature and evaluated in clinical trials. These models were
complemented by a discovery pattern-based approach. Results: Accuracy
classifier based on PubMedBERT achieved the best performance (F1= 0.854) in
classifying semantic predications. Among five knowledge graph completion
models, TransE outperformed others (MR = 0.923, Hits@1=0.417). Some known drugs
linked to COVID-19 in the literature were identified, as well as some candidate
drugs that have not yet been studied. Discovery patterns enabled generation of
plausible hypotheses regarding the relationships between the candidate drugs
and COVID-19. Among them, five highly ranked and novel drugs (paclitaxel, SB
203580, alpha 2-antiplasmin, pyrrolidine dithiocarbamate, and butylated
hydroxytoluene) with their mechanistic explanations were further discussed.
Conclusion: We show that an LBD approach can be feasible for discovering drug
candidates for COVID-19, and for generating mechanistic explanations. Our
approach can be generalized to other diseases as well as to other clinical
questions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1810.11227v1,"From the EM Algorithm to the CM-EM Algorithm for Global Convergence of
  Mixture Models","The Expectation-Maximization (EM) algorithm for mixture models often results
in slow or invalid convergence. The popular convergence proof affirms that the
likelihood increases with Q; Q is increasing in the M -step and non-decreasing
in the E-step. The author found that (1) Q may and should decrease in some
E-steps; (2) The Shannon channel from the E-step is improper and hence the
expectation is improper. The author proposed the CM-EM algorithm (CM means
Channel's Matching), which adds a step to optimize the mixture ratios for the
proper Shannon channel and maximizes G, average log-normalized-likelihood, in
the M-step. Neal and Hinton's Maximization-Maximization (MM) algorithm use F
instead of Q to speed the convergence. Maximizing G is similar to maximizing F.
The new convergence proof is similar to Beal's proof with the variational
method. It first proves that the minimum relative entropy equals the minimum
R-G (R is mutual information), then uses variational and iterative methods that
Shannon et al. use for rate-distortion functions to prove the global
convergence. Some examples show that Q and F should and may decrease in some
E-steps. For the same example, the EM, MM, and CM-EM algorithms need about 36,
18, and 9 iterations respectively.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,0,0,0,1,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.11059v1,Automatic Quantification of Facial Asymmetry using Facial Landmarks,"One-sided facial paralysis causes uneven movements of facial muscles on the
sides of the face. Physicians currently assess facial asymmetry in a subjective
manner based on their clinical experience. This paper proposes a novel method
to provide an objective and quantitative asymmetry score for frontal faces. Our
metric has the potential to help physicians for diagnosis as well as monitoring
the rehabilitation of patients with one-sided facial paralysis. A deep learning
based landmark detection technique is used to estimate style invariant facial
landmark points and dense optical flow is used to generate motion maps from a
short sequence of frames. Six face regions are considered corresponding to the
left and right parts of the forehead, eyes, and mouth. Motion is computed and
compared between the left and the right parts of each region of interest to
estimate the symmetry score. For testing, asymmetric sequences are
synthetically generated from a facial expression dataset. A score equation is
developed to quantify symmetry in both symmetric and asymmetric face sequences.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0405097v1,A Coalgebraic Approach to Kleene Algebra with Tests,"Kleene algebra with tests is an extension of Kleene algebra, the algebra of
regular expressions, which can be used to reason about programs. We develop a
coalgebraic theory of Kleene algebra with tests, along the lines of the
coalgebraic theory of regular expressions based on deterministic automata.
Since the known automata-theoretic presentation of Kleene algebra with tests
does not lend itself to a coalgebraic theory, we define a new interpretation of
Kleene algebra with tests expressions and a corresponding automata-theoretic
presentation. One outcome of the theory is a coinductive proof principle, that
can be used to establish equivalence of our Kleene algebra with tests
expressions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0809.1330v1,"Low-Complexity Coding and Source-Optimized Clustering for Large-Scale
  Sensor Networks","We consider the distributed source coding problem in which correlated data
picked up by scattered sensors has to be encoded separately and transmitted to
a common receiver, subject to a rate-distortion constraint. Although
near-tooptimal solutions based on Turbo and LDPC codes exist for this problem,
in most cases the proposed techniques do not scale to networks of hundreds of
sensors. We present a scalable solution based on the following key elements:
(a) distortion-optimized index assignments for low-complexity distributed
quantization, (b) source-optimized hierarchical clustering based on the
Kullback-Leibler distance and (c) sum-product decoding on specific factor
graphs exploiting the correlation of the data.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0405102v1,A Proof Theoretic Approach to Failure in Functional Logic Programming,"How to extract negative information from programs is an important issue in
logic programming. Here we address the problem for functional logic programs,
from a proof-theoretic perspective. The starting point of our work is CRWL
(Constructor based ReWriting Logic), a well established theoretical framework
for functional logic programming, whose fundamental notion is that of
non-strict non-deterministic function. We present a proof calculus, CRWLF,
which is able to deduce negative information from CRWL-programs. In particular,
CRWLF is able to prove finite failure of reduction within CRWL.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.04249v1,Experimentation with MANETs of Smartphones,"Mobile AdHoc NETworks (MANETs) have been identified as a key emerging
technology for scenarios in which IEEE 802.11 or cellular communications are
either infeasible, inefficient, or cost-ineffective. Smartphones are the most
adequate network nodes in many of these scenarios, but it is not
straightforward to build a network with them. We extensively survey existing
possibilities to build applications on top of ad-hoc smartphone networks for
experimentation purposes, and introduce a taxonomy to classify them. We present
AdHocDroid, an Android package that creates an IP-level MANET of (rooted)
Android smartphones, and make it publicly available to the community.
AdHocDroid supports standard TCP/IP applications, providing real smartphone
IEEE 802.11 MANET and the capability to easily change the routing protocol. We
tested our framework on several smartphones and a laptop. We validate the MANET
running off-the-shelf applications, and reporting on experimental performance
evaluation, including network metrics and battery discharge rate.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1009.6105v1,"The Baire partial quasi-metric space: A mathematical tool for asymptotic
  complexity analysis in Computer Science","In 1994, S.G. Matthews introduced the notion of partial metric space in order
to obtain a suitable mathematical tool for program verification [Ann. New York
Acad. Sci. 728 (1994), 183-197]. He gave an application of this new structure
to parallel computing by means of a partial metric version of the celebrated
Banach fixed point theorem [Theoret. Comput. Sci. 151 (1995), 195-205]. Later
on, M.P. Schellekens introduced the theory of complexity (quasi-metric) spaces
as a part of the development of a topological foundation for the asymptotic
complexity analysis of programs and algorithms [Elec- tronic Notes in Theoret.
Comput. Sci. 1 (1995), 211-232]. The applicability of this theory to the
asymptotic complexity analysis of Divide and Conquer algorithms was also
illustrated by Schellekens. In particular, he gave a new proof, based on the
use of the aforenamed Banach fixed point theorem, of the well-known fact that
Mergesort al- gorithm has optimal asymptotic average running time of computing.
In this paper, motivated by the utility of partial metrics in Computer Science,
we discuss whether the Matthews fixed point theorem is a suitable tool to
analyze the asymptotic complexity of algorithms in the spirit of Schellekens.
Specifically, we show that a slight modification of the well-known Baire
partial metric on the set of all words over an alphabet constitutes an
appropriate tool to carry out the asymptotic complexity analysis of algorithms
via fixed point methods without the need for assuming the convergence condition
inherent to the defini- tion of the complexity space in the Shellekens
framework. Finally, in order to illustrate and to validate the developed theory
we apply our results to analyze the asymptotic complexity of Quicksort,
Mergesort and Largesort algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0407046v1,A Bimachine Compiler for Ranked Tagging Rules,"This paper describes a novel method of compiling ranked tagging rules into a
deterministic finite-state device called a bimachine. The rules are formulated
in the framework of regular rewrite operations and allow unrestricted regular
expressions in both left and right rule contexts. The compiler is illustrated
by an application within a speech synthesis system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0504100v6,A DNA Sequence Compression Algorithm Based on LUT and LZ77,"This article introduces a new DNA sequence compression algorithm which is
based on LUT and LZ77 algorithm. Combined a LUT-based pre-coding routine and
LZ77 compression routine,this algorithm can approach a compression ratio of
1.9bits \slash base and even lower.The biggest advantage of this algorithm is
fast execution, small memory occupation and easy implementation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.04191v2,Exploiting Database Management Systems and Treewidth for Counting,"Bounded treewidth is one of the most cited combinatorial invariants, which
was applied in the literature for solving several counting problems
efficiently. A canonical counting problem is #SAT, which asks to count the
satisfying assignments of a Boolean formula. Recent work shows that
benchmarking instances for #SAT often have reasonably small treewidth. This
paper deals with counting problems for instances of small treewidth. We
introduce a general framework to solve counting questions based on
state-of-the-art database management systems (DBMS). Our framework takes
explicitly advantage of small treewidth by solving instances using dynamic
programming (DP) on tree decompositions (TD). Therefore, we implement the
concept of DP into a DBMS (PostgreSQL), since DP algorithms are already often
given in terms of table manipulations in theory. This allows for elegant
specifications of DP algorithms and the use of SQL to manipulate records and
tables, which gives us a natural approach to bring DP algorithms into practice.
To the best of our knowledge, we present the first approach to employ a DBMS
for algorithms on TDs. A key advantage of our approach is that DBMS naturally
allow to deal with huge tables with a limited amount of main memory (RAM),
parallelization, as well as suspending computation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.03466v1,Unsupervised Portrait Shadow Removal via Generative Priors,"Portrait images often suffer from undesirable shadows cast by casual objects
or even the face itself. While existing methods for portrait shadow removal
require training on a large-scale synthetic dataset, we propose the first
unsupervised method for portrait shadow removal without any training data. Our
key idea is to leverage the generative facial priors embedded in the
off-the-shelf pretrained StyleGAN2. To achieve this, we formulate the shadow
removal task as a layer decomposition problem: a shadowed portrait image is
constructed by the blending of a shadow image and a shadow-free image. We
propose an effective progressive optimization algorithm to learn the
decomposition process. Our approach can also be extended to portrait tattoo
removal and watermark removal. Qualitative and quantitative experiments on a
real-world portrait shadow dataset demonstrate that our approach achieves
comparable performance with supervised shadow removal methods. Our source code
is available at
https://github.com/YingqingHe/Shadow-Removal-via-Generative-Priors.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1103.6246v2,Sparse Vector Distributions and Recovery from Compressed Sensing,"It is well known that the performance of sparse vector recovery algorithms
from compressive measurements can depend on the distribution underlying the
non-zero elements of a sparse vector. However, the extent of these effects has
yet to be explored, and formally presented. In this paper, I empirically
investigate this dependence for seven distributions and fifteen recovery
algorithms. The two morals of this work are: 1) any judgement of the recovery
performance of one algorithm over that of another must be prefaced by the
conditions for which this is observed to be true, including sparse vector
distributions, and the criterion for exact recovery; and 2) a recovery
algorithm must be selected carefully based on what distribution one expects to
underlie the sensed sparse signal.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.12380v2,"International Migration in Academia and Citation Performance: An
  Analysis of German-Affiliated Researchers by Gender and Discipline Using
  Scopus Publications 1996-2020","Germany has become a major country of immigration, as well as a research
powerhouse in Europe. As Germany spends a higher fraction of its GDP on
research and development than most countries with advanced economies, there is
an expectation that Germany should be able to attract and retain international
scholars who have high citation performance. Using an exhaustive set of over
eight million Scopus publications, we analyze the trends in international
migration to and from Germany among published researchers over the past 24
years. We assess changes in institutional affiliations for over one million
researchers who have published with a German affiliation address at some point
during the 1996-2020 period. We show that while Germany has been highly
integrated into the global movement of researchers, with particularly strong
ties to the US, the UK, and Switzerland, the country has been sending more
published researchers abroad than it has attracted. While the balance has been
largely negative over time, analyses disaggregated by gender, citation
performance, and field of research show that compositional differences in
migrant flows may help to alleviate persistent gender inequalities in selected
fields.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2001.08586v1,Integrated Mobile Solutions in an Internet-of-Things Development Model,"The Internet-of-Things (IoT) is a revolutionary technology that is rapidly
changing the world. IoT systems strive to provide automated solutions for
almost every life aspect; traditional devices are becoming connected,
ubiquitous, pervasive, wireless, context-aware, smart, controlled through
mobile solutions, to name but a few. IoT devices can now be found in our
apartments, places of work, cars, buildings, and in almost every aspect of
life. In this investigation, we propose an IoT system Development Model (IDM).
The proposed IDM enables the development of IoT systems from concept to
prototyping. The model comprises concept refinement pyramids, decision trees,
realistic constraint lists, architecture and organization diagrams,
communication interface patterns, use cases, and menus of analysis metrics and
evaluation indicators. The investigation confirms that the proposed model
enjoys several properties, such as, clarity, conciseness, thoroughness,
productivity, etc. The model is deployed for a variety of systems that belong
to heterogeneous areas of application; the model is proven to be effective in
application and successful in integrating mobile solutions. This chapter
includes the presentation of the IDM submodels, the reasoning about their
usefulness, and the technical developments of several systems. The chapter
includes thorough discussions, analysis of the model usability and application,
and in-depth evaluations.",0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
http://arxiv.org/abs/cs/9907028v1,Further Results on Arithmetic Filters for Geometric Predicates,"An efficient technique to solve precision problems consists in using exact
computations. For geometric predicates, using systematically expensive exact
computations can be avoided by the use of filters. The predicate is first
evaluated using rounding computations, and an error estimation gives a
certificate of the validity of the result. In this note, we studies the
statistical efficiency of filters for cosphericity predicate with an assumption
of regular distribution of the points. We prove that the expected value of the
polynomial corresponding to the in sphere test is greater than epsilon with
probability O(epsilon log 1/epsilon) improving the results of a previous paper
by the same authors.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0107033v1,Yet another zeta function and learning,"We study the convergence speed of the batch learning algorithm, and compare
its speed to that of the memoryless learning algorithm and of learning with
memory (as analyzed in joint work with N. Komarova). We obtain precise results
and show in particular that the batch learning algorithm is never worse than
the memoryless learning algorithm (at least asymptotically). Its performance
vis-a-vis learning with full memory is less clearcut, and depends on
certainprobabilistic assumptions. These results necessitate theintroduction of
the moment zeta function of a probability distribution and the study of some of
its properties.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.05772v2,Persistently Exciting Tube MPC,"This paper presents a new approach to deal with the dual problem of system
identification and regulation. The main feature consists of breaking the
control input to the system into a regulator part and a persistently exciting
part. The former is used to regulate the plant using a robust MPC formulation,
in which the latter is treated as a bounded additive disturbance. The
identification process is executed by a simple recursive least squares
algorithm. In order to guarantee sufficient excitation for the identification,
an additional non-convex constraint is enforced over the persistently exciting
part.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.09257v3,"Inspecting state of the art performance and NLP metrics in image-based
  medical report generation","Several deep learning architectures have been proposed over the last years to
deal with the problem of generating a written report given an imaging exam as
input. Most works evaluate the generated reports using standard Natural
Language Processing (NLP) metrics (e.g. BLEU, ROUGE), reporting significant
progress. In this article, we contrast this progress by comparing state of the
art (SOTA) models against weak baselines. We show that simple and even naive
approaches yield near SOTA performance on most traditional NLP metrics. We
conclude that evaluation methods in this task should be further studied towards
correctly measuring clinical accuracy, ideally involving physicians to
contribute to this end.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0708.0741v1,Characterising Web Site Link Structure,"The topological structures of the Internet and the Web have received
considerable attention. However, there has been little research on the
topological properties of individual web sites. In this paper, we consider
whether web sites (as opposed to the entire Web) exhibit structural
similarities. To do so, we exhaustively crawled 18 web sites as diverse as
governmental departments, commercial companies and university departments in
different countries. These web sites consisted of as little as a few thousand
pages to millions of pages. Statistical analysis of these 18 sites revealed
that the internal link structure of the web sites are significantly different
when measured with first and second-order topological properties, i.e.
properties based on the connectivity of an individual or a pairs of nodes.
However, examination of a third-order topological property that consider the
connectivity between three nodes that form a triangle, revealed a strong
correspondence across web sites, suggestive of an invariant. Comparison with
the Web, the AS Internet, and a citation network, showed that this third-order
property is not shared across other types of networks. Nor is the property
exhibited in generative network models such as that of Barabasi and Albert.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.03043v1,"LUXOR: An FPGA Logic Cell Architecture for Efficient Compressor Tree
  Implementations","We propose two tiers of modifications to FPGA logic cell architecture to
deliver a variety of performance and utilization benefits with only minor area
overheads. In the irst tier, we augment existing commercial logic cell
datapaths with a 6-input XOR gate in order to improve the expressiveness of
each element, while maintaining backward compatibility. This new architecture
is vendor-agnostic, and we refer to it as LUXOR. We also consider a secondary
tier of vendor-speciic modifications to both Xilinx and Intel FPGAs, which we
refer to as X-LUXOR+ and I-LUXOR+ respectively. We demonstrate that compressor
tree synthesis using generalized parallel counters (GPCs) is further improved
with the proposed modifications. Using both the Intel adaptive logic module and
the Xilinx slice at the 65nm technology node for a comparative study, it is
shown that the silicon area overhead is less than 0.5% for LUXOR and 5-6% for
LUXOR+, while the delay increments are 1-6% and 3-9% respectively. We
demonstrate that LUXOR can deliver an average reduction of 13-19% in logic
utilization on micro-benchmarks from a variety of domains.BNN benchmarks
benefit the most with an average reduction of 37-47% in logic utilization,
which is due to the highly-efficient mapping of the XnorPopcount operation on
our proposed LUXOR+ logic cells.",0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1003.4418v1,Evaluation of Query Generators for Entity Search Engines,"Dynamic web applications such as mashups need efficient access to web data
that is only accessible via entity search engines (e.g. product or publication
search engines). However, most current mashup systems and applications only
support simple keyword searches for retrieving data from search engines. We
propose the use of more powerful search strategies building on so-called query
generators. For a given set of entities query generators are able to
automatically determine a set of search queries to retrieve these entities from
an entity search engine. We demonstrate the usefulness of query generators for
on-demand web data integration and evaluate the effectiveness and efficiency of
query generators for a challenging real-world integration scenario.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0412069v1,Swarming around Shellfish Larvae,"The collection of wild larvae seed as a source of raw material is a major sub
industry of shellfish aquaculture. To predict when, where and in what
quantities wild seed will be available, it is necessary to track the appearance
and growth of planktonic larvae. One of the most difficult groups to identify,
particularly at the species level are the Bivalvia. This difficulty arises from
the fact that fundamentally all bivalve larvae have a similar shape and colour.
Identification based on gross morphological appearance is limited by the
time-consuming nature of the microscopic examination and by the limited
availability of expertise in this field. Molecular and immunological methods
are also being studied. We describe the application of computational pattern
recognition methods to the automated identification and size analysis of
scallop larvae. For identification, the shape features used are binary
invariant moments; that is, the features are invariant to shift (position
within the image), scale (induced either by growth or differential image
magnification) and rotation. Images of a sample of scallop and non-scallop
larvae covering a range of maturities have been analysed. In order to overcome
the automatic identification, as well as to allow the system to receive new
unknown samples at any moment, a self-organized and unsupervised ant-like
clustering algorithm based on Swarm Intelligence is proposed, followed by
simple k-NNR nearest neighbour classification on the final map. Results achieve
a full recognition rate of 100% under several situations (k =1 or 3).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1112.2141v3,"Resolving Gdel's Incompleteness Myth: Polynomial Equations and
  Dynamical Systems for Algebraic Logic","A new computational method that uses polynomial equations and dynamical
systems to evaluate logical propositions is introduced and applied to Goedel's
incompleteness theorems. The truth value of a logical formula subject to a set
of axioms is computed from the solution to the corresponding system of
polynomial equations. A reference by a formula to its own provability is shown
to be a recurrence relation, which can be either interpreted as such to
generate a discrete dynamical system, or interpreted in a static way to create
an additional simultaneous equation. In this framework the truth values of
logical formulas and other polynomial objectives have complex data structures:
sets of elementary values, or dynamical systems that generate sets of infinite
sequences of such solution-value sets. Besides the routine result that a
formula has a definite elementary value, these data structures encode several
exceptions: formulas that are ambiguous, unsatisfiable, unsteady, or
contingent. These exceptions represent several semantically different types of
undecidability; none causes any fundamental problem for mathematics. It is
simple to calculate that Goedel's formula, which asserts that it cannot be
proven, is exceptional in specific ways: interpreted statically, the formula
defines an inconsistent system of equations (thus it is called unsatisfiable);
interpreted dynamically, it defines a dynamical system that has a periodic
orbit and no fixed point (thus it is called unsteady). These exceptions are not
catastrophic failures of logic; they are accurate mathematical descriptions of
Goedel's self-referential construction. Goedel's analysis does not reveal any
essential incompleteness in formal reasoning systems, nor any barrier to
proving the consistency of such systems by ordinary mathematical means.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1103.2068v2,COMET: A Recipe for Learning and Using Large Ensembles on Massive Data,"COMET is a single-pass MapReduce algorithm for learning on large-scale data.
It builds multiple random forest ensembles on distributed blocks of data and
merges them into a mega-ensemble. This approach is appropriate when learning
from massive-scale data that is too large to fit on a single machine. To get
the best accuracy, IVoting should be used instead of bagging to generate the
training subset for each decision tree in the random forest. Experiments with
two large datasets (5GB and 50GB compressed) show that COMET compares favorably
(in both accuracy and training time) to learning on a subsample of data using a
serial algorithm. Finally, we propose a new Gaussian approach for lazy ensemble
evaluation which dynamically decides how many ensemble members to evaluate per
data point; this can reduce evaluation cost by 100X or more.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.10733v2,"Graph Neural Networks: Methods, Applications, and Opportunities","In the last decade or so, we have witnessed deep learning reinvigorating the
machine learning field. It has solved many problems in the domains of computer
vision, speech recognition, natural language processing, and various other
tasks with state-of-the-art performance. The data is generally represented in
the Euclidean space in these domains. Various other domains conform to
non-Euclidean space, for which graph is an ideal representation. Graphs are
suitable for representing the dependencies and interrelationships between
various entities. Traditionally, handcrafted features for graphs are incapable
of providing the necessary inference for various tasks from this complex data
representation. Recently, there is an emergence of employing various advances
in deep learning to graph data-based tasks. This article provides a
comprehensive survey of graph neural networks (GNNs) in each learning setting:
supervised, unsupervised, semi-supervised, and self-supervised learning.
Taxonomy of each graph based learning setting is provided with logical
divisions of methods falling in the given learning setting. The approaches for
each learning task are analyzed from both theoretical as well as empirical
standpoints. Further, we provide general architecture guidelines for building
GNNs. Various applications and benchmark datasets are also provided, along with
open challenges still plaguing the general applicability of GNNs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0205048v2,Huffman Coding with Letter Costs: A Linear-Time Approximation Scheme,"We give a polynomial-time approximation scheme for the generalization of
Huffman Coding in which codeword letters have non-uniform costs (as in Morse
code, where the dash is twice as long as the dot). The algorithm computes a
(1+epsilon)-approximate solution in time O(n + f(epsilon) log^3 n), where n is
the input size.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1904.13273v1,Detecting Reflections by Combining Semantic and Instance Segmentation,"Reflections in natural images commonly cause false positives in automated
detection systems. These false positives can lead to significant impairment of
accuracy in the tasks of detection, counting and segmentation. Here, inspired
by the recent panoptic approach to segmentation, we show how fusing instance
and semantic segmentation can automatically identify reflection false
positives, without explicitly needing to have the reflective regions labelled.
We explore in detail how state of the art two-stage detectors suffer a loss of
broader contextual features, and hence are unable to learn to ignore these
reflections. We then present an approach to fuse instance and semantic
segmentations for this application, and subsequently show how this reduces
false positive detections in a real world surveillance data with a large number
of reflective surfaces. This demonstrates how panoptic segmentation and related
work, despite being in its infancy, can already be useful in real world
computer vision problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.04024v3,6G Communication: Envisioning the Key Issues and Challenges,"In 2030, we are going to evidence the 6G mobile communication technology,
which will enable the Internet of Everything. Yet 5G has to be experienced by
people worldwide and B5G has to be developed; the researchers have already
started planning, visioning, and gathering requirements of the 6G. Moreover,
many countries have already initiated the research on 6G. 6G promises
connecting every smart device to the Internet from smartphone to intelligent
vehicles. 6G will provide sophisticated and high QoS such as holographic
communication, augmented reality/virtual reality and many more. Also, it will
focus on Quality of Experience (QoE) to provide rich experiences from 6G
technology. Notably, it is very important to vision the issues and challenges
of 6G technology, otherwise, promises may not be delivered on time. The
requirements of 6G poses new challenges to the research community. To achieve
desired parameters of 6G, researchers are exploring various alternatives.
Hence, there are diverse research challenges to envision, from devices to
softwarization. Therefore, in this article, we discuss the future issues and
challenges to be faced by the 6G technology. We have discussed issues and
challenges from every aspect from hardware to the enabling technologies which
will be utilized by 6G.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1610.03704v1,A low-cost indoor and outdoor terrestrial autonomous navigation model,"In this paper, a method for low-cost system design oriented to indoor and
outdoor autonomous navigation is illustrated. In order to provide a motivation
for the solution here presented, a brief discussion of the typical drawbacks of
state-of-the-art technologies is reported. Finally, an application of such a
method for the design of a navigation system for blindfolded people is shown.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1910.07133v2,"Design of a Simple Orthogonal Multiwavelet Filter by Matrix Spectral
  Factorization","We consider the design of an orthogonal symmetric/antisymmetric multiwavelet
from its matrix product filter by matrix spectral factorization (MSF). As a
test problem, we construct a simple matrix product filter with desirable
properties, and factor it using Bauer's method, which in this case can be done
in closed form. The corresponding orthogonal multiwavelet function is derived
using algebraic techniques which allow symmetry to be considered. This leads to
the known orthogonal multiwavelet SA1, which can also be derived directly. We
also give a lifting scheme for SA1, investigate the influence of the number of
significant digits in the calculations, and show some numerical experiments.",0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
http://arxiv.org/abs/1107.1200v1,Time Delays in Membrane Systems and Petri Nets,"Timing aspects in formalisms with explicit resources and parallelism are
investigated, and it is presented a formal link between timed membrane systems
and timed Petri nets with localities. For both formalisms, timing does not
increase the expressive power; however both timed membrane systems and timed
Petri nets are more flexible in describing molecular phenomena where time is a
critical resource. We establish a link between timed membrane systems and timed
Petri nets with localities, and prove an operational correspondence between
them.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0105031v1,State Analysis and Aggregation Study for Multicast-based Micro Mobility,"IP mobility addresses the problem of changing the network point-of-attachment
transparently during movement. Mobile IP is the proposed standard by IETF.
Several studies, however, have shown that Mobile IP has several drawbacks, such
as triangle routing and poor handoff performance. Multicast-based mobility has
been proposed as a promising solution to the above problems, incurring less
end-to-end delays and fast smooth handoff. Nonetheless, such architecture
suffers from multicast state scalability problems with the growth in number of
mobile nodes. This architecture also requires ubiquitous multicast deployment
and more complex security measures. To alleviate these problems, we propose an
intra-domain multicast-based mobility solution. A mobility proxy allocates a
multicast address for each mobile that moves to its domain. The mobile uses
this multicast address within a domain for micro mobility. Also, aggregation is
considered to reduce the multicast state. We conduct multicast state analysis
to study the efficiency of several aggregation techniques. We use extensive
simulation to evaluate our protocol's performance over a variety of real and
generated topologies. We take aggregation gain as metric for our evaluation.
  Our simulation results show that in general leaky aggregation obtains better
gains than perfect aggregation. Also, we notice that aggregation gain increases
with the increase in number of visiting mobile nodes and with the decrease in
number of mobility proxies within a domain.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1401.4879v2,"On the Computing Power of $+$, $-$, and $\times$","Modify the Blum-Shub-Smale model of computation replacing the permitted
computational primitives (the real field operations) with any finite set $B$ of
real functions semialgebraic over the rationals. Consider the class of boolean
decision problems that can be solved in polynomial time in the new model by
machines with no machine constants. How does this class depend on $B$? We prove
that it is always contained in the class obtained for $B = \{+, -, \times\}$.
Moreover, if $B$ is a set of continuous semialgebraic functions containing $+$
and $-$, and such that arbitrarily small numbers can be computed using $B$,
then we have the following dichotomy: either our class is $\mathsf P$ or it
coincides with the class obtained for $B = \{+, -, \times\}$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0911.4985v1,A Type System for a Stochastic CLS,"The Stochastic Calculus of Looping Sequences is suitable to describe the
evolution of microbiological systems, taking into account the speed of the
described activities. We propose a type system for this calculus that models
how the presence of positive and negative catalysers can modify these speeds.
We claim that types are the right abstraction in order to represent the
interaction between elements without specifying exactly the element positions.
Our claim is supported through an example modelling the lactose operon.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0906.5089v1,Comparing and Aggregating Partially Resolved Trees,"We define, analyze, and give efficient algorithms for two kinds of distance
measures for rooted and unrooted phylogenies. For rooted trees, our measures
are based on the topologies the input trees induce on triplets; that is, on
three-element subsets of the set of species. For unrooted trees, the measures
are based on quartets (four-element subsets). Triplet and quartet-based
distances provide a robust and fine-grained measure of the similarities between
trees. The distinguishing feature of our distance measures relative to
traditional quartet and triplet distances is their ability to deal cleanly with
the presence of unresolved nodes, also called polytomies. For rooted trees,
these are nodes with more than two children; for unrooted trees, they are nodes
of degree greater than three.
  Our first class of measures are parametric distances, where there is a
parameter that weighs the difference between an unresolved triplet/quartet
topology and a resolved one. Our second class of measures are based on
Hausdorff distance. Each tree is viewed as a set of all possible ways in which
the tree could be refined to eliminate unresolved nodes. The distance between
the original (unresolved) trees is then taken to be the Hausdorff distance
between the associated sets of fully resolved trees, where the distance between
trees in the sets is the triplet or quartet distance, as appropriate.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1712.05792v1,"Inferring hierarchical structure of spatial and generic complex networks
  through a modeling framework","Our recent paper [Grauwin et al. Sci. Rep. 7 (2017)] demonstrates that
community and hierarchical structure of the networks of human interactions
largely determines the least and should be taken into account while modeling
them. In the present proof-of-concept pre-print the opposite question is
considered: could the hierarchical structure itself be inferred to be best
aligned with the network model? The inference mechanism is provided for both -
spatial networks as well as complex networks in general - through a model based
on hierarchical and (if defined) geographical distances. The mechanism allows
to discover hierarchical and community structure at any desired resolution in
complex networks and in particular - the space-independent structure of the
spatial networks. The approach is illustrated on the example of the interstate
people migration network in USA.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1806.04298v2,Collective Story Writing through Linking Images,"Collaborative creativity is the approach of employing crowd to accomplish
creative tasks. In this paper, we present a collaborative crowdsourcing
platform for writing stories by means of connecting a series of `images'. These
connected images are termed as Image Chains, reflecting successive scenarios.
Users can either start or extend an Image Chain by uploading their own image or
choosing from the available ones. These users are allowed to pen their stories
from the Image Chains. Finally, stories get published based on the number of
votes obtained. This provides an organized framework of story writing unlike
most of the state-of-the-art collaborative editing platforms. Our experiments
on 25 contributors highlight their interest in growing shorter Image Chains but
voting longer Image Chains.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.6645v3,Weakly Supervised Multi-Embeddings Learning of Acoustic Models,"We trained a Siamese network with multi-task same/different information on a
speech dataset, and found that it was possible to share a network for both
tasks without a loss in performance. The first task was to discriminate between
two same or different words, and the second was to discriminate between two
same or different talkers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.13997v1,"Good Counterfactuals and Where to Find Them: A Case-Based Technique for
  Generating Counterfactuals for Explainable AI (XAI)","Recently, a groundswell of research has identified the use of counterfactual
explanations as a potentially significant solution to the Explainable AI (XAI)
problem. It is argued that (a) technically, these counterfactual cases can be
generated by permuting problem-features until a class change is found, (b)
psychologically, they are much more causally informative than factual
explanations, (c) legally, they are GDPR-compliant. However, there are issues
around the finding of good counterfactuals using current techniques (e.g.
sparsity and plausibility). We show that many commonly-used datasets appear to
have few good counterfactuals for explanation purposes. So, we propose a new
case based approach for generating counterfactuals using novel ideas about the
counterfactual potential and explanatory coverage of a case-base. The new
technique reuses patterns of good counterfactuals, present in a case-base, to
generate analogous counterfactuals that can explain new problems and their
solutions. Several experiments show how this technique can improve the
counterfactual potential and explanatory coverage of case-bases that were
previously found wanting.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.01988v1,"Dynamic QoS-Aware Traffic Planning for Time-Triggered Flows with
  Conflict Graphs","Many networked applications, e.g., in the domain of cyber-physical systems,
require strict service guarantees, usually in the form of jitter and latency
bounds, for time-triggered traffic flows. It is a notoriously hard problem to
compute a network-wide traffic plan that satisfies these requirements, and
dynamic changes in the flow set add even more challenges. Existing
traffic-planning methods are ill-suited for dynamic scenarios because they
either suffer from high computational cost, can result in low network
utilization, or provide no explicit guarantees when transitioning to a new
traffic plan that incorporates new flows.
  Therefore, we present a novel approach for dynamic traffic planning of
time-triggered flows. Our conflict-graph based modeling of the traffic planning
problem allows to reconfigure active flows to increase the network utilization,
while also providing per-flow QoS guarantees during the transition to the new
traffic plan. Additionally, we introduce a novel heuristic for computing the
new traffic plans. Evaluations of our prototypical implementation show that we
can efficiently compute new traffic plans in scenarios with hundreds of active
flows for a wide range of scenarios.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.06927v1,"Linnaeus: A highly reusable and adaptable ML based log classification
  pipeline","Logs are a common way to record detailed run-time information in software. As
modern software systems evolve in scale and complexity, logs have become
indispensable to understanding the internal states of the system. At the same
time however, manually inspecting logs has become impractical. In recent times,
there has been more emphasis on statistical and machine learning (ML) based
methods for analyzing logs. While the results have shown promise, most of the
literature focuses on algorithms and state-of-the-art (SOTA), while largely
ignoring the practical aspects. In this paper we demonstrate our end-to-end log
classification pipeline, Linnaeus. Besides showing the more traditional ML
flow, we also demonstrate our solutions for adaptability and re-use,
integration towards large scale software development processes, and how we cope
with lack of labelled data. We hope Linnaeus can serve as a blueprint for, and
inspire the integration of, various ML based solutions in other large scale
industrial settings.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.07327v1,"Dynamic Analysis of the Arrow Distributed Directory Protocol in General
  Networks","The Arrow protocol is a simple and elegant protocol to coordinate exclusive
access to a shared object in a network. The protocol solves the underlying
distributed queueing problem by using path reversal on a pre-computed spanning
tree (or any other tree topology simulated on top of the given network).
  It is known that the Arrow protocol solves the problem with a competitive
ratio of O(log D) on trees of diameter D. This implies a distributed queueing
algorithm with competitive ratio O(s*log D) for general networks with a
spanning tree of diameter D and stretch s. In this work we show that when
running the Arrow protocol on top of the well-known probabilistic tree
embedding of Fakcharoenphol, Rao, and Talwar [STOC 03], we obtain a randomized
distributed queueing algorithm with a competitive ratio of O(log n) even on
general network topologies. The result holds even if the queueing requests
occur in an arbitrarily dynamic and concurrent fashion and even if
communication is asynchronous. From a technical point of view, the main of the
paper shows that the competitive ratio of the Arrow protocol is constant on a
special family of tree topologies, known as hierarchically well separated
trees.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1608.08689v2,Deleting Powers in Words,"We consider the language consisting of all words such that it is possible to
obtain the empty word by iteratively deleting powers. It turns out that in the
case of deleting squares in binary words this language is regular, and in the
case of deleting squares in words over a larger alphabet the language is not
regular. However, for deleting squares over any alphabet we find that this
language can be generated by a linear index grammar which is a mildly context
sensitive grammar formalism. In the general case we show that this language is
generated by an indexed grammar.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1708.08731v1,Active Learning of Input Grammars,"Knowing the precise format of a program's input is a necessary prerequisite
for systematic testing. Given a program and a small set of sample inputs, we
(1) track the data flow of inputs to aggregate input fragments that share the
same data flow through program execution into lexical and syntactic entities;
(2) assign these entities names that are based on the associated variable and
function identifiers; and (3) systematically generalize production rules by
means of membership queries. As a result, we need only a minimal set of sample
inputs to obtain human-readable context-free grammars that reflect valid input
structure. In our evaluation on inputs like URLs, spreadsheets, or
configuration files, our AUTOGRAM prototype obtains input grammars that are
both accurate and very readable - and that can be directly fed into test
generators for comprehensive automated testing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.12702v1,TagLab: A human-centric AI system for interactive semantic segmentation,"Fully automatic semantic segmentation of highly specific semantic classes and
complex shapes may not meet the accuracy standards demanded by scientists. In
such cases, human-centered AI solutions, able to assist operators while
preserving human control over complex tasks, are a good trade-off to speed up
image labeling while maintaining high accuracy levels. TagLab is an open-source
AI-assisted software for annotating large orthoimages which takes advantage of
different degrees of automation; it speeds up image annotation from scratch
through assisted tools, creates custom fully automatic semantic segmentation
models, and, finally, allows the quick edits of automatic predictions. Since
the orthoimages analysis applies to several scientific disciplines, TagLab has
been designed with a flexible labeling pipeline. We report our results in two
different scenarios, marine ecology, and architectural heritage.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.01330v1,Detecting Temporal Correlation via Quantum Random Number Generation,"All computing devices, including quantum computers, must exhibit that for a
given input, an output is produced in accordance with the program. The outputs
generated by quantum computers that fulfill these requirements are not
temporally correlated, however. In a quantum-computing device comprising
solid-state qubits such as superconducting qubits, any operation to rest the
qubits to their initial state faces a practical problem. We applied a
statistical analysis to a collection of random numbers output from a 20-qubit
superconducting-qubit cloud quantum computer using the simplest random number
generation scheme. The analysis indicates temporal correlation in the output of
some sequences obtained from the 20 qubits. This temporal correlation is not
related to the relaxation time of each qubit. We conclude that the correlation
could be a result of a systematic error.",0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0304018v2,Quasiconvex Analysis of Backtracking Algorithms,"We consider a class of multivariate recurrences frequently arising in the
worst case analysis of Davis-Putnam-style exponential time backtracking
algorithms for NP-hard problems. We describe a technique for proving asymptotic
upper bounds on these recurrences, by using a suitable weight function to
reduce the problem to that of solving univariate linear recurrences; show how
to use quasiconvex programming to determine the weight function yielding the
smallest upper bound; and prove that the resulting upper bounds are within a
polynomial factor of the true asymptotics of the recurrence. We develop and
implement a multiple-gradient descent algorithm for the resulting quasiconvex
programs, using a real-number arithmetic package for guaranteed accuracy of the
computed worst case time bounds.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.13580v1,"Deep Learning-Based Automatic Detection of Poorly Positioned Mammograms
  to Minimize Patient Return Visits for Repeat Imaging: A Real-World
  Application","Screening mammograms are a routine imaging exam performed to detect breast
cancer in its early stages to reduce morbidity and mortality attributed to this
disease. In order to maximize the efficacy of breast cancer screening programs,
proper mammographic positioning is paramount. Proper positioning ensures
adequate visualization of breast tissue and is necessary for effective breast
cancer detection. Therefore, breast-imaging radiologists must assess each
mammogram for the adequacy of positioning before providing a final
interpretation of the examination; this often necessitates return patient
visits for additional imaging. In this paper, we propose a deep
learning-algorithm method that mimics and automates this decision-making
process to identify poorly positioned mammograms. Our objective for this
algorithm is to assist mammography technologists in recognizing inadequately
positioned mammograms real-time, improve the quality of mammographic
positioning and performance, and ultimately reducing repeat visits for patients
with initially inadequate imaging. The proposed model showed a true positive
rate for detecting correct positioning of 91.35% in the mediolateral oblique
view and 95.11% in the craniocaudal view. In addition to these results, we also
present an automatically generated report which can aid the mammography
technologist in taking corrective measures during the patient visit.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.04943v1,State-Relabeling Adversarial Active Learning,"Active learning is to design label-efficient algorithms by sampling the most
representative samples to be labeled by an oracle. In this paper, we propose a
state relabeling adversarial active learning model (SRAAL), that leverages both
the annotation and the labeled/unlabeled state information for deriving the
most informative unlabeled samples. The SRAAL consists of a representation
generator and a state discriminator. The generator uses the complementary
annotation information with traditional reconstruction information to generate
the unified representation of samples, which embeds the semantic into the whole
data representation. Then, we design an online uncertainty indicator in the
discriminator, which endues unlabeled samples with different importance. As a
result, we can select the most informative samples based on the discriminator's
predicted state. We also design an algorithm to initialize the labeled pool,
which makes subsequent sampling more efficient. The experiments conducted on
various datasets show that our model outperforms the previous state-of-art
active learning methods and our initially sampling algorithm achieves better
performance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.10870v2,Deep Q-Learning: Theoretical Insights from an Asymptotic Analysis,"Deep Q-Learning is an important reinforcement learning algorithm, which
involves training a deep neural network, called Deep Q-Network (DQN), to
approximate the well-known Q-function. Although wildly successful under
laboratory conditions, serious gaps between theory and practice as well as a
lack of formal guarantees prevent its use in the real world. Adopting a
dynamical systems perspective, we provide a theoretical analysis of a popular
version of Deep Q-Learning under realistic and verifiable assumptions. More
specifically, we prove an important result on the convergence of the algorithm,
characterizing the asymptotic behavior of the learning process. Our result
sheds light on hitherto unexplained properties of the algorithm and helps
understand empirical observations, such as performance inconsistencies even
after training. Unlike previous theories, our analysis accommodates state
Markov processes with multiple stationary distributions. In spite of the focus
on Deep Q-Learning, we believe that our theory may be applied to understand
other deep learning algorithms",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1201.3480v1,"Organic Design of Massively Distributed Systems: A Complex Networks
  Perspective","The vision of Organic Computing addresses challenges that arise in the design
of future information systems that are comprised of numerous, heterogeneous,
resource-constrained and error-prone components or devices. Here, the notion
organic particularly highlights the idea that, in order to be manageable, such
systems should exhibit self-organization, self-adaptation and self-healing
characteristics similar to those of biological systems. In recent years, the
principles underlying many of the interesting characteristics of natural
systems have been investigated from the perspective of complex systems science,
particularly using the conceptual framework of statistical physics and
statistical mechanics. In this article, we review some of the interesting
relations between statistical physics and networked systems and discuss
applications in the engineering of organic networked computing systems with
predictable, quantifiable and controllable self-* properties.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1403.0974v3,Parametrized Algorithms for Random Serial Dictatorship,"Voting and assignment are two of the most fundamental settings in social
choice theory. For both settings, random serial dictatorship (RSD) is a
well-known rule that satisfies anonymity, ex post efficiency, and
strategyproofness. Recently, it was shown that computing the resulting
probabilities is #P-complete both in the voting and assignment setting. In this
paper, we study RSD from a parametrized complexity perspective. More
specifically, we present efficient algorithms to compute the RSD probabilities
under the condition that the number of agent types, alternatives, or objects is
bounded.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.04170v1,A Neuromorphic Paradigm for Online Unsupervised Clustering,"A computational paradigm based on neuroscientific concepts is proposed and
shown to be capable of online unsupervised clustering. Because it is an online
method, it is readily amenable to streaming realtime applications and is
capable of dynamically adjusting to macro-level input changes. All operations,
both training and inference, are localized and efficient. The paradigm is
implemented as a cognitive column that incorporates five key elements: 1)
temporal coding, 2) an excitatory neuron model for inference, 3)
winner-take-all inhibition, 4) a column architecture that combines excitation
and inhibition, 5) localized training via spike timing de-pendent plasticity
(STDP). These elements are described and discussed, and a prototype column is
given. The prototype column is simulated with a semi-synthetic benchmark and is
shown to have performance characteristics on par with classic k-means.
Simulations reveal the inner operation and capabilities of the column with
emphasis on excitatory neuron response functions and STDP implementations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1804.02939v2,Symmetric Circuits for Rank Logic,"Fixed-point logic with rank (FPR) is an extension of fixed-point logic with
counting (FPC) with operators for computing the rank of a matrix over a finite
field. The expressive power of FPR properly extends that of FPC and is
contained in PTime, but not known to be properly contained. We give a circuit
characterization for FPR in terms of families of symmetric circuits with rank
gates, along the lines of that for FPC given by [Anderson and Dawar 2017]. This
requires the development of a broad framework of circuits in which the
individual gates compute functions that are not symmetric (i.e., invariant
under all permutations of their inputs). In the case of FPC, the proof of
equivalence of circuits and logic rests heavily on the assumption that
individual gates compute such symmetric functions and so novel techniques are
required to make this work for FPR.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1812.09204v1,The future of statistical disclosure control,"Statistical disclosure control (SDC) was not created in a single seminal
paper nor following the invention of a new mathematical technique, rather it
developed slowly in response to the practical challenges faced by data
practitioners based at national statistical institutes (NSIs). SDC's subsequent
emergence as a specialised academic field was an outcome of three interrelated
socio-technical changes: (i) the advent of accessible computing as a research
tool in the 1980s meant that it became possible - and then increasingly easy -
for researchers to process larger quantities of data automatically; this
naturally increased demand for such data; (ii) it became possible for data
holders to process and disseminate detailed data as digital files and (iii) the
number of organisations holding data about individuals proliferated. This also
meant the number of potential adversaries with the resources to attack any
given dataset increased exponentially. In this article, we describe the state
of the art for SDC and then discuss the core issues and future challenges. In
particular, we touch on SDC and big data, on SDC and machine learning, and on
SDC and anti-discrimination.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2108.06181v2,"Detecting socially interacting groups using f-formation: A survey of
  taxonomy, methods, datasets, applications, challenges, and future research
  directions","Robots in our daily surroundings are increasing day by day. Their usability
and acceptability largely depend on their explicit and implicit interaction
capability with fellow human beings. As a result, social behavior is one of the
most sought-after qualities that a robot can possess. However, there is no
specific aspect and/or feature that defines socially acceptable behavior and it
largely depends on the situation, application, and society. In this article, we
investigate one such social behavior for collocated robots. Imagine a group of
people is interacting with each other and we want to join the group. We as
human beings do it in a socially acceptable manner, i.e., within the group, we
do position ourselves in such a way that we can participate in the group
activity without disturbing/obstructing anybody. To possess such a quality,
first, a robot needs to determine the formation of the group and then determine
a position for itself, which we humans do implicitly. The theory of f-formation
can be utilized for this purpose. As the types of formations can be very
diverse, detecting the social groups is not a trivial task. In this article, we
provide a comprehensive survey of the existing work on social interaction and
group detection using f-formation for robotics and other applications. We also
put forward a novel holistic survey framework combining all the possible
concerns and modules relevant to this problem. We define taxonomies based on
methods, camera views, datasets, detection capabilities and scale, evaluation
approaches, and application areas. We discuss certain open challenges and
limitations in current literature along with possible future research
directions based on this framework. In particular, we discuss the existing
methods/techniques and their relative merits and demerits, applications, and
provide a set of unsolved but relevant problems in this domain.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0901.3348v1,Nuclear norm minimization for the planted clique and biclique problems,"We consider the problems of finding a maximum clique in a graph and finding a
maximum-edge biclique in a bipartite graph. Both problems are NP-hard. We write
both problems as matrix-rank minimization and then relax them using the nuclear
norm. This technique, which may be regarded as a generalization of compressive
sensing, has recently been shown to be an effective way to solve rank
optimization problems. In the special cases that the input graph has a planted
clique or biclique (i.e., a single large clique or biclique plus diversionary
edges), our algorithm successfully provides an exact solution to the original
instance. For each problem, we provide two analyses of when our algorithm
succeeds. In the first analysis, the diversionary edges are placed by an
adversary. In the second, they are placed at random. In the case of random
edges for the planted clique problem, we obtain the same bound as Alon,
Krivelevich and Sudakov as well as Feige and Krauthgamer, but we use different
techniques.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1001.2086v1,The Isomorphism Problem On Classes of Automatic Structures,"Automatic structures are finitely presented structures where the universe and
all relations can be recognized by finite automata. It is known that the
isomorphism problem for automatic structures is complete for $\Sigma^1_1$; the
first existential level of the analytical hierarchy. Several new results on
isomorphism problems for automatic structures are shown in this paper: (i) The
isomorphism problem for automatic equivalence relations is complete for
$\Pi^0_1$ (first universal level of the arithmetical hierarchy). (ii) The
isomorphism problem for automatic trees of height $n \geq 2$ is
$\Pi^0_{2n-3}$-complete. (iii) The isomorphism problem for automatic linear
orders is not arithmetical. This solves some open questions of Khoussainov,
Rubin, and Stephan.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1703.06858v2,"Analog Transmit Signal Optimization for Undersampled Delay-Doppler
  Estimation","In this work, the optimization of the analog transmit waveform for joint
delay-Doppler estimation under sub-Nyquist conditions is considered. Based on
the Bayesian Cram\'er-Rao lower bound (BCRLB), we derive an estimation
theoretic design rule for the Fourier coefficients of the analog transmit
signal when violating the sampling theorem at the receiver through a wide
analog pre-filtering bandwidth. For a wireless delay-Doppler channel, we obtain
a system optimization problem which can be solved in compact form by using an
Eigenvalue decomposition. The presented approach enables one to explore the
Pareto region spanned by the optimized analog waveforms. Furthermore, we
demonstrate how the framework can be used to reduce the sampling rate at the
receiver while maintaining high estimation accuracy. Finally, we verify the
practical impact by Monte-Carlo simulations of a channel estimation algorithm.",0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1205.6402v1,Constructive Provability Logic,"We present constructive provability logic, an intuitionstic modal logic that
validates the L\""ob rule of G\""odel and L\""ob's provability logic by permitting
logical reflection over provability. Two distinct variants of this logic, CPL
and CPL*, are presented in natural deduction and sequent calculus forms which
are then shown to be equivalent. In addition, we discuss the use of
constructive provability logic to justify stratified negation in logic
programming within an intuitionstic and structural proof theory.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1211.0157v1,Optimal Covering with Mobile Sensors in an Unbounded Region,"Covering a bounded region with minimum number of homogeneous sensor nodes is
a NP-complete problem \cite{Li09}. In this paper we have proposed an {\it id}
based distributed algorithm for optimal coverage in an unbounded region. The
proposed algorithm guarantees maximum spreading in $O(\sqrt{n})$ rounds without
creating any coverage hole. The algorithm executes in synchronous rounds
without exchanging any message.
  We have also explained how our proposed algorithm can achieve optimal energy
consumption and handle random sensor node deployment for optimal spreading.",0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1401.1163v2,"Proceedings of the 3rd MANIAC Challenge, Berlin, Germany, July 27 - 28,
  2013","This is the Proceedings of the 3rd MANIAC Challenge, which was held in
Berlin, Germany, July 27 - 28, 2013.",0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0303017v1,"A Neural Network Assembly Memory Model with Maximum-Likelihood Recall
  and Recognition Properties","It has been shown that a neural network model recently proposed to describe
basic memory performance is based on a ternary/binary coding/decoding algorithm
which leads to a new neural network assembly memory model (NNAMM) providing
maximum-likelihood recall/recognition properties and implying a new memory unit
architecture with Hopfield two-layer network, N-channel time gate, auxiliary
reference memory, and two nested feedback loops. For the data coding used,
conditions are found under which a version of Hopfied network implements
maximum-likelihood convolutional decoding algorithm and, simultaneously, linear
statistical classifier of arbitrary binary vectors with respect to Hamming
distance between vector analyzed and reference vector given. In addition to
basic memory performance and etc, the model explicitly describes the dependence
on time of memory trace retrieval, gives a possibility of one-trial learning,
metamemory simulation, generalized knowledge representation, and distinct
description of conscious and unconscious mental processes. It has been shown
that an assembly memory unit may be viewed as a model of a smallest inseparable
part or an 'atom' of consciousness. Some nontraditional neurobiological
backgrounds (dynamic spatiotemporal synchrony, properties of time dependent and
error detector neurons, early precise spike firing, etc) and the model's
application to solve some interdisciplinary problems from different scientific
fields are discussed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.09158v1,"Evolutionary Multi-Objective Optimization Framework for Mining
  Association Rules","In this paper, two multi-objective optimization frameworks in two variants
(i.e., NSGA-III-ARM-V1, NSGA-III-ARM-V2; and MOEAD-ARM-V1, MOEAD-ARM-V2) are
proposed to find association rules from transactional datasets. The first
framework uses Non-dominated sorting genetic algorithm III (NSGA-III) and the
second uses Decomposition based multi-objective evolutionary algorithm (MOEA/D)
to find the association rules which are diverse, non-redundant and
non-dominated (having high objective function values). In both these
frameworks, there is no need to specify minimum support and minimum confidence.
In the first variant, support, confidence, and lift are considered as objective
functions while in second, confidence, lift, and interestingness are considered
as objective functions. These frameworks are tested on seven different kinds of
datasets including two real-life bank datasets. Our study suggests that
NSGA-III-ARM framework works better than MOEAD-ARM framework in both the
variants across majority of the datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1005.5142v3,Unprovability of the Logical Characterization of Bisimulation,"We quickly review labelled Markov processes (LMP) and provide a
counterexample showing that in general measurable spaces, event bisimilarity
and state bisimilarity differ in LMP. This shows that the logic in Desharnais
[*] does not characterize state bisimulation in non-analytic measurable spaces.
Furthermore we show that, under current foundations of Mathematics, such
logical characterization is unprovable for spaces that are projections of a
coanalytic set. Underlying this construction there is a proof that stationary
Markov processes over general measurable spaces do not have semi-pullbacks.
([*] J. Desharnais, Labelled Markov Processes. School of Computer Science.
McGill University, Montr\'eal (1999))",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1810.01609v2,"Simulating the weak death of the neutron in a femtoscale universe with
  near-Exascale computing","The fundamental particle theory called Quantum Chromodynamics (QCD) dictates
everything about protons and neutrons, from their intrinsic properties to
interactions that bind them into atomic nuclei. Quantities that cannot be fully
resolved through experiment, such as the neutron lifetime (whose precise value
is important for the existence of light-atomic elements that make the sun shine
and life possible), may be understood through numerical solutions to QCD. We
directly solve QCD using Lattice Gauge Theory and calculate nuclear observables
such as neutron lifetime. We have developed an improved algorithm that
exponentially decreases the time-to solution and applied it on the new CORAL
supercomputers, Sierra and Summit. We use run-time autotuning to distribute GPU
resources, achieving 20% performance at low node count. We also developed
optimal application mapping through a job manager, which allows CPU and GPU
jobs to be interleaved, yielding 15% of peak performance when deployed across
large fractions of CORAL.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1709.07528v1,"Defining a Lingua Franca to Open the Black Box of a Nave Bayes
  Recommender","Many AI systems have a black box nature that makes it difficult to understand
how they make their recommendations. This can be unsettling, as the designer
cannot be certain how the system will respond to novelty. To penetrate our
Na\""ive Bayes recommender's black box, we first asked, what do we want to know
from our system, and how can it be obtained? The answers led us to recursively
define a common lexicon with the AI, a lingua franca, using the very items that
the system ranks to create meta-symbols recognized by the system, and enabling
us to understand the system's knowledge in plain terms and at different levels
of abstraction. As one bonus, using its existing knowledge, the lingua franca
can enable the system to extend recommendations to related, but entirely new
areas, ameliorating the cold start problem. We also supplement the lingua
franca with techniques for visualizing the system's knowledge state, develop
metrics for evaluating the meaningfulness of terms in the lingua franca, and
generalize the requirements for developing a similar lingua franca in other
applications.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.1119v2,"Feature selection for classification with class-separability strategy
  and data envelopment analysis","In this paper, a novel feature selection method is presented, which is based
on Class-Separability (CS) strategy and Data Envelopment Analysis (DEA). To
better capture the relationship between features and the class, class labels
are separated into individual variables and relevance and redundancy are
explicitly handled on each class label. Super-efficiency DEA is employed to
evaluate and rank features via their conditional dependence scores on all class
labels, and the feature with maximum super-efficiency score is then added in
the conditioning set for conditional dependence estimation in the next
iteration, in such a way as to iteratively select features and get the final
selected features. Eventually, experiments are conducted to evaluate the
effectiveness of proposed method comparing with four state-of-the-art methods
from the viewpoint of classification accuracy. Empirical results verify the
feasibility and the superiority of proposed feature selection method.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.09161v1,"Hardhats and Bungaloos: Comparing Crowdsourced Design Feedback with Peer
  Design Feedback in the Classroom","Feedback is an important aspect of design education, and crowdsourcing has
emerged as a convenient way to obtain feedback at scale. In this paper, we
investigate how crowdsourced design feedback compares to peer design feedback
within a design-oriented HCI class and across two metrics: perceived quality
and perceived fairness. We also examine the perceived monetary value of
crowdsourced feedback, which provides an interesting contrast to the typical
requester-centric view of the value of labor on crowdsourcing platforms. Our
results reveal that the students (N=106) perceived the crowdsourced design
feedback as inferior to peer design feedback in multiple ways. However, they
also identified various positive aspects of the online crowds that peers cannot
provide. We discuss the meaning of the findings and provide suggestions for
teachers in HCI and other researchers interested in crowd feedback systems on
using crowds as a potential complement to peers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1810.03598v2,Defunctionalization of Higher-Order Constrained Horn Clauses,"Building on the successes of satisfiability modulo theories (SMT), Bj{\o}rner
et al. initiated a research programme advocating Horn constraints as a suitable
basis for automatic program verification. The notion of first-order constrained
Horn clauses has recently been extended to higher-order logic by Cathcart Burn
et al. To exploit the remarkable efficiency of SMT solving, a natural approach
to solve systems of higher-order Horn constraints is to reduce them to systems
of first-order Horn constraints. This paper presents a defunctionalization
algorithm to achieve the reduction.
  Given a well-sorted higher-order constrained Horn clause (HoCHC) problem
instance, the defunctionalization algorithm constructs a first-order
well-sorted constrained Horn clause problem. In addition to well-sortedness of
the algorithm's output, we prove that if an input HoCHC is solvable, then the
result of its defunctionalization is solvable. The converse also holds, which
we prove using a recent result on the continuous semantics of HoCHC. To our
knowledge, this defunctionalization algorithm is the first sound and complete
reduction from systems of higher-order Horn constraints to systems of
first-order Horn constraints.
  We have constructed DefMono, a prototype implementation of the
defunctionalization algorithm. It first defunctionalizes an input HoCHC problem
and then feeds the result into a backend SMT solver. We have evaluated the
performance of DefMono empirically by comparison with two other higher-order
verification tools.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1508.01657v2,"A parameterized complexity view on non-preemptively scheduling
  interval-constrained jobs: few machines, small looseness, and small slack","We study the problem of non-preemptively scheduling $n$ jobs, each job $j$
with a release time $t_j$, a deadline $d_j$, and a processing time $p_j$, on
$m$ parallel identical machines. Cieliebak et al. (2004) considered the two
constraints $|d_j-t_j|\leq \lambda p_j$ and $|d_j-t_j|\leq p_j +\sigma$ and
showed the problem to be NP-hard for any $\lambda>1$ and for any $\sigma\geq
2$. We complement their results by parameterized complexity studies: we show
that, for any $\lambda>1$, the problem remains weakly NP-hard even for $m=2$
and strongly W[1]-hard parameterized by $m$. We present a
pseudo-polynomial-time algorithm for constant $m$ and $\lambda$ and a
fixed-parameter tractability result for the parameter $m$ combined with
$\sigma$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1711.09059v1,"Long Short-Term Memory (LSTM) networks with jet constituents for boosted
  top tagging at the LHC","Multivariate techniques based on engineered features have found wide adoption
in the identification of jets resulting from hadronic top decays at the Large
Hadron Collider (LHC). Recent Deep Learning developments in this area include
the treatment of the calorimeter activation as an image or supplying a list of
jet constituent momenta to a fully connected network. This latter approach
lends itself well to the use of Recurrent Neural Networks. In this work the
applicability of architectures incorporating Long Short-Term Memory (LSTM)
networks is explored. Several network architectures, methods of ordering of jet
constituents, and input pre-processing are studied. The best performing LSTM
network achieves a background rejection of 100 for 50% signal efficiency. This
represents more than a factor of two improvement over a fully connected Deep
Neural Network (DNN) trained on similar types of inputs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0507027v4,Anyone but Him: The Complexity of Precluding an Alternative,"Preference aggregation in a multiagent setting is a central issue in both
human and computer contexts. In this paper, we study in terms of complexity the
vulnerability of preference aggregation to destructive control. That is, we
study the ability of an election's chair to, through such mechanisms as
voter/candidate addition/suppression/partition, ensure that a particular
candidate (equivalently, alternative) does not win. And we study the extent to
which election systems can make it impossible, or computationally costly
(NP-complete), for the chair to execute such control. Among the systems we
study--plurality, Condorcet, and approval voting--we find cases where systems
immune or computationally resistant to a chair choosing the winner nonetheless
are vulnerable to the chair blocking a victory. Beyond that, we see that among
our studied systems no one system offers the best protection against
destructive control. Rather, the choice of a preference aggregation system will
depend closely on which types of control one wishes to be protected against. We
also find concrete cases where the complexity of or susceptibility to control
varies dramatically based on the choice among natural tie-handling rules.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1602.07424v2,"TRIST: Counting Local and Global Triangles in Fully-dynamic Streams
  with Fixed Memory Size","We present TRI\`EST, a suite of one-pass streaming algorithms to compute
unbiased, low-variance, high-quality approximations of the global and local
(i.e., incident to each vertex) number of triangles in a fully-dynamic graph
represented as an adversarial stream of edge insertions and deletions. Our
algorithms use reservoir sampling and its variants to exploit the
user-specified memory space at all times. This is in contrast with previous
approaches which use hard-to-choose parameters (e.g., a fixed sampling
probability) and offer no guarantees on the amount of memory they will use. We
show a full analysis of the variance of the estimations and novel concentration
bounds for these quantities. Our experimental results on very large graphs show
that TRI\`EST outperforms state-of-the-art approaches in accuracy and exhibits
a small update time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1605.06069v3,"A Hierarchical Latent Variable Encoder-Decoder Model for Generating
  Dialogues","Sequential data often possesses a hierarchical structure with complex
dependencies between subsequences, such as found between the utterances in a
dialogue. In an effort to model this kind of generative process, we propose a
neural network-based generative architecture, with latent stochastic variables
that span a variable number of time steps. We apply the proposed model to the
task of dialogue response generation and compare it with recent neural network
architectures. We evaluate the model performance through automatic evaluation
metrics and by carrying out a human evaluation. The experiments demonstrate
that our model improves upon recently proposed models and that the latent
variables facilitate the generation of long outputs and maintain the context.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0909.1590v1,"Efficient and Spontaneous Privacy-Preserving Protocol for Secure
  Vehicular Communications","This paper introduces an efficient and spontaneous privacy-preserving
protocol for vehicular ad-hoc networks based on revocable ring signature. The
proposed protocol has three appealing characteristics: First, it offers
conditional privacy-preservation: while a receiver can verify that a message
issuer is an authorized participant in the system only a trusted authority can
reveal the true identity of a message sender. Second, it is spontaneous: safety
messages can be authenticated locally, without support from the roadside units
or contacting other vehicles. Third, it is efficient by offering fast message
authentication and verification, cost-effective identity tracking in case of a
dispute, and low storage requirements. We use extensive analysis to demonstrate
the merits of the proposed protocol and to contrast it with previously proposed
solutions.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1211.2501v1,FlowME: Lattice-based Traffic Measurement,"Flow-based traffic measurement is a very challenging problem: Managing
counters for each individual traffic flow in hardware resources knowingly
struggle to scale with high-speed links. In this paper we propose a novel
lattice theory-based approach that improves flow-based measurement performances
and scales by keeping the number of the maintained hardware counters to a
minimum (result mathematically established in the paper). The crucial
contribution of the lattice is to map the computational semantics of the packet
processing to user requests for traffic measurement thus allowing for a
better-informed and focused counter assignment.
  An implementation over an Openflow switch, FlowME, was developed and
evaluated upon its memory usage, performance overhead, and processing effort to
generate the minimal solution. Experimental results indicate a significant
decrease in resource consumption.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0502044v1,"The complexity of computing the Hilbert polynomial of smooth
  equidimensional complex projective varieties","We continue the study of counting complexity begun in [Buergisser, Cucker 04]
and [Buergisser, Cucker, Lotz 05] by proving upper and lower bounds on the
complexity of computing the Hilbert polynomial of a homogeneous ideal. We show
that the problem of computing the Hilbert polynomial of a smooth
equidimensional complex projective variety can be reduced in polynomial time to
the problem of counting the number of complex common zeros of a finite set of
multivariate polynomials. Moreover, we prove that the more general problem of
computing the Hilbert polynomial of a homogeneous ideal is polynomial space
hard. This implies polynomial space lower bounds for both the problems of
computing the rank and the Euler characteristic of cohomology groups of
coherent sheaves on projective space, improving the #P-lower bound of Bach (JSC
1999).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.6328v1,Big Data as a Mediator in Science Teaching: A Proposal,"We live in a digital world that, in 2010, crossed the mark of one zettabyte
data. This huge amount of data processed on computers extremely fast with
optimized techniques allows one to find insights in new and emerging types of
data and content and to answer questions that were previously considered beyond
reach. This is the idea of Big Data. Google now offers the Google Correlate
analysis public tool that, from a search term or a series of temporal or
regional data, provides a list of queries on Google whose frequencies follow
patterns that best correlate with the data, according to the Pearson
determination coefficient R2. Of course, correlation does not imply causation.
We believe, however, that there is potential for these big data tools to find
unexpected correlations that may serve as clues to interesting phenomena, from
the pedagogical and even scientific point of view. As far as we know, this is
the first proposal for the use of Big Data in Science Teaching, of
constructionist character, taking as mediators the computer and the public and
free tools such as Google Correlate. It also has an epistemological bias, not
being merely a training in computational infrastructure or predictive
analytics, but aiming at providing students a better understanding of physical
concepts, such as phenomena, observation, measurement, physical laws, theory,
and causality. With it, they would be able to become good Big Data specialists,
the so needed 'data scientists' to solve the challenges of Big Data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/0801.3118v1,Spreadsheet Hell,"This management paper looks at the real world issues faced by practitioners
managing spreadsheets through the production phase of their life cycle. It
draws on the commercial experience of several developers working with large
corporations, either as employees or consultants or contractors. It provides
commercial examples of some of the practicalities involved with spreadsheet use
around the enterprise.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1
http://arxiv.org/abs/2008.07356v1,Estimating action plans for smart poultry houses,"In poultry farming, the systematic choice, update, and implementation of
periodic (t) action plans define the feed conversion rate (FCR[t]), which is an
acceptable measure for successful production. Appropriate action plans provide
tailored resources for broilers, allowing them to grow within the so-called
thermal comfort zone, without wast or lack of resources. Although the
implementation of an action plan is automatic, its configuration depends on the
knowledge of the specialist, tending to be inefficient and error-prone, besides
to result in different FCR[t] for each poultry house. In this article, we claim
that the specialist's perception can be reproduced, to some extent, by
computational intelligence. By combining deep learning and genetic algorithm
techniques, we show how action plans can adapt their performance over the time,
based on previous well succeeded plans. We also implement a distributed network
infrastructure that allows to replicate our method over distributed poultry
houses, for their smart, interconnected, and adaptive control. A supervision
system is provided as interface to users. Experiments conducted over real data
show that our method improves 5% on the performance of the most productive
specialist, staying very close to the optimal FCR[t].",0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1509.01168v1,Semi-described and semi-supervised learning with Gaussian processes,"Propagating input uncertainty through non-linear Gaussian process (GP)
mappings is intractable. This hinders the task of training GPs using uncertain
and partially observed inputs. In this paper we refer to this task as
""semi-described learning"". We then introduce a GP framework that solves both,
the semi-described and the semi-supervised learning problems (where missing
values occur in the outputs). Auto-regressive state space simulation is also
recognised as a special case of semi-described learning. To achieve our goal we
develop variational methods for handling semi-described inputs in GPs, and
couple them with algorithms that allow for imputing the missing values while
treating the uncertainty in a principled, Bayesian manner. Extensive
experiments on simulated and real-world data study the problems of iterative
forecasting and regression/classification with missing values. The results
suggest that the principled propagation of uncertainty stemming from our
framework can significantly improve performance in these tasks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0712.1765v6,Solving Simple Stochastic Games with Few Random Vertices,"Simple stochastic games are two-player zero-sum stochastic games with
turn-based moves, perfect information, and reachability winning conditions. We
present two new algorithms computing the values of simple stochastic games.
Both of them rely on the existence of optimal permutation strategies, a class
of positional strategies derived from permutations of the random vertices. The
""permutation-enumeration"" algorithm performs an exhaustive search among these
strategies, while the ""permutation-improvement"" algorithm is based on
successive improvements, \`a la Hoffman-Karp. Our algorithms improve previously
known algorithms in several aspects. First they run in polynomial time when the
number of random vertices is fixed, so the problem of solving simple stochastic
games is fixed-parameter tractable when the parameter is the number of random
vertices. Furthermore, our algorithms do not require the input game to be
transformed into a stopping game. Finally, the permutation-enumeration
algorithm does not use linear programming, while the permutation-improvement
algorithm may run in polynomial time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1204.0033v1,Transforming Graph Representations for Statistical Relational Learning,"Relational data representations have become an increasingly important topic
due to the recent proliferation of network datasets (e.g., social, biological,
information networks) and a corresponding increase in the application of
statistical relational learning (SRL) algorithms to these domains. In this
article, we examine a range of representation issues for graph-based relational
data. Since the choice of relational data representation for the nodes, links,
and features can dramatically affect the capabilities of SRL algorithms, we
survey approaches and opportunities for relational representation
transformation designed to improve the performance of these algorithms. This
leads us to introduce an intuitive taxonomy for data representation
transformations in relational domains that incorporates link transformation and
node transformation as symmetric representation tasks. In particular, the
transformation tasks for both nodes and links include (i) predicting their
existence, (ii) predicting their label or type, (iii) estimating their weight
or importance, and (iv) systematically constructing their relevant features. We
motivate our taxonomy through detailed examples and use it to survey and
compare competing approaches for each of these tasks. We also discuss general
conditions for transforming links, nodes, and features. Finally, we highlight
challenges that remain to be addressed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.16993v2,Feature Extraction for Novelty Detection in Network Traffic,"Data representation plays a critical role in the performance of novelty
detection (or ``anomaly detection'') methods in machine learning. The data
representation of network traffic often determines the effectiveness of these
models as much as the model itself. The wide range of novel events that network
operators need to detect (e.g., attacks, malware, new applications, changes in
traffic demands) introduces the possibility for a broad range of possible
models and data representations. In each scenario, practitioners must spend
significant effort extracting and engineering features that are most predictive
for that situation or application. While anomaly detection is well-studied in
computer networking, much existing work develops specific models that presume a
particular representation -- often IPFIX/NetFlow. Yet, other representations
may result in higher model accuracy, and the rise of programmable networks now
makes it more practical to explore a broader range of representations. To
facilitate such exploration, we develop a systematic framework, open-source
toolkit, and public Python library that makes it both possible and easy to
extract and generate features from network traffic and perform and end-to-end
evaluation of these representations across most prevalent modern novelty
detection models. We first develop and publicly release an open-source tool, an
accompanying Python library (NetML), and end-to-end pipeline for novelty
detection in network traffic. Second, we apply this tool to five different
novelty detection problems in networking, across a range of scenarios from
attack detection to novel device detection. Our findings general insights and
guidelines concerning which features appear to be more appropriate for
particular situations.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1203.0889v1,Data-Driven Execution of Fast Multipole Methods,"Fast multipole methods have O(N) complexity, are compute bound, and require
very little synchronization, which makes them a favorable algorithm on
next-generation supercomputers. Their most common application is to accelerate
N-body problems, but they can also be used to solve boundary integral
equations. When the particle distribution is irregular and the tree structure
is adaptive, load-balancing becomes a non-trivial question. A common strategy
for load-balancing FMMs is to use the work load from the previous step as
weights to statically repartition the next step. The authors discuss in the
paper another approach based on data-driven execution to efficiently tackle
this challenging load-balancing problem. The core idea consists of breaking the
most time-consuming stages of the FMMs into smaller tasks. The algorithm can
then be represented as a Directed Acyclic Graph (DAG) where nodes represent
tasks, and edges represent dependencies among them. The execution of the
algorithm is performed by asynchronously scheduling the tasks using the QUARK
runtime environment, in a way such that data dependencies are not violated for
numerical correctness purposes. This asynchronous scheduling results in an
out-of-order execution. The performance results of the data-driven FMM
execution outperform the previous strategy and show linear speedup on a
quad-socket quad-core Intel Xeon system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.05738v1,"Preventing Handheld Phone Distraction for Drivers by Sensing the
  Gripping Hand","Handheld phone distraction is the leading cause of traffic accidents.
However, few efforts have been devoted to detecting when the phone distraction
happens, which is a critical input for taking immediate safety measures. This
work proposes a phone-use monitoring system, which detects the start of the
driver's handheld phone use and eliminates the distraction at once.
Specifically, the proposed system emits periodic ultrasonic pulses to sense if
the phone is being held in hand or placed on support surfaces (e.g., seat and
cup holder) by capturing the unique signal interference resulted from the
contact object's damping, reflection and refraction. We derive the short-time
Fourier transform from the microphone data to describe such impacts and develop
a CNN-based binary classifier to discriminate the phone use between the
handheld and the handsfree status. Additionally, we design an adaptive
window-based filter to correct the classification errors and identify each
handheld phone distraction instance, including its start, end, and duration.
Extensive experiments with fourteen people, three phones and two car models
show that our system achieves 99% accuracy of recognizing handheld phone-use
instances and 0.76-second median error to estimate the distraction's start
time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1402.0804v1,"A Measurement-based Analysis of the Energy Consumption of Data Center
  Servers","Energy consumption is a growing issue in data centers, impacting their
economic viability and their public image. In this work we empirically
characterize the power and energy consumed by different types of servers. In
particular, in order to understand the behavior of their energy and power
consumption, we perform measurements in different servers. In each of them, we
exhaustively measure the power consumed by the CPU, the disk, and the network
interface under different configurations, identifying the optimal operational
levels. One interesting conclusion of our study is that the curve that defines
the minimal CPU power as a function of the load is neither linear nor purely
convex as has been previously assumed. Moreover, we find that the efficiency of
the various server components can be maximized by tuning the CPU frequency and
the number of active cores as a function of the system and network load, while
the block size of I/O operations should be always maximized by applications. We
also show how to estimate the energy consumed by an application as a function
of some simple parameters, like the CPU load, and the disk and network
activity. We validate the proposed approach by accurately estimating the energy
of a map-reduce computation in a Hadoop platform.",0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.04641v1,"Filling missing data in point clouds by merging structured and
  unstructured point clouds","Point clouds arising from structured data, mainly as a result of CT scans,
provides special properties on the distribution of points and the distances
between those. Yet often, the amount of data provided can not compare to
unstructured point clouds, i.e. data that arises from 3D light scans or laser
scans. This article hereby proposes an approach to extend structured data and
enhance the quality by inserting selected points from an unstructured point
cloud. The resulting point cloud still has a partial structure that is called
""half-structure"". In this way, missing data that can not be optimally recovered
through other surface reconstruction methods can be completed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.02690v1,"TricycleGAN: Unsupervised Image Synthesis and Segmentation Based on
  Shape Priors","Medical image segmentation is routinely performed to isolate regions of
interest, such as organs and lesions. Currently, deep learning is the state of
the art for automatic segmentation, but is usually limited by the need for
supervised training with large datasets that have been manually segmented by
trained clinicians. The goal of semi-superised and unsupervised image
segmentation is to greatly reduce, or even eliminate, the need for training
data and therefore to minimze the burden on clinicians when training
segmentation models. To this end we introduce a novel network architecture for
capable of unsupervised and semi-supervised image segmentation called
TricycleGAN. This approach uses three generative models to learn translations
between medical images and segmentation maps using edge maps as an intermediate
step. Distinct from other approaches based on generative networks, TricycleGAN
relies on shape priors rather than colour and texture priors. As such, it is
particularly well-suited for several domains of medical imaging, such as
ultrasound imaging, where commonly used visual cues may be absent. We present
experiments with TricycleGAN on a clinical dataset of kidney ultrasound images
and the benchmark ISIC 2018 skin lesion dataset.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0605106v1,Supervisory Control of Fuzzy Discrete Event Systems: A Formal Approach,"Fuzzy {\it discrete event systems} (DESs) were proposed recently by Lin and
Ying [19], which may better cope with the real-world problems with fuzziness,
impreciseness, and subjectivity such as those in biomedicine. As a continuation
of [19], in this paper we further develop fuzzy DESs by dealing with
supervisory control of fuzzy DESs. More specifically, (i) we reformulate the
parallel composition of crisp DESs, and then define the parallel composition of
fuzzy DESs that is equivalent to that in [19]; {\it max-product} and {\it
max-min} automata for modeling fuzzy DESs are considered; (ii) we deal with a
number of fundamental problems regarding supervisory control of fuzzy DESs,
particularly demonstrate controllability theorem and nonblocking
controllability theorem of fuzzy DESs, and thus present the conditions for the
existence of supervisors in fuzzy DESs; (iii) we analyze the complexity for
presenting a uniform criterion to test the fuzzy controllability condition of
fuzzy DESs modeled by max-product automata; in particular, we present in detail
a general computing method for checking whether or not the fuzzy
controllability condition holds, if max-min automata are used to model fuzzy
DESs, and by means of this method we can search for all possible fuzzy states
reachable from initial fuzzy state in max-min automata; also, we introduce the
fuzzy $n$-controllability condition for some practical problems; (iv) a number
of examples serving to illustrate the applications of the derived results and
methods are described; some basic properties related to supervisory control of
fuzzy DESs are investigated. To conclude, some related issues are raised for
further consideration.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.00439v1,"Qualitative Framing of Financial Incentives - A Case of Emotion
  Annotation","Online labor platforms, such as the Amazon Mechanical Turk, provide an
effective framework for eliciting responses to judgment tasks. Previous work
has shown that workers respond best to financial incentives, especially to
extra bonuses. However, most of the tested incentives involve describing the
bonus conditions in formulas instead of plain English. We believe that
different incentives given in English (or in qualitative framing) will result
in differences in workers' performance, especially when task difficulties vary.
In this paper, we report the preliminary results of a crowdsourcing experiment
comparing workers' performance using only qualitative framings of financial
incentives. Our results demonstrate a significant increase in workers'
performance using a specific well-formulated qualitative framing inspired by
the Peer Truth Serum. This positive effect is observed only when the difficulty
of the task is high, while when the task is easy there is no difference of
which incentives to use.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.03595v2,"The Fine-Grained Complexity of Computing the Tutte Polynomial of a
  Linear Matroid","We show that computing the Tutte polynomial of a linear matroid of dimension
$k$ on $k^{O(1)}$ points over a field of $k^{O(1)}$ elements requires
$k^{\Omega(k)}$ time unless the \#ETH---a counting extension of the Exponential
Time Hypothesis of Impagliazzo and Paturi [CCC 1999] due to Dell {\em et al.}
[ACM TALG 2014]---is false. This holds also for linear matroids that admit a
representation where every point is associated to a vector with at most two
nonzero coordinates. We also show that the same is true for computing the Tutte
polynomial of a binary matroid of dimension $k$ on $k^{O(1)}$ points with at
most three nonzero coordinates in each point's vector. This is in sharp
contrast to computing the Tutte polynomial of a $k$-vertex graph (that is, the
Tutte polynomial of a {\em graphic} matroid of dimension $k$---which is
representable in dimension $k$ over the binary field so that every vector has
two nonzero coordinates), which is known to be computable in $2^k k^{O(1)}$
time [Bj\""orklund {\em et al.}, FOCS 2008]. Our lower-bound proofs proceed via
(i) a connection due to Crapo and Rota [1970] between the number of tuples of
codewords of full support and the Tutte polynomial of the matroid associated
with the code; (ii) an earlier-established \#ETH-hardness of counting the
solutions to a bipartite $(d,2)$-CSP on $n$ vertices in $d^{o(n)}$ time; and
(iii) new embeddings of such CSP instances as questions about codewords of full
support in a linear code. We complement these lower bounds with two algorithm
designs. The first design computes the Tutte polynomial of a linear matroid of
dimension~$k$ on $k^{O(1)}$ points in $k^{O(k)}$ operations. The second design
generalizes the Bj\""orklund~{\em et al.} algorithm and runs in
$q^{k+1}k^{O(1)}$ time for linear matroids of dimension $k$ defined over the
$q$-element field by $k^{O(1)}$ points with at most two nonzero coordinates
each.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0110023v2,Set Unification,"The unification problem in algebras capable of describing sets has been
tackled, directly or indirectly, by many researchers and it finds important
applications in various research areas--e.g., deductive databases, theorem
proving, static analysis, rapid software prototyping. The various solutions
proposed are spread across a large literature. In this paper we provide a
uniform presentation of unification of sets, formalizing it at the level of set
theory. We address the problem of deciding existence of solutions at an
abstract level. This provides also the ability to classify different types of
set unification problems. Unification algorithms are uniformly proposed to
solve the unification problem in each of such classes.
  The algorithms presented are partly drawn from the literature--and properly
revisited and analyzed--and partly novel proposals. In particular, we present a
new goal-driven algorithm for general ACI1 unification and a new simpler
algorithm for general (Ab)(Cl) unification.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0411036v2,Feedback Capacity of the First-Order Moving Average Gaussian Channel,"The feedback capacity of the stationary Gaussian additive noise channel has
been open, except for the case where the noise is white. Here we find the
feedback capacity of the stationary first-order moving average additive
Gaussian noise channel in closed form. Specifically, the channel is given by
$Y_i = X_i + Z_i,$ $i = 1, 2, ...,$ where the input $\{X_i\}$ satisfies a power
constraint and the noise $\{Z_i\}$ is a first-order moving average Gaussian
process defined by $Z_i = \alpha U_{i-1} + U_i,$ $|\alpha| \le 1,$ with white
Gaussian innovations $U_i,$ $i = 0,1,....$
  We show that the feedback capacity of this channel is $-\log x_0,$ where
$x_0$ is the unique positive root of the equation $ \rho x^2 = (1-x^2) (1 -
|\alpha|x)^2,$ and $\rho$ is the ratio of the average input power per
transmission to the variance of the noise innovation $U_i$. The optimal coding
scheme parallels the simple linear signalling scheme by Schalkwijk and Kailath
for the additive white Gaussian noise channel -- the transmitter sends a
real-valued information-bearing signal at the beginning of communication and
subsequently refines the receiver's error by processing the feedback noise
signal through a linear stationary first-order autoregressive filter. The
resulting error probability of the maximum likelihood decoding decays
doubly-exponentially in the duration of the communication. This feedback
capacity of the first-order moving average Gaussian channel is very similar in
form to the best known achievable rate for the first-order
\emph{autoregressive} Gaussian noise channel studied by Butman, Wolfowitz, and
Tiernan, although the optimality of the latter is yet to be established.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.02022v1,Programming Language Features for Refinement,"Algorithmic and data refinement are well studied topics that provide a
mathematically rigorous approach to gradually introducing details in the
implementation of software. Program refinements are performed in the context of
some programming language, but mainstream languages lack features for recording
the sequence of refinement steps in the program text. To experiment with the
combination of refinement, automated verification, and language design,
refinement features have been added to the verification-aware programming
language Dafny. This paper describes those features and reflects on some
initial usage thereof.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1012.2496v2,On the Implementation of GNU Prolog,"GNU Prolog is a general-purpose implementation of the Prolog language, which
distinguishes itself from most other systems by being, above all else, a
native-code compiler which produces standalone executables which don't rely on
any byte-code emulator or meta-interpreter. Other aspects which stand out
include the explicit organization of the Prolog system as a multipass compiler,
where intermediate representations are materialized, in Unix compiler
tradition. GNU Prolog also includes an extensible and high-performance finite
domain constraint solver, integrated with the Prolog language but implemented
using independent lower-level mechanisms. This article discusses the main
issues involved in designing and implementing GNU Prolog: requirements, system
organization, performance and portability issues as well as its position with
respect to other Prolog system implementations and the ISO standardization
initiative.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1604.07781v1,"The dynamics of publishing of posts and comments on facebook (the
  russian segment, the first five months of 2013)","The article presents a study of some characteristics of post and comment
publishing in the Russian segment of Facebook. A number of non-trivial results
has been obtained. For example, a significant anomaly has been detected in the
number of user accounts with the rate of publishing posts of approximately two
posts per three days. The analysis has been carried out at the level of basic
characteristics that are shared by most social media platforms. It makes
possible a direct comparison of obtained results with data from other
platforms. The article presents an approach to formalization and ordering of
structural and informational elements on social media platforms. The approach
is based on the representation of these structural elements in the form of a
coherent hierarchy of container objects and their relations. This method allows
to structure and analyze raw data from different social media platforms in a
unified algorithmic design. The described approach is more formal, universal
and constructive than other known approaches.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1307.6894v1,"The operad of temporal wiring diagrams: formalizing a graphical language
  for discrete-time processes","We investigate the hierarchical structure of processes using the mathematical
theory of operads. Information or material enters a given process as a stream
of inputs, and the process converts it to a stream of outputs. Output streams
can then be supplied to other processes in an organized manner, and the
resulting system of interconnected processes can itself be considered a macro
process. To model the inherent structure in this kind of system, we define an
operad $\mathcal{W}$ of black boxes and directed wiring diagrams, and we define
a $\mathcal{W}$-algebra $\mathcal{P}$ of processes (which we call propagators,
after Radul and Sussman). Previous operadic models of wiring diagrams use
undirected wires without length, useful for modeling static systems of
constraints, whereas we use directed wires with length, useful for modeling
dynamic flows of information. We give multiple examples throughout to ground
the ideas.",0,0,0,0,0,1,0,1,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1903.09818v2,"Harnessing Higher-Order (Meta-)Logic to Represent and Reason with
  Complex Ethical Theories","The computer-mechanization of an ambitious explicit ethical theory, Gewirth's
Principle of Generic Consistency, is used to showcase an approach for
representing and reasoning with ethical theories exhibiting complex logical
features like alethic and deontic modalities, indexicals, higher-order
quantification, among others. Harnessing the high expressive power of Church's
type theory as a meta-logic to semantically embed a combination of quantified
non-classical logics, our work pushes existing boundaries in knowledge
representation and reasoning. We demonstrate that intuitive encodings of
complex ethical theories and their automation on the computer are no longer
antipodes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.14454v1,A Stock Options Metaphor for Content Delivery Networks,"The concept of Stock Options is used to address the scarcity of resources,
not adequately addressed by the previous tools of our Prediction Mechanism.
Using a Predictive Reservation Scheme, network and disk resources are being
monitored through well-established techniques (Kernel Regression Estimators) in
a given time frame. Next, an Secondary Market mechanism significantly improves
the efficiency and robustness of our Predictive Reservation Scheme by allowing
the fast exchange of unused (remaining) resources between the Origin Servers
(CDN Clients). This exchange can happen, either by implementing socially
optimal practices or by allowing automatic electronic auctions at the end of
the day or at shorter time intervals. Finally, we further enhance our
Prediction Mechanism; Stock Options are obtained and exercised, depending on
the lack of resources at the end of day. As a result, Origin Servers may
acquire resources (if required) at a normal price. The effectiveness of our
mechanism further improves.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1507.08348v1,Approximating Dense Max 2-CSPs,"In this paper, we present a polynomial-time algorithm that approximates
sufficiently high-value Max 2-CSPs on sufficiently dense graphs to within
$O(N^{\varepsilon})$ approximation ratio for any constant $\varepsilon > 0$.
Using this algorithm, we also achieve similar results for free games,
projection games on sufficiently dense random graphs, and the Densest
$k$-Subgraph problem with sufficiently dense optimal solution. Note, however,
that algorithms with similar guarantees to the last algorithm were in fact
discovered prior to our work by Feige et al. and Suzuki and Tokuyama.
  In addition, our idea for the above algorithms yields the following
by-product: a quasi-polynomial time approximation scheme (QPTAS) for
satisfiable dense Max 2-CSPs with better running time than the known
algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.12101v5,Learning Differentiable Programs with Admissible Neural Heuristics,"We study the problem of learning differentiable functions expressed as
programs in a domain-specific language. Such programmatic models can offer
benefits such as composability and interpretability; however, learning them
requires optimizing over a combinatorial space of program ""architectures"". We
frame this optimization problem as a search in a weighted graph whose paths
encode top-down derivations of program syntax. Our key innovation is to view
various classes of neural networks as continuous relaxations over the space of
programs, which can then be used to complete any partial program. This relaxed
program is differentiable and can be trained end-to-end, and the resulting
training loss is an approximately admissible heuristic that can guide the
combinatorial search. We instantiate our approach on top of the A-star
algorithm and an iteratively deepened branch-and-bound search, and use these
algorithms to learn programmatic classifiers in three sequence classification
tasks. Our experiments show that the algorithms outperform state-of-the-art
methods for program learning, and that they discover programmatic classifiers
that yield natural interpretations and achieve competitive accuracy.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0701014v2,"A Reply to Hofman On: ""Why LP cannot solve large instances of
  NP-complete problems in polynomial time""","Using an approach that seems to be patterned after that of Yannakakis, Hofman
argues that an NP-complete problem cannot be formulated as a polynomial
bounded-sized linear programming problem. He then goes on to propose a
""construct"" that he claims to be a counter-example to recently published linear
programming formulations of the Traveling Salesman Problem (TSP) and the
Quadratic Assignment Problems (QAP), respectively. In this paper, we show that
Hofman's construct is flawed, and provide further proof that his
""counter-example"" is invalid.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.04826v1,Verification of a Merkle Patricia Tree Library Using F*,"A Merkle tree is a data structure for representing a key-value store as a
tree. Each node of a Merkle tree is equipped with a hash value computed from
those of their descendants. A Merkle tree is often used for representing a
state of a blockchain system since it can be used for efficiently auditing the
state in a trustless manner. Due to the safety-critical nature of blockchains,
ensuring the correctness of their implementation is paramount.
  We show our formally verified implementation of the core part of Plebeia
using F*. Plebeia is a library to manipulate an extension of Merkle trees
(called Plebeia trees). It is being implemented as a part of the storage system
of the Tezos blockchain system. To this end, we gradually ported Plebeia to F*;
the OCaml code extracted from the modules ported to F* is linked with the
unverified part of Plebeia. By this gradual porting process, we can obtain a
working code from our partially verified implementation of Plebeia; we
confirmed that the binary passes all the unit tests of Plebeia.
  More specifically, we verified the following properties on the implementation
of Plebeia: (1) Each tree-manipulating function preserves the invariants on the
data structure of a Plebeia tree and satisfies the functional requirements as a
nested key-value store; (2) Each function for serializing/deserializing a
Plebeia tree to/from the low-level storage is implemented correctly; and (3)
The hash function for a Plebeia tree is relatively collision-resistant with
respect to the cryptographic safety of the blake2b hash function. During
porting Plebeia to F*, we found a bug in an old version of Plebeia, which was
overlooked by the tests bundled with the original implementation. To the best
of our knowledge, this is the first work that verifies a production-level
implementation of a Merkle-tree library by F*.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.06769v2,"One Ring to Rule Them All: Certifiably Robust Geometric Perception with
  Outliers","We propose the first general and practical framework to design certifiable
algorithms for robust geometric perception in the presence of a large amount of
outliers. We investigate the use of a truncated least squares (TLS) cost
function, which is known to be robust to outliers, but leads to hard,
nonconvex, and nonsmooth optimization problems. Our first contribution is to
show that -for a broad class of geometric perception problems- TLS estimation
can be reformulated as an optimization over the ring of polynomials and
Lasserre's hierarchy of convex moment relaxations is empirically tight at the
minimum relaxation order (i.e., certifiably obtains the global minimum of the
nonconvex TLS problem). Our second contribution is to exploit the structural
sparsity of the objective and constraint polynomials and leverage basis
reduction to significantly reduce the size of the semidefinite program (SDP)
resulting from the moment relaxation, without compromising its tightness. Our
third contribution is to develop scalable dual optimality certifiers from the
lens of sums-of-squares (SOS) relaxation, that can compute the suboptimality
gap and possibly certify global optimality of any candidate solution (e.g.,
returned by fast heuristics such as RANSAC or graduated non-convexity). Our
dual certifiers leverage Douglas-Rachford Splitting to solve a convex
feasibility SDP. Numerical experiments across different perception problems,
including single rotation averaging, shape alignment, 3D point cloud and mesh
registration, and high-integrity satellite pose estimation, demonstrate the
tightness of our relaxations, the correctness of the certification, and the
scalability of the proposed dual certifiers to large problems, beyond the reach
of current SDP solvers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1507.03685v1,TryLogic tutorial: an approach to Learning Logic by proving and refuting,"Aiming to offer a framework for blended learning to the teaching of proof
theory, the present paper describes an interactive tutorial, called
\textsc{TryLogic}, teaching how to solve logical conjectures either by proofs
or refutations. The paper also describes the integration of our infrastructure
with the Virtual Learning Environment \texttt{Moodle} through the IMS Learning
Tools Interoperability specification, and evaluates the tool we have developed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/2003.13278v2,"A Blackbox Yield Estimation Workflow with Gaussian Process Regression
  Applied to the Design of Electromagnetic Devices","In this paper an efficient and reliable method for stochastic yield
estimation is presented. Since one main challenge of uncertainty quantification
is the computational feasibility, we propose a hybrid approach where most of
the Monte Carlo sample points are evaluated with a surrogate model, and only a
few sample points are reevaluated with the original high fidelity model.
Gaussian Process Regression is a non-intrusive method which is used to build
the surrogate model. Without many prerequisites, this gives us not only an
approximation of the function value, but also an error indicator that we can
use to decide whether a sample point should be reevaluated or not. For two
benchmark problems, a dielectrical waveguide and a lowpass filter, the proposed
methods outperform classic approaches.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.04156v2,"An Efficient Analyses of the Behavior of One Dimensional Chaotic Maps
  using 0-1 Test and Three State Test","In this paper, a rigorous analysis of the behavior of the standard logistic
map, Logistic Tent system (LTS), Logistic-Sine system (LSS) and Tent-Sine
system (TSS) is performed using 0-1 test and three state test (3ST). In this
work, it has been proved that the strength of the chaotic behavior is not
uniform. Through extensive experiment and analysis, the strong and weak chaotic
regions of LTS, LSS and TSS have been identified. This would enable researchers
using these maps, to have better choices of control parameters as key values,
for stronger encryption. In addition, this paper serves as a precursor to
stronger testing practices in cryptosystem research, as Lyapunov exponent alone
has been shown to fail as a true representation of the chaotic nature of a map.",0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0912.4080v3,"Windtalking Computers: Frequency Normalization, Binary Coding Systems
  and Encryption","This paper discusses the application of known techniques, knowledge and
technology in a novel way for encryption. Two distinct and separate methods are
presented.
  Method 1: Alter the symbol set of the language by adding additional redundant
symbols for frequent symbols. This will reduce the high frequency of more
commonly used symbols. Hence, frequency analysis upon ciphertext will not be
possible. Hence, decryption will be possible.
  Method 2: Computers use binary base 2. Most encryption systems use ciphering
to convert data to ciphertext. The author presents the theory and several
possible implementations of a method for computers analogous to speaking
another language. This is done by using a binary base other than base 2. Ex.
Fibonacci, Phi or Prime.
  In addition, steganography may be used for creating alternate binary bases.
  This kind of encryption significantly increases the complexity of decryption.
First the binary base must be known. Only then, can decryption begin.
  This kind of encryption also breaks the transitivity of
plaintext-codebook-binary; the correlation of letters-ASCII-base2. With this
transitivity broken, decryption is logically impossible. Coupled with
encrypting the plaintext, binary encryption makes decryption uncrackable. It
may produce false positives--information theoretic secure, and requires much
more computing power to resolve than is currently used in brute force
decryption. Hence, the assertion that these combination of methods are
computationally secure--impervious to brute force.
  The proposed system has a drawback. It is not as compressed as a base2.
(Similar to adding random padding to the encryption.) However, this is
acceptable, since the goal is very strong encryption:
  Both methods are not decryptable by method uncrackable - by conventional,
statistical means.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1407.6559v1,"Cell-Probe Bounds for Online Edit Distance and Other Pattern Matching
  Problems","We give cell-probe bounds for the computation of edit distance, Hamming
distance, convolution and longest common subsequence in a stream. In this
model, a fixed string of $n$ symbols is given and one $\delta$-bit symbol
arrives at a time in a stream. After each symbol arrives, the distance between
the fixed string and a suffix of most recent symbols of the stream is reported.
The cell-probe model is perhaps the strongest model of computation for showing
data structure lower bounds, subsuming in particular the popular word-RAM
model.
  * We first give an $\Omega((\delta \log n)/(w+\log\log n))$ lower bound for
the time to give each output for both online Hamming distance and convolution,
where $w$ is the word size. This bound relies on a new encoding scheme and for
the first time holds even when $w$ is as small as a single bit.
  * We then consider the online edit distance and longest common subsequence
problems in the bit-probe model ($w=1$) with a constant sized input alphabet.
We give a lower bound of $\Omega(\sqrt{\log n}/(\log\log n)^{3/2})$ which
applies for both problems. This second set of results relies both on our new
encoding scheme as well as a carefully constructed hard distribution.
  * Finally, for the online edit distance problem we show that there is an
$O((\log n)^2/w)$ upper bound in the cell-probe model. This bound gives a
contrast to our new lower bound and also establishes an exponential gap between
the known cell-probe and RAM model complexities.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1307.3207v1,Scalable Eventually Consistent Counters over Unreliable Networks,"Counters are an important abstraction in distributed computing, and play a
central role in large scale geo-replicated systems, counting events such as web
page impressions or social network ""likes"". Classic distributed counters,
strongly consistent, cannot be made both available and partition-tolerant, due
to the CAP Theorem, being unsuitable to large scale scenarios. This paper
defines Eventually Consistent Distributed Counters (ECDC) and presents an
implementation of the concept, Handoff Counters, that is scalable and works
over unreliable networks. By giving up the sequencer aspect of classic
distributed counters, ECDC implementations can be made AP in the CAP design
space, while retaining the essence of counting. Handoff Counters are the first
CRDT (Conflict-free Replicated Data Type) based mechanism that overcomes the
identity explosion problem in naive CRDTs, such as G-Counters (where state size
is linear in the number of independent actors that ever incremented the
counter), by managing identities towards avoiding global propagation and
garbage collecting temporary entries. The approach used in Handoff Counters is
not restricted to counters, being more generally applicable to other data types
with associative and commutative operations.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1201.6134v2,"Synthetic sequence generator for recommender systems - memory biased
  random walk on sequence multilayer network","Personalized recommender systems rely on each user's personal usage data in
the system, in order to assist in decision making. However, privacy policies
protecting users' rights prevent these highly personal data from being publicly
available to a wider researcher audience. In this work, we propose a memory
biased random walk model on multilayer sequence network, as a generator of
synthetic sequential data for recommender systems. We demonstrate the
applicability of the synthetic data in training recommender system models for
cases when privacy policies restrict clickstream publishing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1701.01630v1,Reducing Competitive Cache Misses in Modern Processor Architectures,"The increasing number of threads inside the cores of a multicore processor,
and competitive access to the shared cache memory, become the main reasons for
an increased number of competitive cache misses and performance decline.
Inevitably, the development of modern processor architectures leads to an
increased number of cache misses. In this paper, we make an attempt to
implement a technique for decreasing the number of competitive cache misses in
the first level of cache memory. This technique enables competitive access to
the entire cache memory when there is a hit - but, if there are cache misses,
memory data (by using replacement techniques) is put in a virtual part given to
threads, so that competitive cache misses are avoided. By using a simulator
tool, the results show a decrease in the number of cache misses and performance
increase for up to 15%. The conclusion that comes out of this research is that
cache misses are a real challenge for future processor designers, in order to
hide memory latency.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.09726v1,Revisiting 802.11 Rate Adaptation from Energy Consumption's Perspective,"Rate adaptation in 802.11 WLANs has received a lot of attention from the
research community, with most of the proposals aiming at maximising throughput
based on network conditions. Considering energy consumption, an implicit
assumption is that optimality in throughput implies optimality in energy
efficiency, but this assumption has been recently put into question. In this
paper, we address via analysis and experimentation the relation between
throughput performance and energy efficiency in multi-rate 802.11 scenarios. We
demonstrate the trade-off between these performance figures, confirming that
they may not be simultaneously optimised, and analyse their sensitivity towards
the energy consumption parameters of the device. Our results provide the means
to design novel rate adaptation schemes that takes energy consumption into
account.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0210026v1,Encoding a Taxonomy of Web Attacks with Different-Length Vectors,"Web attacks, i.e. attacks exclusively using the HTTP protocol, are rapidly
becoming one of the fundamental threats for information systems connected to
the Internet. When the attacks suffered by web servers through the years are
analyzed, it is observed that most of them are very similar, using a reduced
number of attacking techniques. It is generally agreed that classification can
help designers and programmers to better understand attacks and build more
secure applications. As an effort in this direction, a new taxonomy of web
attacks is proposed in this paper, with the objective of obtaining a
practically useful reference framework for security applications. The use of
the taxonomy is illustrated by means of multiplatform real world web attack
examples. Along with this taxonomy, important features of each attack category
are discussed. A suitable semantic-dependent web attack encoding scheme is
defined that uses different-length vectors. Possible applications are
described, which might benefit from this taxonomy and encoding scheme, such as
intrusion detection systems and application firewalls.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.12153v1,An Improved Algorithm for Coarse-Graining Cellular Automata,"In studying the predictability of emergent phenomena in complex systems,
Israeli & Goldenfeld (Phys. Rev. Lett., 2004; Phys. Rev. E, 2006) showed how to
coarse-grain (elementary) cellular automata (CA). Their algorithm for finding
coarse-grainings of supercell size $N$ took doubly-exponential $2^{2^N}$-time,
and thus only allowed them to explore supercell sizes $N \leq 4$. Here we
introduce a new, more efficient algorithm for finding coarse-grainings between
any two given CA that allows us to systematically explore all elementary CA
with supercell sizes up to $N=7$, and to explore individual examples of even
larger supercell size. Our algorithm is based on a backtracking search, similar
to the DPLL algorithm with unit propagation for the NP-complete problem of
Boolean Satisfiability.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0812.3893v3,Succinct Greedy Geometric Routing in the Euclidean Plane,"In greedy geometric routing, messages are passed in a network embedded in a
metric space according to the greedy strategy of always forwarding messages to
nodes that are closer to the destination. We show that greedy geometric routing
schemes exist for the Euclidean metric in R^2, for 3-connected planar graphs,
with coordinates that can be represented succinctly, that is, with O(log n)
bits, where n is the number of vertices in the graph. Moreover, our embedding
strategy introduces a coordinate system for R^2 that supports distance
comparisons using our succinct coordinates. Thus, our scheme can be used to
significantly reduce bandwidth, space, and header size over other recently
discovered greedy geometric routing implementations for R^2.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0306017v1,Minimum Model Semantics for Logic Programs with Negation-as-Failure,"We give a purely model-theoretic characterization of the semantics of logic
programs with negation-as-failure allowed in clause bodies. In our semantics
the meaning of a program is, as in the classical case, the unique minimum model
in a program-independent ordering. We use an expanded truth domain that has an
uncountable linearly ordered set of truth values between False (the minimum
element) and True (the maximum), with a Zero element in the middle. The truth
values below Zero are ordered like the countable ordinals. The values above
Zero have exactly the reverse order. Negation is interpreted as reflection
about Zero followed by a step towards Zero; the only truth value that remains
unaffected by negation is Zero. We show that every program has a unique minimum
model M_P, and that this model can be constructed with a T_P iteration which
proceeds through the countable ordinals. Furthermore, we demonstrate that M_P
can also be obtained through a model intersection construction which
generalizes the well-known model intersection theorem for classical logic
programming. Finally, we show that by collapsing the true and false values of
the infinite-valued model M_P to (the classical) True and False, we obtain a
three-valued model identical to the well-founded one.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.07416v2,Combinatorial Game Complexity: An Introduction with Poset Games,"Poset games have been the object of mathematical study for over a century,
but little has been written on the computational complexity of determining
important properties of these games. In this introduction we develop the
fundamentals of combinatorial game theory and focus for the most part on poset
games, of which Nim is perhaps the best-known example. We present the
complexity results known to date, some discovered very recently.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0207008v1,Agent Programming with Declarative Goals,"A long and lasting problem in agent research has been to close the gap
between agent logics and agent programming frameworks. The main reason for this
problem of establishing a link between agent logics and agent programming
frameworks is identified and explained by the fact that agent programming
frameworks have not incorporated the concept of a `declarative goal'. Instead,
such frameworks have focused mainly on plans or `goals-to-do' instead of the
end goals to be realised which are also called `goals-to-be'. In this paper, a
new programming language called GOAL is introduced which incorporates such
declarative goals. The notion of a `commitment strategy' - one of the main
theoretical insights due to agent logics, which explains the relation between
beliefs and goals - is used to construct a computational semantics for GOAL.
Finally, a proof theory for proving properties of GOAL agents is introduced.
Thus, we offer a complete theory of agent programming in the sense that our
theory provides both for a programming framework and a programming logic for
such agents. An example program is proven correct by using this programming
logic.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.02861v1,Concurrent Constraint Conditional-Branching Timed Interactive Scores,"Multimedia scenarios have multimedia content and interactive events
associated with computer programs. Interactive Scores (IS) is a formalism to
represent such scenarios by temporal objects, temporal relations (TRs) and
interactive events. IS describe TRs, but IS cannot represent TRs together with
conditional branching. We propose a model for conditional branching timed IS in
the Non-deterministic Timed Concurrent Constraint (ntcc) calculus. We ran a
prototype of our model in Ntccrt (a real-time capable interpreter for ntcc) and
the response time was acceptable for real-time interaction. An advantage of
ntcc over Max/MSP or Petri Nets is that conditions and global constraints are
represented declaratively.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1407.6915v1,Accelerating Fast Fourier Transforms Using Hadoop and CUDA,"There has been considerable research into improving Fast Fourier Transform
(FFT) performance through parallelization and optimization for specialized
hardware. However, even with those advancements, processing of very large
files, over 1TB in size, still remains prohibitively slow. Analysts performing
signal processing are forced to wait hours or days for results, which results
in a disruption of their workflow and a decrease in productivity. In this paper
we present a unique approach that not only parallelizes the workload over
multi-cores, but distributes the problem over a cluster of graphics processing
unit (GPU)-equipped servers. By utilizing Hadoop and CUDA, we can take
advantage of inexpensive servers while still exceeding the processing power of
a dedicated supercomputer, as demonstrated in our result using Amazon EC2.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0707.0740v1,A Multi Interface Grid Discovery System,"Discovery Systems (DS) can be considered as entry points for global loosely
coupled distributed systems. An efficient Discovery System in essence increases
the performance, reliability and decision making capability of distributed
systems. With the rapid increase in scale of distributed applications, existing
solutions for discovery systems are fast becoming either obsolete or incapable
of handling such complexity. They are particularly ineffective when handling
service lifetimes and providing up-to-date information, poor at enabling
dynamic service access and they can also impose unwanted restrictions on
interfaces to widely available information repositories. In this paper we
present essential the design characteristics, an implementation and a
performance analysis for a discovery system capable of overcoming these
deficiencies in large, globally distributed environments.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.02933v1,"Towards Usability Guidelines for the Design of Effective Arabic
  Websites: Design Practices and Lessons Focusing on Font and Image usage","The Arabic websites constitute 1% of the web content with more than 225
million viewers and 41% Internet penetration. However, there is a lack of
design guidelines related to the selection and use of appropriate font type and
size and images in Arabic websites. Both text and images are vital multimedia
components of websites and thereby were selected for investigation in this
study. The herein paper performed an indepth inspection of font and image
design practices within 73 most visited Arabic websites in Saudi Arabia
according to Alexa Internet ranking in the first quarter of 2019. Our
exhaustive analysis showed discrepancies between the international design
recommendations and the actual design of Arabic websites. There was a
considerable variation and inconsistency in using font types and sizes between
and within the Arabic websites. Arabic Droid Kufi was used mostly for styling
page titles and navigation menus, whilst Tahoma was used for styling
paragraphs. The font size of the Arabic text ranged from 12 to 16 pixels, which
may lead to poor readability. Images were used heavily in the Arabic websites
causing prolonged site loading times. Moreover, the images strongly reflected
the dimensions of the Saudi culture, especially collectivism and masculinity.
Current Arabic web design practices are compared against the findings from past
studies about international designs and lessons aiming at ameliorating the
Arabic web design are inferred.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1904.09322v2,Compositionality of Rewriting Rules with Conditions,"We extend the notion of compositional associative rewriting as recently
studied in the rule algebra framework literature to the setting of rewriting
rules with conditions. Our methodology is category-theoretical in nature, where
the definition of rule composition operations encodes the non-deterministic
sequential concurrent application of rules in Double-Pushout (DPO) and
Sesqui-Pushout (SqPO) rewriting with application conditions based upon
$\mathcal{M}$-adhesive categories. We uncover an intricate interplay between
the category-theoretical concepts of conditions on rules and morphisms, the
compositionality and compatibility of certain shift and transport constructions
for conditions, and thirdly the property of associativity of the composition of
rules.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0407003v1,Insertion Sort is O(n log n),"Traditional Insertion Sort runs in O(n^2) time because each insertion takes
O(n) time. When people run Insertion Sort in the physical world, they leave
gaps between items to accelerate insertions. Gaps help in computers as well.
This paper shows that Gapped Insertion Sort has insertion times of O(log n)
with high probability, yielding a total running time of O(n log n) with high
probability.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1404.3272v3,Efficient Lock-free Binary Search Trees,"In this paper we present a novel algorithm for concurrent lock-free internal
binary search trees (BST) and implement a Set abstract data type (ADT) based on
that. We show that in the presented lock-free BST algorithm the amortized step
complexity of each set operation - {\sc Add}, {\sc Remove} and {\sc Contains} -
is $O(H(n) + c)$, where, $H(n)$ is the height of BST with $n$ number of nodes
and $c$ is the contention during the execution. Our algorithm adapts to
contention measures according to read-write load. If the situation is
read-heavy, the operations avoid helping pending concurrent {\sc Remove}
operations during traversal, and, adapt to interval contention. However, for
write-heavy situations we let an operation help pending {\sc Remove}, even
though it is not obstructed, and so adapt to tighter point contention. It uses
single-word compare-and-swap (\texttt{CAS}) operations. We show that our
algorithm has improved disjoint-access-parallelism compared to similar existing
algorithms. We prove that the presented algorithm is linearizable. To the best
of our knowledge this is the first algorithm for any concurrent tree data
structure in which the modify operations are performed with an additive term of
contention measure.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1712.03693v1,"Faster integer and polynomial multiplication using cyclotomic
  coefficient rings","We present an algorithm that computes the product of two n-bit integers in
O(n log n (4\sqrt 2)^{log^* n}) bit operations. Previously, the best known
bound was O(n log n 6^{log^* n}). We also prove that for a fixed prime p,
polynomials in F_p[X] of degree n may be multiplied in O(n log n 4^{log^* n})
bit operations; the previous best bound was O(n log n 8^{log^* n}).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.03036v1,Removing systematic errors for exoplanet search via latent causes,"We describe a method for removing the effect of confounders in order to
reconstruct a latent quantity of interest. The method, referred to as
half-sibling regression, is inspired by recent work in causal inference using
additive noise models. We provide a theoretical justification and illustrate
the potential of the method in a challenging astronomy application.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.06436v3,Fundamentals of Computing,"These are notes for the course CS-172 I first taught in the Fall 1986 at UC
Berkeley and subsequently at Boston University. The goal was to introduce the
undergraduates to basic concepts of Theory of Computation and to provoke their
interest in further study. Model-dependent effects were systematically ignored.
Concrete computational problems were considered only as illustrations of
general principles. The notes are skeletal: they do have (terse) proofs, but
exercises, references, intuitive comments, examples are missing or inadequate.
The notes can be used for designing a course or by students who want to refresh
the known material or are bright and have access to an instructor for
questions. Each subsection takes about a week of the course.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.14171v1,"SE-KGE: A Location-Aware Knowledge Graph Embedding Model for Geographic
  Question Answering and Spatial Semantic Lifting","Learning knowledge graph (KG) embeddings is an emerging technique for a
variety of downstream tasks such as summarization, link prediction, information
retrieval, and question answering. However, most existing KG embedding models
neglect space and, therefore, do not perform well when applied to (geo)spatial
data and tasks. For those models that consider space, most of them primarily
rely on some notions of distance. These models suffer from higher computational
complexity during training while still losing information beyond the relative
distance between entities. In this work, we propose a location-aware KG
embedding model called SE-KGE. It directly encodes spatial information such as
point coordinates or bounding boxes of geographic entities into the KG
embedding space. The resulting model is capable of handling different types of
spatial reasoning. We also construct a geographic knowledge graph as well as a
set of geographic query-answer pairs called DBGeo to evaluate the performance
of SE-KGE in comparison to multiple baselines. Evaluation results show that
SE-KGE outperforms these baselines on the DBGeo dataset for geographic logic
query answering task. This demonstrates the effectiveness of our
spatially-explicit model and the importance of considering the scale of
different geographic entities. Finally, we introduce a novel downstream task
called spatial semantic lifting which links an arbitrary location in the study
area to entities in the KG via some relations. Evaluation on DBGeo shows that
our model outperforms the baseline by a substantial margin.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1004.1887v1,"Maximized Posteriori Attributes Selection from Facial Salient Landmarks
  for Face Recognition","This paper presents a robust and dynamic face recognition technique based on
the extraction and matching of devised probabilistic graphs drawn on SIFT
features related to independent face areas. The face matching strategy is based
on matching individual salient facial graph characterized by SIFT features as
connected to facial landmarks such as the eyes and the mouth. In order to
reduce the face matching errors, the Dempster-Shafer decision theory is applied
to fuse the individual matching scores obtained from each pair of salient
facial features. The proposed algorithm is evaluated with the ORL and the IITK
face databases. The experimental results demonstrate the effectiveness and
potential of the proposed face recognition technique also in case of partially
occluded faces.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1109.1368v1,Multiple verification in computational modeling of bone pathologies,"We introduce a model checking approach to diagnose the emerging of bone
pathologies. The implementation of a new model of bone remodeling in PRISM has
led to an interesting characterization of osteoporosis as a defective bone
remodeling dynamics with respect to other bone pathologies. Our approach allows
to derive three types of model checking-based diagnostic estimators. The first
diagnostic measure focuses on the level of bone mineral density, which is
currently used in medical practice. In addition, we have introduced a novel
diagnostic estimator which uses the full patient clinical record, here
simulated using the modeling framework. This estimator detects rapid (months)
negative changes in bone mineral density. Independently of the actual bone
mineral density, when the decrease occurs rapidly it is important to alarm the
patient and monitor him/her more closely to detect insurgence of other bone
co-morbidities. A third estimator takes into account the variance of the bone
density, which could address the investigation of metabolic syndromes, diabetes
and cancer. Our implementation could make use of different logical combinations
of these statistical estimators and could incorporate other biomarkers for
other systemic co-morbidities (for example diabetes and thalassemia). We are
delighted to report that the combination of stochastic modeling with formal
methods motivate new diagnostic framework for complex pathologies. In
particular our approach takes into consideration important properties of
biosystems such as multiscale and self-adaptiveness. The multi-diagnosis could
be further expanded, inching towards the complexity of human diseases. Finally,
we briefly introduce self-adaptiveness in formal methods which is a key
property in the regulative mechanisms of biological systems and well known in
other mathematical and engineering areas.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1808.02943v5,Coaxioms: flexible coinductive definitions by inference systems,"We introduce a generalized notion of inference system to support more
flexible interpretations of recursive definitions. Besides axioms and inference
rules with the usual meaning, we allow also coaxioms, which are, intuitively,
axioms which can only be applied ""at infinite depth"" in a proof tree. Coaxioms
allow us to interpret recursive definitions as fixed points which are not
necessarily the least, nor the greatest one, whose existence is guaranteed by a
smooth extension of classical results. This notion nicely subsumes standard
inference systems and their inductive and coinductive interpretation, thus
allowing formal reasoning in cases where the inductive and coinductive
interpretation do not provide the intended meaning, but are rather mixed
together.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.10688v1,A metric Suite for Systematic Quality Assessment of Linked Open Data,"Abstract- The vision of the Linked Open Data (LOD) initiative is to provide a
distributed model for publishing and meaningfully interlinking open data. The
realization of this goal depends strongly on the quality of the data that is
published as a part of the LOD. This paper focuses on the systematic quality
assessment of datasets prior to publication on the LOD cloud. To this end, we
identify important quality deficiencies that need to be avoided and/or resolved
prior to the publication of a dataset. We then propose a set of metrics to
measure these quality deficiencies in a dataset. This way, we enable the
assessment and identification of undesirable quality characteristics of a
dataset through our proposed metrics. This will help publishers to filter out
low-quality data based on the quality assessment results, which in turn enables
data consumers to make better and more informed decisions when using the open
datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0603001v1,BioSig - An application of Octave,"BioSig is an open source software library for biomedical signal processing.
Most users in the field are using Matlab; however, significant effort was
undertaken to provide compatibility to Octave, too. This effort has been widely
successful, only some non-critical components relying on a graphical user
interface are missing. Now, installing BioSig on Octave is as easy as on
Matlab. Moreover, a benchmark test based on BioSig has been developed and the
benchmark results of several platforms are presented.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.10892v2,"Non-Crossing Shortest Paths in Undirected Unweighted Planar Graphs in
  Linear Time","Given a set of well-formed terminal pairs on the external face of an
undirected planar graph with unit edge weights, we give a linear-time algorithm
for computing the union of non-crossing shortest paths joining each terminal
pair, where well-formed means that such a set of non-crossing paths exists.
This allows us to compute distances between each terminal pair, within the same
time bound. We also give a novel concept of incremental shortest path subgraph
of a planar graph, i.e., a partition of the planar embedding in subregions that
preserve distances, that can be of interest itself.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1104.5362v2,"Selected Operations, Algorithms, and Applications of n-Tape Weighted
  Finite-State Machines","A weighted finite-state machine with n tapes (n-WFSM) defines a rational
relation on n strings. It is a generalization of weighted acceptors (one tape)
and transducers (two tapes).
  After recalling some basic definitions about n-ary weighted rational
relations and n-WFSMs, we summarize some central operations on these relations
and machines, such as join and auto-intersection. Unfortunately, due to Post's
Correspondence Problem, a fully general join or auto-intersection algorithm
cannot exist. We recall a restricted algorithm for a class of n-WFSMs.
  Through a series of practical applications, we finally investigate the
augmented descriptive power of n-WFSMs and their join, compared to classical
transducers and their composition. Some applications are not feasible with the
latter. The series includes: the morphological analysis of Semitic languages,
the preservation of intermediate results in transducer cascades, the induction
of morphological rules from corpora, the alignment of lexicon entries, the
automatic extraction of acronyms and their meaning from corpora, and the search
for cognates in a bilingual lexicon.
  All described operations and applications have been implemented with Xerox's
WFSC tool.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1305.2463v1,Determination and (re)parametrization of rational developable surfaces,"The developable surface is an important surface in computer aided design,
geometric modeling and industrial manufactory. It is often given in the stan-
dard parametric form, but it can also be in the implicit form which is commonly
used in algebraic geometry. Not all algebraic developable surfaces have
rational parametrizations. In this paper, we focus on the rational developable
surfaces. For a given algebraic surface, we first determine whether it is
developable by geometric inspection, and we give a rational proper
parametrization for the af- firmative case. For a rational parametric surface,
we can also determine the developability and give a proper reparametrization
for the developable surface.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1002.3174v3,A new approach to content-based file type detection,"File type identification and file type clustering may be difficult tasks that
have an increasingly importance in the field of computer and network security.
Classical methods of file type detection including considering file extensions
and magic bytes can be easily spoofed. Content-based file type detection is a
newer way that is taken into account recently. In this paper, a new
content-based method for the purpose of file type detection and file type
clustering is proposed that is based on the PCA and neural networks. The
proposed method has a good accuracy and is fast enough.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1905.07135v2,"Separating k-Player from t-Player One-Way Communication, with
  Applications to Data Streams","In a $k$-party communication problem, the $k$ players with inputs $x_1, x_2,
\ldots, x_k$, respectively, want to evaluate a function $f(x_1, x_2, \ldots,
x_k)$ using as little communication as possible. We consider the
message-passing model, in which the inputs are partitioned in an arbitrary,
possibly worst-case manner, among a smaller number $t$ of players ($t<k$). The
$t$-player communication cost of computing $f$ can only be smaller than the
$k$-player communication cost, since the $t$ players can trivially simulate the
$k$-player protocol. But how much smaller can it be? We study deterministic and
randomized protocols in the one-way model, and provide separations for product
input distributions, which are optimal for low error probability protocols. We
also provide much stronger separations when the input distribution is
non-product.
  A key application of our results is in proving lower bounds for data stream
algorithms. In particular, we give an optimal $\Omega(\epsilon^{-2}\log(N) \log
\log(mM))$ bits of space lower bound for the fundamental problem of
$(1\pm\epsilon)$-approximating the number $\|x\|_0$ of non-zero entries of an
$n$-dimensional vector $x$ after $m$ integer updates each of magnitude at most
$M$, and with success probability $\ge 2/3$, in a strict turnstile stream. We
additionally prove the matching $\Omega(\epsilon^{-2}\log(N) \log \log(T))$
space lower bound for the problem when we have access to a heavy hitters oracle
with threshold $T$. Our results match the best known upper bounds when
$\epsilon\ge 1/\operatorname{polylog}(mM)$ and when $T =
2^{\operatorname{poly}(1/\epsilon)}$ respectively. It also improves on the
prior $\Omega(\epsilon^{-2}\log(mM) )$ lower bound and separates the complexity
of approximating $L_0$ from approximating the $p$-norm $L_p$ for $p$ bounded
away from $0$, since the latter has an $O(\epsilon^{-2}\log (mM))$ bit upper
bound.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1508.04234v1,Compact Routing Messages in Self-Healing Trees,"Existing compact routing schemes, e.g., Thorup and Zwick [SPAA 2001] and
Chechik [PODC 2013], often have no means to tolerate failures, once the system
has been setup and started. This paper presents, to our knowledge, the first
self-healing compact routing scheme. Besides, our schemes are developed for low
memory nodes, i.e., nodes need only $O(\log^2 n)$ memory, and are thus, compact
schemes.
  We introduce two algorithms of independent interest: The first is CompactFT,
a novel compact version (using only $O(\log n)$ local memory) of the
self-healing algorithm Forgiving Tree of Hayes et al. [PODC 2008]. The second
algorithm (CompactFTZ) combines CompactFT with Thorup-Zwick's tree-based
compact routing scheme [SPAA 2001] to produce a fully compact self-healing
routing scheme. In the self-healing model, the adversary deletes nodes one at a
time with the affected nodes self-healing locally by adding few edges.
CompactFT recovers from each attack in only $O(1)$ time and $\Delta$ messages,
with only +3 degree increase and $O(log \Delta)$ graph diameter increase, over
any sequence of deletions ($\Delta$ is the initial maximum degree).
  Additionally, CompactFTZ guarantees delivery of a packet sent from sender s
as long as the receiver t has not been deleted, with only an additional $O(y
\log \Delta)$ latency, where $y$ is the number of nodes that have been deleted
on the path between $s$ and $t$. If $t$ has been deleted, $s$ gets informed and
the packet removed from the network.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1404.7092v1,Robustness against Power is PSPACE-complete,"Power is a RISC architecture developed by IBM, Freescale, and several other
companies and implemented in a series of POWER processors. The architecture
features a relaxed memory model providing very weak guarantees with respect to
the ordering and atomicity of memory accesses.
  Due to these weaknesses, some programs that are correct under sequential
consistency (SC) show undesirable effects when run under Power. We call these
programs not robust against the Power memory model. Formally, a program is
robust if every computation under Power has the same data and control
dependencies as some SC computation.
  Our contribution is a decision procedure for robustness of concurrent
programs against the Power memory model. It is based on three ideas. First, we
reformulate robustness in terms of the acyclicity of a happens-before relation.
Second, we prove that among the computations with cyclic happens-before
relation there is one in a certain normal form. Finally, we reduce the
existence of such a normal-form computation to a language emptiness problem.
Altogether, this yields a PSPACE algorithm for checking robustness against
Power. We complement it by a matching lower bound to show PSPACE-completeness.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1905.04065v2,Fast electrostatic solvers for kinetic Monte Carlo simulations,"Kinetic Monte Carlo (KMC) is an important computational tool in physics and
chemistry. In contrast to standard Monte Carlo, KMC permits the description of
time dependent dynamical processes and is not restricted to systems in
equilibrium. Recently KMC has been applied successfully in modelling of novel
energy materials such as Lithium-ion batteries and solar cells. We consider
general solid state systems which contain free, interacting particles which can
hop between localised sites in the material. The KMC transition rates for those
hops depend on the change in total potential energy of the system. For charged
particles this requires the frequent calculation of electrostatic interactions,
which is usually the bottleneck of the simulation. To avoid this issue and
obtain results in reasonable times, many studies replace the long-range
potential by a short range approximation. This, however, leads to systematic
errors and unphysical results. On the other hand standard electrostatic solvers
such as Ewald summation or fast Poisson solvers are highly inefficient or
introduce uncontrollable systematic errors at high resolution. In this paper we
describe how the Fast Multipole Method by Greengard and Rokhlin can be adapted
to overcome this issue by dramatically reducing computational costs. We exploit
the fact that each update in the transition rate calculation corresponds to a
single particle move and changes the configuration only by a small amount. This
allows us to construct an algorithm which scales linearly in the number of
charges for each KMC step, something which had not been deemed to be possible
before. We demonstrate the performance and parallel scalability of the method
by implementing it in a performance portable software library. We describe the
high-level Python interface of the code which makes it easy to adapt to
specific cases.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1204.5093v1,"The Distributed Ontology Language (DOL): Ontology Integration and
  Interoperability Applied to Mathematical Formalization","The Distributed Ontology Language (DOL) is currently being standardized
within the OntoIOp (Ontology Integration and Interoperability) activity of
ISO/TC 37/SC 3. It aims at providing a unified framework for (1) ontologies
formalized in heterogeneous logics, (2) modular ontologies, (3) links between
ontologies, and (4) annotation of ontologies.
  This paper focuses on an application of DOL's meta-theoretical features in
mathematical formalization: validating relationships between ontological
formalizations of mathematical concepts in COLORE (Common Logic Repository),
which provide the foundation for formalizing real-world notions such as spatial
and temporal relations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1806.03688v1,"LexNLP: Natural language processing and information extraction for legal
  and regulatory texts","LexNLP is an open source Python package focused on natural language
processing and machine learning for legal and regulatory text. The package
includes functionality to (i) segment documents, (ii) identify key text such as
titles and section headings, (iii) extract over eighteen types of structured
information like distances and dates, (iv) extract named entities such as
companies and geopolitical entities, (v) transform text into features for model
training, and (vi) build unsupervised and supervised models such as word
embedding or tagging models. LexNLP includes pre-trained models based on
thousands of unit tests drawn from real documents available from the SEC EDGAR
database as well as various judicial and regulatory proceedings. LexNLP is
designed for use in both academic research and industrial applications, and is
distributed at https://github.com/LexPredict/lexpredict-lexnlp.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0307022v2,Transformations of Logic Programs with Goals as Arguments,"We consider a simple extension of logic programming where variables may range
over goals and goals may be arguments of predicates. In this language we can
write logic programs which use goals as data. We give practical evidence that,
by exploiting this capability when transforming programs, we can improve
program efficiency.
  We propose a set of program transformation rules which extend the familiar
unfolding and folding rules and allow us to manipulate clauses with goals which
occur as arguments of predicates. In order to prove the correctness of these
transformation rules, we formally define the operational semantics of our
extended logic programming language. This semantics is a simple variant of
LD-resolution. When suitable conditions are satisfied this semantics agrees
with LD-resolution and, thus, the programs written in our extended language can
be run by ordinary Prolog systems.
  Our transformation rules are shown to preserve the operational semantics and
termination.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0510079v2,Evidence with Uncertain Likelihoods,"An agent often has a number of hypotheses, and must choose among them based
on observations, or outcomes of experiments. Each of these observations can be
viewed as providing evidence for or against various hypotheses. All the
attempts to formalize this intuition up to now have assumed that associated
with each hypothesis h there is a likelihood function \mu_h, which is a
probability measure that intuitively describes how likely each observation is,
conditional on h being the correct hypothesis. We consider an extension of this
framework where there is uncertainty as to which of a number of likelihood
functions is appropriate, and discuss how one formal approach to defining
evidence, which views evidence as a function from priors to posteriors, can be
generalized to accommodate this uncertainty.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1305.4912v2,Rules with parameters in modal logic I,"We study admissibility of inference rules and unification with parameters in
transitive modal logics (extensions of K4), in particular we generalize various
results on parameter-free admissibility and unification to the setting with
parameters.
  Specifically, we give a characterization of projective formulas generalizing
Ghilardi's characterization in the parameter-free case, leading to new proofs
of Rybakov's results that admissibility with parameters is decidable and
unification is finitary for logics satisfying suitable frame extension
properties (called cluster-extensible logics in this paper). We construct
explicit bases of admissible rules with parameters for cluster-extensible
logics, and give their semantic description. We show that in the case of
finitely many parameters, these logics have independent bases of admissible
rules, and determine which logics have finite bases.
  As a sideline, we show that cluster-extensible logics have various nice
properties: in particular, they are finitely axiomatizable, and have an
exponential-size model property. We also give a rather general characterization
of logics with directed (filtering) unification.
  In the sequel, we will use the same machinery to investigate the
computational complexity of admissibility and unification with parameters in
cluster-extensible logics, and we will adapt the results to logics with unique
top cluster (e.g., S4.2) and superintuitionistic logics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1402.5708v1,"The Cerebellum: New Computational Model that Reveals its Primary
  Function to Calculate Multibody Dynamics Conform to Lagrange-Euler
  Formulation","Cerebellum is part of the brain that occupies only 10% of the brain volume,
but it contains about 80% of total number of brain neurons. New cerebellar
function model is developed that sets cerebellar circuits in context of
multibody dynamics model computations, as important step in controlling balance
and movement coordination, functions performed by two oldest parts of the
cerebellum. Model gives new functional interpretation for granule cells-Golgi
cell circuit, including distinct function for upper and lower Golgi cell
dendritc trees, and resolves issue of sharing Granule cells between Purkinje
cells. Sets new function for basket cells, and for stellate cells according to
position in molecular layer. New model enables easily and direct integration of
sensory information from vestibular system and cutaneous mechanoreceptors, for
balance, movement and interaction with environments. Model gives explanation of
Purkinje cells convergence on deep-cerebellar nuclei.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0609049v2,"Scanning and Sequential Decision Making for Multi-Dimensional Data -
  Part I: the Noiseless Case","We investigate the problem of scanning and prediction (""scandiction"", for
short) of multidimensional data arrays. This problem arises in several aspects
of image and video processing, such as predictive coding, for example, where an
image is compressed by coding the error sequence resulting from scandicting it.
Thus, it is natural to ask what is the optimal method to scan and predict a
given image, what is the resulting minimum prediction loss, and whether there
exist specific scandiction schemes which are universal in some sense.
  Specifically, we investigate the following problems: First, modeling the data
array as a random field, we wish to examine whether there exists a scandiction
scheme which is independent of the field's distribution, yet asymptotically
achieves the same performance as if this distribution was known. This question
is answered in the affirmative for the set of all spatially stationary random
fields and under mild conditions on the loss function. We then discuss the
scenario where a non-optimal scanning order is used, yet accompanied by an
optimal predictor, and derive bounds on the excess loss compared to optimal
scanning and prediction.
  This paper is the first part of a two-part paper on sequential decision
making for multi-dimensional data. It deals with clean, noiseless data arrays.
The second part deals with noisy data arrays, namely, with the case where the
decision maker observes only a noisy version of the data, yet it is judged with
respect to the original, clean data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1808.02778v1,"Using Applied Behavior Analysis in Software to help Tutor Individuals
  with Autism Spectrum Disorder","There are currently many tutoring software systems which have been designed
for neurotypical children. These systems cover academic topics such as reading
and math, and are made available through various technological mediums. The
majority of these systems were not designed for use by children with special
needs, in particular those who are diagnosed with Autism Spectrum Disorder.
Since the 1970's, studies have been conducted on the use of Applied Behavior
Analysis to help autistic children learn [1]. This teaching methodology is
proven to be very effective, with many patients having their diagnosis of
autism dropped after a few years of treatment. With the advent of ubiquitous
technologies such as mobile devices, it has become apparent that these devices
could also be used to help tutor autistic children on academic subjects such as
reading and math. Though the delivery of tutoring material must be made using
Applied Behavior Analysis techniques, given that ABA therapy is currently the
only form of treatment for Autism Spectrum Disorder endorsed by the US Surgeon
General [2], which further makes the case for incorporating it into an
academics tutoring system tailored for autistic children. In this paper, we
present a mobile software system which can be utilized to tutor children who
are diagnosed with Autism Spectrum Disorder in the subjects of reading and
math. The software makes use of Applied Behavior Analysis techniques such as a
Token Economy system, visual and audible reinforcers, and generalization.
Furthermore, we explore how combining Applied Behavior Analysis and technology,
could help extend the reach of tutoring systems to these children.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1001.2623v1,"A Steganography Based on CT-CDMA Communication Scheme Using Complete
  Complementary Codes","It has been shown that complete complementary codes can be applied into some
communication systems like approximately synchronized CDMA systems because of
its good correlation properties. CT-CDMA is one of the communication systems
based on complete complementary codes. In this system, the information data of
the multiple users can be transmitted by using the same set of complementary
codes through a single frequency band. In this paper, we propose to apply
CT-CDMA systems into a kind of steganography. It is shown that a large amount
of secret data can be embedded in the stego image by the proposed method
through some numerical experiments using color images.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1210.1665v1,"Annotation of Logic Programs for Independent AND-Parallelism by Partial
  Evaluation","Traditional approaches to automatic AND-parallelization of logic programs
rely on some static analysis to identify independent goals that can be safely
and efficiently run in parallel in any possible execution. In this paper, we
present a novel technique for generating annotations for independent
AND-parallelism that is based on partial evaluation. Basically, we augment a
simple partial evaluation procedure with (run-time) groundness and variable
sharing information so that parallel conjunctions are added to the residual
clauses when the conditions for independence are met. In contrast to previous
approaches, our partial evaluator is able to transform the source program in
order to expose more opportunities for parallelism. To the best of our
knowledge, we present the first approach to a parallelizing partial evaluator.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1201.2387v1,Network Coding Meets Information-Centric Networking,"The focus of user behavior in the Internet has changed over the recent years
towards being driven by exchanging and accessing information. Many advances in
networking technologies have utilized this change by focusing on the content of
an exchange rather than the endpoints exchanging the content. Network coding
and information centric networking are two examples of these technology trends,
each being developed largely independent so far. This paper brings these areas
together in an evolutionary as well as explorative setting for a new
internetworking architecture. We outline opportunities for applying network
coding in a novel and performance-enhancing way that could eventually push
forward the case for information centric network itself.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9904011v1,WebScript -- A Scripting Language for the Web,"WebScript is a scripting language for processing Web documents. Designed as
an extension to Jacl, the Java implementation of Tcl, WebScript allows
programmers to manipulate HTML in the same way as Tcl manipulates text strings
and GUI elements. This leads to a completely new way of writing the next
generation of Web applications. This paper presents the motivation behind the
design and implementation of WebScript, an overview of its major features, as
well as some demonstrations of its power.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1708.06839v1,"Back to the Future: an Even More Nearly Optimal Cardinality Estimation
  Algorithm","We describe a new cardinality estimation algorithm that is extremely
space-efficient. It applies one of three novel estimators to the compressed
state of the Flajolet-Martin-85 coupon collection process. In an
apples-to-apples empirical comparison against compressed HyperLogLog sketches,
the new algorithm simultaneously wins on all three dimensions of the
time/space/accuracy tradeoff. Our prototype uses the zstd compression library,
and produces sketches that are smaller than the entropy of HLL, so no possible
implementation of compressed HLL can match its space efficiency. The paper's
technical contributions include analyses and simulations of the three new
estimators, accurate values for the entropies of FM85 and HLL, and a
non-trivial method for estimating a double asymptotic limit via simulation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1701.04182v1,"hMDAP: A Hybrid Framework for Multi-paradigm Data Analytical Processing
  on Spark","We propose hMDAP, a hybrid framework for large-scale data analytical
processing on Spark, to support multi-paradigm process (incl. OLAP, machine
learning, and graph analysis etc.) in distributed environments. The framework
features a three-layer data process module and a business process module which
controls the former. We will demonstrate the strength of hMDAP by using traffic
scenarios in a real world.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.06814v2,Gain and Pain of a Reliable Delay Model,"State-of-the-art digital circuit design tools almost exclusively rely on pure
and inertial delay for timing simulations. While these provide reasonable
estimations at very low execution time in the average case, their ability to
cover complex signal traces is limited. Research has provided the dynamic
Involution Delay Model (IDM) as a promising alternative, which was shown (i) to
depict reality more closely and recently (ii) to be compatible with modern
simulation suites. In this paper we complement these encouraging results by
experimentally exploring the behavioral coverage for more advanced circuits. In
detail we apply the IDM to three simple circuits (a combinatorial loop, an SR
latch and an adder), interpret the delivered results and evaluate the overhead
in realistic settings. Comparisons to digital (inertial delay) and analog
(SPICE) simulations reveal, that the IDM delivers very fine-grained results,
which match analog simulations very closely. Moreover, severe shortcomings of
inertial delay become apparent in our simulations, as it fails to depict a
range of malicious behaviors. Overall the Involution Delay Model hence
represents a viable upgrade to the available delay models in modern digital
timing simulation tools.",0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0806.2555v1,"Frequency of Correctness versus Average-Case Polynomial Time and
  Generalized Juntas","We prove that every distributional problem solvable in polynomial time on the
average with respect to the uniform distribution has a frequently
self-knowingly correct polynomial-time algorithm. We also study some features
of probability weight of correctness with respect to generalizations of
Procaccia and Rosenschein's junta distributions [PR07b].",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0803.0167v1,"Does an awareness of differing types of spreadsheet errors aid end-users
  in identifying spreadsheets errors?","The research presented in this paper establishes a valid, and simplified,
revision of previous spreadsheet error classifications. This investigation is
concerned with the results of a web survey and two web-based gender and
domain-knowledge free spreadsheet error identification exercises. The
participants of the survey and exercises were a test group of professionals
(all of whom regularly use spreadsheets) and a control group of students from
the University of Greenwich (UK). The findings show that over 85% of users are
also the spreadsheet's developer, supporting the revised spreadsheet error
classification. The findings also show that spreadsheet error identification
ability is directly affected both by spreadsheet experience and by error-type
awareness. In particular, that spreadsheet error-type awareness significantly
improves the user's ability to identify, the more surreptitious, qualitative
error.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1
http://arxiv.org/abs/1706.09506v1,Symmetry-guided design of topologies for supercomputer networks,"A family of graphs optimized as the topologies for supercomputer
interconnection networks is proposed. The special needs of such network
topologies, minimal diameter and mean path length, are met by special
constructions of the weight vectors in a representation of the symplectic
algebra. Such theoretical design of topologies can conveniently reconstruct the
mesh and hypercubic graphs, widely used as today's network topologies. Our
symplectic algebraic approach helps generate many classes of graphs suitable
for network topologies.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1810.05814v1,Categorical Aspects of Parameter Learning,"Parameter learning is the technique for obtaining the probabilistic
parameters in conditional probability tables in Bayesian networks from tables
with (observed) data --- where it is assumed that the underlying graphical
structure is known. There are basically two ways of doing so, referred to as
maximal likelihood estimation (MLE) and as Bayesian learning. This paper
provides a categorical analysis of these two techniques and describes them in
terms of basic properties of the multiset monad M, the distribution monad D and
the Giry monad G. In essence, learning is about the reltionships between
multisets (used for counting) on the one hand and probability distributions on
the other. These relationsips will be described as suitable natural
transformations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1401.5269v2,Stable Roommates Problem with Random Preferences,"The stable roommates problem with $n$ agents has worst case complexity
$O(n^2)$ in time and space. Random instances can be solved faster and with less
memory, however. We introduce an algorithm that has average time and space
complexity $O(n^\frac{3}{2})$ for random instances. We use this algorithm to
simulate large instances of the stable roommates problem and to measure the
probabilty $p_n$ that a random instance of size $n$ admits a stable matching.
Our data supports the conjecture that $p_n = \Theta(n^{-1/4})$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1608.08139v2,Where is my Phone ? Personal Object Retrieval from Egocentric Images,"This work presents a retrieval pipeline and evaluation scheme for the problem
of finding the last appearance of personal objects in a large dataset of images
captured from a wearable camera. Each personal object is modelled by a small
set of images that define a query for a visual search engine.The retrieved
results are reranked considering the temporal timestamps of the images to
increase the relevance of the later detections. Finally, a temporal
interleaving of the results is introduced for robustness against false
detections. The Mean Reciprocal Rank is proposed as a metric to evaluate this
problem. This application could help into developing personal assistants
capable of helping users when they do not remember where they left their
personal belongings.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1908.07894v1,"Visualization in the preprocessing phase: an interview study with
  enterprise professionals","The current information age has increasingly required organizations to become
data-driven. However, analyzing and managing raw data is still a challenging
part of the data mining process. Even though we can find interview studies
proposing design implications or recommendations for future visualization
solutions in the data mining scope, they cover the entire workflow and do not
fully focus on the challenges during the preprocessing phase and on how
visualization can support it. Moreover, they do not organize a final list of
insights consolidating the findings of other related studies. Hence, to better
understand the current practice of enterprise professionals in data mining
workflows, in particular during the preprocessing phase, and how visualization
supports this process, we conducted semi-structured interviews with thirteen
data analysts. The discussion about the challenges and opportunities based on
the responses of the interviewees resulted in a list of ten insights. This list
was compared with the closest related works, improving the reliability of our
findings and providing background, as a consolidated set of requirements, for
future visualization research papers applied to visual data exploration in data
mining. Furthermore, we provide greater details on the profile of the data
analysts, the main challenges they face, and the opportunities that arise while
they are engaged in data mining projects in diverse organizational areas.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1106.5694v2,"GPU-Based Heuristic Solver for Linear Sum Assignment Problems Under
  Real-time Constraints","In this paper we modify a fast heuristic solver for the Linear Sum Assignment
Problem (LSAP) for use on Graphical Processing Units (GPUs). The motivating
scenario is an industrial application for P2P live streaming that is moderated
by a central node which is periodically solving LSAP instances for assigning
peers to one another. The central node needs to handle LSAP instances involving
thousands of peers in as near to real-time as possible. Our findings are
generic enough to be applied in other contexts. Our main result is a parallel
version of a heuristic algorithm called Deep Greedy Switching (DGS) on GPUs
using the CUDA programming language. DGS sacrifices absolute optimality in
favor of low computation time and was designed as an alternative to classical
LSAP solvers such as the Hungarian and auctioning methods. The contribution of
the paper is threefold: First, we present the process of trial and error we
went through, in the hope that our experience will be beneficial to adopters of
GPU programming for similar problems. Second, we show the modifications needed
to parallelize the DGS algorithm. Third, we show the performance gains of our
approach compared to both a sequential CPU-based implementation of DGS and a
parallel GPU-based implementation of the auctioning algorithm.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.03025v2,Linear equations for unordered data vectors in $[D]^k\to{}Z^d$,"Following a recently considered generalisation of linear equations to
unordered-data vectors and to ordered-data vectors, we perform a further
generalisation to data vectors that are functions from k-element subsets of the
unordered-data set to vectors of integer numbers. These generalised equations
naturally appear in the analysis of vector addition systems (or Petri nets)
extended so that each token carries a set of unordered data. We show that
nonnegative-integer solvability of linear equations is in nondeterministic
exponential time while integer solvability is in polynomial time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1608.01700v3,"Approximation Algorithms for Clustering Problems with Lower Bounds and
  Outliers","We consider clustering problems with {\em non-uniform lower bounds and
outliers}, and obtain the {\em first approximation guarantees} for these
problems. We have a set $\F$ of facilities with lower bounds $\{L_i\}_{i\in\F}$
and a set $\D$ of clients located in a common metric space
$\{c(i,j)\}_{i,j\in\F\cup\D}$, and bounds $k$, $m$. A feasible solution is a
pair $\bigl(S\sse\F,\sigma:\D\mapsto S\cup\{\mathsf{out}\}\bigr)$, where
$\sigma$ specifies the client assignments, such that $|S|\leq k$,
$|\sigma^{-1}(i)|\geq L_i$ for all $i\in S$, and
$|\sigma^{-1}(\mathsf{out})|\leq m$. In the {\em lower-bounded min-sum-of-radii
with outliers} (\lbksro) problem, the objective is to minimize $\sum_{i\in
S}\max_{j\in\sigma^{-1}(i)}c(i,j)$, and in the {\em lower-bounded $k$-supplier
with outliers} (\lbkso) problem, the objective is to minimize $\max_{i\in
S}\max_{j\in\sigma^{-1}(i)}c(i,j)$.
  We obtain an approximation factor of $12.365$ for \lbksro, which improves to
$3.83$ for the non-outlier version (i.e., $m=0$). These also constitute the
{\em first} approximation bounds for the min-sum-of-radii objective when we
consider lower bounds and outliers {\em separately}. We apply the primal-dual
method to the relaxation where we Lagrangify the $|S|\leq k$ constraint. The
chief technical contribution and novelty of our algorithm is that, departing
from the standard paradigm used for such constrained problems, we obtain an
$O(1)$-approximation {\em despite the fact that we do not obtain a
Lagrangian-multiplier-preserving algorithm for the Lagrangian relaxation}. We
believe that our ideas have {broader applicability to other clustering problems
with outliers as well.}
  We obtain approximation factors of $5$ and $3$ respectively for \lbkso and
its non-outlier version. These are the {\em first} approximation results for
$k$-supplier with {\em non-uniform} lower bounds.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.13352v1,Graph Neural Network for Hamiltonian-Based Material Property Prediction,"Development of next-generation electronic devices for applications call for
the discovery of quantum materials hosting novel electronic, magnetic, and
topological properties. Traditional electronic structure methods require
expensive computation time and memory consumption, thus a fast and accurate
prediction model is desired with increasing importance. Representing the
interactions among atomic orbitals in any material, a material Hamiltonian
provides all the essential elements that control the structure-property
correlations in inorganic compounds. Effective learning of material Hamiltonian
by developing machine learning methodologies therefore offers a transformative
approach to accelerate the discovery and design of quantum materials. With this
motivation, we present and compare several different graph convolution networks
that are able to predict the band gap for inorganic materials. The models are
developed to incorporate two different features: the information of each
orbital itself and the interaction between each other. The information of each
orbital includes the name, relative coordinates with respect to the center of
super cell and the atom number, while the interaction between orbitals are
represented by the Hamiltonian matrix. The results show that our model can get
a promising prediction accuracy with cross-validation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1311.2531v1,Motility at the origin of life: Its characterization and a model,"Due to recent advances in synthetic biology and artificial life, the origin
of life is currently a hot topic of research. We review the literature and
argue that the two traditionally competing ""replicator-first"" and
""metabolism-first"" approaches are merging into one integrated theory of
individuation and evolution. We contribute to the maturation of this more
inclusive approach by highlighting some problematic assumptions that still lead
to an impoverished conception of the phenomenon of life. In particular, we
argue that the new consensus has so far failed to consider the relevance of
intermediate timescales. We propose that an adequate theory of life must
account for the fact that all living beings are situated in at least four
distinct timescales, which are typically associated with metabolism, motility,
development, and evolution. On this view, self-movement, adaptive behavior and
morphological changes could have already been present at the origin of life. In
order to illustrate this possibility we analyze a minimal model of life-like
phenomena, namely of precarious, individuated, dissipative structures that can
be found in simple reaction-diffusion systems. Based on our analysis we suggest
that processes in intermediate timescales could have already been operative in
prebiotic systems. They may have facilitated and constrained changes occurring
in the faster- and slower-paced timescales of chemical self-individuation and
evolution by natural selection, respectively.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.13417v1,"Movie Box office Prediction via Joint Actor Representations and Social
  Media Sentiment","In recent years, driven by the Asian film industry, such as China and India,
the global box office has maintained a steady growth trend. Previous studies
have rarely used long-term, full-sample film data in analysis, lack of research
on actors' social networks. Existing film box office prediction algorithms only
use film meta-data, lack of using social network characteristics and the model
is less interpretable. I propose a FC-GRU-CNN binary classification model in of
box office prediction task, combining five characteristics, including the film
meta-data, Sina Weibo text sentiment, actors' social network measurement, all
pairs shortest path and actors' art contribution. Exploiting long-term memory
ability of GRU layer in long sequences and the mapping ability of CNN layer in
retrieving all pairs shortest path matrix features, proposed model is 14%
higher in accuracy than the current best C-LSTM model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1203.3997v1,CloudGenius: Decision Support for Web Server Cloud Migration,"Cloud computing is the latest computing paradigm that delivers hardware and
software resources as virtualized services in which users are free from the
burden of worrying about the low-level system administration details. Migrating
Web applications to Cloud services and integrating Cloud services into existing
computing infrastructures is non-trivial. It leads to new challenges that often
require innovation of paradigms and practices at all levels: technical,
cultural, legal, regulatory, and social. The key problem in mapping Web
applications to virtualized Cloud services is selecting the best and compatible
mix of software images (e.g., Web server image) and infrastructure services to
ensure that Quality of Service (QoS) targets of an application are achieved.
The fact that, when selecting Cloud services, engineers must consider
heterogeneous sets of criteria and complex dependencies between infrastructure
services and software images, which are impossible to resolve manually, is a
critical issue. To overcome these challenges, we present a framework (called
CloudGenius) which automates the decision-making process based on a model and
factors specifically for Web server migration to the Cloud. CloudGenius
leverages a well known multi-criteria decision making technique, called
Analytic Hierarchy Process, to automate the selection process based on a model,
factors, and QoS parameters related to an application. An example application
demonstrates the applicability of the theoretical CloudGenius approach.
Moreover, we present an implementation of CloudGenius that has been validated
through experiments.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1102.4078v2,"An Analytical Model for Service Profile Based Service Quality of an
  Institutional eLibrary","Devising a scheme for evaluating the service quality of an institutional
electronic library is a difficult and challenging task. The challenge comes
from the fact that the services provided by an institutional electronic library
depend upon the contents requested by the users and the contents housed by the
library. Different types of users might be interested in different types of
contents. In this paper, we propose a technique for evaluating the service
quality of an institutional electronic library. Our scheme is based on the
service profiles of contents requested by the users at the server side which is
hosted at the library. Further, we propose models to analyze the service
quality of an electronic library. For analyzing the service quality, we present
two analytical models. The first one is based on the number of days by which
the item to be served by the library is delayed and the penalty points per day
for the duration for which the item is delayed. The second model is based on
the credits earned by the library if the item is served in a timely fashion,
and the penalties, thereof, if the item is delayed. These models may help in
evaluating the service quality of an electronic library and taking the
corrective measures to improve it.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1707.06817v1,Fluid and Diffusion Limits for Bike Sharing Systems,"Bike sharing systems have rapidly developed around the world, and they are
served as a promising strategy to improve urban traffic congestion and to
decrease polluting gas emissions. So far performance analysis of bike sharing
systems always exists many difficulties and challenges under some more general
factors. In this paper, a more general large-scale bike sharing system is
discussed by means of heavy traffic approximation of multiclass closed queueing
networks with non-exponential factors. Based on this, the fluid scaled
equations and the diffusion scaled equations are established by means of the
numbers of bikes both at the stations and on the roads, respectively.
Furthermore, the scaling processes for the numbers of bikes both at the
stations and on the roads are proved to converge in distribution to a
semimartingale reflecting Brownian motion (SRBM) in a $N^{2}$-dimensional box,
and also the fluid and diffusion limit theorems are obtained. Furthermore,
performance analysis of the bike sharing system is provided. Thus the results
and methodology of this paper provide new highlight in the study of more
general large-scale bike sharing systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.07424v1,"Investigation of Data Deletion Vulnerabilities in NAND Flash Memory
  Based Storage","Semiconductor NAND Flash based memory technology dominates the electronic
Non-Volatile storage media market. Though NAND Flash offers superior
performance and reliability over conventional magnetic HDDs, yet it suffers
from certain data-security vulnerabilities. Such vulnerabilities can expose
sensitive information stored on the media to security risks. It is thus
necessary to study in detail the fundamental reasons behind data-security
vulnerabilities of NAND Flash for use in critical applications. In this paper,
the problem of unreliable data-deletion/sanitization in commercial NAND Flash
media is investigated along with the fundamental reasons leading to such
vulnerabilities. Exhaustive software based data recovery experiments (multiple
iterations) has been carried out on commercial NAND Flash storage media (8 GB
and 16 GB) for different types of filesystems (NTFS and FAT) and OS specific
delete/Erase instructions. 100 % data recovery is obtained for windows and
linux based delete/Erase commands. Inverse effect of performance enhancement
techniques like wear levelling, bad block management etc. is also observed with
the help of software based recovery experiments.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0609121v1,"Approximating Rate-Distortion Graphs of Individual Data: Experiments in
  Lossy Compression and Denoising","Classical rate-distortion theory requires knowledge of an elusive source
distribution. Instead, we analyze rate-distortion properties of individual
objects using the recently developed algorithmic rate-distortion theory. The
latter is based on the noncomputable notion of Kolmogorov complexity. To apply
the theory we approximate the Kolmogorov complexity by standard data
compression techniques, and perform a number of experiments with lossy
compression and denoising of objects from different domains. We also introduce
a natural generalization to lossy compression with side information. To
maintain full generality we need to address a difficult searching problem.
While our solutions are therefore not time efficient, we do observe good
denoising and compression performance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1510.04821v2,The Vampire and the FOOL,"This paper presents new features recently implemented in the theorem prover
Vampire, namely support for first-order logic with a first class boolean sort
(FOOL) and polymorphic arrays. In addition to having a first class boolean
sort, FOOL also contains if-then-else and let-in expressions. We argue that
presented extensions facilitate reasoning-based program analysis, both by
increasing the expressivity of first-order reasoners and by gains in
efficiency.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.08831v1,Efficiently updating a covariance matrix and its LDL decomposition,"Equations are presented which efficiently update or downdate the covariance
matrix of a large number of $m$-dimensional observations. Updates and downdates
to the covariance matrix, as well as mixed updates/downdates, are shown to be
rank-$k$ modifications, where $k$ is the number of new observations added plus
the number of old observations removed. As a result, the update and downdate
equations decrease the required number of multiplications for a modification to
$\Theta((k+1)m^2)$ instead of $\Theta((n+k+1)m^2)$ or $\Theta((n-k+1)m^2)$,
where $n$ is the number of initial observations. Having the rank-$k$ formulas
for the updates also allows a number of other known identities to be applied,
providing a way of applying updates and downdates directly to the inverse and
decompositions of the covariance matrix. To illustrate, we provide an efficient
algorithm for applying the rank-$k$ update to the LDL decomposition of a
covariance matrix.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0109072v1,Higher-Order Pattern Complement and the Strict Lambda-Calculus,"We address the problem of complementing higher-order patterns without
repetitions of existential variables. Differently from the first-order case,
the complement of a pattern cannot, in general, be described by a pattern, or
even by a finite set of patterns. We therefore generalize the simply-typed
lambda-calculus to include an internal notion of strict function so that we can
directly express that a term must depend on a given variable. We show that, in
this more expressive calculus, finite sets of patterns without repeated
variables are closed under complement and intersection. Our principal
application is the transformational approach to negation in higher-order logic
programs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0403008v3,"Polynomial-time computing over quadratic maps I: sampling in real
  algebraic sets","Given a quadratic map Q : K^n -> K^k defined over a computable subring D of a
real closed field K, and a polynomial p(Y_1,...,Y_k) of degree d, we consider
the zero set Z=Z(p(Q(X)),K^n) of the polynomial p(Q(X_1,...,X_n)). We present a
procedure that computes, in (dn)^O(k) arithmetic operations in D, a set S of
(real univariate representations of) sampling points in K^n that intersects
nontrivially each connected component of Z. As soon as k=o(n), this is faster
than the standard methods that all have exponential dependence on n in the
complexity. In particular, our procedure is polynomial-time for constant k. In
contrast, the best previously known procedure (due to A.Barvinok) is only
capable of deciding in n^O(k^2) operations the nonemptiness (rather than
constructing sampling points) of the set Z in the case of p(Y)=sum_i Y_i^2 and
homogeneous Q.
  A by-product of our procedure is a bound (dn)^O(k) on the number of connected
components of Z.
  The procedure consists of exact symbolic computations in D and outputs
vectors of algebraic numbers. It involves extending K by infinitesimals and
subsequent limit computation by a novel procedure that utilizes knowledge of an
explicit isomorphism between real algebraic sets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1211.4347v4,How many software engineering professionals hold this certificate?,"Estimates of quantity of the certificates issued during 10 years of existence
of the professionals certification program in the area of software engineering
implemented by one of the leading professional associations are presented. The
estimates have been obtained by way of processing certificate records openly
accessible at the certification program web-site. Comparison of these estimates
and the known facts about evolution of the certification program indicates that
as of the present day this evolution has not led to a large scale issuance of
these certificates. But the same estimates, possibly, indicate that the meaning
of these certificates differs from what is usually highlighted, and their real
value is much greater. Also these estimates can be viewed, besides everything
else, as reflecting an outcome of a decade long experimental verification of
the known idea about ""software engineering as a mature engineering profession,""
and they possibly show that this idea deserves partial revision.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
http://arxiv.org/abs/cs/0512105v2,"A study of the edge-switching Markov-chain method for the generation of
  random graphs","We study the problem of generating connected random graphs with no self-loops
or multiple edges and that, in addition, have a given degree sequence. The
generation method we focus on is the edge-switching Markov-chain method, whose
functioning depends on a parameter w related to the method's core operation of
an edge switch. We analyze two existing heuristics for adjusting w during the
generation of a graph and show that they result in a Markov chain whose
stationary distribution is uniform, thus ensuring that generation occurs
uniformly at random. We also introduce a novel w-adjusting heuristic which,
even though it does not always lead to a Markov chain, is still guaranteed to
converge to the uniform distribution under relatively mild conditions. We
report on extensive computer experiments comparing the three heuristics'
performance at generating random graphs whose node degrees are distributed as
power laws.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0412031v1,"The Features of the Complex CAD system of Reconstruction of the
  Industrial Plants","The features of designing of reconstruction of the acting plant by its design
department are considered: the results of work are drawings corresponding with
the national standards; large number of the small projects for different acting
objects; variety of the types of the drawings in one project; large paper
archive. The models and methods of developing of the complex CAD system with
friend uniform environment of designing, with setting a profile of operations,
with usage of the general parts of the project, with a series of
problem-oriented subsystems are described on an example of a CAD system
TechnoCAD GlassX",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0910.4711v1,"Parallelization of the LBG Vector Quantization Algorithm for Shared
  Memory Systems","This paper proposes a parallel approach for the Vector Quantization (VQ)
problem in image processing. VQ deals with codebook generation from the input
training data set and replacement of any arbitrary data with the nearest
codevector. Most of the efforts in VQ have been directed towards designing
parallel search algorithms for the codebook, and little has hitherto been done
in evolving a parallelized procedure to obtain an optimum codebook. This
parallel algorithm addresses the problem of designing an optimum codebook using
the traditional LBG type of vector quantization algorithm for shared memory
systems and for the efficient usage of parallel processors. Using the codebook
formed from a training set, any arbitrary input data is replaced with the
nearest codevector from the codebook. The effectiveness of the proposed
algorithm is indicated.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1106.1845v3,Byzantine Broadcast in Point-to-Point Networks using Local Linear Coding,"The goal of Byzantine Broadcast (BB) is to allow a set of fault-free nodes to
agree on information that a source node wants to broadcast to them, in the
presence of Byzantine faulty nodes. We consider design of efficient algorithms
for BB in {\em synchronous} point-to-point networks, where the rate of
transmission over each communication link is limited by its ""link capacity"".
The throughput of a particular BB algorithm is defined as the average number of
bits that can be reliably broadcast to all fault-free nodes per unit time using
the algorithm without violating the link capacity constraints. The {\em
capacity} of BB in a given network is then defined as the supremum of all
achievable BB throughputs in the given network, over all possible BB
algorithms.
  We develop NAB -- a Network-Aware Byzantine broadcast algorithm -- for
arbitrary point-to-point networks consisting of $n$ nodes, wherein the number
of faulty nodes is at most $f$, $f<n/3$, and the network connectivity is at
least $2f+1$. We also prove an upper bound on the capacity of Byzantine
broadcast, and conclude that NAB can achieve throughput at least 1/3 of the
capacity. When the network satisfies an additional condition, NAB can achieve
throughput at least 1/2 of the capacity.
  To the best of our knowledge, NAB is the first algorithm that can achieve a
constant fraction of capacity of Byzantine Broadcast (BB) in arbitrary
point-to-point networks.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0904.4512v1,Bounds on series-parallel slowdown,"We use activity networks (task graphs) to model parallel programs and
consider series-parallel extensions of these networks. Our motivation is
two-fold: the benefits of series-parallel activity networks and the modelling
of programming constructs, such as those imposed by current parallel computing
environments. Series-parallelisation adds precedence constraints to an activity
network, usually increasing its makespan (execution time). The slowdown ratio
describes how additional constraints affect the makespan. We disprove an
existing conjecture positing a bound of two on the slowdown when workload is
not considered. Where workload is known, we conjecture that 4/3 slowdown is
always achievable, and prove our conjecture for small networks using max-plus
algebra. We analyse a polynomial-time algorithm showing that achieving 4/3
slowdown is in exp-APX. Finally, we discuss the implications of our results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1605.03644v1,How to Build Your Network? A Structural Analysis,"Creating new ties in a social network facilitates knowledge exchange and
affects positional advantage. In this paper, we study the process, which we
call network building, of establishing ties between two existing social
networks in order to reach certain structural goals. We focus on the case when
one of the two networks consists only of a single member and motivate this case
from two perspectives. The first perspective is socialization: we ask how a
newcomer can forge relationships with an existing network to place herself at
the center. We prove that obtaining optimal solutions to this problem is
NP-complete, and present several efficient algorithms to solve this problem and
compare them with each other. The second perspective is network expansion: we
investigate how a network may preserve or reduce its diameter through linking
with a new node, hence ensuring small distance between its members. We give two
algorithms for this problem. For both perspectives the experiment demonstrates
that a small number of new links is usually sufficient to reach the respective
goal.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1611.00323v1,Self-Awareness of Cloud Applications,"Cloud applications today deliver an increasingly larger portion of the
Information and Communication Technology (ICT) services. To address the scale,
growth, and reliability of cloud applications, self-aware management and
scheduling are becoming commonplace. How are they used in practice? In this
chapter, we propose a conceptual framework for analyzing state-of-the-art
self-awareness approaches used in the context of cloud applications. We map
important applications corresponding to popular and emerging application
domains to this conceptual framework, and compare the practical
characteristics, benefits, and drawbacks of self-awareness approaches. Last, we
propose a roadmap for addressing open challenges in self-aware cloud and
datacenter applications.",1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
http://arxiv.org/abs/1507.00567v1,"Self-Learning Cloud Controllers: Fuzzy Q-Learning for Knowledge
  Evolution","Cloud controllers aim at responding to application demands by automatically
scaling the compute resources at runtime to meet performance guarantees and
minimize resource costs. Existing cloud controllers often resort to scaling
strategies that are codified as a set of adaptation rules. However, for a cloud
provider, applications running on top of the cloud infrastructure are more or
less black-boxes, making it difficult at design time to define optimal or
pre-emptive adaptation rules. Thus, the burden of taking adaptation decisions
often is delegated to the cloud application. Yet, in most cases, application
developers in turn have limited knowledge of the cloud infrastructure. In this
paper, we propose learning adaptation rules during runtime. To this end, we
introduce FQL4KE, a self-learning fuzzy cloud controller. In particular, FQL4KE
learns and modifies fuzzy rules at runtime. The benefit is that for designing
cloud controllers, we do not have to rely solely on precise design-time
knowledge, which may be difficult to acquire. FQL4KE empowers users to specify
cloud controllers by simply adjusting weights representing priorities in system
goals instead of specifying complex adaptation rules. The applicability of
FQL4KE has been experimentally assessed as part of the cloud application
framework ElasticBench. The experimental results indicate that FQL4KE
outperforms our previously developed fuzzy controller without learning
mechanisms and the native Azure auto-scaling.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1810.05207v3,On Kernel Derivative Approximation with Random Fourier Features,"Random Fourier features (RFF) represent one of the most popular and
wide-spread techniques in machine learning to scale up kernel algorithms.
Despite the numerous successful applications of RFFs, unfortunately, quite
little is understood theoretically on their optimality and limitations of their
performance. Only recently, precise statistical-computational trade-offs have
been established for RFFs in the approximation of kernel values, kernel ridge
regression, kernel PCA and SVM classification. Our goal is to spark the
investigation of optimality of RFF-based approximations in tasks involving not
only function values but derivatives, which naturally lead to optimization
problems with kernel derivatives. Particularly, in this paper, we focus on the
approximation quality of RFFs for kernel derivatives and prove that the
existing finite-sample guarantees can be improved exponentially in terms of the
domain where they hold, using recent tools from unbounded empirical process
theory. Our result implies that the same approximation guarantee is attainable
for kernel derivatives using RFF as achieved for kernel values.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.12090v2,"Estimating Parameters in Mathematical Model for Societal Booms through
  Bayesian Inference Approach","In this study, based on our previous study, we examined the mathematical
properties, especially the stability of the equilibrium for our proposed
mathematical model. By means of the results of the stability in this study, we
also used actual data representing transient booms and resurgent booms, and
conducted parameter estimation for our proposed model using Bayesian inference.
In addition, we conducted a model fitting to five actual data. By this study,
we reconfirmed that we can express the resurgences or minute vibrations of
actual data by means of our proposed model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1806.10920v1,Machine Learning for Mathematical Software,"While there has been some discussion on how Symbolic Computation could be
used for AI there is little literature on applications in the other direction.
However, recent results for quantifier elimination suggest that, given enough
example problems, there is scope for machine learning tools like Support Vector
Machines to improve the performance of Computer Algebra Systems. We survey the
authors own work and similar applications for other mathematical software.
  It may seem that the inherently probabilistic nature of machine learning
tools would invalidate the exact results prized by mathematical software.
However, algorithms and implementations often come with a range of choices
which have no effect on the mathematical correctness of the end result but a
great effect on the resources required to find it, and thus here, machine
learning can have a significant impact.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.13357v1,Reactive Temporal Logic,"Whereas standard treatments of temporal logic are adequate for closed
systems, having no run-time interactions with their environment, they fall
short for reactive systems, interacting with their environments through
synchronisation of actions. This paper introduces reactive temporal logic, a
form of temporal logic adapted for the study of reactive systems. I illustrate
its use by applying it to formulate definitions of a fair scheduler, and of a
correct mutual exclusion protocol. Previous definitions of these concepts were
conceptually much more involved or less precise, leading to debates on whether
or not a given protocol satisfies the implicit requirements.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1803.01472v1,"Teaching the Formalization of Mathematical Theories and Algorithms via
  the Automatic Checking of Finite Models","Education in the practical applications of logic and proving such as the
formal specification and verification of computer programs is substantially
hampered by the fact that most time and effort that is invested in proving is
actually wasted in vain: because of errors in the specifications respectively
algorithms that students have developed, their proof attempts are often
pointless (because the proposition proved is actually not of interest) or a
priori doomed to fail (because the proposition to be proved does actually not
hold), this is a frequent source of frustration and gives formal methods a bad
reputation. RISCAL (RISC Algorithm Language) is a formal specification language
and associated software system that attempts to overcome this problem by making
logic formalization fun rather than a burden. To this end, RISCAL allows
students to easily validate the correctness of instances of propositions
respectively algorithms by automatically evaluating/executing and checking them
on (small) finite models. Thus many/most errors can be quickly detected and
subsequent proof attempts can be focused on propositions that are more/most
likely to be both meaningful and true.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/2109.05083v1,"Preliminary Wildfire Detection Using State-of-the-art PTZ (Pan, Tilt,
  Zoom) Camera Technology and Convolutional Neural Networks","Wildfires are uncontrolled fires in the environment that can be caused by
humans or nature. In 2020 alone, wildfires in California have burned 4.2
million acres, damaged 10,500 buildings or structures, and killed more than 31
people, exacerbated by climate change and a rise in average global
temperatures. This also means there has been an increase in the costs of
extinguishing these treacherous wildfires. The objective of the research is to
detect forest fires in their earlier stages to prevent them from spreading,
prevent them from causing damage to a variety of things, and most importantly,
reduce or eliminate the chances of someone dying from a wildfire. A fire
detection system should be efficient and accurate with respect to extinguishing
wildfires in their earlier stages to prevent the spread of them along with
their consequences. Computer Vision is potentially a more reliable, fast, and
widespread method we need. The current research in the field of preliminary
fire detection has several problems related to unrepresentative data being used
to train models and their existing varied amounts of label imbalance in the
classes of their dataset. We propose a more representative and evenly
distributed data through better settings, lighting, atmospheres, etc., and
class distribution in the entire dataset. After thoroughly examining the
results of this research, it can be inferred that they supported the datasets
strengths by being a viable resource when tested in the real world on
unfamiliar data. This is evident since as the model trains on the dataset, it
is able to generalize on it, hence confirming this is a viable Machine Learning
setting that has practical impact.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1103.2848v1,"Polynomial Weights or Generalized Geometric Weights: Yet Another Scheme
  for Assigning Credits to Multiple Authors","Devising a weight assignment policy for assigning credits to multiple authors
of a manuscript is a challenging task. In this paper, we present a scheme for
assigning credits to multiple authors that we call a polynomial weight
assignment scheme. We compare our scheme with other schemes proposed in the
literature.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1806.03997v1,Endoscopic navigation in the absence of CT imaging,"Clinical examinations that involve endoscopic exploration of the nasal cavity
and sinuses often do not have a reference image to provide structural context
to the clinician. In this paper, we present a system for navigation during
clinical endoscopic exploration in the absence of computed tomography (CT)
scans by making use of shape statistics from past CT scans. Using a deformable
registration algorithm along with dense reconstructions from video, we show
that we are able to achieve submillimeter registrations in in-vivo clinical
data and are able to assign confidence to these registrations using confidence
criteria established using simulated data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0112009v1,DNA Self-Assembly For Constructing 3D Boxes,"We propose a mathematical model of DNA self-assembly using 2D tiles to form
3D nanostructures. This is the first work to combine studies in self-assembly
and nanotechnology in 3D, just as Rothemund and Winfree did in the 2D case. Our
model is a more precise superset of their Tile Assembly Model that facilitates
building scalable 3D molecules. Under our model, we present algorithms to build
a hollow cube, which is intuitively one of the simplest 3D structures to
construct. We also introduce five basic measures of complexity to analyze these
algorithms. Our model and algorithmic techniques are applicable to more complex
2D and 3D nanostructures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.01168v1,Deploying Crowdsourcing for Workflow Driven Business Process,"The main goal of this paper is to discuss how to integrate the possibilities
of crowdsourcing platforms with systems supporting workflow to enable the
engagement and interaction with business tasks of a wider group of people.
Thus, this work is an attempt to expand the functional capabilities of typical
business systems by allowing selected process tasks to be performed by
unlimited human resources. Opening business tasks to crowdsourcing, within
established Business Process Management Systems (BPMS) will improve the
flexibility of company processes and allow for lower work-load and greater
specialization among the staff employed on-site. The presented conceptual work
is based on the current international standards in this field, promoted by
Workflows Management Coalition. To this end, the functioning of business
platforms was analysed and their functionality was presented visually, followed
by a proposal and a discussion of how to implement crowdsourcing into workflow
systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.13638v2,A Standard Grammar for Temporal Logics on Finite Traces,"The heterogeneity of tools that support temporal logic formulae poses several
challenges in terms of interoperability. In particular, a standard syntax for
temporal logic on finite traces, despite similar to the one for infinite
traces, is currently missing. This document proposes a standard grammar for
several temporal logic formalisms interpreted over finite traces, like Linear
Temporal Logic (LTLf), Linear Dynamic Logic (LDLf), Pure-Past Linear Temporal
Logic (PLTLf) and Pure-Past Linear Dynamic Logic (PLDLf).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0205062v1,"Minimizing Cache Misses in Scientific Computing Using Isoperimetric
  Bodies","A number of known techniques for improving cache performance in scientific
computations involve the reordering of the iteration space. Some of these
reorderings can be considered coverings of the iteration space with sets having
small surface-to-volume ratios. Use of such sets may reduce the number of cache
misses in computations of local operators having the iteration space as their
domain. First, we derive lower bounds on cache misses that any algorithm must
suffer while computing a local operator on a grid. Then, we explore coverings
of iteration spaces of structured and unstructured discretization grid
operators which allow us to approach these lower bounds. For structured grids
we introduce a covering by successive minima tiles based on the interference
lattice of the grid. We show that the covering has a small surface-to-volume
ratio and present a computer experiment showing actual reduction of the cache
misses achieved by using these tiles. For planar unstructured grids we show
existence of a covering which reduces the number of cache misses to the level
of that of structured grids. Next, we introduce a class of multidimensional
grids, called starry grids in this paper. These grids represent an abstraction
of unstructured grids used in, for example, molecular simulations and the
solution of partial differential equations. We show that starry grids can be
covered by sets having a low surface-to-volume ratio and, hence have the same
cache efficiency as structured grids. Finally, we present a triangulation of a
three-dimensional cube that has the property that any local operator on the
corresponding grid must incur a significantly larger number of cache misses
than a similar operator on a structured grid of the same size.",0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1409.6873v3,Probabilistic thread algebra,"We add probabilistic features to basic thread algebra and its extensions with
thread-service interaction and strategic interleaving. Here, threads represent
the behaviours produced by instruction sequences under execution and services
represent the behaviours exhibited by the components of execution environments
of instruction sequences. In a paper concerned with probabilistic instruction
sequences, we proposed several kinds of probabilistic instructions and gave an
informal explanation for each of them. The probabilistic features added to the
extension of basic thread algebra with thread-service interaction make it
possible to give a formal explanation in terms of non-probabilistic
instructions and probabilistic services. The probabilistic features added to
the extensions of basic thread algebra with strategic interleaving make it
possible to cover strategies corresponding to probabilistic scheduling
algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1202.6395v1,Functions that preserve p-randomness,"We show that polynomial-time randomness (p-randomness) is preserved under a
variety of familiar operations, including addition and multiplication by a
nonzero polynomial-time computable real number. These results follow from a
general theorem: If $I$ is an open interval in the reals, $f$ is a function
mapping $I$ into the reals, and $r$ in $I$ is p-random, then $f(r)$ is p-random
provided
  1. $f$ is p-computable on the dyadic rational points in $I$, and
  2. $f$ varies sufficiently at $r$, i.e., there exists a real constant $C > 0$
such that either (a) $(f(x) - f(r))/(x-r) > C$ for all $x$ in $I$ with $x \ne
r$, or (b) $(f(x) - f(r))(x-r) < -C$ for all $x$ in $I$ with $x \ne r$.
  Our theorem implies in particular that any analytic function about a
p-computable point whose power series has uniformly p-computable coefficients
preserves p-randomness in its open interval of absolute convergence. Such
functions include all the familiar functions from first-year calculus.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1307.6239v4,Soft Contract Verification,"Behavioral software contracts are a widely used mechanism for governing the
flow of values between components. However, run-time monitoring and enforcement
of contracts imposes significant overhead and delays discovery of faulty
components to run-time.
  To overcome these issues, we present soft contract verification, which aims
to statically prove either complete or partial contract correctness of
components, written in an untyped, higher-order language with first-class
contracts. Our approach uses higher-order symbolic execution, leveraging
contracts as a source of symbolic values including unknown behavioral values,
and employs an updatable heap of contract invariants to reason about
flow-sensitive facts. We prove the symbolic execution soundly approximates the
dynamic semantics and that verified programs can't be blamed.
  The approach is able to analyze first-class contracts, recursive data
structures, unknown functions, and control-flow-sensitive refinements of
values, which are all idiomatic in dynamic languages. It makes effective use of
an off-the-shelf solver to decide problems without heavy encodings. The
approach is competitive with a wide range of existing tools---including type
systems, flow analyzers, and model checkers---on their own benchmarks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1004.5437v1,Parallel algorithms in linear algebra,"This report provides an introduction to algorithms for fundamental linear
algebra problems on various parallel computer architectures, with the emphasis
on distributed-memory MIMD machines. To illustrate the basic concepts and key
issues, we consider the problem of parallel solution of a nonsingular linear
system by Gaussian elimination with partial pivoting. This problem has come to
be regarded as a benchmark for the performance of parallel machines. We
consider its appropriateness as a benchmark, its communication requirements,
and schemes for data distribution to facilitate communication and load
balancing. In addition, we describe some parallel algorithms for orthogonal
(QR) factorization and the singular value decomposition (SVD).",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1004.0256v1,From Playability to a Hierarchical Game Usability Model,"This paper presents a brief review of current game usability models. This
leads to the conception of a high-level game development-centered usability
model that integrates current usability approaches in game industry and game
research.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1
http://arxiv.org/abs/cs/0412115v1,"Reductions in Distributed Computing Part I: Consensus and Atomic
  Commitment Tasks","We introduce several notions of reduction in distributed computing, and
investigate reduction properties of two fundamental agreement tasks, namely
Consensus and Atomic Commitment.
  We first propose the notion of reduction ""a la Karp'', an analog for
distributed computing of the classical Karp reduction. We then define a weaker
reduction which is the analog of Cook reduction. These two reductions are
called K-reduction and C-reduction, respectively.
  We also introduce the notion of C*-reduction which has no counterpart in
classical (namely, non distributed) systems, and which naturally arises when
dealing with symmetric tasks.
  We establish various reducibility and irreducibility theorems with respect to
these three reductions. Our main result is an incomparability statement for
Consensus and Atomic Commitment tasks: we show that they are incomparable with
respect to the C-reduction, except when the resiliency degree is 1, in which
case Atomic Commitment is strictly harder than Consensus. A side consequence of
these results is that our notion of C-reduction is strictly weaker than the one
of K-reduction, even for unsolvable tasks.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.00414v2,"On the Security of Networked Control Systems in Smart Vehicle and its
  Adaptive Cruise Control","With the benefits of Internet of Vehicles (IoV) paradigm, come along
unprecedented security challenges. Among many applications of inter-connected
systems, vehicular networks and smart cars are examples that are already rolled
out. Smart vehicles not only have networks connecting their internal components
e.g. via Controller Area Network (CAN) bus, but also are connected to the
outside world through road side units and other vehicles. In some cases, the
internal and external network packets pass through the same hardware and are
merely isolated by software defined rules. Any misconfiguration opens a window
for the hackers to intrude into vehicles' internal components e.g. central lock
system, Engine Control Unit (ECU), Anti-lock Braking System (ABS) or Adaptive
Cruise Control (ACC) system. Compromise of any of these can lead to disastrous
outcomes. In this paper, we study the security of smart vehicles' adaptive
cruise control systems in the presence of covert attacks. We define two
covert/stealth attacks in the context of cruise control and propose a novel
intrusion detection and compensation method to disclose and respond to such
attacks. More precisely, we focus on the covert cyber attacks that compromise
the integrity of cruise controller and employ a neural network identifier in
the IDS engine to estimate the system output dynamically and compare it against
the ACC output. If any anomaly is detected, an embedded substitute controller
kicks in and takes over the control. We conducted extensive experiments in
MATLAB to evaluate the effectiveness of the proposed scheme in a simulated
environment.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1603.09009v1,Routing under Balance,"We introduce the notion of balance for directed graphs: a weighted directed
graph is $\alpha$-balanced if for every cut $S \subseteq V$, the total weight
of edges going from $S$ to $V\setminus S$ is within factor $\alpha$ of the
total weight of edges going from $V\setminus S$ to $S$. Several important
families of graphs are nearly balanced, in particular, Eulerian graphs (with
$\alpha = 1$) and residual graphs of $(1+\epsilon)$-approximate undirected
maximum flows (with $\alpha=O(1/\epsilon)$).
  We use the notion of balance to give a more fine-grained understanding of
several well-studied routing questions that are considerably harder in directed
graphs. We first revisit oblivious routings in directed graphs. Our main
algorithmic result is an oblivious routing scheme for single-source instances
that achieve an $O(\alpha \cdot \log^3 n / \log \log n)$ competitive ratio. In
the process, we make several technical contributions which may be of
independent interest. In particular, we give an efficient algorithm for
computing low-radius decompositions of directed graphs parameterized by
balance. We also define and construct low-stretch arborescences, a
generalization of low-stretch spanning trees to directed graphs.
  On the negative side, we present new lower bounds for oblivious routing
problems on directed graphs. We show that the competitive ratio of oblivious
routing algorithms for directed graphs is $\Omega(n)$ in general; this result
improves upon the long-standing best known lower bound of $\Omega(\sqrt{n})$
given by Hajiaghayi, Kleinberg, Leighton and R\""acke in 2006. We also show that
our restriction to single-source instances is necessary by showing an
$\Omega(\sqrt{n})$ lower bound for multiple-source oblivious routing in
Eulerian graphs.
  We also give a fast algorithm for the maximum flow problem in balanced
directed graphs.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.09827v3,A Formal Treatment of Contract Signature,"The paper develops a logical understanding of processes for signature of
legal contracts, motivated by applications to legal recognition of smart
contracts on blockchain platforms. A number of axioms and rules of inference
are developed that can be used to justify a ``meeting of the minds''
precondition for contract formation from the fact that certain content has been
signed. In addition to an ``offer and acceptance'' process, the paper considers
``signature in counterparts'', a legal process that permits a contract between
two or more parties to be brought into force by having the parties
independently (possibly, remotely) sign different copies of the contract,
rather than placing their signatures on a common copy at a physical meeting. It
is argued that a satisfactory account of signature in counterparts benefits
from a logic with syntactic self-reference. The axioms used are supported by a
formal semantics, and a number of further properties of the logic are
investigated. In particular, it is shown that the logic implies that when a
contract has been signed, the parties do not just agree, but are in mutual
agreement (a common-knowledge-like notion) about the terms of the contract.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.11182v4,"The Soccer Game, bit by bit: An information-theoretic analysis","We modeled the dynamics of a soccer match based on a network representation
where players are nodes discretely clustered into homogeneous groups. Players
were grouped by physical proximity, supported by the intuitive notion that
competing and same-team players use relative position as a key tactical tool to
contribute to the team's objectives. The model was applied to a set of matches
from a major European national football league, with players' coordinates
sampled at 10Hz, resulting in approx. 60,000 network samples per match. We took
an information theoretic approach to measuring distance between samples and
used it as a proxy for the game dynamics. Significant correlations were found
between measurements and key match events that are empirically known to result
in players jostling for position, such as when striving to get unmarked or to
mark. These events increase the information distance, while breaks in game play
have the opposite effect. By analyzing the frequency spectrum of players'
cluster transitions and their corresponding information distance, it is
possible to build a comprehensive view of player's interactions, useful for
training and strategy development. This analysis can be drilled down to the
level of individual players by quantifying their contribution to cluster
breakup and emergence, building an overall multi-level map that provides
insights into the game dynamics, from the individual player, to the clusters of
interacting players, all the way to the teams and their matches.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.12523v2,A Study of Fake News Reading and Annotating in Social Media Context,"The online spreading of fake news is a major issue threatening entire
societies. Much of this spreading is enabled by new media formats, namely
social networks and online media sites. Researchers and practitioners have been
trying to answer this by characterizing the fake news and devising automated
methods for detecting them. The detection methods had so far only limited
success, mostly due to the complexity of the news content and context and lack
of properly annotated datasets. One possible way to boost the efficiency of
automated misinformation detection methods, is to imitate the detection work of
humans. It is also important to understand the news consumption behavior of
online users. In this paper, we present an eye-tracking study, in which we let
44 lay participants to casually read through a social media feed containing
posts with news articles, some of which were fake. In a second run, we asked
the participants to decide on the truthfulness of these articles. We also
describe a follow-up qualitative study with a similar scenario but this time
with 7 expert fake news annotators. We present the description of both studies,
characteristics of the resulting dataset (which we hereby publish) and several
findings.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2003.09340v4,"Ordered Functional Decision Diagrams: A Functional Semantics For Binary
  Decision Diagrams","We introduce a novel framework, termed $\lambda$DD, that revisits Binary
Decision Diagrams from a purely functional point of view. The framework allows
to classify the already existing variants, including the most recent ones like
Chain-DD and ESRBDD, as implementations of a special class of ordered models.
We enumerate, in a principled way, all the models of this class and isolate its
most expressive model. This new model, termed $\lambda$DD-O-NUCX, is suitable
for both dense and sparse Boolean functions, and is moreover invariant by
negation. The canonicity of $\lambda$DD-O-NUCX is formally verified using the
Coq proof assistant. We furthermore give bounds on the size of the different
diagrams: the potential gain achieved by more expressive models can be at most
linear in the number of variables n.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0811.3782v5,"Real Computation with Least Discrete Advice: A Complexity Theory of
  Nonuniform Computability","It is folklore particularly in numerical and computer sciences that, instead
of solving some general problem f:A->B, additional structural information about
the input x in A (that is any kind of promise that x belongs to a certain
subset A' of A) should be taken advantage of. Some examples from real number
computation show that such discrete advice can even make the difference between
computability and uncomputability. We turn this into a both topological and
combinatorial complexity theory of information, investigating for several
practical problems how much advice is necessary and sufficient to render them
computable.
  Specifically, finding a nontrivial solution to a homogeneous linear equation
A*x=0 for a given singular real NxN-matrix A is possible when knowing
rank(A)=0,1,...,N-1; and we show this to be best possible. Similarly,
diagonalizing (i.e. finding a BASIS of eigenvectors of) a given real symmetric
NxN-matrix is possible when knowing the number of distinct eigenvalues: an
integer between 1 and N (the latter corresponding to the nondegenerate case).
And again we show that N-fold (i.e. roughly log N bits of) additional
information is indeed necessary in order to render this problem (continuous
and) computable; whereas for finding SOME SINGLE eigenvector of A, providing
the truncated binary logarithm of the least-dimensional eigenspace of A--i.e.
Theta(log N)-fold advice--is sufficient and optimal.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1701.03968v1,Attention Allocation Aid for Visual Search,"This paper outlines the development and testing of a novel, feedback-enabled
attention allocation aid (AAAD), which uses real-time physiological data to
improve human performance in a realistic sequential visual search task. Indeed,
by optimizing over search duration, the aid improves efficiency, while
preserving decision accuracy, as the operator identifies and classifies targets
within simulated aerial imagery. Specifically, using experimental eye-tracking
data and measurements about target detectability across the human visual field,
we develop functional models of detection accuracy as a function of search
time, number of eye movements, scan path, and image clutter. These models are
then used by the AAAD in conjunction with real time eye position data to make
probabilistic estimations of attained search accuracy and to recommend that the
observer either move on to the next image or continue exploring the present
image. An experimental evaluation in a scenario motivated from human
supervisory control in surveillance missions confirms the benefits of the AAAD.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0803.0378v2,Thread algebra for poly-threading,"Threads as considered in basic thread algebra are primarily looked upon as
behaviours exhibited by sequential programs on execution. It is a fact of life
that sequential programs are often fragmented. Consequently, fragmented program
behaviours are frequently found. In this paper, we consider this phenomenon. We
extend basic thread algebra with the barest mechanism for sequencing of threads
that are taken for fragments. This mechanism, called poly-threading, supports
both autonomous and non-autonomous thread selection in sequencing. We relate
the resulting theory to the algebraic theory of processes known as ACP and use
it to describe analytic execution architectures suited for fragmented programs.
We also consider the case where the steps of fragmented program behaviours are
interleaved in the ways of non-distributed and distributed multi-threading.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9912001v1,"The phase transition in random Horn satisfiability and its algorithmic
  implications","Let c>0 be a constant, and $\Phi$ be a random Horn formula with n variables
and $m=c\cdot 2^{n}$ clauses, chosen uniformly at random (with repetition) from
the set of all nonempty Horn clauses in the given variables. By analyzing \PUR,
a natural implementation of positive unit resolution, we show that
$\lim_{n\goesto \infty} \PR ({$\Phi$ is satisfiable})= 1-F(e^{-c})$, where
$F(x)=(1-x)(1-x^2)(1-x^4)(1-x^8)... $. Our method also yields as a byproduct an
average-case analysis of this algorithm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1308.2597v3,"A framework for systematic analysis of Open Access journals and its
  application in software engineering and information systems","This article is a contribution towards an understanding of Open Access (OA)
publishing. It proposes an analysis framework of 18 core attributes, divided
into the areas of Bibliographic information, Activity metrics, Economics,
Accessibility, and Predatory issues of OA journals. The framework has been
employed in a systematic analysis of 30 OA journals in software engineering
(SE) and information systems (IS), which were selected among 386 OA journals in
Computer Science from the Directory of OA Journals. An analysis is performed on
the sample of the journals, to provide an overview of the current situation of
OA journals in the fields of SE and IS. The journals are then compared
between-group, according to the presence of a publication fee. A within-group
analysis is performed on the journals requesting publication charges to
authors, in order to understand what is the value added according to different
price ranges. This article offers several contributions. It presents an
overview of OA definitions and models. It provides an analysis framework born
from the observation of data and the literature. It raises the need to study OA
in the fields of SE and IS while offering a first analysis. Finally, it
provides recommendations to readers of OA journals. This paper highlights
several concerns still threatening OA publishing in the fields of SE and IS.
Among them, it is shown that high publication fees are not sufficiently
justified by the publishers, which often lack transparency and may prevent
authors from adopting OA.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1602.07267v2,"On closure operators related to maximal tricliques in tripartite
  hypergraphs","Triadic Formal Concept Analysis (3FCA) was introduced by Lehman and Wille
almost two decades ago. And many researchers work in Data Mining and Formal
Concept Analysis using the notions of closed sets, Galois and closure
operators, closure systems, but up-to-date even though that different
researchers actively work on mining triadic and n-ary relations, a proper
closure operator for enumeration of triconcepts, i.e. maximal triadic cliques
of tripartite hypergaphs, was not introduced. In this paper we show that the
previously introduced operators for obtaining triconcepts and maximal connected
and complete sets (MCCSs) are not always consistent and provide the reader with
a definition of valid closure operator and associated set system. Moreover, we
study the difficulties of related problems from order-theoretic and
combinatorial point view as well as provide the reader with justifications of
the complexity classes of these problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0806.0130v1,Feedback Scheduling of Priority-Driven Control Networks,"With traditional open-loop scheduling of network resources, the
quality-of-control (QoC) of networked control systems (NCSs) may degrade
significantly in the presence of limited bandwidth and variable workload. The
goal of this work is to maximize the overall QoC of NCSs through dynamically
allocating available network bandwidth. Based on codesign of control and
scheduling, an integrated feedback scheduler is developed to enable flexible
QoC management in dynamic environments. It encompasses a cascaded feedback
scheduling module for sampling period adjustment and a direct feedback
scheduling module for priority modification. The inherent characteristics of
priority-driven control networks make it feasible to implement the proposed
feedback scheduler in real-world systems. Extensive simulations show that the
proposed approach leads to significant QoC improvement over the traditional
open-loop scheduling scheme under both underloaded and overloaded network
conditions.",0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1309.5223v1,"JRC EuroVoc Indexer JEX - A freely available multi-label categorisation
  tool","EuroVoc (2012) is a highly multilingual thesaurus consisting of over 6,700
hierarchically organised subject domains used by European Institutions and many
authorities in Member States of the European Union (EU) for the classification
and retrieval of official documents. JEX is JRC-developed multi-label
classification software that learns from manually labelled data to
automatically assign EuroVoc descriptors to new documents in a profile-based
category-ranking task. The JEX release consists of trained classifiers for 22
official EU languages, of parallel training data in the same languages, of an
interface that allows viewing and amending the assignment results, and of a
module that allows users to re-train the tool on their own document
collections. JEX allows advanced users to change the document representation so
as to possibly improve the categorisation result through linguistic
pre-processing. JEX can be used as a tool for interactive EuroVoc descriptor
assignment to increase speed and consistency of the human categorisation
process, or it can be used fully automatically. The output of JEX is a
language-independent EuroVoc feature vector lending itself also as input to
various other Language Technology tasks, including cross-lingual clustering and
classification, cross-lingual plagiarism detection, sentence selection and
ranking, and more.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.00827v1,"Automatic Expansion of Domain-Specific Affective Models for Web
  Intelligence Applications","Sentic computing relies on well-defined affective models of different
complexity - polarity to distinguish positive and negative sentiment, for
example, or more nuanced models to capture expressions of human emotions. When
used to measure communication success, even the most granular affective model
combined with sophisticated machine learning approaches may not fully capture
an organisation's strategic positioning goals. Such goals often deviate from
the assumptions of standardised affective models. While certain emotions such
as Joy and Trust typically represent desirable brand associations, specific
communication goals formulated by marketing professionals often go beyond such
standard dimensions. For instance, the brand manager of a television show may
consider fear or sadness to be desired emotions for its audience. This article
introduces expansion techniques for affective models, combining common and
commonsense knowledge available in knowledge graphs with language models and
affective reasoning, improving coverage and consistency as well as supporting
domain-specific interpretations of emotions. An extensive evaluation compares
the performance of different expansion techniques: (i) a quantitative
evaluation based on the revisited Hourglass of Emotions model to assess
performance on complex models that cover multiple affective categories, using
manually compiled gold standard data, and (ii) a qualitative evaluation of a
domain-specific affective model for television programme brands. The results of
these evaluations demonstrate that the introduced techniques support a variety
of embeddings and pre-trained models. The paper concludes with a discussion on
applying this approach to other scenarios where affective model resources are
scarce.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1610.07530v2,Target Set Selection in Dense Graph Classes,"In this paper, we study the Target Set Selection problem from a parameterized
complexity perspective. Here for a given graph and a threshold for each vertex,
the task is to find a set of vertices (called a target set) which activates the
whole graph during the following iterative process. A vertex outside the active
set becomes active if the number of so far activated vertices in its
neighborhood is at least its threshold.
  We give two parameterized algorithms for a special case where each vertex has
the threshold set to the half of its neighbors (the so-called Majority Target
Set Selection problem) for parameterizations by the neighborhood diversity and
the twin cover number of the input graph.
  We complement these results from the negative side. We give a hardness proof
for the Majority Target Set Selection problem when parameterized by (a
restriction of) the modular-width - a natural generalization of both previous
structural parameters. We also show the Target Set Selection problem
parameterized by the neighborhood diversity or by the twin cover number is
W[1]-hard when there is no restriction on the thresholds.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1908.06089v1,Batch 1: Definition of several Weather & Climate Dwarfs,"This document is one of the deliverable reports created for the ESCAPE
project. ESCAPE stands for Energy-efficient Scalable Algorithms for Weather
Prediction at Exascale. The project develops world-class, extreme-scale
computing capabilities for European operational numerical weather prediction
and future climate models. This is done by identifying weather & climate dwarfs
which are key patterns in terms of computation and communication (in the spirit
of the Berkeley dwarfs). These dwarfs are then optimised for different hardware
architectures (single and multi-node) and alternative algorithms are explored.
Performance portability is addressed through the use of domain specific
languages.
  This deliverable contains the description of the characteristics of the
weather & climate dwarfs that form key functional components of prediction
models in terms of the science that they encapsulate and in terms of
computational cost they impose on the forecast production. The ESCAPE work flow
between work packages centres on these dwarfs and hence their selection, their
performance assessment, code adaptation and optimization is crucial for the
success of the project. At this stage of ESCAPE, a selection of established and
new dwarfs has been made, their documentation been compiled and the software
been made available on the software exchange platform. The selection of dwarfs
will be extended throughout the course of the project (see Deliverable D1.2).
  The current selection includes the spectral transforms, the cloud
microphysics scheme, two and three-dimensional elliptic solvers, a bi-Fourier
spectral transform, an interpolation needed for the semi-Lagrangian advection
scheme and a first version of the semi-Lagrangian advection scheme itself. This
deliverable includes their scientific description and the guidance for
installation, execution and testing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1512.02802v2,Lively quantum walks on cycles,"We introduce a family of quantum walks on cycles parametrized by their
liveliness, defined by the ability to execute a long-range move. We investigate
the behaviour of the probability distribution and time-averaged probability
distribution. We show that the liveliness parameter, controlling the magnitude
of the additional long-range move, has a direct impact on the periodicity of
the limiting distribution. We also show that the introduced model provides a
method for network exploration which is robust against trapping.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.05013v1,"PWPAE: An Ensemble Framework for Concept Drift Adaptation in IoT Data
  Streams","As the number of Internet of Things (IoT) devices and systems have surged,
IoT data analytics techniques have been developed to detect malicious
cyber-attacks and secure IoT systems; however, concept drift issues often occur
in IoT data analytics, as IoT data is often dynamic data streams that change
over time, causing model degradation and attack detection failure. This is
because traditional data analytics models are static models that cannot adapt
to data distribution changes. In this paper, we propose a Performance Weighted
Probability Averaging Ensemble (PWPAE) framework for drift adaptive IoT anomaly
detection through IoT data stream analytics. Experiments on two public datasets
show the effectiveness of our proposed PWPAE method compared against
state-of-the-art methods.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.09413v1,Learnable Programming: Blocks and Beyond,"Blocks-based programming has become the lingua franca for introductory
coding. Studies have found that experience with blocks-based programming can
help beginners learn more traditional text-based languages. We explore how
blocks environments improve learnability for novices by 1) favoring recognition
over recall, 2) reducing cognitive load, and 3) preventing errors. Increased
usability of blocks programming has led to widespread adoption within
introductory programming contexts across a range of ages. Ongoing work explores
further reducing barriers to programming, supporting novice programmers in
expanding their programming skills, and transitioning to textual programming.
New blocks frameworks are making it easier to access a variety of APIs through
blocks environments, opening the doors to a greater diversity of programming
domains and supporting greater experimentation for novices and professionals
alike.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1503.03128v3,Efficient Straggler Replication in Large-scale Parallel Computing,"In a cloud computing job with many parallel tasks, the tasks on the slowest
machines (straggling tasks) become the bottleneck in the job completion.
Computing frameworks such as MapReduce and Spark tackle this by replicating the
straggling tasks and waiting for any one copy to finish. Despite being adopted
in practice, there is little analysis of how replication affects the latency
and the cost of additional computing resources. In this paper we provide a
framework to analyze this latency-cost trade-off and find the best replication
strategy by answering design questions such as: 1) when to replicate straggling
tasks, 2) how many replicas to launch, and 3) whether to kill the original copy
or not. Our analysis reveals that for certain execution time distributions, a
small amount of task replication can drastically reduce both latency as well as
the cost of computing resources. We also propose an algorithm to estimate the
latency and cost based on the empirical distribution of task execution time.
Evaluations using samples in the Google Cluster Trace suggest further latency
and cost reduction compared to the existing replication strategy used in
MapReduce.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0111055v2,Overview of the NSTX Control System,"The National Spherical Torus Experiment (NSTX) is an innovative magnetic
fusion device that was constructed by the Princeton Plasma Physics Laboratory
(PPPL) in collaboration with the Oak Ridge National Laboratory, Columbia
University, and the University of Washington at Seattle. Since achieving first
plasma in 1999, the device has been used for fusion research through an
international collaboration of over twenty institutions. The NSTX is operated
through a collection of control systems that encompass a wide range of
technology, from hardwired relay controls to real-time control systems with
giga-FLOPS of capability. This paper presents a broad introduction to the
control systems used on NSTX, with an emphasis on the computing controls, data
acquisition, and synchronization systems.",0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.15133v1,Algebraic 3D Graphic Statics: Constrained Areas,"This research provides algorithms and numerical methods to geometrically
control the magnitude of the internal and external forces in the reciprocal
diagrams of 3D/Polyhedral Graphic statics (3DGS). In 3DGS, the form of the
structure and its equilibrium of forces is represented by two polyhedral
diagrams that are geometrically and topologically related. The areas of the
faces of the force diagram represent the magnitude of the internal and external
forces in the system. For the first time, the methods of this research allow
the user to control and constrain the areas and edge lengths of the faces of
general polyhedrons that can be convex, self-intersecting, or concave. As a
result, a designer can explicitly control the force magnitudes in the force
diagram and explore the equilibrium of a variety of compression and
tension-combined funicular structural forms. In this method, a quadratic
formulation is used to compute the area of a single face based on its edge
lengths. The approach is applied to manipulating the face geometry with a
predefined area and the edge lengths. Subsequently, the geometry of the
polyhedron is updated with newly changed faces. This approach is a multi-step
algorithm where each step includes computing the geometry of a single face and
updating the polyhedral geometry. One of the unique results of this framework
is the construction of the zero-area, self-intersecting faces, where the sum of
the signed areas of a self-intersecting face is zero, representing a member
with zero force in the form diagram. The methodology of this research can
clarify the equilibrium of some systems that could not be previously justified
using reciprocal polyhedral diagrams. Therefore, it generalizes the principle
of the equilibrium of polyhedral frames and opens a completely new horizon in
the design of highly-sophisticated funicular polyhedral structures beyond
compression-only systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0212004v1,Minimal-Change Integrity Maintenance Using Tuple Deletions,"We address the problem of minimal-change integrity maintenance in the context
of integrity constraints in relational databases. We assume that
integrity-restoration actions are limited to tuple deletions. We identify two
basic computational issues: repair checking (is a database instance a repair of
a given database?) and consistent query answers (is a tuple an answer to a
given query in every repair of a given database?). We study the computational
complexity of both problems, delineating the boundary between the tractable and
the intractable. We consider denial constraints, general functional and
inclusion dependencies, as well as key and foreign key constraints. Our results
shed light on the computational feasibility of minimal-change integrity
maintenance. The tractable cases should lead to practical implementations. The
intractability results highlight the inherent limitations of any integrity
enforcement mechanism, e.g., triggers or referential constraint actions, as a
way of performing minimal-change integrity maintenance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.11028v1,"On physics-informed data-driven isotropic and anisotropic constitutive
  models through probabilistic machine learning and space-filling sampling","Data-driven constitutive modeling is an emerging field in computational solid
mechanics with the prospect of significantly relieving the computational costs
of hierarchical computational methods. Traditionally, these surrogates have
been trained using datasets which map strain inputs to stress outputs directly.
Data-driven constitutive models for elastic and inelastic materials have
commonly been developed based on artificial neural networks (ANNs), which
recently enabled the incorporation of physical laws in the construction of
these models. However, ANNs do not offer convergence guarantees and are reliant
on user-specified parameters. In contrast to ANNs, Gaussian process regression
(GPR) is based on nonparametric modeling principles as well as on fundamental
statistical knowledge and hence allows for strict convergence guarantees. GPR
however has the major disadvantage that it scales poorly as datasets get large.
In this work we present a physics-informed data-driven constitutive modeling
approach for isostropic and anisotropic materials based on probabilistic
machine learning that can be used in the big data context. The trained GPR
surrogates are able to respect physical principles such as material frame
indifference, material symmetry, thermodynamic consistency, stress-free
undeformed configuration, and the local balance of angular momentum.
Furthermore, this paper presents the first sampling approach that directly
generates space-filling points in the invariant space corresponding to bounded
domain of the gradient deformation tensor. Overall, the presented approach is
tested on synthetic data from isotropic and anisotropic constitutive laws and
shows surprising accuracy even far beyond the limits of the training domain,
indicating that the resulting surrogates can efficiently generalize as they
incorporate knowledge about the underlying physics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0205021v1,An Overview of a Grid Architecture for Scientific Computing,"This document gives an overview of a Grid testbed architecture proposal for
the NorduGrid project. The aim of the project is to establish an inter-Nordic
testbed facility for implementation of wide area computing and data handling.
The architecture is supposed to define a Grid system suitable for solving data
intensive problems at the Large Hadron Collider at CERN. We present the various
architecture components needed for such a system. After that we go on to give a
description of the dynamics by showing the task flow.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.11855v3,Auctions Between Regret-Minimizing Agents,"We analyze a scenario in which software agents implemented as
regret-minimizing algorithms engage in a repeated auction on behalf of their
users. We study first-price and second-price auctions, as well as their
generalized versions (e.g., as those used for ad auctions). Using both
theoretical analysis and simulations, we show that, surprisingly, in
second-price auctions the players have incentives to misreport their true
valuations to their own learning agents, while in the first-price auction it is
a dominant strategy for all players to truthfully report their valuations to
their agents.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.07042v1,Minimisation of Event Structures,"Event structures are fundamental models in concurrency theory, providing a
representation of events in computation and of their relations, notably
concurrency, conflict and causality. In this paper we present a theory of
minimisation for event structures. Working in a class of event structures that
generalises many stable event structure models in the literature (e.g., prime,
asymmetric, flow and bundle event structures), we study a notion of
behaviour-preserving quotient, referred to as a folding, taking (hereditary)
history preserving bisimilarity as a reference behavioural equivalence. We show
that for any event structure a folding producing a uniquely determined minimal
quotient always exists. We observe that each event structure can be seen as the
folding of a prime event structure, and that all foldings between general event
structures arise from foldings of (suitably defined) corresponding prime event
structures. This gives a special relevance to foldings in the class of prime
event structures, which are studied in detail. We identify folding conditions
for prime and asymmetric event structures, and show that also prime event
structures always admit a unique minimal quotient (while this is not the case
for various other event structure models).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.04638v2,"A Spacetime Approach to Generalized Cognitive Reasoning in Multi-scale
  Learning","In modern machine learning, pattern recognition replaces realtime semantic
reasoning. The mapping from input to output is learned with fixed semantics by
training outcomes deliberately. This is an expensive and static approach which
depends heavily on the availability of a very particular kind of prior raining
data to make inferences in a single step. Conventional semantic network
approaches, on the other hand, base multi-step reasoning on modal logics and
handcrafted ontologies, which are ad hoc, expensive to construct, and fragile
to inconsistency. Both approaches may be enhanced by a hybrid approach, which
completely separates reasoning from pattern recognition. In this report, a
quasi-linguistic approach to knowledge representation is discussed, motivated
by spacetime structure. Tokenized patterns from diverse sources are integrated
to build a lightly constrained and approximately scale-free network. This is
then be parsed with very simple recursive algorithms to generate
`brainstorming' sets of reasoned knowledge.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0602017v1,Quasi-Linear Soft Tissue Models Revisited,"Incompressibility, nonlinear deformation under stress and viscoelasticity are
the fingerprint of soft tissue mechanical behavior. In order to model soft
tissues appropriately, we must pursue to complete these requirements. In this
work we revisited different soft tissue quasi-linear model possibilities in
trying to achieve for this commitment.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1703.02819v1,"Introduction to Formal Concept Analysis and Its Applications in
  Information Retrieval and Related Fields","This paper is a tutorial on Formal Concept Analysis (FCA) and its
applications. FCA is an applied branch of Lattice Theory, a mathematical
discipline which enables formalisation of concepts as basic units of human
thinking and analysing data in the object-attribute form. Originated in early
80s, during the last three decades, it became a popular human-centred tool for
knowledge representation and data analysis with numerous applications. Since
the tutorial was specially prepared for RuSSIR 2014, the covered FCA topics
include Information Retrieval with a focus on visualisation aspects, Machine
Learning, Data Mining and Knowledge Discovery, Text Mining and several others.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1610.07965v1,Higher-Order Linearisability,"Linearisability is a central notion for verifying concurrent libraries: a
given library is proven safe if its operational history can be rearranged into
a new sequential one which, in addition, satisfies a given specification.
Linearisability has been examined for libraries in which method arguments and
method results are of ground type, including libraries parameterised with such
methods. In this paper we extend linearisability to the general higher-order
setting: methods can be passed as arguments and returned as values. A library
may also depend on abstract methods of any order. We use this generalised
notion to show correctness of several higher-order example libraries.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1112.0992v1,"The Web economy: goods, users, models and policies","Web emerged as an antidote to the rapidly increasing quantity of accumulated
knowledge and become successful because it facilitates massive participation
and communication with minimum costs. Today, its enormous impact, scale and
dynamism in time and space make very difficult (and sometimes impossible) to
measure and anticipate the effects in human society. In addition to that, we
demand from the Web to be fast, secure, reliable, all-inclusive and trustworthy
in any transaction. The scope of the present article is to review a part of the
Web economy literature that will help us to identify its major participants and
their functions. The goal is to understand how the Web economy differs from the
traditional setting and what implications have these differences. Secondarily,
we attempt to establish a minimal common understanding about the incentives and
properties of the Web economy. In this direction the concept of Web Goods and a
new classification of Web Users are introduced and analyzed This article, is
not, by any means, a thorough review of the economic literature related to the
Web. We focus only on its relevant part that models the Web as a standalone
economic artifact with native functionality and processes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1308.3508v2,"A General Optimization Technique for High Quality Community Detection in
  Complex Networks","Recent years have witnessed the development of a large body of algorithms for
community detection in complex networks. Most of them are based upon the
optimization of objective functions, among which modularity is the most common,
though a number of alternatives have been suggested in the scientific
literature. We present here an effective general search strategy for the
optimization of various objective functions for community detection purposes.
When applied to modularity, on both real-world and synthetic networks, our
search strategy substantially outperforms the best existing algorithms in terms
of final scores of the objective function; for description length, its
performance is on par with the original Infomap algorithm. The execution time
of our algorithm is on par with non-greedy alternatives present in literature,
and networks of up to 10,000 nodes can be analyzed in time spans ranging from
minutes to a few hours on average workstations, making our approach readily
applicable to tasks which require the quality of partitioning to be as high as
possible, and are not limited by strict time constraints. Finally, based on the
most effective of the available optimization techniques, we compare the
performance of modularity and code length as objective functions, in terms of
the quality of the partitions one can achieve by optimizing them. To this end,
we evaluated the ability of each objective function to reconstruct the
underlying structure of a large set of synthetic and real-world networks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1208.2649v1,Survey and Analysis of Production Distributed Computing Infrastructures,"This report has two objectives. First, we describe a set of the production
distributed infrastructures currently available, so that the reader has a basic
understanding of them. This includes explaining why each infrastructure was
created and made available and how it has succeeded and failed. The set is not
complete, but we believe it is representative.
  Second, we describe the infrastructures in terms of their use, which is a
combination of how they were designed to be used and how users have found ways
to use them. Applications are often designed and created with specific
infrastructures in mind, with both an appreciation of the existing capabilities
provided by those infrastructures and an anticipation of their future
capabilities. Here, the infrastructures we discuss were often designed and
created with specific applications in mind, or at least specific types of
applications. The reader should understand how the interplay between the
infrastructure providers and the users leads to such usages, which we call
usage modalities. These usage modalities are really abstractions that exist
between the infrastructures and the applications; they influence the
infrastructures by representing the applications, and they influence the ap-
plications by representing the infrastructures.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1809.11042v1,"Can female fertility management mobile apps be sustainable and
  contribute to female health care? Harnessing the power of patient generated
  data ; Analysis of the organizations active in this e-Health segment","In recent years, personal health technologies have emerged that allow
patients to collect a wide range of health-related data outside the clinic.
These patient-generated data (PGD) reflect patients everyday behaviors
including physical activity, mood, diet, sleep, and symptoms. However, major
players and academics alike, have ignored the case where these patients or
normal people are women. Is analyzed the eHealth segment of female fertility
planning mobile apps (in US called: period trackers) and its possible
extensions to other female health care mobile services. The market potential is
very large although age segmentation applies. These apps help women record and
plan their menstruation cycles, their fertility periods, and ease with relevant
personalized advice all the uncomfort. As an illustration, the case of a
European app service supplier is described in depth. The services of ten
worldwide suppliers are compared in terms of functionality, adoption,
organization, financial and business aspects. The research question: Can female
fertility management mobile apps be sustainable and contribute to female health
care, is researched by a combination of academic literature study, testing of 7
essential hypotheses, and a limited user driven experimental demand analysis.
Quality and impact metrics from a user point of view are proposed. The
conclusion is a moderate yes to the research question, with several conditions.
Further research and innovative ideas, as well as marketing and strategic
directions are provided, incl. associations with male fertility apps.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/cs/0007027v1,"Efficient cache use for stencil operations on structured discretization
  grids","We derive tight bounds on cache misses for evaluation of explicit stencil
operators on structured grids. Our lower bound is based on the isoperimetrical
property of the discrete octahedron. Our upper bound is based on good surface
to volume ratio of a parallelepiped spanned by a reduced basis of the inter-
ference lattice of a grid. Measurements show that our algorithm typically
reduces the number of cache misses by factor of three relative to a compiler
optimized code. We show that stencil calculations on grids whose interference
lattice have a short vector feature abnormally high numbers of cache misses. We
call such grids unfavorable and suggest to avoid these in computations by
appropriate padding. By direct measurements on MIPS R10000 we show a good
correlation of abnormally high cache misses and unfavorable three-dimensional
grids.",0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0609058v1,"The JRC-Acquis: A multilingual aligned parallel corpus with 20+
  languages","We present a new, unique and freely available parallel corpus containing
European Union (EU) documents of mostly legal nature. It is available in all 20
official EUanguages, with additional documents being available in the languages
of the EU candidate countries. The corpus consists of almost 8,000 documents
per language, with an average size of nearly 9 million words per language.
Pair-wise paragraph alignment information produced by two different aligners
(Vanilla and HunAlign) is available for all 190+ language pair combinations.
Most texts have been manually classified according to the EUROVOC subject
domains so that the collection can also be used to train and test multi-label
classification algorithms and keyword-assignment software. The corpus is
encoded in XML, according to the Text Encoding Initiative Guidelines. Due to
the large number of parallel texts in many languages, the JRC-Acquis is
particularly suitable to carry out all types of cross-language research, as
well as to test and benchmark text analysis software across different languages
(for instance for alignment, sentence splitting and term extraction).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1411.7554v3,Impact of redundant checks on the LP decoding thresholds of LDPC codes,"Feldman et al.(2005) asked whether the performance of the LP decoder can be
improved by adding redundant parity checks to tighten the LP relaxation. We
prove that for LDPC codes, even if we include all redundant checks,
asymptotically there is no gain in the LP decoder threshold on the BSC under
certain conditions on the base Tanner graph. First, we show that if the graph
has bounded check-degree and satisfies a condition which we call asymptotic
strength, then including high degree redundant checks in the LP does not
significantly improve the threshold in the following sense: for each constant
delta>0, there is a constant k>0 such that the threshold of the LP decoder
containing all redundant checks of degree at most k improves by at most delta
upon adding to the LP all redundant checks of degree larger than k. We conclude
that if the graph satisfies a rigidity condition, then including all redundant
checks does not improve the threshold of the base LP. We call the graph
asymptotically strong if the LP decoder corrects a constant fraction of errors
even if the LLRs of the correct variables are arbitrarily small. By building on
the work of Feldman et al.(2007) and Viderman(2013), we show that asymptotic
strength follows from sufficiently large expansion. We also give a geometric
interpretation of asymptotic strength in terms pseudocodewords. We call the
graph rigid if the minimum weight of a sum of check nodes involving a cycle
tends to infinity as the block length tends to infinity. Under the assumptions
that the graph girth is logarithmic and the minimum check degree is at least 3,
rigidity is equivalent to the nondegeneracy property that adding at least
logarithmically many checks does not give a constant weight check. We argue
that nondegeneracy is a typical property of random check-regular graphs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.01913v1,Secure and Privacy-Preserving Federated Learning via Co-Utility,"The decentralized nature of federated learning, that often leverages the
power of edge devices, makes it vulnerable to attacks against privacy and
security. The privacy risk for a peer is that the model update she computes on
her private data may, when sent to the model manager, leak information on those
private data. Even more obvious are security attacks, whereby one or several
malicious peers return wrong model updates in order to disrupt the learning
process and lead to a wrong model being learned. In this paper we build a
federated learning framework that offers privacy to the participating peers as
well as security against Byzantine and poisoning attacks. Our framework
consists of several protocols that provide strong privacy to the participating
peers via unlinkable anonymity and that are rationally sustainable based on the
co-utility property. In other words, no rational party is interested in
deviating from the proposed protocols. We leverage the notion of co-utility to
build a decentralized co-utile reputation management system that provides
incentives for parties to adhere to the protocols. Unlike privacy protection
via differential privacy, our approach preserves the values of model updates
and hence the accuracy of plain federated learning; unlike privacy protection
via update aggregation, our approach preserves the ability to detect bad model
updates while substantially reducing the computational overhead compared to
methods based on homomorphic encryption.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/0710.1153v2,"Verification of Ptime Reducibility for system F Terms: Type Inference
  in<br> Dual Light Affine Logic","In a previous work Baillot and Terui introduced Dual light affine logic
(DLAL) as a variant of Light linear logic suitable for guaranteeing complexity
properties on lambda calculus terms: all typable terms can be evaluated in
polynomial time by beta reduction and all Ptime functions can be represented.
In the present work we address the problem of typing lambda-terms in
second-order DLAL. For that we give a procedure which, starting with a term
typed in system F, determines whether it is typable in DLAL and outputs a
concrete typing if there exists any. We show that our procedure can be run in
time polynomial in the size of the original Church typed system F term.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.09850v4,A Theoretical Computer Science Perspective on Consciousness,"The quest to understand consciousness, once the purview of philosophers and
theologians, is now actively pursued by scientists of many stripes. This paper
studies consciousness from the perspective of theoretical computer science. It
formalizes the Global Workspace Theory (GWT) originated by cognitive
neuroscientist Bernard Baars and further developed by him, Stanislas Dehaene,
and others. Our major contribution lies in the precise formal definition of a
Conscious Turing Machine (CTM), also called a Conscious AI. We define the CTM
in the spirit of Alan Turing's simple yet powerful definition of a computer,
the Turing Machine (TM). We are not looking for a complex model of the brain
nor of cognition but for a simple model of (the admittedly complex concept of)
consciousness. After formally defining CTM, we give a formal definition of
consciousness in CTM. We then suggest why the CTM has the feeling of
consciousness. The reasonableness of the definitions and explanations can be
judged by how well they agree with commonly accepted intuitive concepts of
human consciousness, the breadth of related concepts that the model explains
easily and naturally, and the extent of its agreement with scientific evidence.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.00196v2,All-Hex Meshing Strategies For Densely Packed Spheres,"We develop an all-hex meshing strategy for the interstitial space in beds of
densely packed spheres that is tailored to turbulent flow simulations based on
the spectral element method (SEM). The SEM achieves resolution through elevated
polynomial order N and requires two to three orders of magnitude fewer elements
than standard finite element approaches do. These reduced element counts place
stringent requirements on mesh quality and conformity. Our meshing algorithm is
based on a Voronoi decomposition of the sphere centers. Facets of the Voronoi
cells are tessellated into quads that are swept to the sphere surface to
generate a high-quality base mesh. Refinements to the algorithm include edge
collapse to remove slivers, node insertion to balance resolution, localized
refinement in the radial direction about each sphere, and mesh optimization. We
demonstrate geometries with 10^2-10^5 spheres using approximately 300 elements
per sphere (for three radial layers), along with mesh quality metrics, timings,
flow simulations, and solver performance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.3539v3,"Pattern Recognition in Narrative: Tracking Emotional Expression in
  Context","Using geometric data analysis, our objective is the analysis of narrative,
with narrative of emotion being the focus in this work. The following two
principles for analysis of emotion inform our work. Firstly, emotion is
revealed not as a quality in its own right but rather through interaction. We
study the 2-way relationship of Ilsa and Rick in the movie Casablanca, and the
3-way relationship of Emma, Charles and Rodolphe in the novel {\em Madame
Bovary}. Secondly, emotion, that is expression of states of mind of subjects,
is formed and evolves within the narrative that expresses external events and
(personal, social, physical) context. In addition to the analysis methodology
with key aspects that are innovative, the input data used is crucial. We use,
firstly, dialogue, and secondly, broad and general description that
incorporates dialogue. In a follow-on study, we apply our unsupervised
narrative mapping to data streams with very low emotional expression. We map
the narrative of Twitter streams. Thus we demonstrate map analysis of general
narratives.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1906.00861v4,"Mind the Gap: Trade-Offs between Distributed Ledger Technology
  Characteristics","When developing peer-to-peer applications on Distributed Ledger Technology
(DLT), a crucial decision is the selection of a suitable DLT design (e.g.,
Ethereum) because it is hard to change the underlying DLT design post hoc. To
facilitate the selection of suitable DLT designs, we review DLT characteristics
and identify trade-offs between them. Furthermore, we assess how DLT designs
account for these trade-offs and we develop archetypes for DLT designs that
cater to specific quality requirements. The main purpose of our article is to
introduce scientific and practical audiences to the intricacies of DLT designs
and to support development of viable applications on DLT.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
http://arxiv.org/abs/2101.04339v3,Locality Sensitive Hashing for Efficient Similar Polygon Retrieval,"Locality Sensitive Hashing (LSH) is an effective method of indexing a set of
items to support efficient nearest neighbors queries in high-dimensional
spaces. The basic idea of LSH is that similar items should produce hash
collisions with higher probability than dissimilar items.
  We study LSH for (not necessarily convex) polygons, and use it to give
efficient data structures for similar shape retrieval. Arkin et al. represent
polygons by their ""turning function"" - a function which follows the angle
between the polygon's tangent and the $ x $-axis while traversing the perimeter
of the polygon. They define the distance between polygons to be variations of
the $ L_p $ (for $p=1,2$) distance between their turning functions. This metric
is invariant under translation, rotation and scaling (and the selection of the
initial point on the perimeter) and therefore models well the intuitive notion
of shape resemblance.
  We develop and analyze LSH near neighbor data structures for several
variations of the $ L_p $ distance for functions (for $p=1,2$). By applying our
schemes to the turning functions of a collection of polygons we obtain
efficient near neighbor LSH-based structures for polygons. To tune our
structures to turning functions of polygons, we prove some new properties of
these turning functions that may be of independent interest.
  As part of our analysis, we address the following problem which is of
independent interest. Find the vertical translation of a function $ f $ that is
closest in $ L_1 $ distance to a function $ g $. We prove tight bounds on the
approximation guarantee obtained by the translation which is equal to the
difference between the averages of $ g $ and $ f $.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.10598v3,"Embracing New Techniques in Deep Learning for Estimating Image
  Memorability","Various work has suggested that the memorability of an image is consistent
across people, and thus can be treated as an intrinsic property of an image.
Using computer vision models, we can make specific predictions about what
people will remember or forget. While older work has used now-outdated deep
learning architectures to predict image memorability, innovations in the field
have given us new techniques to apply to this problem. Here, we propose and
evaluate five alternative deep learning models which exploit developments in
the field from the last five years, largely the introduction of residual neural
networks, which are intended to allow the model to use semantic information in
the memorability estimation process. These new models were tested against the
prior state of the art with a combined dataset built to optimize both
within-category and across-category predictions. Our findings suggest that the
key prior memorability network had overstated its generalizability and was
overfit on its training set. Our new models outperform this prior model,
leading us to conclude that Residual Networks outperform simpler convolutional
neural networks in memorability regression. We make our new state-of-the-art
model readily available to the research community, allowing memory researchers
to make predictions about memorability on a wider range of images.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.03759v1,Sparsifying the Operators of Fast Matrix Multiplication Algorithms,"Fast matrix multiplication algorithms may be useful, provided that their
running time is good in practice. Particularly, the leading coefficient of
their arithmetic complexity needs to be small. Many sub-cubic algorithms have
large leading coefficients, rendering them impractical. Karstadt and Schwartz
(SPAA'17, JACM'20) demonstrated how to reduce these coefficients by sparsifying
an algorithm's bilinear operator. Unfortunately, the problem of finding optimal
sparsifications is NP-Hard.
  We obtain three new methods to this end, and apply them to existing fast
matrix multiplication algorithms, thus improving their leading coefficients.
These methods have an exponential worst case running time, but run fast in
practice and improve the performance of many fast matrix multiplication
algorithms. Two of the methods are guaranteed to produce leading coefficients
that, under some assumptions, are optimal.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.05023v3,"Facial Electromyography-based Adaptive Virtual Reality Gaming for
  Cognitive Training","Cognitive training has shown promising results for delivering improvements in
human cognition related to attention, problem solving, reading comprehension
and information retrieval. However, two frequently cited problems in cognitive
training literature are a lack of user engagement with the training programme,
and a failure of developed skills to generalise to daily life. This paper
introduces a new cognitive training (CT) paradigm designed to address these two
limitations by combining the benefits of gamification, virtual reality (VR),
and affective adaptation in the development of an engaging, ecologically valid,
CT task. Additionally, it incorporates facial electromyography (EMG) as a means
of determining user affect while engaged in the CT task. This information is
then utilised to dynamically adjust the game's difficulty in real-time as users
play, with the aim of leading them into a state of flow. Affect recognition
rates of 64.1% and 76.2%, for valence and arousal respectively, were achieved
by classifying a DWT-Haar approximation of the input signal using kNN. The
affect-aware VR cognitive training intervention was then evaluated with a
control group of older adults. The results obtained substantiate the notion
that adaptation techniques can lead to greater feelings of competence and a
more appropriate challenge of the user's skills.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
http://arxiv.org/abs/2008.01363v2,Illustrations of non-Euclidean geometry in virtual reality,"Mathematical objects are generally abstract and not very approachable.
Illustrations and interactive visualizations help both students and
professionals to comprehend mathematical material and to work with it. This
approach lends itself particularly well to geometrical objects. An example for
this category of mathematical objects are hyperbolic geometric spaces. When
Euclid lay down the foundations of mathematics, his formulation of geometry
reflected the surrounding space, as humans perceive it. For about two
millennia, it remained unclear whether there are alternative geometric spaces
that carry their own, unique mathematical properties and that do not reflect
human every-day perceptions. Finally, in the early 19th century, several
mathematicians described such geometries, which do not follow Euclid's rules
and which were at first interesting solely from a pure mathematical point of
view. These descriptions were not very accessible as mathematicians approached
the geometries via complicated collections of formulae. Within the following
decades, visualization aided the new concepts and two-dimensional versions of
these illustrations even appeared in artistic works. Furthermore, certain
aspects of Einstein's theory of relativity provided applications for
non-Euclidean geometric spaces. With the rise of computer graphics towards the
end of the twentieth century, three-dimensional illustrations became available
to explore these geometries and their non-intuitive properties. However, just
as the canvas confines the two-dimensional depictions, the computer monitor
confines these three-dimensional visualizations. Only virtual reality recently
made it possible to present immersive experiences of non-Euclidean geometries.
In virtual reality, users have completely new opportunities to encounter
geometric properties and effects that are not present in their surrounding
Euclidean world.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0911.5046v2,Integrating the Probabilistic Models BM25/BM25F into Lucene,"This document describes the BM25 and BM25F implementation using the Lucene
Java Framework. Both models have stood out at TREC by their performance and are
considered as state-of-the-art in the IR community. BM25 is applied to
retrieval on plain text documents, that is for documents that do not contain
fields, while BM25F is applied to documents with structure.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1204.0423v11,"On voting intentions inference from Twitter content: a case study on UK
  2010 General Election","This is a report, where preliminary work regarding the topic of voting
intention inference from Social Media - such as Twitter - is presented. Our
case study is the UK 2010 General Election and we are focusing on predicting
the percentages of voting intention polls (conducted by YouGov) for the three
major political parties - Conservatives, Labours and Liberal Democrats - during
a 5-month period before the election date (May 6, 2010). We form three
methodologies for extracting positive or negative sentiment from tweets, which
build on each other, and then propose two supervised models for turning
sentiment into voting intention percentages. Interestingly, when the content of
tweets is enriched by attaching synonymous words, a significant improvement on
inference performance is achieved reaching a mean absolute error of 4.34% +/-
2.13%; in that case, the predictions are also shown to be statistically
significant. The presented methods should be considered as work-in-progress;
limitations and suggestions for future work appear in the final section of this
script.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.04915v1,"Managing Data Lineage of O&G Machine Learning Models: The Sweet Spot for
  Shale Use Case","Machine Learning (ML) has increased its role, becoming essential in several
industries. However, questions around training data lineage, such as ""where has
the dataset used to train this model come from?""; the introduction of several
new data protection legislation; and, the need for data governance
requirements, have hindered the adoption of ML models in the real world. In
this paper, we discuss how data lineage can be leveraged to benefit the ML
lifecycle to build ML models to discover sweet-spots for shale oil and gas
production, a major application in the Oil and Gas O&G Industry.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.12209v2,"Can Advanced Type Systems Be Usable? An Empirical Study of Ownership,
  Assets, and Typestate in Obsidian","Some blockchain programs (smart contracts) have included serious security
vulnerabilities. Obsidian is a new typestate-oriented programming language that
uses a strong type system to rule out some of these vulnerabilities. Although
Obsidian was designed to promote usability to make it as easy as possible to
write programs, strong type systems can cause a language to be difficult to
use. In particular, ownership, typestate, and assets, which Obsidian uses to
provide safety guarantees, have not seen broad adoption together in popular
languages and result in significant usability challenges. We performed an
empirical study with 20 participants comparing Obsidian to Solidity, which is
the language most commonly used for writing smart contracts today. We observed
that Obsidian participants were able to successfully complete more of the
programming tasks than the Solidity participants. We also found that the
Solidity participants commonly inserted asset-related bugs, which Obsidian
detects at compile time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0505072v1,Steganographic Codes -- a New Problem of Coding Theory,"To study how to design steganographic algorithm more efficiently, a new
coding problem -- steganographic codes (abbreviated stego-codes) -- is
presented in this paper. The stego-codes are defined over the field with
$q(q\ge2)$ elements. Firstly a method of constructing linear stego-codes is
proposed by using the direct sum of vector subspaces. And then the problem of
linear stego-codes is converted to an algebraic problem by introducing the
concept of $t$th dimension of vector space. And some bounds on the length of
stego-codes are obtained, from which the maximum length embeddable (MLE) code
is brought up. It is shown that there is a corresponding relation between MLE
codes and perfect error-correcting codes. Furthermore the classification of all
MLE codes and a lower bound on the number of binary MLE codes are obtained
based on the corresponding results on perfect codes. Finally hiding redundancy
is defined to value the performance of stego-codes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0404007v1,Polarity sensitivity and evaluation order in type-logical grammar,"We present a novel, type-logical analysis of_polarity sensitivity_: how
negative polarity items (like ""any"" and ""ever"") or positive ones (like ""some"")
are licensed or prohibited. It takes not just scopal relations but also linear
order into account, using the programming-language notions of delimited
continuations and evaluation order, respectively. It thus achieves greater
empirical coverage than previous proposals.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.10397v2,NVCache: A Plug-and-Play NVMM-based I/O Booster for Legacy Systems,"This paper introduces NVCache, an approach that uses a non-volatile main
memory (NVMM) as a write cache to improve the write performance of legacy
applications. We compare NVCache against file systems tailored for NVMM
(Ext4-DAX and NOVA) and with I/O-heavy applications (SQLite, RocksDB). Our
evaluation shows that NVCache reaches the performance level of the existing
state-of-the-art systems for NVMM, but without their limitations: NVCache does
not limit the size of the stored data to the size of the NVMM, and works
transparently with unmodified legacy applications, providing additional
persistence guarantees even when their source code is not available.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0710.0431v2,New Counting Codes for Distributed Video Coding,"This paper introduces a new counting code. Its design was motivated by
distributed video coding where, for decoding, error correction methods are
applied to improve predictions. Those error corrections sometimes fail which
results in decoded values worse than the initial prediction. Our code exploits
the fact that bit errors are relatively unlikely events: more than a few bit
errors in a decoded pixel value are rare. With a carefully designed counting
code combined with a prediction those bit errors can be corrected and sometimes
the original pixel value recovered. The error correction improves
significantly. Our new code not only maximizes the Hamming distance between
adjacent (or ""near 1"") codewords but also between nearby (for example ""near 2"")
codewords. This is why our code is significantly different from the well-known
maximal counting sequences which have maximal average Hamming distance.
Fortunately, the new counting code can be derived from Gray Codes for every
code word length (i.e. bit depth).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1605.02045v1,Fast Compatibility Testing for Phylogenies with Nested Taxa,"Semi-labeled trees are phylogenies whose internal nodes may be labeled by
higher-order taxa. Thus, a leaf labeled Mus musculus could nest within a
subtree whose root node is labeled Rodentia, which itself could nest within a
subtree whose root is labeled Mammalia. Suppose we are given collection
$\mathcal P$ of semi-labeled trees over various subsets of a set of taxa. The
ancestral compatibility problem asks whether there is a semi-labeled tree
$\mathcal T$ that respects the clusterings and the ancestor/descendant
relationships implied by the trees in $\mathcal P$. We give a
$\tilde{O}(M_{\mathcal{P}})$ algorithm for the ancestral compatibility problem,
where $M_{\mathcal{P}}$ is the total number of nodes and edges in the trees in
$\mathcal P$. Unlike the best previous algorithm, the running time of our
method does not depend on the degrees of the nodes in the input trees.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0911.0508v1,Optimization and Evaluation of Nested Queries and Procedures,"Many database applications perform complex data retrieval and update tasks.
Nested queries, and queries that invoke user-defined functions, which are
written using a mix of procedural and SQL constructs, are often used in such
applications. A straight-forward evaluation of such queries involves repeated
execution of parameterized sub-queries or blocks containing queries and
procedural code.
  An important problem that arises while optimizing nested queries as well as
queries with joins, aggregates and set operations is the problem of finding an
optimal sort order from a factorial number of possible sort orders. We show
that even a special case of this problem is NP-Hard, and present practical
heuristics that are effective and easy to incorporate in existing query
optimizers.
  We also consider iterative execution of queries and updates inside complex
procedural blocks such as user-defined functions and stored procedures.
Parameter batching is an important means of improving performance as it enables
set-orientated processing. The key challenge to parameter batching lies in
rewriting a given procedure/function to process a batch of parameter values. We
propose a solution, based on program analysis and rewrite rules, to automate
the generation of batched forms of procedures and replace iterative database
calls within imperative loops with a single call to the batched form.
  We present experimental results for the proposed techniques, and the results
show significant gains in performance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1908.06807v2,Boomerang: Real-Time I/O Meets Legacy Systems,"This paper presents Boomerang, an I/O system that integrates a legacy
non-real-time OS with one that is customized for timing-sensitive tasks. A
relatively small RTOS benefits from the pre-existing libraries, drivers and
services of the legacy system. Additionally, timing-critical tasks are isolated
from less critical tasks by securely partitioning machine resources among the
separate OSes. Boomerang guarantees end-to-end processing delays on input data
that requires outputs to be generated within specific time bounds.
  We show how to construct composable task pipelines in Boomerang that combine
functionality spanning a custom RTOS and a legacy Linux system. By dedicating
time-critical I/O to the RTOS, we ensure that complementary services provided
by Linux are sufficiently predictable to meet end-to-end service guarantees.
While Boomerang benefits from spatial isolation, it also outperforms a
standalone Linux system using deadline-based CPU reservations for pipeline
tasks. We also show how Boomerang outperforms a virtualized system called ACRN,
designed for automotive systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.2304v2,"Declaratively solving tricky Google Code Jam problems with Prolog-based
  ECLiPSe CLP system","In this paper we demonstrate several examples of solving challenging
algorithmic problems from the Google Code Jam programming contest with the
Prolog-based ECLiPSe system using declarative techniques like constraint logic
programming and linear (integer) programming. These problems were designed to
be solved by inventing clever algorithms and efficiently implementing them in a
conventional imperative programming language, but we present relatively simple
declarative programs in ECLiPSe that are fast enough to find answers within the
time limit imposed by the contest rules. We claim that declarative programming
with ECLiPSe is better suited for solving certain common kinds of programming
problems offered in Google Code Jam than imperative programming. We show this
by comparing the mental steps required to come up with both kinds of solutions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1604.07849v2,"Distributed rotational and translational maneuvering of rigid formations
  and their applications","Recently it has been reported that range-measurement inconsistency, or
equivalently mismatches in prescribed inter-agent distances, may prevent the
popular gradient controllers from guiding rigid formations of mobile agents to
converge to their desired shape, and even worse from standing still at any
location. In this paper, instead of treating mismatches as the source of ill
performance, we take them as design parameters and show that by introducing
such a pair of parameters per distance constraint, distributed controller
achieving simultaneously both formation and motion control can be designed that
not only encompasses the popular gradient control, but more importantly allows
us to achieve constant collective translation, rotation or their combination
while guaranteeing asymptotically no distortion in the formation shape occurs.
Such motion control results are then applied to (a) the alignment of formations
orientations and (b) enclosing and tracking a moving target. Besides rigorous
mathematical proof, experiments using mobile robots are demonstrated to show
the satisfying performances of the proposed formation-motion distributed
controller.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.05352v3,"All One Needs to Know about Metaverse: A Complete Survey on
  Technological Singularity, Virtual Ecosystem, and Research Agenda","Since the popularisation of the Internet in the 1990s, the cyberspace has
kept evolving. We have created various computer-mediated virtual environments
including social networks, video conferencing, virtual 3D worlds (e.g., VR
Chat), augmented reality applications (e.g., Pokemon Go), and Non-Fungible
Token Games (e.g., Upland). Such virtual environments, albeit non-perpetual and
unconnected, have bought us various degrees of digital transformation. The term
`metaverse' has been coined to further facilitate the digital transformation in
every aspect of our physical lives. At the core of the metaverse stands the
vision of an immersive Internet as a gigantic, unified, persistent, and shared
realm. While the metaverse may seem futuristic, catalysed by emerging
technologies such as Extended Reality, 5G, and Artificial Intelligence, the
digital `big bang' of our cyberspace is not far away. This survey paper
presents the first effort to offer a comprehensive framework that examines the
latest metaverse development under the dimensions of state-of-the-art
technologies and metaverse ecosystems, and illustrates the possibility of the
digital `big bang'. First, technologies are the enablers that drive the
transition from the current Internet to the metaverse. We thus examine eight
enabling technologies rigorously - Extended Reality, User Interactivity
(Human-Computer Interaction), Artificial Intelligence, Blockchain, Computer
Vision, IoT and Robotics, Edge and Cloud computing, and Future Mobile Networks.
In terms of applications, the metaverse ecosystem allows human users to live
and play within a self-sustaining, persistent, and shared realm. Therefore, we
discuss six user-centric factors -- Avatar, Content Creation, Virtual Economy,
Social Acceptability, Security and Privacy, and Trust and Accountability.
Finally, we propose a concrete research agenda for the development of the
metaverse.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1202.4107v1,"Unsupervised Threshold for Automatic Extraction of Dolphin Dorsal Fin
  Outlines from Digital Photographs in DARWIN (Digital Analysis and Recognition
  of Whale Images on a Network)","At least two software packages---DARWIN, Eckerd College, and FinScan, Texas
A&M---exist to facilitate the identification of cetaceans---whales, dolphins,
porpoises---based upon the naturally occurring features along the edges of
their dorsal fins. Such identification is useful for biological studies of
population, social interaction, migration, etc. The process whereby fin
outlines are extracted in current fin-recognition software packages is manually
intensive and represents a major user input bottleneck: it is both time
consuming and visually fatiguing. This research aims to develop automated
methods (employing unsupervised thresholding and morphological processing
techniques) to extract cetacean dorsal fin outlines from digital photographs
thereby reducing manual user input. Ideally, automatic outline generation will
improve the overall user experience and improve the ability of the software to
correctly identify cetaceans. Various transformations from color to gray space
were examined to determine which produced a grayscale image in which a suitable
threshold could be easily identified. To assist with unsupervised thresholding,
a new metric was developed to evaluate the jaggedness of figures (""pixelarity"")
in an image after thresholding. The metric indicates how cleanly a threshold
segments background and foreground elements and hence provides a good measure
of the quality of a given threshold. This research results in successful
extractions in roughly 93% of images, and significantly reduces user-input
time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0408005v1,Educational Content Management - A Cellular Approach,"In recent times online educational applications more and more are requested
to provide self-consistent learning offers for students at the university
level. Consequently they need to cope with the wide range of complexity and
interrelations university course teaching brings along. An urgent need to
overcome simplistically linked HTMLc ontent pages becomes apparent. In the
present paper we discuss a schematic concept of educational content
construction from information cells and introduce its implementation on the
storage and runtime layer. Starting from cells content is annotated according
to didactic needs, structured for dynamic arrangement, dynamically decorated
with hyperlinks and, as all works are based on XML, open to any presentation
layer. Data can be variably accessed through URIs built on semantic path-names
and edited via an adaptive authoring toolbox. Our content management approach
is based on the more general Multimedia Information Repository MIR. and allows
for personalisation, as well. MIR is an open system supporting the standards
XML, Corba and JNDI.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0
http://arxiv.org/abs/1204.3249v1,Process algebra with conditionals in the presence of epsilon,"In a previous paper, we presented several extensions of ACP with conditional
expressions, including one with a retrospection operator on conditions to allow
for looking back on conditions under which preceding actions have been
performed. In this paper, we add a constant for a process that is only capable
of terminating successfully to those extensions of ACP, which can be very
useful in applications. It happens that in all cases the addition of this
constant is unproblematic.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1908.02215v1,On cylindrical regression in three-dimensional Euclidean space,"The three-dimensional cylindrical regression problem is a problem of finding
a cylinder best fitting a group of points in three-dimensional Euclidean space.
The words best fitting are usually understood in the sense of the minimum root
mean square deflection of the given points from a cylinder to be found. In this
form the problem has no analytic solution. If one replaces the root mean square
averaging by a certain biquadratic averaging, the resulting problem has an
almost analytic solution. This solution is reproduced in the present paper in a
coordinate-free form.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0004001v1,"A Theory of Universal Artificial Intelligence based on Algorithmic
  Complexity","Decision theory formally solves the problem of rational agents in uncertain
worlds if the true environmental prior probability distribution is known.
Solomonoff's theory of universal induction formally solves the problem of
sequence prediction for unknown prior distribution. We combine both ideas and
get a parameterless theory of universal Artificial Intelligence. We give strong
arguments that the resulting AIXI model is the most intelligent unbiased agent
possible. We outline for a number of problem classes, including sequence
prediction, strategic games, function minimization, reinforcement and
supervised learning, how the AIXI model can formally solve them. The major
drawback of the AIXI model is that it is uncomputable. To overcome this
problem, we construct a modified algorithm AIXI-tl, which is still effectively
more intelligent than any other time t and space l bounded agent. The
computation time of AIXI-tl is of the order tx2^l. Other discussed topics are
formal definitions of intelligence order relations, the horizon problem and
relations of the AIXI theory to other AI approaches.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1005.2638v1,"Hierarchical Clustering for Finding Symmetries and Other Patterns in
  Massive, High Dimensional Datasets","Data analysis and data mining are concerned with unsupervised pattern finding
and structure determination in data sets. ""Structure"" can be understood as
symmetry and a range of symmetries are expressed by hierarchy. Such symmetries
directly point to invariants, that pinpoint intrinsic properties of the data
and of the background empirical domain of interest. We review many aspects of
hierarchy here, including ultrametric topology, generalized ultrametric,
linkages with lattices and other discrete algebraic structures and with p-adic
number representations. By focusing on symmetries in data we have a powerful
means of structuring and analyzing massive, high dimensional data stores. We
illustrate the powerfulness of hierarchical clustering in case studies in
chemistry and finance, and we provide pointers to other published case studies.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9906004v1,Cascaded Grammatical Relation Assignment,"In this paper we discuss cascaded Memory-Based grammatical relations
assignment. In the first stages of the cascade, we find chunks of several types
(NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local,
temporal). In the last stage, we assign grammatical relations to pairs of
chunks. We studied the effect of adding several levels to this cascaded
classifier and we found that even the less performing chunkers enhanced the
performance of the relation finder.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0208012v1,"Online Scientific Data Curation, Publication, and Archiving","Science projects are data publishers. The scale and complexity of current and
future science data changes the nature of the publication process. Publication
is becoming a major project component. At a minimum, a project must preserve
the ephemeral data it gathers. Derived data can be reconstructed from metadata,
but metadata is ephemeral. Longer term, a project should expect some archive to
preserve the data. We observe that pub-lished scientific data needs to be
available forever ? this gives rise to the data pyramid of versions and to data
inflation where the derived data volumes explode. As an example, this article
describes the Sloan Digital Sky Survey (SDSS) strategies for data publication,
data access, curation, and preservation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,1,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.05414v1,"Verification and Reachability Analysis of Fractional-Order Differential
  Equations Using Interval Analysis","Interval approaches for the reachability analysis of initial value problems
for sets of classical ordinary differential equations have been investigated
and implemented by many researchers during the last decades. However, there
exist numerous applications in computational science and engineering, where
continuous-time system dynamics cannot be described adequately by integer-order
differential equations. Especially in cases in which long-term memory effects
are observed, fractional-order system representations are promising to describe
the dynamics, on the one hand, with sufficient accuracy and, on the other hand,
to limit the number of required state variables and parameters to a reasonable
amount. Real-life applications for such fractional-order models can, among
others, be found in the field of electrochemistry, where methods for impedance
spectroscopy are typically used to identify fractional-order models for the
charging/discharging behavior of batteries or for the dynamic relation between
voltage and current in fuel cell systems if operated in a non-stationary state.
This paper aims at presenting an iterative method for reachability analysis of
fractional-order systems that is based on an interval arithmetic extension of
Mittag-Leffler functions. An illustrating example, inspired by a low-order
model of battery systems concludes this contribution.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1810.05874v2,"Embedded deep learning in ophthalmology: Making ophthalmic imaging
  smarter","Deep learning has recently gained high interest in ophthalmology, due to its
ability to detect clinically significant features for diagnosis and prognosis.
Despite these significant advances, little is known about the ability of
various deep learning systems to be embedded within ophthalmic imaging devices,
allowing automated image acquisition. In this work, we will review the existing
and future directions for ""active acquisition"" embedded deep learning, leading
to as high quality images with little intervention by the human operator. In
clinical practice, the improved image quality should translate into more robust
deep learning-based clinical diagnostics. Embedded deep learning will be
enabled by the constantly improving hardware performance with low cost. We will
briefly review possible computation methods in larger clinical systems.
Briefly, they can be included in a three-layer framework composed of edge, fog
and cloud layers, the former being performed at a device-level. Improved edge
layer performance via ""active acquisition"" serves as an automatic data curation
operator translating to better quality data in electronic health records
(EHRs), as well as on the cloud layer, for improved deep learning-based
clinical data mining.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1810.00267v1,"Older Adults and Crowdsourcing: Android TV App for Evaluating TEDx
  Subtitle Quality","In this paper we describe the insights from an exploratory qualitative pilot
study testing the feasibility of a solution that would encourage older adults
to participate in online crowdsourcing tasks in a non-computer scenario.
Therefore, we developed an Android TV application using Amara API to retrieve
subtitles for TEDx talks which allows the participants to detect and categorize
errors to support the quality of the translation and transcription processes.
It relies on the older adults' innate skills as long-time native language users
and the motivating factors of this socially and personally beneficial task. The
study allowed us to verify the underlying concept of using Smart TVs as
interfaces for crowdsourcing, as well as possible barriers, including the
interface, configuration issues, topics and the process itself. We have also
assessed the older adults' interaction and engagement with this TV-enabled
online crowdsourcing task and we are convinced that the design of our setup
addresses some key barriers to crowdsourcing by older adults. It also validates
avenues for further research in this area focused on such considerations as
autonomy and freedom of choice, familiarity, physical and cognitive comfort as
well as building confidence and the edutainment value.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1004.2697v4,Assume-Guarantee Synthesis for Digital Contract Signing,"We study the automatic synthesis of fair non-repudiation protocols, a class
of fair exchange protocols, used for digital contract signing. First, we show
how to specify the objectives of the participating agents and the trusted third
party (TTP) as path formulas in LTL and prove that the satisfaction of these
objectives imply fairness; a property required of fair exchange protocols. We
then show that weak (co-operative) co-synthesis and classical (strictly
competitive) co-synthesis fail, whereas assume-guarantee synthesis (AGS)
succeeds. We demonstrate the success of assume-guarantee synthesis as follows:
(a) any solution of assume-guarantee synthesis is attack-free; no subset of
participants can violate the objectives of the other participants; (b) the
Asokan-Shoup-Waidner (ASW) certified mail protocol that has known
vulnerabilities is not a solution of AGS; (c) the Kremer-Markowitch (KM)
non-repudiation protocol is a solution of AGS; and (d) AGS presents a new and
symmetric fair non-repudiation protocol that is attack-free. To our knowledge
this is the first application of synthesis to fair non-repudiation protocols,
and our results show how synthesis can both automatically discover
vulnerabilities in protocols and generate correct protocols. The solution to
assume-guarantee synthesis can be computed efficiently as the secure
equilibrium solution of three-player graph games.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.14125v1,"AirSPEC: An IoT-empowered Air Quality Monitoring System integrated with
  a Machine Learning Framework to Detect and Predict defined Air Quality
  parameters","The air that surrounds us is the cardinal source of respiration of all
life-forms. Therefore, it is undoubtedly vital to highlight that balanced air
quality is utmost important to the respiratory health of all living beings,
environmental homeostasis, and even economical equilibrium. Nevertheless, a
gradual deterioration of air quality has been observed in the last few decades,
due to the continuous increment of polluted emissions from automobiles and
industries into the atmosphere. Even though many people have scarcely
acknowledged the depth of the problem, the persistent efforts of determined
parties, including the World Health Organization, have consistently pushed the
boundaries for a qualitatively better global air homeostasis, by facilitating
technology-driven initiatives to timely detect and predict air quality in
regional and global scales. However, the existing frameworks for air quality
monitoring lack the capability of real-time responsiveness and flexible
semantic distribution. In this paper, a novel Internet of Things framework is
proposed which is easily implementable, semantically distributive, and
empowered by a machine learning model. The proposed system is equipped with a
NodeRED dashboard which processes, visualizes, and stores the primary sensor
data that are acquired through a public air quality sensor network, and
further, the dashboard is integrated with a machine-learning model to obtain
temporal and geo-spatial air quality predictions. ESP8266 NodeMCU is
incorporated as a subscriber to the NodeRED dashboard via a message queuing
telemetry transport broker to communicate quantitative air quality data or
alarming emails to the end-users through the developed web and mobile
applications. Therefore, the proposed system could become highly beneficial in
empowering public engagement in air quality through an unoppressive,
data-driven, and semantic framework.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.10217v1,"Shape Inference and Grammar Induction for Example-based Procedural
  Generation","Designers increasingly rely on procedural generation for automatic generation
of content in various industries. These techniques require extensive knowledge
of the desired content, and about how to actually implement such procedural
methods. Algorithms for learning interpretable generative models from example
content could alleviate both difficulties. We propose SIGI, a novel method for
inferring shapes and inducing a shape grammar from grid-based 3D building
examples. This interpretable grammar is well-suited for co-creative design.
Applied to Minecraft buildings, we show how the shape grammar can be used to
automatically generate new buildings in a similar style.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1703.05647v2,Replicable Parallel Branch and Bound Search,"Combinatorial branch and bound searches are a common technique for solving
global optimisation and decision problems. Their performance often depends on
good search order heuristics, refined over decades of algorithms research.
Parallel search necessarily deviates from the sequential search order,
sometimes dramatically and unpredictably, e.g. by distributing work at random.
This can disrupt effective search order heuristics and lead to unexpected and
highly variable parallel performance. The variability makes it hard to reason
about the parallel performance of combinatorial searches.
  This paper presents a generic parallel branch and bound skeleton, implemented
in Haskell, with replicable parallel performance. The skeleton aims to preserve
the search order heuristic by distributing work in an ordered fashion, closely
following the sequential search order. We demonstrate the generality of the
approach by applying the skeleton to 40 instances of three combinatorial
problems: Maximum Clique, 0/1 Knapsack and Travelling Salesperson. The
overheads of our Haskell skeleton are reasonable: giving slowdown factors of
between 1.9 and 6.2 compared with a class-leading, dedicated, and highly
optimised C++ Maximum Clique solver. We demonstrate scaling up to 200 cores of
a Beowulf cluster, achieving speedups of 100x for several Maximum Clique
instances. We demonstrate low variance of parallel performance across all
instances of the three combinatorial problems and at all scales up to 200
cores, with median Relative Standard Deviation (RSD) below 2%. Parallel solvers
that do not follow the sequential search order exhibit far higher variance,
with median RSD exceeding 85% for Knapsack.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0504083v1,On the Unicity Distance of Stego Key,"Steganography is about how to send secret message covertly. And the purpose
of steganalysis is to not only detect the existence of the hidden message but
also extract it. So far there have been many reliable detecting methods on
various steganographic algorithms, while there are few approaches that can
extract the hidden information. In this paper, the difficulty of extracting
hidden information, which is essentially a kind of privacy, is analyzed with
information-theoretic method in the terms of unicity distance of steganographic
key (abbreviated stego key). A lower bound for the unicity distance is
obtained, which shows the relations between key rate, message rate, hiding
capacity and difficulty of extraction. Furthermore the extracting attack to
steganography is viewed as a special kind of cryptanalysis, and an effective
method on recovering the stego key of popular LSB replacing steganography in
spatial images is presented by combining the detecting technique of
steganalysis and correlation attack of cryptanalysis together. The analysis for
this method and experimental results on steganographic software ``Hide and Seek
4.1"" are both accordant with the information-theoretic conclusion.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.00706v1,Turing Number: How Far Are You to A. M. Turing Award?,"The ACM A.M. Turing Award is commonly acknowledged as the highest distinction
in the realm of computer science. Since 1960s, it has been awarded to computer
scientists who made outstanding contributions. The significance of this award
is far-reaching to the laureates as well as their research teams. However,
unlike the Nobel Prize that has been extensively investigated, little research
has been done to explore this most important award. To this end, we propose the
Turing Number (TN) index to measure how far a specific scholar is to this
award. Inspired by previous works on Erdos Number and Bacon Number, this index
is defined as the shortest path between a given scholar to any Turing Award
Laureate. Experimental results suggest that TN can reflect the closeness of
collaboration between scholars and Turing Award Laureates. With the correlation
analysis between TN and metrics from the bibliometric-level and network-level,
we demonstrate that TN has the potential of reflecting a scholar's academic
influence and reputation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1201.4764v1,Matroid Prophet Inequalities,"Consider a gambler who observes a sequence of independent, non-negative
random numbers and is allowed to stop the sequence at any time, claiming a
reward equal to the most recent observation. The famous prophet inequality of
Krengel, Sucheston, and Garling asserts that a gambler who knows the
distribution of each random variable can achieve at least half as much reward,
in expectation, as a ""prophet"" who knows the sampled values of each random
variable and can choose the largest one. We generalize this result to the
setting in which the gambler and the prophet are allowed to make more than one
selection, subject to a matroid constraint. We show that the gambler can still
achieve at least half as much reward as the prophet; this result is the best
possible, since it is known that the ratio cannot be improved even in the
original prophet inequality, which corresponds to the special case of rank-one
matroids. Generalizing the result still further, we show that under an
intersection of p matroid constraints, the prophet's reward exceeds the
gambler's by a factor of at most O(p), and this factor is also tight.
  Beyond their interest as theorems about pure online algorithms or optimal
stopping rules, these results also have applications to mechanism design. Our
results imply improved bounds on the ability of sequential posted-price
mechanisms to approximate Bayesian optimal mechanisms in both single-parameter
and multi-parameter settings. In particular, our results imply the first
efficiently computable constant-factor approximations to the Bayesian optimal
revenue in certain multi-parameter settings.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1401.5775v3,"Trusty URIs: Verifiable, Immutable, and Permanent Digital Artifacts for
  Linked Data","To make digital resources on the web verifiable, immutable, and permanent, we
propose a technique to include cryptographic hash values in URIs. We call them
trusty URIs and we show how they can be used for approaches like
nanopublications to make not only specific resources but their entire reference
trees verifiable. Digital artifacts can be identified not only on the byte
level but on more abstract levels such as RDF graphs, which means that
resources keep their hash values even when presented in a different format. Our
approach sticks to the core principles of the web, namely openness and
decentralized architecture, is fully compatible with existing standards and
protocols, and can therefore be used right away. Evaluation of our reference
implementations shows that these desired properties are indeed accomplished by
our approach, and that it remains practical even for very large files.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1701.01636v1,"Evaluation of Business-Oriented Performance Metrics in e-Commerce using
  Web-based Simulation","The Web 2.0 paradigm has radically changed the way businesses are run all
around the world. Moreover, e-Commerce has overcome in daily shopping
activities. For management teams, the assessment, evaluation, and forecasting
of online incomes and other business-oriented performance measures have become
'a holy grail', the ultimate question imposing their current and future
e-Commerce projects. Within the paper, we describe the development of a
Web-based simulation model, suitable for their estimation, taking into account
multiple operation profiles and scenarios. Specifically, we put focus on
introducing specific classes of e-Customers, as well as the workload
characterization of an arbitrary e-Commerce website. On the other hand, we
employ and embed the principles of the system thinking approach and the system
dynamics into the proposed solution. As a result, a complete simulation model
has been developed, available online. The model, which includes numerous
adjustable input variables, can be successfully utilized in making
'what-if'-like insights into a plethora of business-oriented performance
metrics for an arbitrary e-Commerce website. This project is, also, a great
example of the power delivered by InsightMaker, free-of-charge Web-based
software, suitable for a collaborative online development of models following
the systems thinking paradigm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/cs/0302038v1,Tight Logic Programs,"This note is about the relationship between two theories of negation as
failure -- one based on program completion, the other based on stable models,
or answer sets. Francois Fages showed that if a logic program satisfies a
certain syntactic condition, which is now called ``tightness,'' then its stable
models can be characterized as the models of its completion. We extend the
definition of tightness and Fages' theorem to programs with nested expressions
in the bodies of rules, and study tight logic programs containing the
definition of the transitive closure of a predicate.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.06381v1,Scalable Querying of Nested Data,"While large-scale distributed data processing platforms have become an
attractive target for query processing, these systems are problematic for
applications that deal with nested collections. Programmers are forced either
to perform non-trivial translations of collection programs or to employ
automated flattening procedures, both of which lead to performance problems.
These challenges only worsen for nested collections with skewed cardinalities,
where both handcrafted rewriting and automated flattening are unable to enforce
load balancing across partitions.
  In this work, we propose a framework that translates a program manipulating
nested collections into a set of semantically equivalent shredded queries that
can be efficiently evaluated. The framework employs a combination of query
compilation techniques, an efficient data representation for nested
collections, and automated skew-handling. We provide an extensive experimental
evaluation, demonstrating significant improvements provided by the framework in
diverse scenarios for nested collection programs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1809.04684v1,Fair lending needs explainable models for responsible recommendation,"The financial services industry has unique explainability and fairness
challenges arising from compliance and ethical considerations in credit
decisioning. These challenges complicate the use of model machine learning and
artificial intelligence methods in business decision processes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.06180v2,aether: Distributed system emulation in Common Lisp,"We describe a Common Lisp package suitable for the high-level design,
specification, simulation, and instrumentation of real-time distributed
algorithms and hardware on which to run them. We discuss various design
decisions around the package structure, and we explore their consequences with
small examples.",0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.01286v2,"Operator Splitting for Adaptive Radiation Therapy with Nonlinear Health
  Dynamics","We present an optimization-based approach to radiation treatment planning
over time. Our approach formulates treatment planning as an optimal control
problem with nonlinear patient health dynamics derived from the standard
linear-quadratic cell survival model. As the formulation is nonconvex, we
propose a method for obtaining an approximate solution by solving a sequence of
convex optimization problems. This method is fast, efficient, and robust to
model error, adapting readily to changes in the patient's health between
treatment sessions. Moreover, we show that it can be combined with the operator
splitting method ADMM to produce an algorithm that is highly scalable and can
handle large clinical cases. We introduce an open-source Python implementation
of our algorithm, AdaRad, and demonstrate its performance on several examples.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1810.01272v2,"Disrupting the Coming Robot Stampedes: Designing Resilient Information
  Ecologies","Machines are designed to communicate widely and efficiently. Humans, less so.
We evolved social structures that function best as small subgroups interacting
within larger populations. Technology changes this dynamic, by allowing all
individuals to be connected at the speed of light. A dense, tightly connected
population can behave like a single agent. In animals, this happens in
constrained areas where stampedes can easily form. Machines do not need these
kinds of conditions. The very techniques used to design best-of-breed solutions
may increase the risk of dangerous mass behaviors among homogeneous machines.
In this paper we argue that ecologically-based design principles such as the
presence of diversity are a broadly effective strategy to defend against
unintended consequences at scale.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1204.5086v1,"Reimplementing the Mathematical Subject Classification (MSC) as a Linked
  Open Dataset","The Mathematics Subject Classification (MSC) is a widely used scheme for
classifying documents in mathematics by subject. Its traditional, idiosyncratic
conceptualization and representation makes the scheme hard to maintain and
requires custom implementations of search, query and annotation support. This
limits uptake e.g. in semantic web technologies in general and the creation and
exploration of connections between mathematics and related domains (e.g.
science) in particular.
  This paper presents the new official implementation of the MSC2010 as a
Linked Open Dataset, building on SKOS (Simple Knowledge Organization System).
We provide a brief overview of the dataset's structure, its available
implementations, and first applications.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0310002v2,The Graphics Card as a Streaming Computer,"Massive data sets have radically changed our understanding of how to design
efficient algorithms; the streaming paradigm, whether it in terms of number of
passes of an external memory algorithm, or the single pass and limited memory
of a stream algorithm, appears to be the dominant method for coping with large
data.
  A very different kind of massive computation has had the same effect at the
level of the CPU. The most prominent example is that of the computations
performed by a graphics card. The operations themselves are very simple, and
require very little memory, but require the ability to perform many
computations extremely fast and in parallel to whatever degree possible. What
has resulted is a stream processor that is highly optimized for stream
computations. An intriguing side effect of this is the growing use of a
graphics card as a general purpose stream processing engine. In an
ever-increasing array of applications, researchers are discovering that
performing a computation on a graphics card is far faster than performing it on
a CPU, and so are using a GPU as a stream co-processor.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.08095v1,"Automatic Differentiation via Effects and Handlers: An Implementation in
  Frank","Automatic differentiation (AD) is an important family of algorithms which
enables derivative based optimization. We show that AD can be simply
implemented with effects and handlers by doing so in the Frank language. By
considering how our implementation behaves in Frank's operational semantics, we
show how our code performs the dynamic creation of programs during evaluation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1402.3788v1,Using of GPUs for cluster analysis of large data by K-means method,"This problem was solved within the framework of the grant project ""Solving of
problems of cluster analysis with application of parallel algorithms and cloud
technologies"" in the Institute of Mathematics and Mathematical Modelling in
Almaty. The problem of cluster analysis for the large amount of data is very
important in different areas of science - genetics, biology, sociology etc. At
the same time, such statistical known packages as STATISTICA, STADIA, SYSTAT
and others do not allow to solve large problems. The new algorithm that uses
the high processing power of GPUs for solving clustering problems by the
K-means method was developed. This algorithm is implemented as a C++
application in Microsoft Visual Studio 2010 with using the GPU Nvidia GeForce
660. The developed software package for solving clustering problems by the
method of K - means with using GPUs allows us to handle up to 2 million records
with number of features up to 25. The gain in the computing time is in factor
5. We plan to increase this factor up to 20-30 after improving the algorithms.",0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.08372v2,An Evaluation of Coarse-Grained Locking for Multicore Microkernels,"The trade-off between coarse- and fine-grained locking is a well understood
issue in operating systems. Coarse-grained locking provides lower overhead
under low contention, fine-grained locking provides higher scalability under
contention, though at the expense of implementation complexity and re- duced
best-case performance.
  We revisit this trade-off in the context of microkernels and tightly-coupled
cores with shared caches and low inter-core migration latencies. We evaluate
performance on two architectures: x86 and ARM MPCore, in the former case also
utilising transactional memory (Intel TSX). Our thesis is that on such
hardware, a well-designed microkernel, with short system calls, can take
advantage of coarse-grained locking on modern hardware, avoid the run-time and
complexity cost of multiple locks, enable formal verification, and still
achieve scalability comparable to fine-grained locking.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1408.3048v2,"Accelerating Cosmic Microwave Background map-making procedure through
  preconditioning","Estimation of the sky signal from sequences of time ordered data is one of
the key steps in Cosmic Microwave Background (CMB) data analysis, commonly
referred to as the map-making problem. Some of the most popular and general
methods proposed for this problem involve solving generalised least squares
(GLS) equations with non-diagonal noise weights given by a block-diagonal
matrix with Toeplitz blocks. In this work we study new map-making solvers
potentially suitable for applications to the largest anticipated data sets.
They are based on iterative conjugate gradient (CG) approaches enhanced with
novel, parallel, two-level preconditioners. We apply the proposed solvers to
examples of simulated non-polarised and polarised CMB observations, and a set
of idealised scanning strategies with sky coverage ranging from nearly a full
sky down to small sky patches. We discuss in detail their implementation for
massively parallel computational platforms and their performance for a broad
range of parameters characterising the simulated data sets. We find that our
best new solver can outperform carefully-optimised standard solvers used today
by a factor of as much as 5 in terms of the convergence rate and a factor of up
to $4$ in terms of the time to solution, and to do so without significantly
increasing the memory consumption and the volume of inter-processor
communication. The performance of the new algorithms is also found to be more
stable and robust, and less dependent on specific characteristics of the
analysed data set. We therefore conclude that the proposed approaches are well
suited to address successfully challenges posed by new and forthcoming CMB data
sets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1211.4123v1,Interaction-Oriented Software Engineering: Concepts and Principles,"Following established tradition, software engineering today is rooted in a
conceptually centralized way of thinking. The primary SE artifact is a
specification of a machine -- a computational artifact -- that would meet the
(elicited and) stated requirements. Therein lies a fundamental mismatch with
(open) sociotechnical systems, which involve multiple autonomous social
participants or principals who interact with each other to further their
individual goals. No central machine governs the behaviors of the various
principals.
  We introduce Interaction-Oriented Software Engineering (IOSE) as an approach
expressly suited to the needs of open sociotechnical systems. In IOSE,
specifying a system amounts to specifying the interactions among the principals
as protocols. IOSE reinterprets the classical software engineering principles
of modularity, abstraction, separation of concerns, and encapsulation in a
manner that accords with the realities of sociotechnical systems. To highlight
the novelty of IOSE, we show where well-known SE methodologies, especially
those that explicitly aim to address either sociotechnical systems or the
modeling of interactions among autonomous principals, fail to satisfy the IOSE
principles.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.07355v1,A chatbot architecture for promoting youth resilience,"E-health technologies have the potential to provide scalable and accessible
interventions for youth mental health. As part of a developing an ecosystem of
e-screening and e-therapy tools for New Zealand young people, a dialog agent,
Headstrong, has been designed to promote resilience with methods grounded in
cognitive behavioral therapy and positive psychology. This paper describes the
architecture underlying the chatbot. The architecture supports a range of over
20 activities delivered in a 4-week program by relatable personas. The
architecture provides a visual authoring interface to its content management
system. In addition to supporting the original adolescent resilience chatbot,
the architecture has been reused to create a 3-week 'stress-detox' intervention
for undergraduates, and subsequently for a chatbot to support young people with
the impacts of the COVID-19 pandemic, with all three systems having been used
in field trials. The Headstrong architecture illustrates the feasibility of
creating a domain-focused authoring environment in the context of e-therapy
that supports non-technical expert input and rapid deployment.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.13081v1,The Dark Side of Unikernels for Machine Learning,"This paper analyzes the shortcomings of unikernels as a method of deployment
for machine learning inferencing applications as well as provides insights and
analysis on future work in this space. The findings of this paper advocate for
a tool to enable management of dependent libraries in a unikernel to enable a
more ergonomic build process as well as take advantage of the inherent security
and performance benefits of unikernels.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1210.8024v2,Emergent Spiking in Non-Ideal Memristor Networks,"Memristors have uses as artificial synapses and perform well in this role in
simulations with artificial spiking neurons. Our experiments show that
memristor networks natively spike and can exhibit emergent oscillations and
bursting spikes. Networks of near-ideal memristors exhibit behaviour similar to
a single memristor and combine in circuits like resistors do. Spiking is more
likely when filamentary memristors are used or the circuits have a higher
degree of compositional complexity (i.e. a larger number of anti-series or
anti-parallel interactions). 3-memristor circuits with the same memristor
polarity (low compositional complexity) are stabilised and do not show spiking
behaviour. 3-memristor circuits with anti-series and/or anti-parallel
compositions show richer and more complex dynamics than 2-memristor spiking
circuits. We show that the complexity of these dynamics can be quantified by
calculating (using partial auto-correlation functions) the minimum order
auto-regression function that could fit it. We propose that these oscillations
and spikes may be similar phenomena to brainwaves and neural spike trains and
suggest that these behaviours can be used to perform neuromorphic computation.",0,0,0,1,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1205.1171v1,Divide-and-Conquer 3D Convex Hulls on the GPU,"We describe a pure divide-and-conquer parallel algorithm for computing 3D
convex hulls. We implement that algorithm on GPU hardware, and find a
significant speedup over comparable CPU implementations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.09326v1,"Cinderella's shoe won't fit Soundarya: An audit of facial processing
  tools on Indian faces","The increasing adoption of facial processing systems in India is fraught with
concerns of privacy, transparency, accountability, and missing procedural
safeguards. At the same time, we also know very little about how these
technologies perform on the diverse features, characteristics, and skin tones
of India's 1.34 billion-plus population. In this paper, we test the face
detection and facial analysis functions of four commercial facial processing
tools on a dataset of Indian faces. The tools display varying error rates in
the face detection and gender and age classification functions. The gender
classification error rate for Indian female faces is consistently higher
compared to that of males -- the highest female error rate being 14.68%. In
some cases, this error rate is much higher than that shown by previous studies
for females of other nationalities. Age classification errors are also high.
Despite taking into account an acceptable error margin of plus or minus 10
years from a person's actual age, age prediction failures are in the range of
14.3% to 42.2%. These findings point to the limited accuracy of facial
processing tools, particularly for certain demographic groups, and the need for
more critical thinking before adopting such systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
http://arxiv.org/abs/1408.0510v1,"A note on ""The Need for End-to-End Evaluation of Cloud Availability""","Cloud availability is a major performance parameter for cloud platforms, but
there are very few measurements on commercial platforms, and most of them rely
on outage reports as appeared on specialized sites, providers' dashboards, or
the general press. A paper recently presented at the PAM 2014 conference by Hu
et alii reports the results of a measurement campaign. In this note, the
results of that paper are summarized, highlighting sources of inaccuracy and
some possible improvements. In particular, the use of a low probing frequency
could lead to non detection of short outages, as well as to an inaccurate
estimation of the outage duration statistics. Overcoming this lack of accuracy
is relevant to properly assess SLA violations and establish the basis for
insurance claims.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1107.3593v1,Privacy-Enhanced Methods for Comparing Compressed DNA Sequences,"In this paper, we study methods for improving the efficiency and privacy of
compressed DNA sequence comparison computations, under various querying
scenarios. For instance, one scenario involves a querier, Bob, who wants to
test if his DNA string, $Q$, is close to a DNA string, $Y$, owned by a data
owner, Alice, but Bob does not want to reveal $Q$ to Alice and Alice is willing
to reveal $Y$ to Bob \emph{only if} it is close to $Q$. We describe a
privacy-enhanced method for comparing two compressed DNA sequences, which can
be used to achieve the goals of such a scenario. Our method involves a
reduction to set differencing, and we describe a privacy-enhanced protocol for
set differencing that achieves absolute privacy for Bob (in the information
theoretic sense), and a quantifiable degree of privacy protection for Alice.
One of the important features of our protocols, which makes them ideally suited
to privacy-enhanced DNA sequence comparison problems, is that the communication
complexity of our solutions is proportional to a threshold that bounds the
cardinality of the set differences that are of interest, rather than the
cardinality of the sets involved (which correlates to the length of the DNA
sequences). Moreover, in our protocols, the querier, Bob, can easily compute
the set difference only if its cardinality is close to or below a specified
threshold.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2111.07904v2,"Training Data Reduction for Performance Models of Data Analytics Jobs in
  the Cloud","Distributed dataflow systems like Apache Flink and Apache Spark simplify
processing large amounts of data on clusters in a data-parallel manner.
However, choosing suitable cluster resources for distributed dataflow jobs in
both type and number is difficult, especially for users who do not have access
to previous performance metrics. One approach to overcoming this issue is to
have users share runtime metrics to train context-aware performance models that
help find a suitable configuration for the job at hand. A problem when sharing
runtime data instead of trained models or model parameters is that the data
size can grow substantially over time.
  This paper examines several clustering techniques to minimize training data
size while keeping the associated performance models accurate. Our results
indicate that efficiency gains in data transfer, storage, and model training
can be achieved through training data reduction. In the evaluation of our
solution on a dataset of runtime data from 930 unique distributed dataflow
jobs, we observed that, on average, a 75% data reduction only increases
prediction errors by one percentage point.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1506.00717v1,"Assessing Efficiency-Effectiveness Tradeoffs in Multi-Stage Retrieval
  Systems Without Using Relevance Judgments","Large-scale retrieval systems are often implemented as a cascading sequence
of phases -- a first filtering step, in which a large set of candidate
documents are extracted using a simple technique such as Boolean matching
and/or static document scores; and then one or more ranking steps, in which the
pool of documents retrieved by the filter is scored more precisely using dozens
or perhaps hundreds of different features. The documents returned to the user
are then taken from the head of the final ranked list. Here we examine methods
for measuring the quality of filtering and preliminary ranking stages, and show
how to use these measurements to tune the overall performance of the system.
Standard top-weighted metrics used for overall system evaluation are not
appropriate for assessing filtering stages, since the output is a set of
documents, rather than an ordered sequence of documents. Instead, we use an
approach in which a quality score is computed based on the discrepancy between
filtered and full evaluation. Unlike previous approaches, our methods do not
require relevance judgments, and thus can be used with virtually any query set.
We show that this quality score directly correlates with actual differences in
measured effectiveness when relevance judgments are available. Since the
quality score does not require relevance judgments, it can be used to identify
queries that perform particularly poorly for a given filter. Using these
methods, we explore a wide range of filtering options using thousands of
queries, categorize the relative merits of the different approaches, and
identify useful parameter combinations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.3612v2,Global disease monitoring and forecasting with Wikipedia,"Infectious disease is a leading threat to public health, economic stability,
and other key social structures. Efforts to mitigate these impacts depend on
accurate and timely monitoring to measure the risk and progress of disease.
Traditional, biologically-focused monitoring techniques are accurate but costly
and slow; in response, new techniques based on social internet data such as
social media and search queries are emerging. These efforts are promising, but
important challenges in the areas of scientific peer review, breadth of
diseases and countries, and forecasting hamper their operational usefulness.
  We examine a freely available, open data source for this use: access logs
from the online encyclopedia Wikipedia. Using linear models, language as a
proxy for location, and a systematic yet simple article selection procedure, we
tested 14 location-disease combinations and demonstrate that these data
feasibly support an approach that overcomes these challenges. Specifically, our
proof-of-concept yields models with $r^2$ up to 0.92, forecasting value up to
the 28 days tested, and several pairs of models similar enough to suggest that
transferring models from one location to another without re-training is
feasible.
  Based on these preliminary results, we close with a research agenda designed
to overcome these challenges and produce a disease monitoring and forecasting
system that is significantly more effective, robust, and globally comprehensive
than the current state of the art.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0110067v1,Analysis of Investment Policy in Belarus,"The optimal planning trajectory is analyzed on the basis of the growth model
with effectiveness. The saving per capital value has to be rather high
initially with smooth decrement in the future years.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1308.6149v1,The Extreme Right Filter Bubble,"Due to its status as the most popular video sharing platform, YouTube plays
an important role in the online strategy of extreme right groups, where it is
often used to host associated content such as music and other propaganda. In
this paper, we develop a categorization suitable for the analysis of extreme
right channels found on YouTube. By combining this with an NMF-based topic
modelling method, we categorize channels originating from links propagated by
extreme right Twitter accounts. This method is also used to categorize related
channels, which are determined using results returned by YouTube's related
video service. We identify the existence of a ""filter bubble"", whereby users
who access an extreme right YouTube video are highly likely to be recommended
further extreme right content.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.11020v2,Efficient reductions and algorithms for variants of Subset Sum,"Given $(a_1, \dots, a_n, t) \in \mathbb{Z}_{\geq 0}^{n + 1}$, the Subset Sum
problem ($\mathsf{SSUM}$) is to decide whether there exists $S \subseteq [n]$
such that $\sum_{i \in S} a_i = t$. There is a close variant of the
$\mathsf{SSUM}$, called $\mathsf{Subset~Product}$. Given positive integers
$a_1, ..., a_n$ and a target integer $t$, the $\mathsf{Subset~Product}$ problem
asks to determine whether there exists a subset $S \subseteq [n]$ such that
$\prod_{i \in S} a_i=t$. There is a pseudopolynomial time dynamic programming
algorithm, due to Bellman (1957) which solves the $\mathsf{SSUM}$ and
$\mathsf{Subset~Product}$ in $O(nt)$ time and $O(t)$ space.
  In the first part, we present {\em search} algorithms for variants of the
Subset Sum problem. Our algorithms are parameterized by $k$, which is a given
upper bound on the number of realisable sets (i.e.,~number of solutions,
summing exactly $t$). We show that $\mathsf{SSUM}$ with a unique solution is
already NP-hard, under randomized reduction. This makes the regime of
parametrized algorithms, in terms of $k$, very interesting.
  Subsequently, we present an $\tilde{O}(k\cdot (n+t))$ time deterministic
algorithm, which finds the hamming weight of all the realisable sets for a
subset sum instance. We also give a poly$(knt)$-time and $O(\log(knt))$-space
deterministic algorithm that finds all the realisable sets for a subset sum
instance.
  In the latter part, we present a simple and elegant randomized $\tilde{O}(n +
t)$ time algorithm for $\mathsf{Subset~Product}$. Moreover, we also present a
poly$(nt)$ time and $O(\log^2 (nt))$ space deterministic algorithm for the
same. We study these problems in the unbounded setting as well. Our algorithms
use multivariate FFT, power series and number-theoretic techniques, introduced
by Jin and Wu (SOSA'19) and Kane (2010).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1811.01774v1,"SCAV'18: Report of the 2nd International Workshop on Safe Control of
  Autonomous Vehicles","This report summarizes the discussions, open issues, take-away messages, and
conclusions of the 2nd SCAV workshop.",0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1203.6102v1,A Programmer-Centric Approach to Program Verification in ATS,"Formal specification is widely employed in the construction of high-quality
software. However, there is often a huge gap between formal specification and
actual implementation. While there is already a vast body of work on software
testing and verification, the task to ensure that an implementation indeed
meets its specification is still undeniably of great difficulty. ATS is a
programming language equipped with a highly expressive type system that allows
the programmer to specify and implement and then verify within the language
itself that an implementation meets its specification. In this paper, we
present largely through examples a programmer-centric style of program
verification that puts emphasis on requesting the programmer to explain in a
literate fashion why his or her code works. This is a solid step in the pursuit
of software construction that is verifiably correct according to specification.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.2449v1,RTChoke: Efficient Real-Time Traffic Chokepoint Detection and Monitoring,"We present a novel efficient adaptive sensing and monitoring solution for a
system of mobile sensing devices that support traffic monitoring applications.
We make a key observation that much of the variance in commute times arises at
a few congestion hotspots, and a reliable estimate of congestion can be
obtained by selectively monitoring congestion just at these hotspots. We design
a smartphone application and a backend system that automatically identifies and
monitors congestion hotspots. The solution has low resource footprint in terms
of both battery usage on the sensing devices and the network bytes used for
uploading data. When a user is not inside any hotspot zone, adaptive sampling
conserves battery power and reduces network usage, while ensuring that any new
hotspots can be effectively identified. Our results show that our application
consumes 40- 80% less energy than a periodic sampling system for different
routes in our experiments, with similar accuracy of congestion information. The
system can be used for a variety of applications such as automatic congestion
alerts to users approaching hotspots, reliable end-to-end commute time
estimates and effective alternate route suggestions.",0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1807.03479v1,Efficient Reassembling of Three-Regular Planar Graphs,"A reassembling of a simple graph G = (V,E) is an abstraction of a problem
arising in earlier studies of network analysis. There are several equivalent
definitions of graph reassembling; in this report we use a definition which
makes it closest to the notion of graph carving. A reassembling is a rooted
binary tree whose nodes are subsets of V and whose leaf nodes are singleton
sets, with each of the latter containing a distinct vertex of G. The parent of
two nodes in the reassembling is the union of the two children's vertex sets.
The root node of the reassembling is the full set V. The edge-boundary degree
of a node in the reassembling is the number of edges in G that connect vertices
in the node's set to vertices not in the node's set. A reassembling's
alpha-measure is the largest edge-boundary degree of any node in the
reassembling. A reassembling of G is alpha-optimal if its alpha-measure is the
minimum among all alpha-measures of G's reassemblings.
  The problem of finding an alpha-optimal reassembling of a simple graph in
general was already shown to be NP-hard.
  In this report we present an algorithm which, given a 3-regular plane graph G
= (V,E) as input, returns a reassembling of G with an alpha-measure independent
of n (number of vertices in G) and upper-bounded by 2k, where k is the
edge-outerplanarity of G. (Edge-outerplanarity is distinct but closely related
to the usual notion of outerplanarity; as with outerplanarity, for a fixed
edge-outerplanarity k, the number n of vertices can be arbitrarily large.) Our
algorithm runs in time linear in n. Moreover, we construct a class of
$3$-regular plane graphs for which this alpha-measure is optimal, by proving
that 2k is the lower bound on the alpha-measure of any reassembling of a graph
in that class.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1406.0333v1,"Collection of abstracts of the Workshop on Triangulations in Geometry
  and Topology at CG Week 2014 in Kyoto","This workshop about triangulations of manifolds in computational geometry and
topology was held at the 2014 CG-Week in Kyoto, Japan.
  It focussed on computational and combinatorial questions regarding
triangulations, with the goal of bringing together researchers working on
various aspects of triangulations and of fostering a closer collaboration
within the computational geometry and topology community.
  Triangulations are highly suitable for computations due to their clear
combinatorial structure. As a consequence, they have been successfully employed
in discrete algorithms to solve purely theoretical problems in a broad variety
of mathematical research areas (knot theory, polytope theory, 2- and 3-manifold
topology, geometry, and others). However, due to the large variety of
applications, requirements vary from field to field and thus different types of
triangulations, different tools, and different frameworks are used in different
areas of research. This is why today closely related research areas are
sometimes largely disjoint leaving potential reciprocal benefits unused.
  To address these potentials a workshop on Triangulations was held at
Oberwolfach Research Institute in 2012. Since then many new collaborations
between researchers of different mathematical communities have been
established. Regarding the computational geometry community, the theory of
manifolds continues to contribute to advances in more applied areas of the
field. Many researchers are interested in fundamental mathematical research
about triangulations and thus will benefit from a broad set of knowledge about
different research areas using different techniques.
  We hope that this workshop brought together researchers from many different
fields of computational geometry to have fruitful discussions which will lead
to new interdisciplinary collaborations and solutions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1811.09702v1,"Homogeneity-Based Transmissive Process to Model True and False News in
  Social Networks","An overwhelming number of true and false news stories are posted and shared
in social networks, and users diffuse the stories based on multiple factors.
Diffusion of news stories from one user to another depends not only on the
stories' content and the genuineness but also on the alignment of the topical
interests between the users. In this paper, we propose a novel Bayesian
nonparametric model that incorporates homogeneity of news stories as the key
component that regulates the topical similarity between the posting and sharing
users' topical interests. Our model extends hierarchical Dirichlet process to
model the topics of the news stories and incorporates Bayesian Gaussian process
latent variable model to discover the homogeneity values. We train our model on
a real-world social network dataset and find homogeneity values of news stories
that strongly relate to their labels of genuineness and their contents.
Finally, we show that the supervised version of our model predicts the labels
of news stories better than the state-of-the-art neural network and Bayesian
models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2109.09709v1,Information Dynamics and The Arrow of Time,"Time appears to pass irreversibly. In light of CPT symmetry, the Universe's
initial condition is thought to be somehow responsible. We propose a model, the
stochastic partitioned cellular automaton (SPCA), in which to study the
mechanisms and consequences of emergent irreversibility. While their most
natural definition is probabilistic, we show that SPCA dynamics can be made
deterministic and reversible, by attaching randomly initialized degrees of
freedom. This property motivates analogies to classical field theories. We
develop the foundations of non-equilibrium statistical mechanics on SPCAs. Of
particular interest are the second law of thermodynamics, and a mutual
information law which proves fundamental in non-equilibrium settings. We
believe that studying the dynamics of information on SPCAs will yield insights
on foundational topics in computer engineering, the sciences, and the
philosophy of mind. As evidence of this, we discuss several such applications,
including an extension of Landauer's principle, and sketch a physical
justification of the causal decision theory that underlies the so-called
psychological arrow of time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.06355v1,"Optimizing FPGA-based Accelerator Design for Large-Scale Molecular
  Similarity Search","Molecular similarity search has been widely used in drug discovery to
identify structurally similar compounds from large molecular databases rapidly.
With the increasing size of chemical libraries, there is growing interest in
the efficient acceleration of large-scale similarity search. Existing works
mainly focus on CPU and GPU to accelerate the computation of the Tanimoto
coefficient in measuring the pairwise similarity between different molecular
fingerprints. In this paper, we propose and optimize an FPGA-based accelerator
design on exhaustive and approximate search algorithms. On exhaustive search
using BitBound & folding, we analyze the similarity cutoff and folding level
relationship with search speedup and accuracy, and propose a scalable
on-the-fly query engine on FPGAs to reduce the resource utilization and
pipeline interval. We achieve a 450 million compounds-per-second processing
throughput for a single query engine. On approximate search using hierarchical
navigable small world (HNSW), a popular algorithm with high recall and query
speed. We propose an FPGA-based graph traversal engine to utilize a high
throughput register array based priority queue and fine-grained distance
calculation engine to increase the processing capability. Experimental results
show that the proposed FPGA-based HNSW implementation has a 103385 query per
second (QPS) on the Chembl database with 0.92 recall and achieves a 35x speedup
than the existing CPU implementation on average. To the best of our knowledge,
our FPGA-based implementation is the first attempt to accelerate molecular
similarity search algorithms on FPGA and has the highest performance among
existing approaches.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1703.07562v1,Snafu: Function-as-a-Service (FaaS) Runtime Design and Implementation,"Snafu, or Snake Functions, is a modular system to host, execute and manage
language-level functions offered as stateless (micro-)services to diverse
external triggers. The system interfaces resemble those of commercial FaaS
providers but its implementation provides distinct features which make it
overall useful to research on FaaS and prototyping of FaaS-based applications.
This paper argues about the system motivation in the presence of already
existing alternatives, its design and architecture, the open source
implementation and collected metrics which characterise the system.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0101003v1,"Signal-Theoretic Characterization of Waveguide Mesh Geometries for
  Models of Two--Dimensional Wave Propagation in Elastic Media","Waveguide Meshes are efficient and versatile models of wave propagation along
a multidimensional ideal medium. The choice of the mesh geometry affects both
the computational cost and the accuracy of simulations. In this paper, we focus
on 2D geometries and use multidimensional sampling theory to compare the
square, triangular, and hexagonal meshes in terms of sampling efficiency and
dispersion error under conditions of critical sampling. The analysis shows that
the triangular geometry exhibits the most desirable tradeoff between accuracy
and computational cost.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.09561v1,Normalized Attention Without Probability Cage,"Attention architectures are widely used; they recently gained renewed
popularity with Transformers yielding a streak of state of the art results.
Yet, the geometrical implications of softmax-attention remain largely
unexplored. In this work we highlight the limitations of constraining attention
weights to the probability simplex and the resulting convex hull of value
vectors. We show that Transformers are sequence length dependent biased towards
token isolation at initialization and contrast Transformers to simple max- and
sum-pooling - two strong baselines rarely reported. We propose to replace the
softmax in self-attention with normalization, yielding a hyperparameter and
data-bias robust, generally applicable architecture. We support our insights
with empirical results from more than 25,000 trained models. All results and
implementations are made available.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.00393v1,"Exploring Choice Overload in Related-Article Recommendations in Digital
  Libraries","We investigate the problem of choice overload - the difficulty of making a
decision when faced with many options - when displaying related-article
recommendations in digital libraries. So far, research regarding to how many
items should be displayed has mostly been done in the fields of media
recommendations and search engines. We analyze the number of recommendations in
current digital libraries. When browsing fullscreen with a laptop or desktop
PC, all display a fixed number of recommendations. 72% display three, four, or
five recommendations, none display more than ten. We provide results from an
empirical evaluation conducted with GESIS' digital library Sowiport, with
recommendations delivered by recommendations-as-a-service provider Mr. DLib. We
use click-through rate as a measure of recommendation effectiveness based on
3.4 million delivered recommendations. Our results show lower click-through
rates for higher numbers of recommendations and twice as many clicked
recommendations when displaying ten instead of one related-articles. Our
results indicate that users might quickly feel overloaded by choice.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1908.10410v3,"Visualization of Very Large High-Dimensional Data Sets as Minimum
  Spanning Trees","The chemical sciences are producing an unprecedented amount of large,
high-dimensional data sets containing chemical structures and associated
properties. However, there are currently no algorithms to visualize such data
while preserving both global and local features with a sufficient level of
detail to allow for human inspection and interpretation. Here, we propose a
solution to this problem with a new data visualization method, TMAP, capable of
representing data sets of up to millions of data points and arbitrary high
dimensionality as a two-dimensional tree (http://tmap.gdb.tools).
Visualizations based on TMAP are better suited than t-SNE or UMAP for the
exploration and interpretation of large data sets due to their tree-like
nature, increased local and global neighborhood and structure preservation, and
the transparency of the methods the algorithm is based on. We apply TMAP to the
most used chemistry data sets including databases of molecules such as ChEMBL,
FDB17, the Natural Products Atlas, DSSTox, as well as to the MoleculeNet
benchmark collection of data sets. We also show its broad applicability with
further examples from biology, particle physics, and literature.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1406.1701v1,"A computational study of the effects of remodelled electrophysiology and
  mechanics on initiation of ventricular fibrillation in human heart failure","The study of pathological cardiac conditions such as arrhythmias, a major
cause of mortality in heart failure, is becoming increasingly informed by
computational simulation, numerically modelling the governing equations. This
can provide insight where experimental work is constrained by technical
limitations and/or ethical issues.
  As the models become more realistic, the construction of efficient and
accurate computational models becomes increasingly challenging. In particular,
recent developments have started to couple the electrophysiology models with
mechanical models in order to investigate the effect of tissue deformation on
arrhythmogenesis, thus introducing an element of nonlinearity into the
mathematical representation. This paper outlines a biophysically-detailed
computational model of coupled electromechanical cardiac activity which uses
the finite element method to approximate both electrical and mechanical systems
on unstructured, deforming, meshes. An ILU preconditioner is applied to improve
performance of the solver.
  This software is used to examine the role of electrophysiology, fibrosis and
mechanical deformation on the stability of spiral wave dynamics in human
ventricular tissue by applying it to models of both healthy and failing tissue.
The latter was simulated by modifying (i) cellular electrophysiological
properties, to generate an increased action potential duration and altered
intracellular calcium handling, and (ii) tissue-level properties, to simulate
the gap junction remodelling, fibrosis and increased tissue stiffness seen in
heart failure. The resulting numerical experiments suggest that, for the chosen
mathematical models of electrophysiology and mechanical response, introducing
tissue level fibrosis can have a destabilising effect on the dynamics, while
the net effect of the electrophysiological remodelling stabilises the system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0704.1783v3,Unicast and Multicast Qos Routing with Soft Constraint Logic Programming,"We present a formal model to represent and solve the unicast/multicast
routing problem in networks with Quality of Service (QoS) requirements. To
attain this, first we translate the network adapting it to a weighted graph
(unicast) or and-or graph (multicast), where the weight on a connector
corresponds to the multidimensional cost of sending a packet on the related
network link: each component of the weights vector represents a different QoS
metric value (e.g. bandwidth, cost, delay, packet loss). The second step
consists in writing this graph as a program in Soft Constraint Logic
Programming (SCLP): the engine of this framework is then able to find the best
paths/trees by optimizing their costs and solving the constraints imposed on
them (e.g. delay < 40msec), thus finding a solution to QoS routing problems.
Moreover, c-semiring structures are a convenient tool to model QoS metrics. At
last, we provide an implementation of the framework over scale-free networks
and we suggest how the performance can be improved.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1303.5800v1,Line-Constrained Geometric Server Placement,"In this paper we present new algorithmic solutions for several constrained
geometric server placement problems. We consider the problems of computing the
1-center and obnoxious 1-center of a set of line segments, constrained to lie
on a line segment, and the problem of computing the K-median of a set of
points, constrained to lie on a line. The presented algorithms have
applications in many types of distributed systems, as well as in various fields
which make use of distributed systems for running some of their applications
(like chemistry, metallurgy, physics, etc.).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1108.4142v3,Dynamic Pricing with Limited Supply,"We consider the problem of dynamic pricing with limited supply. A seller has
$k$ identical items for sale and is facing $n$ potential buyers (""agents"") that
are arriving sequentially. Each agent is interested in buying one item. Each
agent's value for an item is an IID sample from some fixed distribution with
support $[0,1]$. The seller offers a take-it-or-leave-it price to each arriving
agent (possibly different for different agents), and aims to maximize his
expected revenue.
  We focus on ""prior-independent"" mechanisms -- ones that do not use any
information about the distribution. They are desirable because knowing the
distribution is unrealistic in many practical scenarios. We study how the
revenue of such mechanisms compares to the revenue of the optimal offline
mechanism that knows the distribution (""offline benchmark"").
  We present a prior-independent dynamic pricing mechanism whose revenue is at
most $O((k \log n)^{2/3})$ less than the offline benchmark, for every
distribution that is regular. In fact, this guarantee holds without *any*
assumptions if the benchmark is relaxed to fixed-price mechanisms. Further, we
prove a matching lower bound. The performance guarantee for the same mechanism
can be improved to $O(\sqrt{k} \log n)$, with a distribution-dependent
constant, if $k/n$ is sufficiently small. We show that, in the worst case over
all demand distributions, this is essentially the best rate that can be
obtained with a distribution-specific constant.
  On a technical level, we exploit the connection to multi-armed bandits (MAB).
While dynamic pricing with unlimited supply can easily be seen as an MAB
problem, the intuition behind MAB approaches breaks when applied to the setting
with limited supply. Our high-level conceptual contribution is that even the
limited supply setting can be fruitfully treated as a bandit problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/cs/0404004v1,Dealing With Curious Players in Secure Networks,"In secure communications networks there are a great number of user
behavioural problems, which need to be dealt with. Curious players pose a very
real and serious threat to the integrity of such a network. By traversing a
network a Curious player could uncover secret information, which that user has
no need to know, by simply posing as a loyalty check. Loyalty checks are done
simply to gauge the integrity of the network with respect to players who act in
a malicious manner. We wish to propose a method, which can deal with Curious
players trying to obtain ""Need to Know"" information using a combined
Fault-tolerant, Cryptographic and Game Theoretic Approach.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.01285v2,"All Factors Should Matter! Reference Checklist for Describing Research
  Conditions in Pursuit of Comparable IVR Experiments","A significant problem with immersive virtual reality (IVR) experiments is the
ability to compare research conditions. VR kits and IVR environments are
complex and diverse but researchers from different fields, e.g. ICT,
psychology, or marketing, often neglect to describe them with a level of detail
sufficient to situate their research on the IVR landscape. Careful reporting of
these conditions may increase the applicability of research results and their
impact on the shared body of knowledge on HCI and IVR. Based on literature
review, our experience, practice and a synthesis of key IVR factors, in this
article we present a reference checklist for describing research conditions of
IVR experiments. Including these in publications will contribute to the
comparability of IVR research and help other researchers decide to what extent
reported results are relevant to their own research goals. The compiled
checklist is a ready-to-use reference tool and takes into account key hardware,
software and human factors as well as diverse factors connected to visual,
audio, tactile, and other aspects of interaction.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0104008v1,Event Indexing Systems for Efficient Selection and Analysis of HERA Data,"The design and implementation of two software systems introduced to improve
the efficiency of offline analysis of event data taken with the ZEUS Detector
at the HERA electron-proton collider at DESY are presented. Two different
approaches were made, one using a set of event directories and the other using
a tag database based on a commercial object-oriented database management
system. These are described and compared. Both systems provide quick direct
access to individual collision events in a sequential data store of several
terabytes, and they both considerably improve the event analysis efficiency. In
particular the tag database provides a very flexible selection mechanism and
can dramatically reduce the computing time needed to extract small subsamples
from the total event sample. Gains as large as a factor 20 have been obtained.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.08141v1,Minimum Fill-In: Inapproximability and Almost Tight Lower Bounds,"Given an $n*n$ sparse symmetric matrix with $m$ nonzero entries, performing
Gaussian elimination may turn some zeroes into nonzero values. To maintain the
matrix sparse, we would like to minimize the number $k$ of these changes, hence
called the minimum fill-in problem. Agrawal et al.~[FOCS'90] developed the
first approximation algorithm, based on early heuristics by George [SIAM J
Numer Anal 10] and by Lipton et al.~[SIAM J Numer Anal 16]. The objective
function they used is $m+k$, the number of nonzero elements after elimination.
An approximation algorithm using $k$ as the objective function was presented by
Natanzon et al.~[STOC'98]. These two versions are incomparable in terms of
approximation.
  Parameterized algorithms for the problem was first studied by Kaplan et
al.~[FOCS'94]. Fomin & Villanger [SODA'12] recently gave an algorithm running
in time $2^{O(\sqrt{k} \log k)}+n^{O(1)}$.
  Hardness results of this problem are surprisingly scarce, and the few known
ones are either weak or have to use nonstandard complexity conjectures. The
only inapproximability result by Wu et al.~[IJCAI'15] applies to only the
objective function $m+k$, and is grounded on the Small Set Expansion
Conjecture. The only nontrivial parameterized lower bounds, by Bliznets et
al.~[SODA'16], include a very weak one based on ETH, and a strong one based on
hardness of subexponential-time approximation of the minimum bisection problem
on regular graphs. For both versions of the problem, we exclude the existence
of PTASs, assuming P$\ne$NP, and the existence of $2^{O(n^{1-\delta})}$-time
approximation schemes for any positive $\delta$, assuming ETH. It also implies
a $2^{O(k^{1/2-\delta})} n^{O(1)}$ parameterized lower bound. Behind these
results is a new reduction from vertex cover, which might be of its own
interest: All previous reductions for similar problems are from some kind of
graph layout problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1202.4833v1,"Integrating DGSs and GATPs in an Adaptative and Collaborative
  Blended-Learning Web-Environment","The area of geometry with its very strong and appealing visual contents and
its also strong and appealing connection between the visual content and its
formal specification, is an area where computational tools can enhance, in a
significant way, the learning environments.
  The dynamic geometry software systems (DGSs) can be used to explore the
visual contents of geometry. This already mature tools allows an easy
construction of geometric figures build from free objects and elementary
constructions. The geometric automated theorem provers (GATPs) allows formal
deductive reasoning about geometric constructions, extending the reasoning via
concrete instances in a given model to formal deductive reasoning in a
geometric theory.
  An adaptative and collaborative blended-learning environment where the DGS
and GATP features could be fully explored would be, in our opinion a very rich
and challenging learning environment for teachers and students.
  In this text we will describe the Web Geometry Laboratory a Web environment
incorporating a DGS and a repository of geometric problems, that can be used in
a synchronous and asynchronous fashion and with some adaptative and
collaborative features.
  As future work we want to enhance the adaptative and collaborative aspects of
the environment and also to incorporate a GATP, constructing a dynamic and
individualised learning environment for geometry.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/2009.12153v2,A Systematic Review on Model Watermarking for Neural Networks,"Machine learning (ML) models are applied in an increasing variety of domains.
The availability of large amounts of data and computational resources
encourages the development of ever more complex and valuable models. These
models are considered intellectual property of the legitimate parties who have
trained them, which makes their protection against stealing, illegitimate
redistribution, and unauthorized application an urgent need. Digital
watermarking presents a strong mechanism for marking model ownership and,
thereby, offers protection against those threats. This work presents a taxonomy
identifying and analyzing different classes of watermarking schemes for ML
models. It introduces a unified threat model to allow structured reasoning on
and comparison of the effectiveness of watermarking methods in different
scenarios. Furthermore, it systematizes desired security requirements and
attacks against ML model watermarking. Based on that framework, representative
literature from the field is surveyed to illustrate the taxonomy. Finally,
shortcomings and general limitations of existing approaches are discussed, and
an outlook on future research directions is given.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1806.04973v1,OpenEDGAR: Open Source Software for SEC EDGAR Analysis,"OpenEDGAR is an open source Python framework designed to rapidly construct
research databases based on the Electronic Data Gathering, Analysis, and
Retrieval (EDGAR) system operated by the US Securities and Exchange Commission
(SEC). OpenEDGAR is built on the Django application framework, supports
distributed compute across one or more servers, and includes functionality to
(i) retrieve and parse index and filing data from EDGAR, (ii) build tables for
key metadata like form type and filer, (iii) retrieve, parse, and update CIK to
ticker and industry mappings, (iv) extract content and metadata from filing
documents, and (v) search filing document contents. OpenEDGAR is designed for
use in both academic research and industrial applications, and is distributed
under MIT License at https://github.com/LexPredict/openedgar.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1411.3967v2,Relatively Complete Counterexamples for Higher-Order Programs,"In this paper, we study the problem of generating inputs to a higher-order
program causing it to error. We first study the problem in the setting of PCF,
a typed, core functional language and contribute the first relatively complete
method for constructing counterexamples for PCF programs. The method is
relatively complete in the sense of Hoare logic; completeness is reduced to the
completeness of a first-order solver over the base types of PCF. In practice,
this means an SMT solver can be used for the effective, automated generation of
higher-order counterexamples for a large class of programs.
  We achieve this result by employing a novel form of symbolic execution for
higher-order programs. The remarkable aspect of this symbolic execution is that
even though symbolic higher-order inputs and values are considered, the path
condition remains a first-order formula. Our handling of symbolic function
application enables the reconstruction of higher-order counterexamples from
this first-order formula.
  After establishing our main theoretical results, we sketch how to apply the
approach to untyped, higher-order, stateful languages with first-class
contracts and show how counterexample generation can be used to detect contract
violations in this setting. To validate our approach, we implement a tool
generating counterexamples for erroneous modules written in Racket.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1308.1733v1,"Invitation to Ezhil: A Tamil Programming Language for Early
  Computer-Science Education","Ezhil is a Tamil programming language with support for imperative
programming, with mixed use of Tamil and English identifiers and
function-names. Ezhil programing system is targeted toward the K-12 (junior
high-school) level Tamil speaking students, as an early introduction to
thinking like a computer-scientist. We believe this 'numeracy' knowledge is
easily transferred over from a native language (Tamil) to the pervasive English
language programming systems, in Java, dot-Net, Ruby or Python. Ezhil is an
effort to improve access to computing in the 21st Century.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/cs/0306051v2,A data Grid testbed environment in Gigabit WAN with HPSS,"For data analysis of large-scale experiments such as LHC Atlas and other
Japanese high energy and nuclear physics projects, we have constructed a Grid
test bed at ICEPP and KEK. These institutes are connected to national
scientific gigabit network backbone called SuperSINET. In our test bed, we have
installed NorduGrid middleware based on Globus, and connected 120TB HPSS at KEK
as a large scale data store. Atlas simulation data at ICEPP has been
transferred and accessed using SuperSINET. We have tested various performances
and characteristics of HPSS through this high speed WAN. The measurement
includes comparison between computing and storage resources are tightly coupled
with low latency LAN and long distant WAN.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1207.3597v1,On Distributability of Petri Nets,"We formalise a general concept of distributed systems as sequential
components interacting asynchronously. We define a corresponding class of Petri
nets, called LSGA nets, and precisely characterise those system specifications
which can be implemented as LSGA nets up to branching ST-bisimilarity with
explicit divergence.",0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.10865v1,"On Specialization of a Program Model of Naive Pattern Matching in
  Strings (Extended Abstract)","We have proved that for any pattern p the tail recursive program model of
naive pattern matching may be automatically specialized w.r.t. the pattern p to
a specialized version of the so-called KMP-algorithm, using the Higman-Kruskal
relation that controls the unfolding/folding. Given an input string, the
corresponding residual program finds the first occurrence of p in the string in
linear time on the string length. The current state of the automated program
specialization art based on unfolding/folding is too weak in order to be able
to reproduce the proof, done by hands, of the uniform property above, while it
known before that program specialization is sometimes able to produce the
KMP-algorithm for a few concrete static patterns.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0211016v4,"Efficient Solving of Quantified Inequality Constraints over the Real
  Numbers","Let a quantified inequality constraint over the reals be a formula in the
first-order predicate language over the structure of the real numbers, where
the allowed predicate symbols are $\leq$ and $<$. Solving such constraints is
an undecidable problem when allowing function symbols such $\sin$ or $\cos$. In
the paper we give an algorithm that terminates with a solution for all, except
for very special, pathological inputs. We ensure the practical efficiency of
this algorithm by employing constraint programming techniques.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1401.6169v2,Parsimonious Topic Models with Salient Word Discovery,"We propose a parsimonious topic model for text corpora. In related models
such as Latent Dirichlet Allocation (LDA), all words are modeled
topic-specifically, even though many words occur with similar frequencies
across different topics. Our modeling determines salient words for each topic,
which have topic-specific probabilities, with the rest explained by a universal
shared model. Further, in LDA all topics are in principle present in every
document. By contrast our model gives sparse topic representation, determining
the (small) subset of relevant topics for each document. We derive a Bayesian
Information Criterion (BIC), balancing model complexity and goodness of fit.
Here, interestingly, we identify an effective sample size and corresponding
penalty specific to each parameter type in our model. We minimize BIC to
jointly determine our entire model -- the topic-specific words,
document-specific topics, all model parameter values, {\it and} the total
number of topics -- in a wholly unsupervised fashion. Results on three text
corpora and an image dataset show that our model achieves higher test set
likelihood and better agreement with ground-truth class labels, compared to LDA
and to a model designed to incorporate sparsity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1804.10520v1,Using Machine Learning to Improve Cylindrical Algebraic Decomposition,"Cylindrical Algebraic Decomposition (CAD) is a key tool in computational
algebraic geometry, best known as a procedure to enable Quantifier Elimination
over real-closed fields. However, it has a worst case complexity doubly
exponential in the size of the input, which is often encountered in practice.
It has been observed that for many problems a change in algorithm settings or
problem formulation can cause huge differences in runtime costs, changing
problem instances from intractable to easy. A number of heuristics have been
developed to help with such choices, but the complicated nature of the
geometric relationships involved means these are imperfect and can sometimes
make poor choices. We investigate the use of machine learning (specifically
support vector machines) to make such choices instead.
  Machine learning is the process of fitting a computer model to a complex
function based on properties learned from measured data. In this paper we apply
it in two case studies: the first to select between heuristics for choosing a
CAD variable ordering; the second to identify when a CAD problem instance would
benefit from Groebner Basis preconditioning. These appear to be the first such
applications of machine learning to Symbolic Computation. We demonstrate in
both cases that the machine learned choice outperforms human developed
heuristics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1804.11301v1,"Stochastic Shortest Paths and Weight-Bounded Properties in Markov
  Decision Processes","The paper deals with finite-state Markov decision processes (MDPs) with
integer weights assigned to each state-action pair. New algorithms are
presented to classify end components according to their limiting behavior with
respect to the accumulated weights. These algorithms are used to provide
solutions for two types of fundamental problems for integer-weighted MDPs.
First, a polynomial-time algorithm for the classical stochastic shortest path
problem is presented, generalizing known results for special classes of
weighted MDPs. Second, qualitative probability constraints for weight-bounded
(repeated) reachability conditions are addressed. Among others, it is shown
that the problem to decide whether a disjunction of weight-bounded reachability
conditions holds almost surely under some scheduler belongs to $\textrm{NP}\cap
\textrm{coNP}$, is solvable in pseudo-polynomial time and is at least as hard
as solving two-player mean-payoff games, while the corresponding problem for
universal quantification over schedulers is solvable in polynomial time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1511.05324v2,Understanding Human-Machine Networks: A Cross-Disciplinary Survey,"In the current hyper-connected era, modern Information and Communication
Technology systems form sophisticated networks where not only do people
interact with other people, but also machines take an increasingly visible and
participatory role. Such human-machine networks (HMNs) are embedded in the
daily lives of people, both for personal and professional use. They can have a
significant impact by producing synergy and innovations. The challenge in
designing successful HMNs is that they cannot be developed and implemented in
the same manner as networks of machines nodes alone, nor following a wholly
human-centric view of the network. The problem requires an interdisciplinary
approach. Here, we review current research of relevance to HMNs across many
disciplines. Extending the previous theoretical concepts of socio-technical
systems, actor-network theory, cyber-physical-social systems, and social
machines, we concentrate on the interactions among humans and between humans
and machines. We identify eight types of HMNs: public-resource computing,
crowdsourcing, web search engines, crowdsensing, online markets, social media,
multiplayer online games and virtual worlds, and mass collaboration. We
systematically select literature on each of these types and review it with a
focus on implications for designing HMNs. Moreover, we discuss risks associated
with HMNs and identify emerging design and development trends.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1010.3007v4,"From Low-Distortion Norm Embeddings to Explicit Uncertainty Relations
  and Efficient Information Locking","The existence of quantum uncertainty relations is the essential reason that
some classically impossible cryptographic primitives become possible when
quantum communication is allowed. One direct operational manifestation of these
uncertainty relations is a purely quantum effect referred to as information
locking. A locking scheme can be viewed as a cryptographic protocol in which a
uniformly random n-bit message is encoded in a quantum system using a classical
key of size much smaller than n. Without the key, no measurement of this
quantum state can extract more than a negligible amount of information about
the message, in which case the message is said to be ""locked"". Furthermore,
knowing the key, it is possible to recover, that is ""unlock"", the message. In
this paper, we make the following contributions by exploiting a connection
between uncertainty relations and low-distortion embeddings of L2 into L1. We
introduce the notion of metric uncertainty relations and connect it to
low-distortion embeddings of L2 into L1. A metric uncertainty relation also
implies an entropic uncertainty relation. We prove that random bases satisfy
uncertainty relations with a stronger definition and better parameters than
previously known. Our proof is also considerably simpler than earlier proofs.
We apply this result to show the existence of locking schemes with key size
independent of the message length. We give efficient constructions of metric
uncertainty relations. The bases defining these metric uncertainty relations
are computable by quantum circuits of almost linear size. This leads to the
first explicit construction of a strong information locking scheme. Moreover,
we present a locking scheme that is close to being implementable with current
technology. We apply our metric uncertainty relations to exhibit communication
protocols that perform quantum equality testing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.04298v3,"An Algorithm for Reversible Logic Circuit Synthesis Based on Tensor
  Decomposition","An algorithm for reversible logic synthesis is proposed. The task is, for a
given $n$-bit substitution map $P_n: \{0,1\}^n \rightarrow \{0,1\}^n$, to find
a sequence of reversible logic gates that implements the map. The gate library
adopted in this work consists of multiple-controlled Toffoli gates denoted by
$C^m\!X$, where $m$ is the number of control bits that ranges from 0 to $n-1$.
Controlled gates with large $m \,\,(>2)$ are then further decomposed into
$C^0\!X$, $C^1\!X$, and $C^2\!X$ gates. A primary concern in designing the
algorithm is to reduce the use of $C^2\!X$ gate (also known as Toffoli gate)
which is known to be universal.
  The main idea is to view an $n$-bit substitution map as a rank-$2n$ tensor
and to transform it such that the resulting map can be written as a tensor
product of a rank-($2n-2$) tensor and the $2\times 2$ identity matrix. Let
$\mathcal{P}_n$ be a set of all $n$-bit substitution maps. What we try to find
is a size reduction map $\mathcal{A}_{\rm red}: \mathcal{P}_n \rightarrow
\{P_n: P_n = P_{n-1} \otimes I_2\}$. %, where $I_m$ is the $m\times m$ identity
matrix. One can see that the output $P_{n-1} \otimes I_2$ acts nontrivially on
$n-1$ bits only, meaning that the map to be synthesized becomes $P_{n-1}$. The
size reduction process is iteratively applied until it reaches tensor product
of only $2 \times 2$ matrices.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1107.3119v2,Experimenting with Transitive Verbs in a DisCoCat,"Formal and distributional semantic models offer complementary benefits in
modeling meaning. The categorical compositional distributional (DisCoCat) model
of meaning of Coecke et al. (arXiv:1003.4394v1 [cs.CL]) combines aspected of
both to provide a general framework in which meanings of words, obtained
distributionally, are composed using methods from the logical setting to form
sentence meaning. Concrete consequences of this general abstract setting and
applications to empirical data are under active study (Grefenstette et al.,
arxiv:1101.0309; Grefenstette and Sadrzadeh, arXiv:1106.4058v1 [cs.CL]). . In
this paper, we extend this study by examining transitive verbs, represented as
matrices in a DisCoCat. We discuss three ways of constructing such matrices,
and evaluate each method in a disambiguation task developed by Grefenstette and
Sadrzadeh (arXiv:1106.4058v1 [cs.CL]).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.06255v6,"Correlation Filters for Unmanned Aerial Vehicle-Based Aerial Tracking: A
  Review and Experimental Evaluation","Aerial tracking, which has exhibited its omnipresent dedication and splendid
performance, is one of the most active applications in the remote sensing
field. Especially, unmanned aerial vehicle (UAV)-based remote sensing system,
equipped with a visual tracking approach, has been widely used in aviation,
navigation, agriculture,transportation, and public security, etc. As is
mentioned above, the UAV-based aerial tracking platform has been gradually
developed from research to practical application stage, reaching one of the
main aerial remote sensing technologies in the future. However, due to the
real-world onerous situations, e.g., harsh external challenges, the vibration
of the UAV mechanical structure (especially under strong wind conditions), the
maneuvering flight in complex environment, and the limited computation
resources onboard, accuracy, robustness, and high efficiency are all crucial
for the onboard tracking methods. Recently, the discriminative correlation
filter (DCF)-based trackers have stood out for their high computational
efficiency and appealing robustness on a single CPU, and have flourished in the
UAV visual tracking community. In this work, the basic framework of the
DCF-based trackers is firstly generalized, based on which, 23 state-of-the-art
DCF-based trackers are orderly summarized according to their innovations for
solving various issues. Besides, exhaustive and quantitative experiments have
been extended on various prevailing UAV tracking benchmarks, i.e., UAV123,
UAV123@10fps, UAV20L, UAVDT, DTB70, and VisDrone2019-SOT, which contain 371,903
frames in total. The experiments show the performance, verify the feasibility,
and demonstrate the current challenges of DCF-based trackers onboard UAV
tracking.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1011.3852v1,iCare: A Mobile Health Monitoring System for the Elderly,"This paper describes a mobile health monitoring system called iCare for the
elderly. We use wireless body sensors and smart phones to monitor the wellbeing
of the elderly. It can offer remote monitoring for the elderly anytime anywhere
and provide tailored services for each person based on their personal health
condition. When detecting an emergency, the smart phone will automatically
alert pre-assigned people who could be the old people's family and friends, and
call the ambulance of the emergency centre. It also acts as the personal health
information system and the medical guidance which offers one communication
platform and the medical knowledge database so that the family and friends of
the served people can cooperate with doctors to take care of him/her. The
system also features some unique functions that cater to the living demands of
the elderly, including regular reminder, quick alarm, medical guidance, etc.
iCare is not only a real-time health monitoring system for the elderly, but
also a living assistant which can make their lives more convenient and
comfortable.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1809.02665v1,"DreamNLP: Novel NLP System for Clinical Report Metadata Extraction using
  Count Sketch Data Streaming Algorithm: Preliminary Results","Extracting information from electronic health records (EHR) is a challenging
task since it requires prior knowledge of the reports and some natural language
processing algorithm (NLP). With the growing number of EHR implementations,
such knowledge is increasingly challenging to obtain in an efficient manner. We
address this challenge by proposing a novel methodology to analyze large sets
of EHRs using a modified Count Sketch data streaming algorithm termed DreamNLP.
By using DreamNLP, we generate a dictionary of frequently occurring terms or
heavy hitters in the EHRs using low computational memory compared to
conventional counting approach other NLP programs use. We demonstrate the
extraction of the most important breast diagnosis features from the EHRs in a
set of patients that underwent breast imaging. Based on the analysis,
extraction of these terms would be useful for defining important features for
downstream tasks such as machine learning for precision medicine.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1901.01970v2,Decision-making and Fuzzy Temporal Logic,"This paper shows that the fuzzy temporal logic can model figures of thought
to describe decision-making behaviors. In order to exemplify, some economic
behaviors observed experimentally were modeled from problems of choice
containing time, uncertainty and fuzziness. Related to time preference, it is
noted that the subadditive discounting is mandatory in positive rewards
situations and, consequently, results in the magnitude effect and time effect,
where the last has a stronger discounting for earlier delay periods (as in, one
hour, one day), but a weaker discounting for longer delay periods (for
instance, six months, one year, ten years). In addition, it is possible to
explain the preference reversal (change of preference when two rewards proposed
on different dates are shifted in the time). Related to the Prospect Theory, it
is shown that the risk seeking and the risk aversion are magnitude dependents,
where the risk seeking may disappear when the values to be lost are very high.",0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0504106v1,"A Distributed Multimedia Communication System and its Applications to
  E-Learning","In this paper we report on a multimedia communication system including a
VCoIP (Video Conferencing over IP) software with a distributed architecture and
its applications for teaching scenarios. It is a simple, ready-to-use scheme
for distributed presenting, recording and streaming multimedia content. We also
introduce and investigate concepts and experiments to IPv6 user and session
mobility, with the special focus on real-time video group communication.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1408.0605v1,"ITEM: Immersive Telepresence for Entertainment and Meetings - A
  Practical Approach","This paper presents an Immersive Telepresence system for Entertainment and
Meetings (ITEM). The system aims to provide a radically new video communication
experience by seamlessly merging participants into the same virtual space to
allow a natural interaction among them and shared collaborative contents. With
the goal to make a scalable, flexible system for various business solutions as
well as easily accessible by massive consumers, we address the challenges in
the whole pipeline of media processing, communication, and displaying in our
design and realization of such a system. Particularly, in this paper we focus
on the system aspects that maximize the end-user experience, optimize the
system and network resources, and enable various teleimmersive application
scenarios. In addition, we also present a few key technologies, i.e. fast
object-based video coding for real world data and spatialized audio capture and
3D sound localization for group teleconferencing. Our effort is to investigate
and optimize the key system components and provide an efficient end-to-end
optimization and integration by considering user needs and preferences.
Extensive experiments show the developed system runs reliably and comfortably
in real time with a minimal setup requirement (e.g. a webcam and/or a depth
camera, an optional microphone array, a laptop/desktop connected to the public
Internet) for teleimmersive communication. With such a really minimal
deployment requirement, we present a variety of interesting applications and
user experiences created by ITEM.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0011045v1,Index Assignment for Multichannel Communication under Failure,"We consider the problem of multiple description scalar quantizers and
describing the achievable rate-distortion tuples in that setting. We formulate
it as a combinatorial optimization problem of arranging numbers in a matrix to
minimize the maximum difference between the largest and the smallest number in
any row or column. We develop a technique for deriving lower bounds on the
distortion at given channel rates. The approach is constructive, thus allowing
an algorithm that gives a closely matching upper bound. For the case of two
communication channels with equal rates, the bounds coincide, thus giving the
precise lowest achievable distortion at fixed rates. The bounds are within a
small constant for higher number of channels. To the best of our knowledge,
this is the first result concerning systems with more than two communication
channels. The problem is also equivalent to the bandwidth minimization problem
of Hamming graphs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0707.1151v6,"Logic, Design & Organization of PTVD-SHAM; A Parallel Time Varying &
  Data Super-helical Access Memory","This paper encompasses a super helical memory system's design, 'Boolean logic
& image-logic' as a theoretical concept of an invention-model to 'store
time-data' in terms of anticipating the best memory location ever for
data/time. A waterfall effect is deemed to assist the process of
potential-difference output-switch into diverse logic states in quantum dot
computational methods via utilizing coiled carbon nanotubes (CCNTs) and carbon
nanotube field effect transistors (CNFETs). A 'quantum confinement' is thus
derived for a flow of particles in a categorized quantum well substrate with a
normalized capacitance rectifying high B-field flux into electromagnetic
induction. Multi-access of coherent sequences of 'qubit addressing' is gained
in any magnitude as pre-defined for the orientation of array displacement.
Briefly, Gaussian curvature of k<0 is debated in aim of specifying the 2D
electron gas characteristics in scenarios where data is stored in short
intervals versus long ones e.g. when k'>(k<0) for greater CCNT diameters,
space-time continuum is folded by chance for the particle. This benefits from
Maxwell-Lorentz theory in Minkowski's space-time viewpoint alike to crystal
oscillators for precise data timing purposes and radar systems e.g., time
varying self-clocking devices in diverse geographic locations. This application
could also be optional for data depository versus extraction, in the best
supercomputer system's locations, autonomously. For best performance in
minimizing current limiting mechanisms including electromigration, a multilevel
metallization and implant process forming elevated sources/drains for the
circuit's staircase pyramidal construction, is discussed accordingly.",0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.00589v3,Robust Image Reconstruction with Misaligned Structural Information,"Multi-modality (or multi-channel) imaging is becoming increasingly important
and more widely available, e.g. hyperspectral imaging in remote sensing,
spectral CT in material sciences as well as multi-contrast MRI and PET-MR in
medicine. Research in the last decades resulted in a plethora of mathematical
methods to combine data from several modalities. State-of-the-art methods,
often formulated as variational regularization, have shown to significantly
improve image reconstruction both quantitatively and qualitatively. Almost all
of these models rely on the assumption that the modalities are perfectly
registered, which is not the case in most real world applications. We propose a
variational framework which jointly performs reconstruction and registration,
thereby overcoming this hurdle. Our approach is the first to achieve this for
different modalities and outranks established approaches in terms of accuracy
of both reconstruction and registration. Numerical results on simulated and
real data show the potential of the proposed strategy for various applications
in multi-contrast MRI, PET-MR, and hyperspectral imaging: typical misalignments
between modalities such as rotations, translations, zooms can be effectively
corrected during the reconstruction process. Therefore the proposed framework
allows the robust exploitation of shared information across multiple modalities
under real conditions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.03003v1,"Fault Tree Analysis: Identifying Maximum Probability Minimal Cut Sets
  with MaxSAT","In this paper, we present a novel MaxSAT-based technique to compute Maximum
Probability Minimal Cut Sets (MPMCSs) in fault trees. We model the MPMCS
problem as a Weighted Partial MaxSAT problem and solve it using a parallel
SAT-solving architecture. The results obtained with our open source tool
indicate that the approach is effective and efficient.",0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0407044v1,Reduced cost-based ranking for generating promising subproblems,"In this paper, we propose an effective search procedure that interleaves two
steps: subproblem generation and subproblem solution. We mainly focus on the
first part. It consists of a variable domain value ranking based on reduced
costs. Exploiting the ranking, we generate, in a Limited Discrepancy Search
tree, the most promising subproblems first. An interesting result is that
reduced costs provide a very precise ranking that allows to almost always find
the optimal solution in the first generated subproblem, even if its dimension
is significantly smaller than that of the original problem. Concerning the
proof of optimality, we exploit a way to increase the lower bound for
subproblems at higher discrepancies. We show experimental results on the TSP
and its time constrained variant to show the effectiveness of the proposed
approach, but the technique could be generalized for other problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1811.11242v1,Wrangling Messy CSV Files by Detecting Row and Type Patterns,"It is well known that data scientists spend the majority of their time on
preparing data for analysis. One of the first steps in this preparation phase
is to load the data from the raw storage format. Comma-separated value (CSV)
files are a popular format for tabular data due to their simplicity and
ostensible ease of use. However, formatting standards for CSV files are not
followed consistently, so each file requires manual inspection and potentially
repair before the data can be loaded, an enormous waste of human effort for a
task that should be one of the simplest parts of data science. The first and
most essential step in retrieving data from CSV files is deciding on the
dialect of the file, such as the cell delimiter and quote character. Existing
dialect detection approaches are few and non-robust. In this paper, we propose
a dialect detection method based on a novel measure of data consistency of
parsed data files. Our method achieves 97% overall accuracy on a large corpus
of real-world CSV files and improves the accuracy on messy CSV files by almost
22% compared to existing approaches, including those in the Python standard
library.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0808.3693v1,Providing Virtual Execution Environments: A Twofold Illustration,"Platform virtualization helps solving major grid computing challenges: share
resource with flexible, user-controlled and custom execution environments and
in the meanwhile, isolate failures and malicious code. Grid resource management
tools will evolve to embrace support for virtual resource.
  We present two open source projects that transparently supply virtual
execution environments. Tycoon has been developed at HP Labs to optimise
resource usage in creating an economy where users bid to access virtual
machines and compete for CPU cycles. SmartDomains provides a peer-to-peer layer
that automates virtual machines deployment using a description language and
deployment engine from HP Labs. These projects demonstrate both client-server
and peer-to-peer approaches to virtual resource management. The first case
makes extensive use of virtual machines features for dynamic resource
allocation. The second translates virtual machines capabilities into a
sophisticated language where resource management components can be plugged in
configurations and architectures defined at deployment time.
  We propose to share our experience at CERN openlab developing SmartDomains
and deploying Tycoon to give an illustrative introduction to emerging research
in virtual resource management.",0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.04513v2,"An experiment exploring the theoretical and methodological challenges in
  developing a semi-automated approach to analysis of small-N qualitative data","This paper experiments with designing a semi-automated qualitative data
analysis (QDA) algorithm to analyse 20 transcripts by using freeware.
Text-mining (TM) and QDA were guided by frequency and association measures,
because these statistics remain robust when the sample size is small. The
refined TM algorithm split the text into various sizes based on a manually
revised dictionary. This lemmatisation approach may reflect the context of the
text better than uniformly tokenising the text into one single size. TM results
were used for initial coding. Code repacking was guided by association measures
and external data to implement a general inductive QDA approach. The
information retrieved by TM and QDA was depicted in subgraphs for comparisons.
The analyses were completed in 6-7 days. Both algorithms retrieved contextually
consistent and relevant information. However, the QDA algorithm retrieved more
specific information than TM alone. The QDA algorithm does not strictly comply
with the convention of TM or of QDA, but becomes a more efficient, systematic
and transparent text analysis approach than a conventional QDA approach.
Scaling up QDA to reliably discover knowledge from text was exactly the
research purpose. This paper also sheds light on understanding the relations
between information technologies, theory and methodologies.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1501.04457v1,Mechanism Design with Strategic Mediators,"We consider the problem of designing mechanisms that interact with strategic
agents through strategic intermediaries (or mediators), and investigate the
cost to society due to the mediators' strategic behavior. Selfish agents with
private information are each associated with exactly one strategic mediator,
and can interact with the mechanism exclusively through that mediator. Each
mediator aims to optimize the combined utility of his agents, while the
mechanism aims to optimize the combined utility of all agents. We focus on the
problem of facility location on a metric induced by a publicly known tree. With
non-strategic mediators, there is a dominant strategy mechanism that is
optimal. We show that when both agents and mediators act strategically, there
is no dominant strategy mechanism that achieves any approximation. We, thus,
slightly relax the incentive constraints, and define the notion of a two-sided
incentive compatible mechanism. We show that the $3$-competitive deterministic
mechanism suggested by Procaccia and Tennenholtz (2013) and Dekel et al. (2010)
for lines extends naturally to trees, and is still $3$-competitive as well as
two-sided incentive compatible. This is essentially the best possible. We then
show that by allowing randomization one can construct a $2$-competitive
randomized mechanism that is two-sided incentive compatible, and this is also
essentially tight. This result also closes a gap left in the work of Procaccia
and Tennenholtz (2013) and Lu et al. (2009) for the simpler problem of
designing strategy-proof mechanisms for weighted agents with no mediators on a
line, while extending to the more general model of trees. We also investigate a
further generalization of the above setting where there are multiple levels of
mediators.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1209.1977v1,Ten times eighteen,"We consider the following simple game: We are given a table with ten slots
indexed one to ten. In each of the ten rounds of the game, three dice are
rolled and the numbers are added. We then put this number into any free slot.
For each slot, we multiply the slot index with the number in this slot, and add
up the products. The goal of the game is to maximize this score. In more
detail, we play the game many times, and try to maximize the sum of scores or,
equivalently, the expected score. We present a strategy to optimally play this
game with respect to the expected score. We then modify our strategy so that we
need only polynomial time and space. Finally, we show that knowing all ten
rolls in advance, results in a relatively small increase in score. Although the
game has a random component and requires a non-trivial strategy to be solved
optimally, this strategy needs only polynomial time and space.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
http://arxiv.org/abs/1908.10078v1,"A Comparative Study of Younger and Older Adults' Interaction with a
  Crowdsourcing Android TV App for Detecting Errors in TEDx Video Subtitles","In this paper we report the results of a pilot study comparing the older and
younger adults' interaction with an Android TV application which enables users
to detect errors in video subtitles. Overall, the interaction with the
TV-mediated crowdsourcing system relying on language profficiency was seen as
intuitive, fun and accessible, but also cognitively demanding; more so for
younger adults who focused on the task of detecting errors, than for older
adults who concentrated more on the meaning and edutainment aspect of the
videos. We also discuss participants' motivations and preliminary
recommendations for the design of TV-enabled crowdsourcing tasks and subtitle
QA systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.06681v1,Low precision logarithmic number systems: Beyond base-2,"Logarithmic number systems (LNS) are used to represent real numbers in many
applications using a constant base raised to a fixed-point exponent making its
distribution exponential. This greatly simplifies hardware multiply, divide and
square root. LNS with base-2 is most common, but in this paper we show that for
low-precision LNS the choice of base has a significant impact.
  We make four main contributions. First, LNS is not closed under addition and
subtraction, so the result is approximate. We show that choosing a suitable
base can manipulate the distribution to reduce the average error. Second, we
show that low-precision LNS addition and subtraction can be implemented
efficiently in logic rather than commonly used ROM lookup tables, the
complexity of which can be reduced by an appropriate choice of base. A similar
effect is shown where the result of arithmetic has greater precision than the
input. Third, where input data from external sources is not expected to be in
LNS, we can reduce the conversion error by selecting a LNS base to match the
expected distribution of the input. Thus, there is no one base which gives the
global optimum, and base selection is a trade-off between different factors.
Fourth, we show that circuits realized in LNS require lower area and power
consumption for short word lengths.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0806.1569v1,Wireless Sensor/Actuator Network Design for Mobile Control Applications,"Wireless sensor/actuator networks (WSANs) are emerging as a new generation of
sensor networks. Serving as the backbone of control applications, WSANs will
enable an unprecedented degree of distributed and mobile control. However, the
unreliability of wireless communications and the real-time requirements of
control applications raise great challenges for WSAN design. With emphasis on
the reliability issue, this paper presents an application-level design
methodology for WSANs in mobile control applications. The solution is generic
in that it is independent of the underlying platforms, environment, control
system models, and controller design. To capture the link quality
characteristics in terms of packet loss rate, experiments are conducted on a
real WSAN system. From the experimental observations, a simple yet efficient
method is proposed to deal with unpredictable packet loss on actuator nodes.
Trace-based simulations give promising results, which demonstrate the
effectiveness of the proposed approach.",0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.07908v2,Label Tree Embeddings for Acoustic Scene Classification,"We present in this paper an efficient approach for acoustic scene
classification by exploring the structure of class labels. Given a set of class
labels, a category taxonomy is automatically learned by collectively optimizing
a clustering of the labels into multiple meta-classes in a tree structure. An
acoustic scene instance is then embedded into a low-dimensional feature
representation which consists of the likelihoods that it belongs to the
meta-classes. We demonstrate state-of-the-art results on two different datasets
for the acoustic scene classification task, including the DCASE 2013 and LITIS
Rouen datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.12063v2,"Balanced dynamic multiple travelling salesmen: algorithms and continuous
  approximations","Dynamic routing occurs when customers are not known in advance, e.g. for
real-time routing. Two heuristics are proposed that solve the balanced dynamic
multiple travelling salesmen problem (BD-mTSP). These heuristics represent
operational (tactical) tools for dynamic (online, real-time) routing. Several
types and scopes of dynamics are proposed. Particular attention is given to
sequential dynamics. The balanced dynamic closest vehicle heuristic (BD-CVH)
and the balanced dynamic assignment vehicle heuristic (BD-AVH) are applied to
this type of dynamics. The algorithms are applied to a wide range of test
instances. Taxi services and palette transfers in warehouses demonstrate how to
use the BD-mTSP algorithms in real-world scenarios. Continuous approximation
models for the BD-mTSP's are derived and serve as strategic tools for dynamic
routing. The models express route lengths using vehicles, customers, and
dynamic scopes without the need of running an algorithm. A machine learning
approach was used to obtain regression models. The mean absolute percentage
error of two of these models is below 3%.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1808.04203v1,"Xcos on Web as a promising learning tool for Bachelor's of
  Electromechanics modeling of technical objects","Research goals: to identify the perspective learning simulation tool for
Bachelors of Electromechanics. Research objectives: to prove the feasibility of
using the simulation system Xcos on Web as a tool of forming of future
Bachelors of Electromechanics competence in modeling of technical objects.
Research object: the use of imitative simulation systems to learning the
Bachelors of Electromechanics. Research subject: the use Xcos on Web in
learning modeling of technical objects the Bachelors of Electromechanics.
Research methods used: the analysis of existing software usage experience.
Research results. The imitative simulation system Xcos on Web is a promising
cloud-based learning tool for Bachelor's of Electromechanics modeling of
technical objects. The main conclusions and recommendations: 1. The use of
simulation systems, such as Scilab Xcos, is a necessary part of Bachelor of
Electromechanics professional training. 2. Cloud-based learning environment
built on the integrative usage of mobile Internet devices promotes the forming
of Bachelor's of Electromechanics professional competencies. 3. Implementation
the full Scilab Xcos functionality at Xcos on Web creates conditions for
transition in Bachelor's of Electromechanics learning the simulation of
technical objects to the use of mobile Internet devices.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/0705.0612v1,"Privacy - an Issue for eLearning? A Trend Analysis Reflecting the
  Attitude of European eLearning Users","Availing services provided via the Internet became a widely accepted means in
organising one's life. Beside others, eLearning goes with this trend as well.
But, while employing Internet service makes life more convenient, at the same
time, it raises risks with respect to the protection of the users' privacy.
This paper analyses the attitudes of eLearning users towards their privacy by,
initially, pointing out terminology and legal issues connected with privacy.
Further, the concept and implementation as well as a result analysis of a
conducted study is presented, which explores the problem area from different
perspectives. The paper will show that eLearning users indeed care for the
protection of their personal information when using eLearning services.
However, their attitudes and behaviour slightly differ. In conclusion, we
provide first approaches of assisting possibilities for users how to resolve
the difference of requirements and their actual activities with respect to
privacy protection.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,1,0,0,0
http://arxiv.org/abs/1405.6090v1,"Using the Regular Chains Library to build cylindrical algebraic
  decompositions by projecting and lifting","Cylindrical algebraic decomposition (CAD) is an important tool, both for
quantifier elimination over the reals and a range of other applications.
Traditionally, a CAD is built through a process of projection and lifting to
move the problem within Euclidean spaces of changing dimension. Recently, an
alternative approach which first decomposes complex space using triangular
decomposition before refining to real space has been introduced and implemented
within the RegularChains Library of Maple. We here describe a freely available
package ProjectionCAD which utilises the routines within the RegularChains
Library to build CADs by projection and lifting. We detail how the projection
and lifting algorithms were modified to allow this, discuss the motivation and
survey the functionality of the package.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0904.3912v2,Refutation of Aslam's Proof that NP = P,"Aslam presents an algorithm he claims will count the number of perfect
matchings in any incomplete bipartite graph with an algorithm in the
function-computing version of NC, which is itself a subset of FP. Counting
perfect matchings is known to be #P-complete; therefore if Aslam's algorithm is
correct, then NP=P. However, we show that Aslam's algorithm does not correctly
count the number of perfect matchings and offer an incomplete bipartite graph
as a concrete counter-example.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1510.05091v1,"Reasoning About Information Flow Security of Separation Kernels with
  Channel-based Communication","Assurance of information flow security by formal methods is mandated in
security certification of separation kernels. As an industrial standard for
separation kernels, ARINC 653 has been complied with by mainstream separation
kernels. Security of functionalities defined in ARINC 653 is thus very
important for the development and certification of separation kernels. This
paper presents the first effort to formally specify and verify separation
kernels with ARINC 653 channel-based communication. We provide a reusable
formal specification and security proofs for separation kernels in
Isabelle/HOL. During reasoning about information flow security, we find some
security flaws in the ARINC 653 standard, which can cause information leakage,
and fix them in our specification. We also validate the existence of the
security flaws in two open-source ARINC 653 compliant separation kernels.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1601.07810v2,"The Unfitted Discontinuous Galerkin Method for Solving the EEG Forward
  Problem","Objective: The purpose of this study is to introduce and evaluate the
unfitted discontinuous Galerkin finite element method (UDG-FEM) for solving the
electroencephalography (EEG) forward problem. Methods: This new approach for
source analysis does not use a geometry conforming volume triangulation, but
instead uses a structured mesh that does not resolve the geometry. The geometry
is described using level set functions and is incorporated implicitly in its
mathematical formulation. As no triangulation is necessary, the complexity of a
simulation pipeline and the need for manual interaction for patient specific
simulations can be reduced and is comparable with that of the FEM for
hexahedral meshes. In addition, it maintains conservation laws on a discrete
level. Here, we present the theory for UDG-FEM forward modeling, its
verification using quasi-analytical solutions in multi-layer sphere models and
an evaluation in a comparison with a discontinuous Galerkin (DG-FEM) method on
hexahedral and on conforming tetrahedral meshes. We furthermore apply the
UDG-FEM forward approach in a realistic head model simulation study. Results:
The given results show convergence and indicate a good overall accuracy of the
UDG-FEM approach. UDG-FEM performs comparable or even better than DG-FEM on a
conforming tetrahedral mesh while providing a less complex simulation pipeline.
When compared to DG-FEM on hexahedral meshes, an overall better accuracy is
achieved. Conclusion: The UDG-FEM approach is an accurate, flexible and
promising method to solve the EEG forward problem. Significance: This study
shows the first application of the UDG-FEM approach to the EEG forward problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.08336v2,Override and update,"Override and update are natural constructions for combining partial
functions, which arise in various program specification contexts. We use an
unexpected connection with combinatorial geometry to provide a complete finite
system of equational axioms for the first order theory of the override and
update constructions on partial functions, resolving the main unsolved problem
in the area.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.01408v1,"Paradigm Shift Through the Integration of Physical Methodology and Data
  Science","Data science methodologies, which have undergone significant developments
recently, provide flexible representational performance and fast computational
means to address the challenges faced by traditional scientific methodologies
while revealing unprecedented challenges such as the interpretability of
computations and the demand for extrapolative predictions on the amount of
data. Methods that integrate traditional physical and data science
methodologies are new methods of mathematical analysis that complement both
methodologies and are being studied in various scientific fields. This paper
highlights the significance and importance of such integrated methods from the
viewpoint of scientific theory. Additionally, a comprehensive survey of
specific methods and applications are conducted, and the current state of the
art in relevant research fields are summarized.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0002004v1,Stochastic Model Checking for Multimedia,"Modern distributed systems include a class of applications in which
non-functional requirements are important. In particular, these applications
include multimedia facilities where real time constraints are crucial to their
correct functioning. In order to specify such systems it is necessary to
describe that events occur at times given by probability distributions and
stochastic automata have emerged as a useful technique by which such systems
can be specified and verified.
  However, stochastic descriptions are very general, in particular they allow
the use of general probability distribution functions, and therefore their
verification can be complex. In the last few years, model checking has emerged
as a useful verification tool for large systems.
  In this paper we describe two model checking algorithms for stochastic
automata. These algorithms consider how properties written in a simple
probabilistic real-time logic can be checked against a given stochastic
automaton.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1005.0146v1,"The Formulator MathML Editor Project: User-Friendly Authoring of Content
  Markup Documents","Implementation of an editing process for Content MathML formulas in common
visual style is a real challenge for a software developer who does not really
want the user to have to understand the structure of Content MathML in order to
edit an expression, since it is expected that users are often not that
technically minded. In this paper, we demonstrate how this aim is achieved in
the context of the Formulator project and discuss features of this MathML
editor, which provides a user with a WYSIWYG editing style while authoring
MathML documents with Content or mixed markup. We also present the approach
taken to enhance availability of the MathML editor to end-users, demonstrating
an online version of the editor that runs inside a Web browser.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1305.2494v3,"Computing Solution Operators of Boundary-value Problems for Some Linear
  Hyperbolic Systems of PDEs","We discuss possibilities of application of Numerical Analysis methods to
proving computability, in the sense of the TTE approach, of solution operators
of boundary-value problems for systems of PDEs. We prove computability of the
solution operator for a symmetric hyperbolic system with computable real
coefficients and dissipative boundary conditions, and of the Cauchy problem for
the same system (we also prove computable dependence on the coefficients) in a
cube $Q\subseteq\mathbb R^m$. Such systems describe a wide variety of physical
processes (e.g. elasticity, acoustics, Maxwell equations). Moreover, many
boundary-value problems for the wave equation also can be reduced to this case,
thus we partially answer a question raised in Weihrauch and Zhong (2002).
Compared with most of other existing methods of proving computability for PDEs,
this method does not require existence of explicit solution formulas and is
thus applicable to a broader class of (systems of) equations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1402.2508v1,Data Compaction - Compression without Decompression,"Data compaction is a new approach for lossless and lossy compression of
read-only array data. The biggest advantage over existing approaches is the
possibility to access compressed data without any decompression. This makes
data compaction most suitable for systems that could currently not apply
compression techniques due to real-time or memory constraints. This is true for
the majority of all computers, i.e. a wide range of embedded systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1908.05944v2,Parallel Computation of Alpha Complex for Biomolecules,"The alpha complex, a subset of the Delaunay triangulation, has been
extensively used as the underlying representation for biomolecular structures.
We propose a GPU-based parallel algorithm for the computation of the alpha
complex, which exploits the knowledge of typical spatial distribution and sizes
of atoms in a biomolecule. Unlike existing methods, this algorithm does not
require prior construction of the Delaunay triangulation. The algorithm
computes the alpha complex in two stages. The first stage proceeds in a
bottom-up fashion and computes a superset of the edges, triangles, and
tetrahedra belonging to the alpha complex. The false positives from this
estimation stage are removed in a subsequent pruning stage to obtain the
correct alpha complex. Computational experiments on several biomolecules
demonstrate the superior performance of the algorithm, up to a factor of 50
when compared to existing methods that are optimized for biomolecules.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1202.2921v1,Evaluation strategies for monadic computations,"Monads have become a powerful tool for structuring effectful computations in
functional programming, because they make the order of effects explicit. When
translating pure code to a monadic version, we need to specify evaluation order
explicitly. Two standard translations give call-by-value and call-by-name
semantics. The resulting programs have different structure and types, which
makes revisiting the choice difficult.
  In this paper, we translate pure code to monadic using an additional
operation malias that abstracts out the evaluation strategy. The malias
operation is based on computational comonads; we use a categorical framework to
specify the laws that are required to hold about the operation.
  For any monad, we show implementations of malias that give call-by-value and
call-by-name semantics. Although we do not give call-by-need semantics for all
monads, we show how to turn certain monads into an extended monad with
call-by-need semantics, which partly answers an open question. Moreover, using
our unified translation, it is possible to change the evaluation strategy of
functional code translated to the monadic form without changing its structure
or types.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1804.05037v2,Reactive Control Improvisation,"Reactive synthesis is a paradigm for automatically building
correct-by-construction systems that interact with an unknown or adversarial
environment. We study how to do reactive synthesis when part of the
specification of the system is that its behavior should be random. Randomness
can be useful, for example, in a network protocol fuzz tester whose output
should be varied, or a planner for a surveillance robot whose route should be
unpredictable. However, existing reactive synthesis techniques do not provide a
way to ensure random behavior while maintaining functional correctness. Towards
this end, we generalize the recently-proposed framework of control
improvisation (CI) to add reactivity. The resulting framework of reactive
control improvisation provides a natural way to integrate a randomness
requirement with the usual functional specifications of reactive synthesis over
a finite window. We theoretically characterize when such problems are
realizable, and give a general method for solving them. For specifications
given by reachability or safety games or by deterministic finite automata, our
method yields a polynomial-time synthesis algorithm. For various other types of
specifications including temporal logic formulas, we obtain a polynomial-space
algorithm and prove matching PSPACE-hardness results. We show that all of these
randomized variants of reactive synthesis are no harder in a
complexity-theoretic sense than their non-randomized counterparts.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0105017v1,Optimization Over Zonotopes and Training Support Vector Machines,"We make a connection between classical polytopes called zonotopes and Support
Vector Machine (SVM) classifiers. We combine this connection with the ellipsoid
method to give some new theoretical results on training SVMs. We also describe
some special properties of soft margin C-SVMs as parameter C goes to infinity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1804.10264v2,Nonlinearity in stock networks,"Stock networks, constructed from stock price time series, are a
well-established tool for the characterization of complex behavior in stock
markets. Following Mantegna's seminal paper, the linear Pearson's correlation
coefficient between pairs of stocks has been the usual way to determine network
edges. Recently, possible effects of nonlinearity on the graph-theoretical
properties of such networks have been demonstrated when using nonlinear
measures such as mutual information instead of linear correlation. In this
paper, we quantitatively characterize the nonlinearity in stock time series and
the effect it has on stock network properties. This is achieved by a systematic
multi-step approach that allows us to quantify the nonlinearity of coupling;
correct its effects wherever it is caused by simple univariate non-Gaussianity;
potentially localize in space and time any remaining strong sources of this
nonlinearity; and, finally, study the effect nonlinearity has on global network
properties. By applying this multi-step approach to stocks included in three
prominent indices (NYSE100, FTSE100 and SP500), we establish that the apparent
nonlinearity that has been observed is largely due to univariate
non-Gaussianity. Furthermore, strong nonstationarity in a few specific stocks
may play a role. In particular, the sharp decrease in some stocks during the
global financial crisis of 2008 gives rise to apparent nonlinear dependencies
among stocks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.04257v1,"Augmenting Teleportation in Virtual Reality With Discrete Rotation
  Angles","Locomotion is one of the most essential interaction tasks in virtual reality
(VR) with teleportation being widely accepted as the state-of-the-art
locomotion technique at the time of this writing. A major draw-back of
teleportation is the accompanying physical rotation that is necessary to adjust
the users' orientation either before or after teleportation. This is a limiting
factor for tethered head-mounted displays (HMDs) and static body postures and
can induce additional simulator sickness for HMDs with three degrees-of-freedom
(DOF) due to missing parallax cues. To avoid physical rotation, previous work
proposed discrete rotation at fixed intervals (InPlace) as a controller-based
technique with low simulator sickness, yet the impact of varying intervals on
spatial disorientation, user presence and performance remains to be explored.
An unevaluated technique found in commercial VR games is reorientation during
the teleportation process (TeleTurn), which prevents physical rotation but
potentially increases interaction time due to its continuous orientation
selection. In an exploratory user study, where participants were free to apply
both techniques, we evaluated the impact of rotation parameters of either
technique on user performance and preference. Our results indicate that
discrete InPlace rotation introduced no significant spatial disorientation,
while user presence scores were increased. Discrete TeleTurn and teleportation
without rotation was ranked higher and achieved a higher presence score than
continuous TeleTurn, which is the current state-of-the-art found in VR games.
Based on observations, that participants avoided TeleTurn rotation when
discrete InPlace rotation was available, we distilled guidelines for designing
teleportation without physical rotation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.00991v1,"Computational Performance of a Germline Variant Calling Pipeline for
  Next Generation Sequencing","With the booming of next generation sequencing technology and its
implementation in clinical practice and life science research, the need for
faster and more efficient data analysis methods becomes pressing in the field
of sequencing. Here we report on the evaluation of an optimized germline
mutation calling pipeline, HummingBird, by assessing its performance against
the widely accepted BWA-GATK pipeline. We found that the HummingBird pipeline
can significantly reduce the running time of the primary data analysis for
whole genome sequencing and whole exome sequencing while without significantly
sacrificing the variant calling accuracy. Thus, we conclude that expansion of
such software usage will help to improve the primary data analysis efficiency
for next generation sequencing.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1008.3585v1,"Ultrametric and Generalized Ultrametric in Computational Logic and in
  Data Analysis","Following a review of metric, ultrametric and generalized ultrametric, we
review their application in data analysis. We show how they allow us to explore
both geometry and topology of information, starting with measured data. Some
themes are then developed based on the use of metric, ultrametric and
generalized ultrametric in logic. In particular we study approximation chains
in an ultrametric or generalized ultrametric context. Our aim in this work is
to extend the scope of data analysis by facilitating reasoning based on the
data analysis; and to show how quantitative and qualitative data analysis can
be incorporated into logic programming.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.00867v2,"Accessible Data Curation and Analytics for International-Scale Citizen
  Science Datasets","The Covid Symptom Study, a smartphone-based surveillance study on COVID-19
symptoms in the population, is an exemplar of big data citizen science. Over
4.7 million participants and 189 million unique assessments have been logged
since its introduction in March 2020. The success of the Covid Symptom Study
creates technical challenges around effective data curation for two reasons.
Firstly, the scale of the dataset means that it can no longer be easily
processed using standard software on commodity hardware. Secondly, the size of
the research group means that replicability and consistency of key analytics
used across multiple publications becomes an issue. We present ExeTera, an open
source data curation software designed to address scalability challenges and to
enable reproducible research across an international research group for
datasets such as the Covid Symptom Study dataset.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.04659v3,Artificial Intelligence: A Child's Play,"We discuss the objectives of any endeavor in creating artificial
intelligence, AI, and provide a possible alternative. Intelligence might be an
unintended consequence of curiosity left to roam free, best exemplified by a
frolicking infant. This suggests that our attempts at AI could have been
misguided. What we actually need to strive for can be termed artificial
curiosity, AC, and intelligence happens as a consequence of those efforts. For
this unintentional yet welcome aftereffect to set in a foundational list of
guiding principles needs to be present. We start with the intuition for this
line of reasoning and formalize it with a series of definitions, assumptions,
ingredients, models and iterative improvements that will be necessary to make
the incubation of intelligence a reality. Our discussion provides conceptual
modifications to the Turing Test and to Searle's Chinese room argument. We
discuss the future implications for society as AI becomes an integral part of
life.
  We provide a road-map for creating intelligence with the technical parts
relegated to the appendix so that the article is accessible to a wide audience.
The central techniques in our formal approach to creating intelligence draw
upon tools and concepts widely used in physics, cognitive science, psychology,
evolutionary biology, statistics, linguistics, communication systems, pattern
recognition, marketing, economics, finance, information science and
computational theory highlighting that solutions for creating artificial
intelligence have to transcend the artificial barriers between various fields
and be highly multi-disciplinary.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.16016v1,Lucas-Interpretation on Isabelle's Functions,"Software tools of Automated Reasoning are too sophisticated for general use
in mathematics education and respective reasoning, while Lucas-Interpretation
provides a general concept for integrating such tools into educational software
with the purpose to reliably and flexibly check formal input of students. This
paper gives the first technically concise description of Lucas-Interpretation
at the occasion of migrating a prototype implementation to the function package
of the proof assistant Isabelle. The description shows straightforward
adaptations of Isabelle's programming language and shows, how simple migration
of the interpreter was, since the design (before the function package has been
introduced to Isabelle) recognised appropriateness of Isabelle's terms as
middle end. The paper gives links into the code in an open repository as
invitation to readers for re-using the prototyped code or adopt the general
concept. And since the prototype has been designed before the function package
was implemented, the paper is an opportunity for recording lessons learned from
Isabelle's development of code structure.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/2001.11695v2,"Modeling Perception Errors towards Robust Decision Making in Autonomous
  Vehicles","Sensing and Perception (S&P) is a crucial component of an autonomous system
(such as a robot), especially when deployed in highly dynamic environments
where it is required to react to unexpected situations. This is particularly
true in case of Autonomous Vehicles (AVs) driving on public roads. However, the
current evaluation metrics for perception algorithms are typically designed to
measure their accuracy per se and do not account for their impact on the
decision making subsystem(s). This limitation does not help developers and
third party evaluators to answer a critical question: is the performance of a
perception subsystem sufficient for the decision making subsystem to make
robust, safe decisions? In this paper, we propose a simulation-based
methodology towards answering this question. At the same time, we show how to
analyze the impact of different kinds of sensing and perception errors on the
behavior of the autonomous system.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1502.00130v1,The Search for Computational Intelligence,"We define and explore in simulation several rules for the local evolution of
generative rules for 1D and 2D cellular automata. Our implementation uses
strategies from conceptual blending. We discuss potential applications to
modelling social dynamics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0108018v1,Bipartite graph partitioning and data clustering,"Many data types arising from data mining applications can be modeled as
bipartite graphs, examples include terms and documents in a text corpus,
customers and purchasing items in market basket analysis and reviewers and
movies in a movie recommender system. In this paper, we propose a new data
clustering method based on partitioning the underlying bipartite graph. The
partition is constructed by minimizing a normalized sum of edge weights between
unmatched pairs of vertices of the bipartite graph. We show that an approximate
solution to the minimization problem can be obtained by computing a partial
singular value decomposition (SVD) of the associated edge weight matrix of the
bipartite graph. We point out the connection of our clustering algorithm to
correspondence analysis used in multivariate analysis. We also briefly discuss
the issue of assigning data objects to multiple clusters. In the experimental
results, we apply our clustering algorithm to the problem of document
clustering to illustrate its effectiveness and efficiency.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
