id,title,abstract,A.1,B.1,B.2,B.3,B.4,B.5,B.6,B.7,B.8,C.0,C.1,C.2,C.3,C.4,C.5,D.0,D.1,D.2,D.3,D.4,E.0,E.1,E.2,E.3,E.4,E.5,F.0,F.1,F.2,F.3,F.4,G.0,G.1,G.2,G.3,G.4,H.0,H.1,H.2,H.3,H.4,H.5,I.0,I.1,I.2,I.3,I.4,I.5,I.6,I.7,J.1,J.2,J.3,J.4,J.5,J.6,J.7,K.2,K.3,K.4,K.5,K.6,K.7,K.8
http://arxiv.org/abs/1803.09681v2,I/O Logic in HOL --- First Steps,"A semantical embedding of input/output logic in classical higher-order logic
is presented. This embedding enables the mechanisation and automation of
reasoning tasks in input/output logic with off-the-shelf higher-order theorem
provers and proof assistants. The key idea for the solution presented here
results from the analysis of an inaccurate previous embedding attempt, which we
will discuss as well.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0505002v1,"Tight Lower Bounds for Query Processing on Streaming and External Memory
  Data","We study a clean machine model for external memory and stream processing. We
show that the number of scans of the external data induces a strict hierarchy
(as long as work space is sufficiently small, e.g., polylogarithmic in the size
of the input). We also show that neither joins nor sorting are feasible if the
product of the number $r(n)$ of scans of the external memory and the size
$s(n)$ of the internal memory buffers is sufficiently small, e.g., of size
$o(\sqrt[5]{n})$. We also establish tight bounds for the complexity of XPath
evaluation and filtering.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.02371v2,Computing maximal autarkies with few and simple oracle queries,"We consider the algorithmic task of computing a maximal autarky for a
clause-set F, i.e., a partial assignment which satisfies every clause of F it
touches, and where this property is destroyed by adding any non-empty set of
further assignments. We employ SAT solvers as oracles, using various
capabilities. Using the standard SAT oracle, log_2(n(F)) oracle calls suffice,
where n(F) is the number of variables, but the drawback is that (translated)
cardinality constraints are employed, which makes this approach less efficient
in practice. Using an extended SAT oracle, motivated by the capabilities of
modern SAT solvers, we show how to compute maximal autarkies with 2 n(F)^{1/2}
simpler oracle calls. This novel algorithm combines the previous two main
approaches, based on the autarky-resolution duality and on SAT translations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0312018v1,Mapping Subsets of Scholarly Information,"We illustrate the use of machine learning techniques to analyze, structure,
maintain, and evolve a large online corpus of academic literature. An emerging
field of research can be identified as part of an existing corpus, permitting
the implementation of a more coherent community structure for its
practitioners.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.02065v3,Computational Approaches for Zero Forcing and Related Problems,"In this paper, we propose computational approaches for the zero forcing
problem, the connected zero forcing problem, and the problem of forcing a graph
within a specified number of timesteps. Our approaches are based on a
combination of integer programming models and combinatorial algorithms, and
include formulations for zero forcing as a dynamic process, and as a
set-covering problem. We explore several solution strategies for these models,
test them on various types of graphs, and show that they are competitive with
the state-of-the-art algorithm for zero forcing. Our proposed algorithms for
connected zero forcing and for controlling the number of zero forcing timesteps
are the first general-purpose computational methods for these problems, and are
superior to brute force computation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0310043v3,"Value-at-Risk and Expected Shortfall for Quadratic portfolio of
  securities with mixture of elliptic Distributed Risk Factors","Generally, in the financial literature, the notion of quadratic VaR is
implicitly confused with the Delta-Gamma VaR, because more authors dealt with
portfolios that contains derivatives instruments.
  In this paper, we postpone to estimate the Value-at-Risk of a quadratic
portfolio of securities (i.e equities) without the Delta and Gamma greeks, when
the joint log-returns changes with multivariate elliptic distribution. We have
reduced the estimation of the quadratic VaR of such portfolio to a resolution
of one dimensional integral equation. To illustrate our method, we give special
attention to the mixture of normal and mixture of t-student distribution. For
given VaR, when joint Risk Factors changes with elliptic distribution, we show
how to estimate an Expected Shortfall .",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.04436v1,Large Scale Measurement on the Adoption of Encrypted DNS,"Several encryption proposals for DNS have been presented since 2016, but
their adoption was not comprehensively studied yet. This research measured the
current adoption of DoH (DNS over HTTPS), DoT (DNS over TLS), and DoQ (DNS over
QUIC) for five months at the beginning of 2021 by three different organizations
with global coverage. By comparing the total values, amount of requests per
user, and the seasonality of the traffic, it was possible to obtain the current
adoption trends. Moreover, we actively scanned the Internet for still-unknown
working DoH servers and we compared them with a novel curated list of
well-known DoH servers. We conclude that despite growing in 2020, during the
first five months of 2021 there was statistically significant evidence that the
average amount of Internet traffic for DoH, DoT and DoQ remained stationary.
However, we found that the amount of, still unknown and ready to use, DoH
servers grew 4 times. These measurements suggest that even though the amount of
encrypted DNS is currently not growing, there may probably be more connections
soon to those unknown DoH servers for benign and malicious purposes.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0101017v1,Checking Properties within Fairness and Behavior Abstractions,"This paper is motivated by the fact that verifying liveness properties under
a fairness condition is often problematic, especially when abstraction is used.
It shows that using a more abstract notion than truth under fairness,
specifically the concept of a property being satisfied within fairness can lead
to interesting possibilities. Technically, it is first established that
deciding satisfaction within fairness is a PSPACE-complete problem and it is
shown that properties satisfied within fairness can always be satisfied by some
fair implementation. Thereafter, the interaction between behavior abstraction
and satisfaction within fairness is studied and it is proved that satisfaction
of properties within fairness can be verified on behavior abstractions, if the
abstraction homomorphism is weakly continuation-closed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1112.5381v1,"Improving the Efficiency of Approximate Inference for Probabilistic
  Logical Models by means of Program Specialization","We consider the task of performing probabilistic inference with probabilistic
logical models. Many algorithms for approximate inference with such models are
based on sampling. From a logic programming perspective, sampling boils down to
repeatedly calling the same queries on a knowledge base composed of a static
part and a dynamic part. The larger the static part, the more redundancy there
is in these repeated calls. This is problematic since inefficient sampling
yields poor approximations.
  We show how to apply logic program specialization to make sampling-based
inference more efficient. We develop an algorithm that specializes the
definitions of the query predicates with respect to the static part of the
knowledge base. In experiments on real-world data we obtain speedups of up to
an order of magnitude, and these speedups grow with the data-size.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1802.01515v2,Robust Vertex Enumeration for Convex Hulls in High Dimensions,"Computation of the vertices of the convex hull of a set $S$ of $n$ points in
$\mathbb{R} ^m$ is a fundamental problem in computational geometry,
optimization, machine learning and more. We present ""All Vertex Triangle
Algorithm"" (AVTA), a robust and efficient algorithm for computing the subset
$\overline S$ of all $K$ vertices of $conv(S)$, the convex hull of $S$. If
$\Gamma_*$ is the minimum of the distances from each vertex to the convex hull
of the remaining vertices, given any $\gamma \leq \gamma_* = \Gamma_*/R$, $R$
the diameter of $S$, $AVTA$ computes $\overline S$ in $O(nK(m+ \gamma^{-2}))$
operations. If $\gamma_*$ is unknown but $K$ is known, AVTA computes $\overline
S$ in $O(nK(m+ \gamma_*^{-2})) \log(\gamma_*^{-1})$ operations. More generally,
given $t \in (0,1)$, AVTA computes a subset $\overline S^t$ of $\overline S$ in
$O(n |\overline S^t|(m+ t^{-2}))$ operations, where the distance between any $p
\in conv(S)$ to $conv(\overline S^t)$ is at most $t R$. Next we consider AVTA
where input is $S_\varepsilon$, an $\varepsilon$ perturbation of $S$. Assuming
a bound on $\varepsilon$ in terms of the minimum of the distances of vertices
of $conv(S)$ to the convex hull of the remaining point of $S$, we derive
analogous complexity bounds for computing $\overline S_\varepsilon$. We also
analyze AVTA under random projections of $S$ or $S_\varepsilon$. Finally, via
AVTA we design new practical algorithms for two popular machine learning
problems: topic modeling and non-negative matrix factorization. For topic
models AVTA leads to significantly better reconstruction of the topic-word
matrix than state of the art approaches~\cite{arora2013practical,
bansal2014provable}. For non-negative matrix AVTA is competitive with existing
methods~\cite{arora2012computing}. Empirically AVTA is robust and can handle
larger amounts of noise than existing methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.08719v1,Faster SVM Training via Conjugate SMO,"We propose an improved version of the SMO algorithm for training
classification and regression SVMs, based on a Conjugate Descent procedure.
This new approach only involves a modest increase on the computational cost of
each iteration but, in turn, usually results in a substantial decrease in the
number of iterations required to converge to a given precision. Besides, we
prove convergence of the iterates of this new Conjugate SMO as well as a linear
rate when the kernel matrix is positive definite. We have implemented Conjugate
SMO within the LIBSVM library and show experimentally that it is faster for
many hyper-parameter configurations, being often a better option than second
order SMO when performing a grid-search for SVM tuning.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0908.0764v1,"Learning about Potential Users of Collaborative Information Retrieval
  Systems","One of the key components of designing usable and useful collaborative
information retrieval systems is to understand the needs of the users of these
systems. Our research team has been exploring collaborative information
behavior in a variety of organizational settings. Our research goals have been
two-fold: First, to develop a conceptual understanding of collaborative
information behavior and second, gather requirements for the design of
collaborative information retrieval systems. In this paper, we present a brief
overview of our fieldwork in a three different organizational settings, discuss
our methodology for collecting data on collaborative information behavior, and
highlight some lessons that we are learning about potential users of
collaborative information retrieval systems in these domains.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0102019v1,Easy and Hard Constraint Ranking in OT: Algorithms and Complexity,"We consider the problem of ranking a set of OT constraints in a manner
consistent with data.
  We speed up Tesar and Smolensky's RCD algorithm to be linear on the number of
constraints. This finds a ranking so each attested form x_i beats or ties a
particular competitor y_i. We also generalize RCD so each x_i beats or ties all
possible competitors.
  Alas, this more realistic version of learning has no polynomial algorithm
unless P=NP! Indeed, not even generation does. So one cannot improve
qualitatively upon brute force:
  Merely checking that a single (given) ranking is consistent with given forms
is coNP-complete if the surface forms are fully observed and Delta_2^p-complete
if not. Indeed, OT generation is OptP-complete. As for ranking, determining
whether any consistent ranking exists is coNP-hard (but in Delta_2^p) if the
forms are fully observed, and Sigma_2^p-complete if not.
  Finally, we show that generation and ranking are easier in derivational
theories: in P, and NP-complete.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0511057v1,Quantized Indexing: Beyond Arithmetic Coding,"Quantized Indexing is a fast and space-efficient form of enumerative
(combinatorial) coding, the strongest among asymptotically optimal universal
entropy coding algorithms. The present advance in enumerative coding is similar
to that made by arithmetic coding with respect to its unlimited precision
predecessor, Elias coding. The arithmetic precision, execution time, table
sizes and coding delay are all reduced by a factor O(n) at a redundancy below
2*log(e)/2^g bits/symbol (for n input symbols and g-bit QI precision). Due to
its tighter enumeration, QI output redundancy is below that of arithmetic
coding (which can be derived as a lower accuracy approximation of QI). The
relative compression gain vanishes in large n and in high entropy limits and
increases for shorter outputs and for less predictable data. QI is
significantly faster than the fastest arithmetic coders, from factor 6 in high
entropy limit to over 100 in low entropy limit (`typically' 10-20 times
faster). These speedups are result of using only 3 adds, 1 shift and 2 array
lookups (all in 32 bit precision) per less probable symbol and no coding
operations for the most probable symbol . Further, the exact enumeration
algorithm is sharpened and its lattice walks formulation is generalized. A new
numeric type with a broader applicability, sliding window integer, is
introduced.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.11975v1,"GEViTRec: Data Reconnaissance Through Recommendation Using a
  Domain-Specific Prevalence Visualization Design Space","Genomic Epidemiology (genEpi) is a branch of public health that uses many
different data types including tabular, network, genomic, and geographic, to
identify and contain outbreaks of deadly diseases. Due to the volume and
variety of data, it is challenging for genEpi domain experts to conduct data
reconnaissance; that is, have an overview of the data they have and make
assessments toward its quality, completeness, and suitability. We present an
algorithm for data reconnaissance through automatic visualization
recommendation, GEViTRec. Our approach handles a broad variety of dataset types
and automatically generates coordinated combinations of charts, in contrast to
existing systems that primarily focus on singleton visual encodings of tabular
datasets. We automatically detect linkages across multiple input datasets by
analyzing non-numeric attribute fields, creating an entity graph within which
we analyze and rank paths. For each high-ranking path, we specify chart
combinations with spatial and color alignments between shared fields, using a
gradual binding approach to transform initial partial specifications of
singleton charts to complete specifications that are aligned and oriented
consistently. A novel aspect of our approach is its combination of
domain-agnostic elements with domain-specific information that is captured
through a domain-specific visualization prevalence design space. Our
implementation is applied to both synthetic data and real data from an Ebola
outbreak. We compare GEViTRec's output to what previous visualization
recommendation systems would generate, and to manually crafted visualizations
used by practitioners. We conducted formative evaluations with ten genEpi
experts to assess the relevance and interpretability of our results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1202.5025v1,The Price of Anarchy for Network Formation in an Adversary Model,"We study network formation with n players and link cost \alpha > 0. After the
network is built, an adversary randomly deletes one link according to a certain
probability distribution. Cost for player v incorporates the expected number of
players to which v will become disconnected. We show existence of equilibria
and a price of stability of 1+o(1) under moderate assumptions on the adversary
and n \geq 9.
  As the main result, we prove bounds on the price of anarchy for two special
adversaries: one removes a link chosen uniformly at random, while the other
removes a link that causes a maximum number of player pairs to be separated.
For unilateral link formation we show a bound of O(1) on the price of anarchy
for both adversaries, the constant being bounded by 10+o(1) and 8+o(1),
respectively. For bilateral link formation we show O(1+\sqrt{n/\alpha}) for one
adversary (if \alpha > 1/2), and \Theta(n) for the other (if \alpha > 2
considered constant and n \geq 9). The latter is the worst that can happen for
any adversary in this model (if \alpha = \Omega(1)). This points out
substantial differences between unilateral and bilateral link formation.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.14427v2,"Bayesian Surface Warping Approach For Rectifying Geological Boundaries
  Using Displacement Likelihood And Evidence From Geochemical Assays","This paper presents a Bayesian framework for manipulating mesh surfaces with
the aim of improving the positional integrity of the geological boundaries that
they seek to represent. The assumption is that these surfaces, created
initially using sparse data, capture the global trend and provide a reasonable
approximation of the stratigraphic, mineralisation and other types of
boundaries for mining exploration, but they are locally inaccurate at scales
typically required for grade estimation. The proposed methodology makes local
spatial corrections automatically to maximise the agreement between the
modelled surfaces and observed samples. Where possible, vertices on a mesh
surface are moved to provide a clear delineation, for instance, between ore and
waste material across the boundary based on spatial and compositional analysis;
using assay measurements collected from densely spaced, geo-registered blast
holes. The maximum a posteriori (MAP) solution ultimately considers the
chemistry observation likelihood in a given domain. Furthermore, it is guided
by an apriori spatial structure which embeds geological domain knowledge and
determines the likelihood of a displacement estimate. The results demonstrate
that increasing surface fidelity can significantly improve grade estimation
performance based on large-scale model validation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.00346v1,Minimum Viable Model Estimates for Machine Learning Projects,"Prioritization of machine learning projects requires estimates of both the
potential ROI of the business case and the technical difficulty of building a
model with the required characteristics. In this work we present a technique
for estimating the minimum required performance characteristics of a predictive
model given a set of information about how it will be used. This technique will
result in robust, objective comparisons between potential projects. The
resulting estimates will allow data scientists and managers to evaluate whether
a proposed machine learning project is likely to succeed before any modelling
needs to be done.
  The technique has been implemented into the open source application MinViME
(Minimum Viable Model Estimator) which can be installed via the PyPI python
package management system, or downloaded directly from the GitHub repository.
Available at https://github.com/john-hawkins/MinViME",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1909.01748v1,Probabilities in Session Types,"This paper deals with the probabilistic behaviours of distributed systems
described by a process calculus considering both probabilistic internal choices
and nondeterministic external choices. For this calculus we define and study a
typing system which extends the multiparty session types in order to deal also
with probabilistic behaviours. The calculus and its typing system are motivated
and illustrated by a running example.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1307.8211v1,Formal verification of a proof procedure for the description logic ALC,"Description Logics (DLs) are a family of languages used for the
representation and reasoning on the knowledge of an application domain, in a
structured and formal manner. In order to achieve this objective, several
provers, such as RACER and FaCT++, have been implemented, but these provers
themselves have not been yet certified. In order to ensure the soundness of
derivations in these DLs, it is necessary to formally verify the deductions
applied by these reasoners. Formal methods offer powerful tools for the
specification and verification of proof procedures, among them there are
methods for proving properties such as soundness, completeness and termination
of a proof procedure. In this paper, we present the definition of a proof
procedure for the Description Logic ALC, based on a semantic tableau method. We
ensure validity of our prover by proving its soundness, completeness and
termination properties using Isabelle proof assistant. The proof proceeds in
two phases, first by establishing these properties on an abstract level, and
then by instantiating them for an implementation based on lists.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1801.02592v1,"Simulations to Analyze Cellular Voting Systems for Side Effects of
  Democratic Redistricting","Motivated by the problem of partisan gerrymandering, we introduce an
electoral system for a representative democracy called democratic cellular
voting, designed to make modern packing and cracking strategies irrelevant by
allowing districts to be influenced directly by voters through elections. We
introduce an example of a democratic cellular voting system, called CV0, that
is suitable for dynamic modelling. We develop a modification of the theory of
discrete Markov chains using the algebraic structure of the semiring
$[0,\infty]$, which is used as a space of correlation coefficients. We use this
to measure voter preferences and model representatives, voters, and districts
in computationally feasible models with a guarantee of long-term stability.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0711.2087v1,Query Evaluation and Optimization in the Semantic Web,"We address the problem of answering Web ontology queries efficiently. An
ontology is formalized as a Deductive Ontology Base (DOB), a deductive database
that comprises the ontology's inference axioms and facts. A cost-based query
optimization technique for DOB is presented. A hybrid cost model is proposed to
estimate the cost and cardinality of basic and inferred facts. Cardinality and
cost of inferred facts are estimated using an adaptive sampling technique,
while techniques of traditional relational cost models are used for estimating
the cost of basic facts and conjunctive ontology queries. Finally, we implement
a dynamic-programming optimization algorithm to identify query evaluation plans
that minimize the number of intermediate inferred facts. We modeled a subset of
the Web ontology language OWL Lite as a DOB, and performed an experimental
study to analyze the predictive capacity of our cost model and the benefits of
the query optimization technique. Our study has been conducted over synthetic
and real-world OWL ontologies, and shows that the techniques are accurate and
improve query performance. To appear in Theory and Practice of Logic
Programming (TPLP).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1909.09429v4,A True AR Authoring Tool for Interactive Virtual Museums,"In this work, a new and innovative way of spatial computing that appeared
recently in the bibliography called True Augmented Reality (AR), is employed in
cultural heritage preservation. This innovation could be adapted by the Virtual
Museums of the future to enhance the quality of experience. It emphasises, the
fact that a visitor will not be able to tell, at a first glance, if the
artefact that he/she is looking at is real or not and it is expected to draw
the visitors' interest. True AR is not limited to artefacts but extends even to
buildings or life-sized character simulations of statues. It provides the best
visual quality possible so that the users will not be able to tell the real
objects from the augmented ones. Such applications can be beneficial for future
museums, as with True AR, 3D models of various exhibits, monuments, statues,
characters and buildings can be reconstructed and presented to the visitors in
a realistic and innovative way. We also propose our Virtual Reality Sample
application, a True AR playground featuring basic components and tools for
generating interactive Virtual Museum applications, alongside a 3D
reconstructed character (the priest of Asinou church) facilitating the
storyteller of the augmented experience.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.05633v1,"Enhanced Audit Techniques Empowered by the Reinforcement Learning
  Pertaining to IFRS 16 Lease","The purpose of accounting audit is to have clear understanding on the
financial activities of a company, which can be enhanced by machine learning or
reinforcement learning as numeric analysis better than manual analysis can be
made. For the purpose of assessment on the relevance, completeness and accuracy
of the information produced by entity pertaining to the newly implemented
International Financial Reporting Standard 16 Lease (IFRS 16) is one of such
candidates as its characteristic of requiring the understanding on the nature
of contracts and its complete analysis from listing up without omission, which
can be enhanced by the digitalization of contracts for the purpose of creating
the lists, still leaving the need of auditing cash flows of companies for the
possible omission due to the potential error at the stage of data collection,
especially for entities with various short or middle term business sites and
related leases, such as construction entities.
  The implementation of the reinforcement learning and its well-known code is
to be made for the purpose of drawing the possibility and utilizability of
interpreters from domain knowledge to numerical system, also can be called
'gamification interpreter' or 'numericalization interpreter' which can be
referred or compared to the extrapolation with nondimensional numbers, such as
Froude Number, in physics, which was a source of inspiration at this study.
Studies on the interpreters can be able to empower the utilizability of
artificial general intelligence in domain and commercial area.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1703.00037v1,Managing the Public to Manage Data: Citizen Science and Astronomy,"Citizen science projects recruit members of the public as volunteers to
process and produce datasets. These datasets must win the trust of the
scientific community. The task of securing credibility involves, in part,
applying standard scientific procedures to clean these datasets. However,
effective management of volunteer behavior also makes a significant
contribution to enhancing data quality. Through a case study of Galaxy Zoo, a
citizen science project set up to generate datasets based on volunteer
classifications of galaxy morphologies, this paper explores how those involved
in running the project manage volunteers. The paper focuses on how methods for
crediting volunteer contributions motivate volunteers to provide higher quality
contributions and to behave in a way that better corresponds to statistical
assumptions made when combining volunteer contributions into datasets. These
methods have made a significant contribution to the success of the project in
securing trust in these datasets, which have been well used by other
scientists. Implications for practice are then presented for citizen science
projects, providing a list of considerations to guide choices regarding how to
credit volunteer contributions to improve the quality and trustworthiness of
citizen science-produced datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1410.7091v1,On Some Distributed Disorder Detection,"Multivariate data sources with components of different information value seem
to appear frequently in practice. Models in which the components change their
homogeneity at different times are of significant importance. The fact whether
any changes are influential for the whole process is determined not only by the
moments of the change, but also depends on which coordinates. This is
particularly important in issues such as reliability analysis of complex
systems and the location of an intruder in surveillance systems. In this paper
we developed a mathematical model for such sources of signals with discrete
time having the Markov property given the times of change. The research also
comprises a multivariate detection of the transition probabilities changes at
certain sensitivity level in the multidimensional process. Additionally, the
observation of the random vector is depicted. Each chosen coordinate forms the
Markov process with different transition probabilities before and after some
unknown moment. The aim of statisticians is to estimate the moments based on
the observation of the process. The Bayesian approach is used with the risk
function depending on measure of chance of a false alarm and some cost of
overestimation. The moment of the system's disorder is determined by the
detection of transition probabilities changes at some coordinates. The overall
modeling of the critical coordinates is based on the simple game.",0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1404.1947v1,Eine entscheidbare Klasse n-stelliger Horn-Pr√§dikate,"Similar to a tree grammar, a Horn theory can be used to describe an infinite
set of terms. In this paper, we present a class of Horn theories such that the
set of definable predicates is closed wrt. conjunction and such that the
satisfiability of a predicate is decidable. This extends previous results on
Horn clauses with unary predicates.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.05451v1,"H2Opus: A distributed-memory multi-GPU software package for non-local
  operators","Hierarchical $\mathcal{H}^2$-matrices are asymptotically optimal
representations for the discretizations of non-local operators such as those
arising in integral equations or from kernel functions. Their $O(N)$ complexity
in both memory and operator application makes them particularly suited for
large-scale problems. As a result, there is a need for software that provides
support for distributed operations on these matrices to allow large-scale
problems to be represented. In this paper, we present high-performance,
distributed-memory GPU-accelerated algorithms and implementations for
matrix-vector multiplication and matrix recompression of hierarchical matrices
in the $\mathcal{H}^2$ format.
  The algorithms are a new module of H2Opus, a performance-oriented package
that supports a broad variety of $\mathcal{H}^2$-matrix operations on CPUs and
GPUs. Performance in the distributed GPU setting is achieved by marshaling the
tree data of the hierarchical matrix representation to allow batched kernels to
be executed on the individual GPUs. MPI is used for inter-process
communication. We optimize the communication data volume and hide much of the
communication cost with local compute phases of the algorithms. Results show
near-ideal scalability up to 1024 NVIDIA V100 GPUs on Summit, with performance
exceeding 2.3 Tflop/s/GPU for the matrix-vector multiplication, and 670
Gflops/s/GPU for matrix compression, which involves batched QR and SVD
operations.
  We illustrate the flexibility and efficiency of the library by solving a 2D
variable diffusivity integral fractional diffusion problem with an algebraic
multigrid-preconditioned Krylov solver and demonstrate scalability up to 16M
degrees of freedom problems on 64 GPUs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0710.0169v6,"Evaluation experiments on related terms search in Wikipedia: Information
  Content and Adapted HITS (In Russian)","The classification of metrics and algorithms search for related terms via
WordNet, Roget's Thesaurus, and Wikipedia was extended to include adapted HITS
algorithm. Evaluation experiments on Information Content and adapted HITS
algorithm are described. The test collection of Russian word pairs with
human-assigned similarity judgments is proposed.
  -----
  Klassifikacija metrik i algoritmov poiska semanticheski blizkih slov v
tezaurusah WordNet, Rozhe i jenciklopedii Vikipedija rasshirena adaptirovannym
HITS algoritmom. S pomow'ju jeksperimentov v Vikipedii oceneny metrika
Information Content i adaptirovannyj algoritm HITS. Predlozhen resurs dlja
ocenki semanticheskoj blizosti russkih slov.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1509.04040v1,Programs as proofs,"The Curry-Howard correspondence is about a relationship between types and
programs on the one hand and propositions and proofs on the other. The
implications for programming language design and program verification is an
active field of research.
  Transformer-like semantics of internal definitions that combine a defining
computation and an application will be presented. By specialisation for a given
defining computation one can derive inference rules for applications of defined
operations.
  With semantics of that kind for every operation, each application identifies
an axiom in a logic defined by the programming language, so a language can be
considered a theory.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9906014v1,Evaluation of the NLP Components of the OVIS2 Spoken Dialogue System,"The NWO Priority Programme Language and Speech Technology is a 5-year
research programme aiming at the development of spoken language information
systems. In the Programme, two alternative natural language processing (NLP)
modules are developed in parallel: a grammar-based (conventional, rule-based)
module and a data-oriented (memory-based, stochastic, DOP) module. In order to
compare the NLP modules, a formal evaluation has been carried out three years
after the start of the Programme. This paper describes the evaluation procedure
and the evaluation results. The grammar-based component performs much better
than the data-oriented one in this comparison.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0205061v1,"Aging, double helix and small world property in genetic algorithms","Over a quarter of century after the invention of genetic algorithms and
miriads of their modifications, as well as successful implementations, we are
still lacking many essential details of thorough analysis of it's inner
working. One of such fundamental questions is: how many generations do we need
to solve the optimization problem? This paper tries to answer this question,
albeit in a fuzzy way, making use of the double helix concept. As a byproduct
we gain better understanding of the ways, in which the genetic algorithm may be
fine tuned.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1311.2400v3,Look-Ahead Removal for Top-Down Tree Transducers,"Top-down tree transducers are a convenient formalism for describing tree
transformations. They can be equipped with regular look-ahead, which allows
them to inspect a subtree before processing it. In certain cases, such a
look-ahead can be avoided and the transformation can be realized by a
transducer without look-ahead. Removing the look-ahead from a transducer, if
possible, is technically highly challenging. For a restricted class of
transducers with look-ahead, namely those that are total, deterministic,
ultralinear, and bounded erasing, we present an algorithm that, for a given
transducer from that class, (1) decides whether it is equivalent to a total
deterministic transducer without look-ahead, and (2) constructs such a
transducer if the answer is positive. For the whole class of total
deterministic transducers with look-ahead we present a similar algorithm, which
assumes that a so-called difference bound is known for the given transducer.
The designer of a transducer can usually also determine a difference bound for
it.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.11097v1,Cut Locus Realizations on Convex Polyhedra,"We prove that every positively-weighted tree T can be realized as the cut
locus C(x) of a point x on a convex polyhedron P, with T weights matching C(x)
lengths. If T has n leaves, P has (in general) n+1 vertices. We show there are
in fact a continuum of polyhedra P each realizing T for some x on P. Three main
tools in the proof are properties of the star unfolding of P, Alexandrov's
gluing theorem, and a cut-locus partition lemma. The construction of P from T
is surprisingly simple.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.10720v2,A program logic for fresh name generation,"We present a program logic for Pitts and Stark's {\nu}-calculus, an extension
of the call-by-value simply-typed {\lambda}-calculus with a mechanism for the
generation of fresh names. Names can be compared for (in)-equality, producing
programs with subtle observable properties. Hidden names produced by
interactions between generation and abstraction are captured logically with a
second-order quantifier over type contexts. We illustrate usage of the logic
through reasoning about well-known difficult cases from the literature.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1809.04711v2,Linear Algebra and Duality of Neural Networks,"Bases, mappings, projections and metrics, natural for Neural network
training, are introduced. Graph-theoretical interpretation is offered.
Non-Gaussianity naturally emerges, even in relatively simple datasets. Training
statistics, hierarchies and energies are analyzed, from physics point of view.
Duality between observables (for example, pixels) and observations is
established. Relationship between exact and numerical solutions is studied.
Physics and financial mathematics interpretations of a key problem are offered.
Examples support all new concepts.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1811.01926v4,contextual: Evaluating Contextual Multi-Armed Bandit Problems in R,"Over the past decade, contextual bandit algorithms have been gaining in
popularity due to their effectiveness and flexibility in solving sequential
decision problems---from online advertising and finance to clinical trial
design and personalized medicine. At the same time, there are, as of yet,
surprisingly few options that enable researchers and practitioners to simulate
and compare the wealth of new and existing bandit algorithms in a standardized
way. To help close this gap between analytical research and empirical
evaluation the current paper introduces the object-oriented R package
""contextual"": a user-friendly and, through its object-oriented structure,
easily extensible framework that facilitates parallelized comparison of
contextual and context-free bandit policies through both simulation and offline
analysis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2007.01223v1,Verifiably Safe Exploration for End-to-End Reinforcement Learning,"Deploying deep reinforcement learning in safety-critical settings requires
developing algorithms that obey hard constraints during exploration. This paper
contributes a first approach toward enforcing formal safety constraints on
end-to-end policies with visual inputs. Our approach draws on recent advances
in object detection and automated reasoning for hybrid dynamical systems. The
approach is evaluated on a novel benchmark that emphasizes the challenge of
safely exploring in the presence of hard constraints. Our benchmark draws from
several proposed problem sets for safe learning and includes problems that
emphasize challenges such as reward signals that are not aligned with safety
constraints. On each of these benchmark problems, our algorithm completely
avoids unsafe behavior while remaining competitive at optimizing for as much
reward as is safe. We also prove that our method of enforcing the safety
constraints preserves all safe policies from the original environment.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.03560v3,"Fractional Reduced Differential Transform Method for
  Belousov-Zhabotinsky reaction model","In this paper, Belousov-Zhabotinsky (B-Z) reaction model with Caputo
fractional time derivative is investigated by the fractional reduced
differential transform method (FRDTM) methods, an iterative technique. The
outcome using FRDTM method reveals an efficiency with high accuracy and minimal
computations for numerical solutions. Moreover, the solution profiles which
demonstrate the behavior of the obtained result are presented.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.00443v3,"Quantum arithmetic operations based on quantum Fourier transform on
  signed integers","The quantum Fourier transform (QFT) brings efficiency in many respects,
especially usage of resource, for most operations on quantum computers. In this
study, the existing QFT-based and non-QFT-based quantum arithmetic operations
are examined. The capabilities of QFT-based addition and multiplication are
improved with some modifications. The proposed operations are compared with the
nearest quantum arithmetic operations. Furthermore, novel QFT-based
subtraction, division and exponentiation operations are presented. The proposed
arithmetic operations can perform nonmodular operations on all signed numbers
without any limitation by using less resources. In addition, novel quantum
circuits of two's complement, absolute value and comparison operations are also
presented by using the proposed QFT-based addition and subtraction operations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.06312v2,Matching Logic,"This paper presents matching logic, a first-order logic (FOL) variant for
specifying and reasoning about structure by means of patterns and pattern
matching. Its sentences, the patterns, are constructed using variables,
symbols, connectives and quantifiers, but no difference is made between
function and predicate symbols. In models, a pattern evaluates into a power-set
domain (the set of values that match it), in contrast to FOL where functions
and predicates map into a regular domain. Matching logic uniformly generalizes
several logical frameworks important for program analysis, such as:
propositional logic, algebraic specification, FOL with equality, modal logic,
and separation logic. Patterns can specify separation requirements at any level
in any program configuration, not only in the heaps or stores, without any
special logical constructs for that: the very nature of pattern matching is
that if two structures are matched as part of a pattern, then they can only be
spatially separated. Like FOL, matching logic can also be translated into pure
predicate logic with equality, at the same time admitting its own sound and
complete proof system. A practical aspect of matching logic is that FOL
reasoning with equality remains sound, so off-the-shelf provers and SMT solvers
can be used for matching logic reasoning. Matching logic is particularly
well-suited for reasoning about programs in programming languages that have an
operational semantics, but it is not limited to this.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0006018v1,"Accuracy, Coverage, and Speed: What Do They Mean to Users?","Speech is becoming increasingly popular as an interface modality, especially
in hands- and eyes-busy situations where the use of a keyboard or mouse is
difficult. However, despite the fact that many have hailed speech as being
inherently usable (since everyone already knows how to talk), most users of
speech input are left feeling disappointed by the quality of the interaction.
Clearly, there is much work to be done on the design of usable spoken
interfaces. We believe that there are two major problems in the design of
speech interfaces, namely, (a) the people who are currently working on the
design of speech interfaces are, for the most part, not interface designers and
therefore do not have as much experience with usability issues as we in the CHI
community do, and (b) speech, as an interface modality, has vastly different
properties than other modalities, and therefore requires different usability
measures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.04598v1,mpENC Multi-Party Encrypted Messaging Protocol design document,"This document is a technical overview and discussion of our work, a protocol
for secure group messaging. By secure we mean for the actual users i.e.
end-to-end security, as opposed to ""secure"" for irrelevant third parties.
  Our work provides everything needed to run a messaging session between real
users on top of a real transport protocol. That is, we specify not just a key
exchange, but when and how to run these relative to transport-layer events; how
to achieve liveness properties such as reliability and consistency, that are
time-sensitive and lie outside of the send-receive logic that cryptography-only
protocols often restrict themselves to; and offer suggestions for displaying
accurate (i.e. secure) but not overwhelming information in user interfaces.
  We aim towards a general-purpose unified protocol. In other words, we'd
prefer to avoid creating a completely new protocol merely to support
automation, or asynchronity, or a different transport protocol. This would add
complexity to the overall ecosystem of communications protocols. It is simply
unnecessary if the original protocol is designed well, as we have tried to do.
  That aim is not complete -- our full protocol system, as currently
implemented, is suitable only for use with certain instant messaging protocols.
However, we have tried to separate out conceptually-independent concerns, and
solve these individually using minimal assumptions even if other components
make extra assumptions. This means that many components of our full system can
be reused in future protocol extensions, and we know exactly which components
must be replaced in order to lift the existing constraints on our full system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1901.09069v1,Word Embeddings: A Survey,"This work lists and describes the main recent strategies for building
fixed-length, dense and distributed representations for words, based on the
distributional hypothesis. These representations are now commonly called word
embeddings and, in addition to encoding surprisingly good syntactic and
semantic information, have been proven useful as extra features in many
downstream NLP tasks.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.6341v1,Efficient Model Learning for Human-Robot Collaborative Tasks,"We present a framework for learning human user models from joint-action
demonstrations that enables the robot to compute a robust policy for a
collaborative task with a human. The learning takes place completely
automatically, without any human intervention. First, we describe the
clustering of demonstrated action sequences into different human types using an
unsupervised learning algorithm. These demonstrated sequences are also used by
the robot to learn a reward function that is representative for each type,
through the employment of an inverse reinforcement learning algorithm. The
learned model is then used as part of a Mixed Observability Markov Decision
Process formulation, wherein the human type is a partially observable variable.
With this framework, we can infer, either offline or online, the human type of
a new user that was not included in the training set, and can compute a policy
for the robot that will be aligned to the preference of this new user and will
be robust to deviations of the human actions from prior demonstrations. Finally
we validate the approach using data collected in human subject experiments, and
conduct proof-of-concept demonstrations in which a person performs a
collaborative task with a small industrial robot.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.04534v1,The Role of Cooperation in Responsible AI Development,"In this paper, we argue that competitive pressures could incentivize AI
companies to underinvest in ensuring their systems are safe, secure, and have a
positive social impact. Ensuring that AI systems are developed responsibly may
therefore require preventing and solving collective action problems between
companies. We note that there are several key factors that improve the
prospects for cooperation in collective action problems. We use this to
identify strategies to improve the prospects for industry cooperation on the
responsible development of AI.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/cs/0602018v1,"Improving the CSIEC Project and Adapting It to the English Teaching and
  Learning in China","In this paper after short review of the CSIEC project initialized by us in
2003 we present the continuing development and improvement of the CSIEC project
in details, including the design of five new Microsoft agent characters
representing different virtual chatting partners and the limitation of
simulated dialogs in specific practical scenarios like graduate job application
interview, then briefly analyze the actual conditions and features of its
application field: web-based English education in China. Finally we introduce
our efforts to adapt this system to the requirements of English teaching and
learning in China and point out the work next to do.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1210.0852v1,Detecting multiword phrases in mathematical text corpora,"We present an approach for detecting multiword phrases in mathematical text
corpora. The method used is based on characteristic features of mathematical
terminology. It makes use of a software tool named Lingo which allows to
identify words by means of previously defined dictionaries for specific word
classes as adjectives, personal names or nouns. The detection of multiword
groups is done algorithmically. Possible advantages of the method for indexing
and information retrieval and conclusions for applying dictionary-based methods
of automatic indexing instead of stemming procedures are discussed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.05689v3,Bidimensionality and Kernels,"Bidimensionality Theory was introduced by [E.D. Demaine, F.V. Fomin,
M.Hajiaghayi, and D.M. Thilikos. Subexponential parameterized algorithms on
graphs of bounded genus and H-minor-free graphs, J. ACM, 52 (2005),
pp.866--893] as a tool to obtain sub-exponential time parameterized algorithms
on H-minor-free graphs. In [E.D. Demaine and M.Hajiaghayi, Bidimensionality:
new connections between FPT algorithms and PTASs, in Proceedings of the 16th
Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), SIAM, 2005,
pp.590--601] this theory was extended in order to obtain polynomial time
approximation schemes (PTASs) for bidimensional problems. In this work, we
establish a third meta-algorithmic direction for bidimensionality theory by
relating it to the existence of linear kernels for parameterized problems. In
particular, we prove that every minor (respectively contraction) bidimensional
problem that satisfies a separation property and is expressible in Countable
Monadic Second Order Logic (CMSO), admits a linear kernel for classes of graphs
that exclude a fixed graph (respectively an apex graph) H as a minor. Our
results imply that a multitude of bidimensional problems g graph classes. For
most of these problems no polynomial kernels on H-minor-free graphs were known
prior to our work.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0811.3116v1,"Geometric properties of satisfying assignments of random
  $Œµ$-1-in-k SAT","We study the geometric structure of the set of solutions of random
$\epsilon$-1-in-k SAT problem. For $l\geq 1$, two satisfying assignments $A$
and $B$ are $l$-connected if there exists a sequence of satisfying assignments
connecting them by changing at most $l$ bits at a time.
  We first prove that w.h.p. two assignments of a random $\epsilon$-1-in-$k$
SAT instance are $O(\log n)$-connected, conditional on being satisfying
assignments. Also, there exists $\epsilon_{0}\in (0,\frac{1}{k-2})$ such that
w.h.p. no two satisfying assignments at distance at least $\epsilon_{0}\cdot n$
form a ""hole"" in the set of assignments. We believe that this is true for all
$\epsilon >0$, and thus satisfying assignments of a random 1-in-$k$ SAT
instance form a single cluster.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1102.0989v2,Axiomatic Attribution for Multilinear Functions,"We study the attribution problem, that is, the problem of attributing a
change in the value of a characteristic function to its independent variables.
We make three contributions. First, we propose a formalization of the problem
based on a standard cost sharing model. Second, we show that there is a unique
attribution method that satisfies Dummy, Additivity, Conditional Nonnegativity,
Affine Scale Invariance, and Anonymity for all characteristic functions that
are the sum of a multilinear function and an additive function. We term this
the Aumann-Shapley-Shubik method. Conversely, we show that such a uniqueness
result does not hold for characteristic functions outside this class. Third, we
study multilinear characteristic functions in detail; we describe a
computationally efficient implementation of the Aumann-Shapley-Shubik method
and discuss practical applications to pay-per-click advertising and portfolio
analysis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2112.07395v1,Handwritten text generation and strikethrough characters augmentation,"We introduce two data augmentation techniques, which, used with a
Resnet-BiLSTM-CTC network, significantly reduce Word Error Rate (WER) and
Character Error Rate (CER) beyond best-reported results on handwriting text
recognition (HTR) tasks. We apply a novel augmentation that simulates
strikethrough text (HandWritten Blots) and a handwritten text generation method
based on printed text (StackMix), which proved to be very effective in HTR
tasks. StackMix uses weakly-supervised framework to get character boundaries.
Because these data augmentation techniques are independent of the network used,
they could also be applied to enhance the performance of other networks and
approaches to HTR. Extensive experiments on ten handwritten text datasets show
that HandWritten Blots augmentation and StackMix significantly improve the
quality of HTR models",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1507.04578v1,Accessible Banking: Experiences and Future Directions,"This is a short position paper drawing on experience working with the UK
banking industry and their disabled and ageing customers in the Business
Disability Forum, a UK non-profit member organisation funded by a large body of
UK private and public sector businesses. We describe some commonly reported
problems of disabled customers who use modern banking technologies, relating
them to UK law and best practice. We describe some of the recent banking
industry innovations and the hope they may offer for improved inclusive and
accessible multi-channel banking.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2002.08777v1,"Do you comply with AI? -- Personalized explanations of learning
  algorithms and their impact on employees' compliance behavior","Machine Learning algorithms are technological key enablers for artificial
intelligence (AI). Due to the inherent complexity, these learning algorithms
represent black boxes and are difficult to comprehend, therefore influencing
compliance behavior. Hence, compliance with the recommendations of such
artifacts, which can impact employees' task performance significantly, is still
subject to research - and personalization of AI explanations seems to be a
promising concept in this regard. In our work, we hypothesize that, based on
varying backgrounds like training, domain knowledge and demographic
characteristics, individuals have different understandings and hence mental
models about the learning algorithm. Personalization of AI explanations,
related to the individuals' mental models, may thus be an instrument to affect
compliance and therefore employee task performance. Our preliminary results
already indicate the importance of personalized explanations in industry
settings and emphasize the importance of this research endeavor.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2105.12037v1,"Information algebras of coherent sets of gambles in general possibility
  spaces","In this paper, we show that coherent sets of gambles can be embedded into the
algebraic structure of information algebra. This leads firstly, to a new
perspective of the algebraic and logical structure of desirability and
secondly, it connects desirability, hence imprecise probabilities, to other
formalism in computer science sharing the same underlying structure. Both the
domain-free and the labeled view of the information algebra of coherent sets of
gambles are presented, considering general possibility spaces.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.00935v1,Toward equipping Artificial Moral Agents with multiple ethical theories,"Artificial Moral Agents (AMA's) is a field in computer science with the
purpose of creating autonomous machines that can make moral decisions akin to
how humans do. Researchers have proposed theoretical means of creating such
machines, while philosophers have made arguments as to how these machines ought
to behave, or whether they should even exist. Of the currently theorised AMA's,
all research and design has been done with either none or at most one specified
normative ethical theory as basis. This is problematic because it narrows down
the AMA's functional ability and versatility which in turn causes moral
outcomes that a limited number of people agree with (thereby undermining an
AMA's ability to be moral in a human sense). As solution we design a
three-layer model for general normative ethical theories that can be used to
serialise the ethical views of people and businesses for an AMA to use during
reasoning. Four specific ethical norms (Kantianism, divine command theory,
utilitarianism, and egoism) were modelled and evaluated as proof of concept for
normative modelling. Furthermore, all models were serialised to XML/XSD as
proof of support for computerisation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1501.05983v1,A new efficient Matching method for web services substitution,"The internet is considered as the most extensive market in the world. To keep
its gradual reputation, it must confront real problems that result from its
distribution and from the diversity of the protocols used to insure
communications. The Web service technology has diminished significantly the
effects of distribution and heterogeneity, but there are several problems that
weaken their performance (unavailability, load increase of use, high cost of
CPU time...). Faced with this situation, we are forced to move in the direction
of the substitution of web services. In this context, we propose an effective
technique of substitution based on a new method of matching that allows
detecting and expressing the matching between the web services pairwise by
considering that each of them is ontology. Also, our method performs a
discovery of the most similar web service to that to be replaced by using an
efficient method of similarity measurement.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1309.1761v1,"Convergence of Nearest Neighbor Pattern Classification with Selective
  Sampling","In the panoply of pattern classification techniques, few enjoy the intuitive
appeal and simplicity of the nearest neighbor rule: given a set of samples in
some metric domain space whose value under some function is known, we estimate
the function anywhere in the domain by giving the value of the nearest sample
per the metric. More generally, one may use the modal value of the m nearest
samples, where m is a fixed positive integer (although m=1 is known to be
admissible in the sense that no larger value is asymptotically superior in
terms of prediction error). The nearest neighbor rule is nonparametric and
extremely general, requiring in principle only that the domain be a metric
space. The classic paper on the technique, proving convergence under
independent, identically-distributed (iid) sampling, is due to Cover and Hart
(1967). Because taking samples is costly, there has been much research in
recent years on selective sampling, in which each sample is selected from a
pool of candidates ranked by a heuristic; the heuristic tries to guess which
candidate would be the most ""informative"" sample. Lindenbaum et al. (2004)
apply selective sampling to the nearest neighbor rule, but their approach
sacrifices the austere generality of Cover and Hart; furthermore, their
heuristic algorithm is complex and computationally expensive. Here we report
recent results that enable selective sampling in the original Cover-Hart
setting. Our results pose three selection heuristics and prove that their
nearest neighbor rule predictions converge to the true pattern. Two of the
algorithms are computationally cheap, with complexity growing linearly in the
number of samples. We believe that these results constitute an important
advance in the art.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1006.5038v1,"Algorithmic Solutions for Several Offline Constrained Resource
  Processing and Data Transfer Multicriteria Optimization Problems","In this paper we present novel algorithmic solutions for several resource
processing and data transfer multicriteria optimization problems. The results
of most of the presented techniques are strategies which solve the considered
problems (almost) optimally. Thus, the developed algorithms construct
intelligent strategies which can be implemented by agents in specific
situations. All the described solutions make use of the properties of the
considered problems and, thus, they are not applicable to a very general class
of problems. However, by considering the specific details of each problem, we
were able to obtain very efficient results.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.14693v2,AM-RRT*: Informed Sampling-based Planning with Assisting Metric,"In this paper, we present a new algorithm that extends RRT* and RT-RRT* for
online path planning in complex, dynamic environments. Sampling-based
approaches often perform poorly in environments with narrow passages, a feature
common to many indoor applications of mobile robots as well as computer games.
Our method extends RRT-based sampling methods to enable the use of an assisting
distance metric to improve performance in environments with obstacles. This
assisting metric, which can be any metric that has better properties than the
Euclidean metric when line of sight is blocked, is used in combination with the
standard Euclidean metric in such a way that the algorithm can reap benefits
from the assisting metric while maintaining the desirable properties of
previous RRT variants - namely probabilistic completeness in tree coverage and
asymptotic optimality in path length. We also introduce a new method of
targeted rewiring, aimed at shortening search times and path lengths in tasks
where the goal shifts repeatedly. We demonstrate that our method offers
considerable improvements over existing multi-query planners such as RT-RRT*
when using diffusion distance as an assisting metric; finding near-optimal
paths with a decrease in search time of several orders of magnitude.
Experimental results show planning times reduced by 99.5% and path lengths by
9.8% over existing real-time RRT planners in a variety of environments.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1208.5451v1,"Are You Imitating Me? Unsupervised Sparse Modeling for Group Activity
  Analysis from a Single Video","A framework for unsupervised group activity analysis from a single video is
here presented. Our working hypothesis is that human actions lie on a union of
low-dimensional subspaces, and thus can be efficiently modeled as sparse linear
combinations of atoms from a learned dictionary representing the action's
primitives. Contrary to prior art, and with the primary goal of spatio-temporal
action grouping, in this work only one single video segment is available for
both unsupervised learning and analysis without any prior training information.
After extracting simple features at a single spatio-temporal scale, we learn a
dictionary for each individual in the video during each short time lapse. These
dictionaries allow us to compare the individuals' actions by producing an
affinity matrix which contains sufficient discriminative information about the
actions in the scene leading to grouping with simple and efficient tools. With
diverse publicly available real videos, we demonstrate the effectiveness of the
proposed framework and its robustness to cluttered backgrounds, changes of
human appearance, and action variability.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0401027v1,ClassdescMP: Easy MPI programming in C++,"ClassdescMP is a distributed memory parallel programming system for use with
C++ and MPI. It uses the Classdesc reflection system to ease the task of
building complicated messages to be sent between processes. It doesn't hide the
underlying MPI API, so it is an augmentation of MPI capabilities. Users can
still call standard MPI function calls if needed for performance reasons.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.00872v1,"Convergence Analysis and System Design for Federated Learning over
  Wireless Networks","Federated learning (FL) has recently emerged as an important and promising
learning scheme in IoT, enabling devices to jointly learn a model without
sharing their raw data sets. However, as the training data in FL is not
collected and stored centrally, FL training requires frequent model exchange,
which is largely affected by the wireless communication network. Therein,
limited bandwidth and random package loss restrict interactions in training.
Meanwhile, the insufficient message synchronization among distributed clients
could also affect FL convergence. In this paper, we analyze the convergence
rate of FL training considering the joint impact of communication network and
training settings. Further by considering the training costs in terms of time
and power, the optimal scheduling problems for communication networks are
formulated. The developed theoretical results can be used to assist the system
parameter selections and explain the principle of how the wireless
communication system could influence the distributed training process and
network scheduling.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0304020v2,A direct sum theorem in communication complexity via message compression,"We prove lower bounds for the direct sum problem for two-party bounded error
randomised multiple-round communication protocols. Our proofs use the notion of
information cost of a protocol, as defined by Chakrabarti, Shi, Wirth and Yao
and refined further by Bar-Yossef, Jayram, Kumar and Sivakumar. Our main
technical result is a `compression' theorem saying that, for any probability
distribution $\mu$ over the inputs, a $k$-round private coin bounded error
protocol for a function $f$ with information cost $c$ can be converted into a
$k$-round deterministic protocol for $f$ with bounded distributional error and
communication cost $O(kc)$. We prove this result using a substate theorem about
relative entropy and a rejection sampling argument. Our direct sum result
follows from this `compression' result via elementary information theoretic
arguments.
  We also consider the direct sum problem in quantum communication. Using a
probabilistic argument, we show that messages cannot be compressed in this
manner even if they carry small information. Hence, new techniques may be
necessary to tackle the direct sum problem in quantum communication.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1904.05540v1,Privacy protocols,"Security protocols enable secure communication over insecure channels.
Privacy protocols enable private interactions over secure channels. Security
protocols set up secure channels using cryptographic primitives. Privacy
protocols set up private channels using secure channels. But just like some
security protocols can be broken without breaking the underlying cryptography,
some privacy protocols can be broken without breaking the underlying security.
Such privacy attacks have been used to leverage e-commerce against targeted
advertising from the outset; but their depth and scope became apparent only
with the overwhelming advent of influence campaigns in politics. The blurred
boundaries between privacy protocols and privacy attacks present a new
challenge for protocol analysis. Covert channels turn out to be concealed not
only below overt channels, but also above: subversions, and the level-below
attacks are supplemented by sublimations and the level-above attacks.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0
http://arxiv.org/abs/0911.2619v1,Active Flows in Diagnostic of Troubleshooting on Backbone Links,"This paper aims to identify the operational region of a link in terms of its
utilization and alert operators at the point where the link becomes overloaded
and requires a capacity upgrade. The number of active flows is considered the
real network state and is proposed to use a proxy for utilization. The Gaussian
approximation gives the expression for the confidence interval on an
operational region. The easy rule has been formulated to display the network
defects by means of measurements of router loading and number of active flows.
Mean flow performance is considered as the basic universal index characterized
quality of network services provided to single user.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1912.08786v1,Why we need an AI-resilient society,"Artificial intelligence is considered as a key technology. It has a huge
impact on our society. Besides many positive effects, there are also some
negative effects or threats. Some of these threats to society are well-known,
e.g., weapons or killer robots. But there are also threats that are ignored.
These unknown-knowns or blind spots affect privacy, and facilitate manipulation
and mistaken identities. We cannot trust data, audio, video, and identities any
more. Democracies are able to cope with known threats, the known-knowns.
Transforming unknown-knowns to known-knowns is one important cornerstone of
resilient societies. An AI-resilient society is able to transform threats
caused by new AI tecchnologies such as generative adversarial networks.
Resilience can be seen as a positive adaptation of these threats. We propose
three strategies how this adaptation can be achieved: awareness, agreements,
and red flags. This article accompanies the TEDx talk ""Why we urgently need an
AI-resilient society"", see https://youtu.be/f6c2ngp7rqY.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1102.1111v1,Treelicious: a System for Semantically Navigating Tagged Web Pages,"Collaborative tagging has emerged as a popular and effective method for
organizing and describing pages on the Web. We present Treelicious, a system
that allows hierarchical navigation of tagged web pages. Our system enriches
the navigational capabilities of standard tagging systems, which typically
exploit only popularity and co-occurrence data. We describe a prototype that
leverages the Wikipedia category structure to allow a user to semantically
navigate pages from the Delicious social bookmarking service. In our system a
user can perform an ordinary keyword search and browse relevant pages but is
also given the ability to broaden the search to more general topics and narrow
it to more specific topics. We show that Treelicious indeed provides an
intuitive framework that allows for improved and effective discovery of
knowledge.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.07791v1,DFM: A Performance Baseline for Deep Feature Matching,"A novel image matching method is proposed that utilizes learned features
extracted by an off-the-shelf deep neural network to obtain a promising
performance. The proposed method uses pre-trained VGG architecture as a feature
extractor and does not require any additional training specific to improve
matching. Inspired by well-established concepts in the psychology area, such as
the Mental Rotation paradigm, an initial warping is performed as a result of a
preliminary geometric transformation estimate. These estimates are simply based
on dense matching of nearest neighbors at the terminal layer of VGG network
outputs of the images to be matched. After this initial alignment, the same
approach is repeated again between reference and aligned images in a
hierarchical manner to reach a good localization and matching performance. Our
algorithm achieves 0.57 and 0.80 overall scores in terms of Mean Matching
Accuracy (MMA) for 1 pixel and 2 pixels thresholds respectively on Hpatches
dataset, which indicates a better performance than the state-of-the-art.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1804.06601v1,Independent Distributions on a Multi-Branching AND-OR Tree of Height 2,"We investigate an AND-OR tree T and a probability distribution d on the truth
assignments to the leaves. Tarsi (1983) showed that if d is an independent and
identical distribution (IID) such that probability of a leaf having value 0 is
neither 0 nor 1 then, under a certain assumptions, there exists an optimal
algorithm that is depth-first. We investigate the case where d is an
independent distribution (ID) and probability depends on each leaf. It is known
that in this general case, if height is greater than or equal to 3, Tarsi-type
result does not hold. It is also known that for a complete binary tree of
height 2, Tarsi-type result certainly holds. In this paper, we ask whether
Tarsi-type result holds for an AND-OR tree of height 2. Here, a child node of
the root is either an OR-gate or a leaf: The number of child nodes of an
internal node is arbitrary, and depends on an internal node. We give an
affirmative answer. Our strategy of the proof is to reduce the problem to the
case of directional algorithms. We perform induction on the number of leaves,
and modify Tarsi's method to suite height 2 trees. We discuss why our proof
does not apply to height 3 trees.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0502066v1,On the Complexity of Real Functions,"We develop a notion of computability and complexity of functions over the
reals, which seems to be very natural when one tries to determine just how
""difficult"" a certain function is. This notion can be viewed as an extension of
both BSS computability [Blum, Cucker, Shub, Smale 1998], and bit computability
in the tradition of computable analysis [Weihrauch 2000] as it relies on the
latter but allows some discontinuities and multiple values.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.06981v2,"FlexiTerm: A more efficient implementation of flexible multi-word term
  recognition","Terms are linguistic signifiers of domain-specific concepts. Automated
recognition of multi-word terms (MWT) in free text is a sequence labelling
problem, which is commonly addressed using supervised machine learning methods.
Their need for manual annotation of training data makes it difficult to port
such methods across domains. FlexiTerm, on the other hand, is a fully
unsupervised method for MWT recognition from domain-specific corpora.
Originally implemented in Java as a proof of concept, it did not scale well,
thus offering little practical value in the context of big data. In this paper,
we describe its re-implementation in Python and compare the performance of
these two implementations. The results demonstrated major improvements in terms
of efficiency, which allow FlexiTerm to transition from the proof of concept to
the production-grade application.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.08844v1,"Brain Tumor Classification Using Deep Learning Technique -- A Comparison
  between Cropped, Uncropped, and Segmented Lesion Images with Different Sizes","Deep Learning is the newest and the current trend of the machine learning
field that paid a lot of the researchers' attention in the recent few years. As
a proven powerful machine learning tool, deep learning was widely used in
several applications for solving various complex problems that require
extremely high accuracy and sensitivity, particularly in the medical field. In
general, brain tumor is one of the most common and aggressive malignant tumor
diseases which is leading to a very short expected life if it is diagnosed at
higher grade. Based on that, brain tumor grading is a very critical step after
detecting the tumor in order to achieve an effective treating plan. In this
paper, we used Convolutional Neural Network (CNN) which is one of the most
widely used deep learning architectures for classifying a dataset of 3064 T1
weighted contrast-enhanced brain MR images for grading (classifying) the brain
tumors into three classes (Glioma, Meningioma, and Pituitary Tumor). The
proposed CNN classifier is a powerful tool and its overall performance with
accuracy of 98.93% and sensitivity of 98.18% for the cropped lesions, while the
results for the uncropped lesions are 99% accuracy and 98.52% sensitivity and
the results for segmented lesion images are 97.62% for accuracy and 97.40%
sensitivity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.08852v1,Let's See Clearly: Contaminant Artifact Removal for Moving Cameras,"Contaminants such as dust, dirt and moisture adhering to the camera lens can
greatly affect the quality and clarity of the resulting image or video. In this
paper, we propose a video restoration method to automatically remove these
contaminants and produce a clean video. Our approach first seeks to detect
attention maps that indicate the regions that need to be restored. In order to
leverage the corresponding clean pixels from adjacent frames, we propose a flow
completion module to hallucinate the flow of the background scene to the
attention regions degraded by the contaminants. Guided by the attention maps
and completed flows, we propose a recurrent technique to restore the input
frame by fetching clean pixels from adjacent frames. Finally, a multi-frame
processing stage is used to further process the entire video sequence in order
to enforce temporal consistency. The entire network is trained on a synthetic
dataset that approximates the physical lighting properties of contaminant
artifacts. This new dataset and our novel framework lead to our method that is
able to address different contaminants and outperforms competitive restoration
approaches both qualitatively and quantitatively.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1712.04076v2,In a Nutshell -- The Sequential Parameter Optimization Toolbox,"The performance of optimization algorithms relies crucially on their
parameterizations. Finding good parameter settings is called algorithm tuning.
The sequential parameter optimization (SPOT) package for R is a toolbox for
tuning and understanding simulation and optimization algorithms. Model-based
investigations are common approaches in simulation and optimization. Sequential
parameter optimization has been developed, because there is a strong need for
sound statistical analysis of simulation and optimization algorithms. SPOT
includes methods for tuning based on classical regression and analysis of
variance techniques; tree-based models such as CART and random forest; Gaussian
process models (Kriging), and combinations of different meta-modeling
approaches. Using a simple simulated annealing algorithm, we will demonstrate
how optimization algorithms can be tuned using SPOT. The underling concepts of
the SPOT approach are explained. This includes key techniques such as
exploratory fitness landscape analysis and sensititvity analysis. Many examples
illustrate how SPOT can be used for understanding the performance of algorithms
and gaining insight into algorithm's behavior. Furthermore, we demonstrate how
SPOT can be used as an optimizer and how a sophisticated ensemble approach is
able to combine several meta models via stacking. This article exemplifies how
SPOT can be used for automatic and interactive tuning.",1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0407028v1,Effects of Language Modeling on Speech-driven Question Answering,"We integrate automatic speech recognition (ASR) and question answering (QA)
to realize a speech-driven QA system, and evaluate its performance. We adapt an
N-gram language model to natural language questions, so that the input of our
system can be recognized with a high accuracy. We target WH-questions which
consist of the topic part and fixed phrase used to ask about something. We
first produce a general N-gram model intended to recognize the topic and
emphasize the counts of the N-grams that correspond to the fixed phrases. Given
a transcription by the ASR engine, the QA engine extracts the answer candidates
from target documents. We propose a passage retrieval method robust against
recognition errors in the transcription. We use the QA test collection produced
in NTCIR, which is a TREC-style evaluation workshop, and show the effectiveness
of our method by means of experiments.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.11465v1,"Hybridization of interval methods and evolutionary algorithms for
  solving difficult optimization problems","Reliable global optimization is dedicated to finding a global minimum in the
presence of rounding errors. The only approaches for achieving a numerical
proof of global optimality are interval branch and bound methods that
interleave branching of the search-space and pruning of the subdomains that
cannot contain an optimal solution. It is of the utmost importance: i) to
compute sharp enclosures of the objective function and the constraints on a
given subdomain; ii) to find a good approximation (an upper bound) of the
global minimum.
  State-of-the-art solvers are generally integrative methods, that is they
embed local optimization algorithms to compute a good upper bound of the global
minimum over each subspace. In this document, we propose a cooperative
framework in which interval methods cooperate with evolutionary algorithms. The
latter are stochastic algorithms in which a population of candidate solutions
iteratively evolves in the search-space to reach satisfactory solutions.
Evolutionary algorithms, endowed with operators that help individuals escape
from local minima, are particularly suited for difficult problems on which
traditional methods struggle to converge.
  Within our cooperative solver Charibde, the evolutionary algorithm and the
interval-based algorithm run in parallel and exchange bounds, solutions and
search-space via message passing. A novel strategy prevents premature
convergence toward local minima. A comparison of Charibde with state-of-the-art
solvers (GlobSol, IBBA, Ibex) on a benchmark of difficult problems shows that
Charibde converges faster by an order of magnitude. New optimality results are
provided for five multimodal problems, for which few solutions were available
in the literature. Finally, we provide the first numerical proof of optimality
for the open Lennard-Jones cluster problem with five atoms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.06667v1,"Understanding the Information Needs and Practices of Human Supporters of
  an Online Mental Health Intervention to Inform Machine Learning Applications","In the context of digital therapy interventions, such as internet-delivered
Cognitive Behavioral Therapy (iCBT) for the treatment of depression and
anxiety, extensive research has shown how the involvement of a human supporter
or coach, who assists the person undergoing treatment, improves user engagement
in therapy and leads to more effective health outcomes than unsupported
interventions. Seeking to maximize the effects and outcomes of this human
support, the research investigates how new opportunities provided through
recent advances in the field of AI and machine learning (ML) can contribute
useful data insights to effectively support the work practices of iCBT
supporters. This paper reports detailed findings of an interview study with 15
iCBT supporters that deepens understanding of their existing work practices and
information needs with the aim to meaningfully inform the development of
useful, implementable ML applications particularly in the context of iCBT
treatment for depression and anxiety. The analysis contributes (1) a set of six
themes that summarize the strategies and challenges that iCBT supporters
encounter in providing effective, personalized feedback to their mental health
clients; and in response to these learnings, (2) presents for each theme
concrete opportunities for how methods of ML could help support and address
identified challenges and information needs. It closes with reflections on
potential social, emotional and pragmatic implications of introducing new
machine-generated data insights within supporter-led client review practices.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1302.6900v2,"Exploiting Independent Subformulas: A Faster Approximation Scheme for
  #k-SAT","We present an improvement on Thurley's recent randomized approximation scheme
for #k-SAT where the task is to count the number of satisfying truth
assignments of a Boolean function {\Phi} given as an n-variable k-CNF. We
introduce a novel way to identify independent substructures of {\Phi} and can
therefore reduce the size of the search space considerably. Our randomized
algorithm works for any k. For #3-SAT, it runs in time
O(\epsilon^{-2}*1.51426^n), for #4-SAT, it runs in time
O(\epsilon^{-2}*1.60816^n), with error bound \epsilon.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.01050v3,"Implicit automata in typed $Œª$-calculi II: streaming transducers
  vs categorical semantics","We characterize regular string transductions as programs in a linear
$\lambda$-calculus with additives. One direction of this equivalence is proved
by encoding copyless streaming string transducers (SSTs), which compute regular
functions, into our $\lambda$-calculus. For the converse, we consider a
categorical framework for defining automata and transducers over words, which
allows us to relate register updates in SSTs to the semantics of the linear
$\lambda$-calculus in a suitable monoidal closed category. To illustrate the
relevance of monoidal closure to automata theory, we also leverage this notion
to give abstract generalizations of the arguments showing that copyless SSTs
may be determinized and that the composition of two regular functions may be
implemented by a copyless SST. Our main result is then generalized from strings
to trees using a similar approach. In doing so, we exhibit a connection between
a feature of streaming tree transducers and the multiplicative/additive
distinction of linear logic.
  Keywords: MSO transductions, implicit complexity, Dialectica categories,
Church encodings",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.13204v1,Graph2Plan: Learning Floorplan Generation from Layout Graphs,"We introduce a learning framework for automated floorplan generation which
combines generative modeling using deep neural networks and user-in-the-loop
designs to enable human users to provide sparse design constraints. Such
constraints are represented by a layout graph. The core component of our
learning framework is a deep neural network, Graph2Plan, which converts a
layout graph, along with a building boundary, into a floorplan that fulfills
both the layout and boundary constraints. Given an input building boundary, we
allow a user to specify room counts and other layout constraints, which are
used to retrieve a set of floorplans, with their associated layout graphs, from
a database. For each retrieved layout graph, along with the input boundary,
Graph2Plan first generates a corresponding raster floorplan image, and then a
refined set of boxes representing the rooms. Graph2Plan is trained on RPLAN, a
large-scale dataset consisting of 80K annotated floorplans. The network is
mainly based on convolutional processing over both the layout graph, via a
graph neural network (GNN), and the input building boundary, as well as the
raster floorplan images, via conventional image convolution.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.10603v1,"Rotation-constrained optical see-through headset calibration
  withbare-hand alignment","The inaccessibility of user-perceived reality remains an open issue in
pursuing the accurate calibration of optical see-through (OST) head-mounted
displays (HMDs). Manual user alignment is usually required to collect a set of
virtual-to-real correspondences, so that a default or an offline display
calibration can be updated to account for the user's eye position(s). Current
alignment-based calibration procedures usually require point-wise alignments
between rendered image point(s) and associated physical landmark(s) of a target
calibration tool. As each alignment can only provide one or a few
correspondences, repeated alignments are required to ensure calibration
quality.
  This work presents an accurate and tool-less online OST calibration method to
update an offline-calibrated eye-display model. The user's bare hand is
markerlessly tracked by a commercial RGBD camera anchored to the OST headset to
generate a user-specific cursor for correspondence collection. The required
alignment is object-wise, and can provide thousands of unordered corresponding
points in tracked space. The collected correspondences are registered by a
proposed rotation-constrained iterative closest point (rcICP) method to
optimise the viewpoint-related calibration parameters. We implemented such a
method for the Microsoft HoloLens 1. The resiliency of the proposed procedure
to noisy data was evaluated through simulated tests and real experiments
performed with an eye-replacement camera. According to the simulation test, the
rcICP registration is robust against possible user-induced rotational
misalignment. With a single alignment, our method achieves 8.81 arcmin (1.37
mm) positional error and 1.76 degree rotational error by camera-based tests in
the arm-reach distance, and 10.79 arcmin (7.71 pixels) reprojection error by
user tests.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1910.13988v1,"Auto-Annotation Quality Prediction for Semi-Supervised Learning with
  Ensembles","Auto-annotation by ensemble of models is an efficient method of learning on
unlabeled data. Wrong or inaccurate annotations generated by the ensemble may
lead to performance degradation of the trained model. To deal with this problem
we propose filtering the auto-labeled data using a trained model that predicts
the quality of the annotation from the degree of consensus between ensemble
models. Using semantic segmentation as an example, we show the advantage of the
proposed auto-annotation filtering over training on data contaminated with
inaccurate labels.
  Moreover, our experimental results show that in the case of semantic
segmentation, the performance of a state-of-the-art model can be achieved by
training it with only a fraction (30$\%$) of the original manually labeled data
set, and replacing the rest with the auto-annotated, quality filtered labels.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1808.05342v1,Formalisation of a frame stack semantics for a Java-like language,"We present a Coq formalisation of the small-step operational semantics of
Jafun, a small Java-like language with classes. This format of semantics makes
it possible to naturally specify and prove invariants that should hold at each
computation step. In contrast to the Featherweight Java approach the semantics
explicitly manipulates frame stack of method calls. Thanks to that one can
express properties of computation that depend on execution of particular
methods.
  On the basis of the semantics, we developed a type system that makes it
possible to delineate a notion of a compound value and classify certain methods
as extensional functions operating on them. In our formalisation we make a
mechanised proof that the operational semantics for the untyped version of the
semantics agrees with the one for the typed one. We discuss different methods
to make such formalisation effort and provide experiments that substantiate it.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.09804v1,Capacity Achieving Uncoded PIR Protocol based on Combinatorial Designs,"In this paper we study the problem of private information retrieval where a
user seeks to retrieve one of the $F$ files from a cluster of $N$ non-colluding
servers without revealing the identity of the requested file. In our setting
the servers are storage constrained in that they can only store a fraction
$\mu=t/N$ of each file. Furthermore, we assume that the files are stored in an
uncoded fashion. The rate of a PIR protocol is defined as the ratio of the file
size and the total number of bits downloaded. The maximum achievable rate is
referred to as capacity. It was previously shown that there are capacity
achieving PIR protocols when the file size is $N^F$ and complete files were
stored on all the servers. These results were further extended for the case
when servers store only a fraction of each file. However, the subpacketization
$v$ of the files required is exponential in the number of servers $N$. We
propose a novel uncoded PIR protocol based on combinatorial designs that are
also capacity achieving when the file size is $v \times t^F$. Our protocol has
linear subpacketization in the number of servers in contrast to previous work
in storage constrained uncoded PIR schemes. In the proposed PIR protocol, the
given system is projected to multiple instances of reduced systems with
replicated servers having full storage capacity. The subfiles stored in these
various instances are separately retrieved and lifted to solve the PIR problem
for the original system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1112.4703v1,Abstracting Path Conditions for Effective Symbolic Execution,"We present an algorithm for tests generation tools based on symbolic
execution. The algorithm is supposed to help in situations, when a tool is
repeatedly failing to cover some code by tests. The algorithm then provides the
tool a necessary condition strongly narrowing space of program paths, which
must be checked for reaching the uncovered code. We also discuss integration of
the algorithm into the tools and we provide experimental results showing a
potential of the algorithm to be valuable in the tools, when properly
implemented there.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0305021v1,"Creating Prototypes for Fast Classification in Dempster-Shafer
  Clustering","We develop a classification method for incoming pieces of evidence in
Dempster-Shafer theory. This methodology is based on previous work with
clustering and specification of originally nonspecific evidence. This
methodology is here put in order for fast classification of future incoming
pieces of evidence by comparing them with prototypes representing the clusters,
instead of making a full clustering of all evidence. This method has a
computational complexity of O(M * N) for each new piece of evidence, where M is
the maximum number of subsets and N is the number of prototypes chosen for each
subset. That is, a computational complexity independent of the total number of
previously arrived pieces of evidence. The parameters M and N are typically
fixed and domain dependent in any application.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0604020v4,"Approximation Algorithms for Restricted Cycle Covers Based on Cycle
  Decompositions","A cycle cover of a graph is a set of cycles such that every vertex is part of
exactly one cycle. An L-cycle cover is a cycle cover in which the length of
every cycle is in the set L. The weight of a cycle cover of an edge-weighted
graph is the sum of the weights of its edges.
  We come close to settling the complexity and approximability of computing
L-cycle covers. On the one hand, we show that for almost all L, computing
L-cycle covers of maximum weight in directed and undirected graphs is APX-hard
and NP-hard. Most of our hardness results hold even if the edge weights are
restricted to zero and one.
  On the other hand, we show that the problem of computing L-cycle covers of
maximum weight can be approximated within a factor of 2 for undirected graphs
and within a factor of 8/3 in the case of directed graphs. This holds for
arbitrary sets L.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1305.1157v1,Parallel String Sample Sort,"We discuss how string sorting algorithms can be parallelized on modern
multi-core shared memory machines. As a synthesis of the best sequential string
sorting algorithms and successful parallel sorting algorithms for atomic
objects, we propose string sample sort. The algorithm makes effective use of
the memory hierarchy, uses additional word level parallelism, and largely
avoids branch mispredictions. Additionally, we parallelize variants of multikey
quicksort and radix sort that are also useful in certain situations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0810.5516v1,Symbolic model checking of tense logics on rational Kripke models,"We introduce the class of rational Kripke models and study symbolic model
checking of the basic tense logic Kt and some extensions of it in models from
that class. Rational Kripke models are based on (generally infinite) rational
graphs, with vertices labeled by the words in some regular language and
transitions recognized by asynchronous two-head finite automata, also known as
rational transducers. Every atomic proposition in a rational Kripke model is
evaluated in a regular set of states. We show that every formula of Kt has an
effectively computable regular extension in every rational Kripke model, and
therefore local model checking and global model checking of Kt in rational
Kripke models are decidable. These results are lifted to a number of extensions
of Kt. We study and partly determine the complexity of the model checking
procedures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1309.3060v4,On SAT representations of XOR constraints,"We study the representation of systems S of linear equations over the
two-element field (aka xor- or parity-constraints) via conjunctive normal forms
F (boolean clause-sets). First we consider the problem of finding an
""arc-consistent"" representation (""AC""), meaning that unit-clause propagation
will fix all forced assignments for all possible instantiations of the
xor-variables. Our main negative result is that there is no polysize
AC-representation in general. On the positive side we show that finding such an
AC-representation is fixed-parameter tractable (fpt) in the number of
equations. Then we turn to a stronger criterion of representation, namely
propagation completeness (""PC"") --- while AC only covers the variables of S,
now all the variables in F (the variables in S plus auxiliary variables) are
considered for PC. We show that the standard translation actually yields a PC
representation for one equation, but fails so for two equations (in fact
arbitrarily badly). We show that with a more intelligent translation we can
also easily compute a translation to PC for two equations. We conjecture that
computing a representation in PC is fpt in the number of equations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1905.10774v1,Justifications of Welfare Guarantees under Normalized Utilities,"It is standard in computational social choice to analyse welfare
considerations under the assumptions of normalized utilities. In this note, we
summarize some common reasons for this approach. We then mention another
justification which is ignored but has solid normative appeal. The central
concept used in the `new' justification can also be used more widely as a
social objective.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1008.4627v1,"Matching Dependencies with Arbitrary Attribute Values: Semantics, Query
  Answering and Integrity Constraints","Matching dependencies (MDs) were introduced to specify the identification or
matching of certain attribute values in pairs of database tuples when some
similarity conditions are satisfied. Their enforcement can be seen as a natural
generalization of entity resolution. In what we call the ""pure case"" of MDs,
any value from the underlying data domain can be used for the value in common
that does the matching. We investigate the semantics and properties of data
cleaning through the enforcement of matching dependencies for the pure case. We
characterize the intended clean instances and also the ""clean answers"" to
queries as those that are invariant under the cleaning process. The complexity
of computing clean instances and clean answers to queries is investigated.
Tractable and intractable cases depending on the MDs and queries are
identified. Finally, we establish connections with database ""repairs"" under
integrity constraints.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.07461v1,"On Relation between Constraint Answer Set Programming and Satisfiability
  Modulo Theories","Constraint answer set programming is a promising research direction that
integrates answer set programming with constraint processing. It is often
informally related to the field of satisfiability modulo theories. Yet, the
exact formal link is obscured as the terminology and concepts used in these two
research areas differ. In this paper, we connect these two research areas by
uncovering the precise formal relation between them. We believe that this work
will booster the cross-fertilization of the theoretical foundations and the
existing solving methods in both areas. As a step in this direction we provide
a translation from constraint answer set programs with integer linear
constraints to satisfiability modulo linear integer arithmetic that paves the
way to utilizing modern satisfiability modulo theories solvers for computing
answer sets of constraint answer set programs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1402.6510v4,"Further improvements of determinization methods for fuzzy finite
  automata","In this paper we combine determinization and state reduction methods into
two-in-one algorithms that simultaneously perform determinization and state
reduction. These algorithms perform better than all previous determinization
algorithms for fuzzy finite automata, developed by Belohlavek [Inform Sciences
143 (2002) 205-209], Li and Pedrycz [Fuzzy Set Syst 156 (2005) 68-92],
Ignjatovi\'c et al. [Inform Sciences 178 (2008) 164-180], and Jan\v{c}i\'c et
al. [Inform Sciences 181 (2011) 1358-1368], in the sense that they produce
smaller automata, while require the same computation time. The only exception
is the Brzozowski type determinization algorithm developed recently by
Jan\v{c}i\'c and \'Ciri\'c [Fuzzy Set Syst (2014), to appear], which produces a
minimal crisp-deterministic fuzzy automaton, but the algorithms created here
can also be used within the Brzozowski type algorithm and improve its
performances.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1810.11404v2,Fixpoint Games on Continuous Lattices,"Many analysis and verifications tasks, such as static program analyses and
model-checking for temporal logics reduce to the solution of systems of
equations over suitable lattices. Inspired by recent work on lattice-theoretic
progress measures, we develop a game-theoretical approach to the solution of
systems of monotone equations over lattices, where for each single equation
either the least or greatest solution is taken. A simple parity game, referred
to as fixpoint game, is defined that provides a correct and complete
characterisation of the solution of equation systems over continuous lattices,
a quite general class of lattices widely used in semantics. For powerset
lattices the fixpoint game is intimately connected with classical parity games
for $\mu$-calculus model-checking, whose solution can exploit as a key tool
Jurdzi\'nski's small progress measures. We show how the notion of progress
measure can be naturally generalised to fixpoint games over continuous lattices
and we prove the existence of small progress measures. Our results lead to a
constructive formulation of progress measures as (least) fixpoints. We refine
this characterisation by introducing the notion of selection that allows one to
constrain the plays in the parity game, enabling an effective (and possibly
efficient) solution of the game, and thus of the associated verification
problem. We also propose a logic for specifying the moves of the existential
player that can be used to systematically derive simplified equations for
efficiently computing progress measures. We discuss potential applications to
the model-checking of latticed $\mu$-calculi and to the solution of fixpoint
equations systems over the reals.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1005.2273v1,Likelihood that a pseudorandom sequence generator has optimal properties,"The authors prove that the probability of choosing a nonlinear filter of
m-sequences with optimal properties, that is, maximum period and maximum linear
complexity, tends assymptotically to 1 as the linear feedback shift register
length increases.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.07663v1,Summing Up Smart Transitions,"Some of the most significant high-level properties of currencies are the sums
of certain account balances. Properties of such sums can ensure the integrity
of currencies and transactions. For example, the sum of balances should not be
changed by a transfer operation. Currencies manipulated by code present a
verification challenge to mathematically prove their integrity by reasoning
about computer programs that operate over them, e.g., in Solidity. The ability
to reason about sums is essential: even the simplest ERC-20 token standard of
the Ethereum community provides a way to access the total supply of balances.
  Unfortunately, reasoning about code written against this interface is
non-trivial: the number of addresses is unbounded, and establishing global
invariants like the preservation of the sum of the balances by operations like
transfer requires higher-order reasoning. In particular, automated reasoners do
not provide ways to specify summations of arbitrary length.
  In this paper, we present a generalization of first-order logic which can
express the unbounded sum of balances. We prove the decidablity of one of our
extensions and the undecidability of a slightly richer one. We introduce
first-order encodings to automate reasoning over software transitions with
summations. We demonstrate the applicability of our results by using SMT
solvers and first-order provers for validating the correctness of common
transitions in smart contracts.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1310.7801v3,"EPOBF: Energy Efficient Allocation of Virtual Machines in High
  Performance Computing Cloud","Cloud computing has become more popular in provision of computing resources
under virtual machine (VM) abstraction for high performance computing (HPC)
users to run their applications. A HPC cloud is such cloud computing
environment. One of challenges of energy efficient resource allocation for VMs
in HPC cloud is tradeoff between minimizing total energy consumption of
physical machines (PMs) and satisfying Quality of Service (e.g. performance).
On one hand, cloud providers want to maximize their profit by reducing the
power cost (e.g. using the smallest number of running PMs). On the other hand,
cloud customers (users) want highest performance for their applications. In
this paper, we focus on the scenario that scheduler does not know global
information about user jobs and user applications in the future. Users will
request shortterm resources at fixed start times and non interrupted durations.
We then propose a new allocation heuristic (named Energy-aware and Performance
per watt oriented Bestfit (EPOBF)) that uses metric of performance per watt to
choose which most energy-efficient PM for mapping each VM (e.g. maximum of MIPS
per Watt). Using information from Feitelson's Parallel Workload Archive to
model HPC jobs, we compare the proposed EPOBF to state of the art heuristics on
heterogeneous PMs (each PM has multicore CPU). Simulations show that the EPOBF
can reduce significant total energy consumption in comparison with state of the
art allocation heuristics.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0610168v1,Presentation Theorems for Coded Character Sets,"The notion of 'presentation', as used in combinatorial group theory, is
applied to coded character sets(CCSs) - sets which facilitate the interchange
of messages in a digital computer network(DCN) . By grouping each element of
the set into two portions and using the idea of group presentation(whereby a
group is specified by its set of generators and its set of relators), the
presentation of a CCS is described. This is illustrated using the Extended
Binary Coded Decimal Interchange Code(EBCDIC) which is one of the most popular
CCSs in DCNs.
  Key words: Group presentation, coded character set, digital computer network",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1503.07189v1,"Optimal control in Markov decision processes via distributed
  optimization","Optimal control synthesis in stochastic systems with respect to quantitative
temporal logic constraints can be formulated as linear programming problems.
However, centralized synthesis algorithms do not scale to many practical
systems. To tackle this issue, we propose a decomposition-based distributed
synthesis algorithm. By decomposing a large-scale stochastic system modeled as
a Markov decision process into a collection of interacting sub-systems, the
original control problem is formulated as a linear programming problem with a
sparse constraint matrix, which can be solved through distributed optimization
methods. Additionally, we propose a decomposition algorithm which automatically
exploits, if exists, the modular structure in a given large-scale system. We
illustrate the proposed methods through robotic motion planning examples.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0110031v1,"Depth-3 Arithmetic Circuits for S^2_n(X) and Extensions of the
  Graham-Pollack Theorem","We consider the problem of computing the second elementary symmetric
polynomial S^2_n(X) using depth-three arithmetic circuits of the form ""sum of
products of linear forms"". We consider this problem over several fields and
determine EXACTLY the number of multiplication gates required. The lower bounds
are proved for inhomogeneous circuits where the linear forms are allowed to
have constants; the upper bounds are proved in the homogeneous model. For reals
and rationals, the number of multiplication gates required is exactly n-1; in
most other cases, it is \ceil{n/2}. This problem is related to the
Graham-Pollack theorem in algebraic graph theory. In particular, our results
answer the following question of Babai and Frankl: what is the minimum number
of complete bipartite graphs required to cover each edge of a complete graph an
odd number of times? We show that for infinitely many n, the answer is
\ceil{n/2}.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1407.0699v1,"Enumeration of Spanning Trees Using Edge Exchange with Minimal
  Partitioning","In this thesis, Minimal Partitioning (MP) algorithm, an innovative algorithm
for enumerating all the spanning trees in an undirected graph is presented.
  While MP algorithm uses a computational tree graph to traverse all possible
spanning trees by the edge exchange technique, it has two unique properties
compared to previous algorithms. In the first place, the algorithm maintains a
state of minimal partition size in the spanning tree due to edge deletion. This
is realized by swapping peripheral edges, more precisely leaf edges, in most of
edge exchange operations. Consequently, the main structure of the spanning
trees is preserved during the steps of the enumeration process. This extra
constraint proves to be advantageous in many applications where the partition
size is a factor in the solution cost. Secondly, we introduce, and utilize, the
new concept of edge promotion: the exchanged edges always share one end.
Practically, and as a result of this property, the interface between the two
partitions of the spanning tree during edge exchange has to be maintained from
one side only.
  For a graph $G(V,E)$, MP algorithm requires $O(log V+E/V)$ expected time and
$OV log V)$ worst case time for generating each spanning tree. MP algorithm
requires a total expected space limit of $O(E log V)$ with worst case limit of
$O(EV)$. Like all edge exchange algorithms, MP algorithm retains the advantage
of compacted output of $O(1)$ per spanning tree by listing the relative
differences only.
  Three sample real-world applications of spanning trees enumeration are
explored and the effects of using MP algorithm are studied. Namely:
construction of nets of polyhedra, multi-robots spanning tree routing, and
computing the electric current in edges of a network. We report that MP
algorithm outperforms other algorithm by $O(V)$ time complexity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.04642v1,"Torch-Points3D: A Modular Multi-Task Frameworkfor Reproducible Deep
  Learning on 3D Point Clouds","We introduce Torch-Points3D, an open-source framework designed to facilitate
the use of deep networks on3D data. Its modular design, efficient
implementation, and user-friendly interfaces make it a relevant tool for
research and productization alike. Beyond multiple quality-of-life features,
our goal is to standardize a higher level of transparency and reproducibility
in 3D deep learning research, and to lower its barrier to entry. In this paper,
we present the design principles of Torch-Points3D, as well as extensive
benchmarks of multiple state-of-the-art algorithms and inference schemes across
several datasets and tasks. The modularity of Torch-Points3D allows us to
design fair and rigorous experimental protocols in which all methods are
evaluated in the same conditions. The Torch-Points3D repository
:https://github.com/nicolas-chaulet/torch-points3d",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.01331v1,"Neighborhood Matters: Influence Maximization in Social Networks with
  Limited Access","Influence maximization (IM) aims at maximizing the spread of influence by
offering discounts to influential users (called seeding). In many applications,
due to user's privacy concern, overwhelming network scale etc., it is hard to
target any user in the network as one wishes. Instead, only a small subset of
users is initially accessible. Such access limitation would significantly
impair the influence spread, since IM often relies on seeding high degree
users, which are particularly rare in such a small subset due to the power-law
structure of social networks. In this paper, we attempt to solve the limited IM
in real-world scenarios by the adaptive approach with seeding and diffusion
uncertainty considered. Specifically, we consider fine-grained discounts and
assume users accept the discount probabilistically. The diffusion process is
depicted by the independent cascade model. To overcome the access limitation,
we prove the set-wise friendship paradox (FP) phenomenon that neighbors have
higher degree in expectation, and propose a two-stage seeding model with the FP
embedded, where neighbors are seeded. On this basis, for comparison we
formulate the non-adaptive case and adaptive case, both proven to be NP-hard.
In the non-adaptive case, discounts are allocated to users all at once. We show
the monotonicity of influence spread w.r.t. discount allocation and design a
two-stage coordinate descent framework to decide the discount allocation. In
the adaptive case, users are sequentially seeded based on observations of
existing seeding and diffusion results. We prove the adaptive submodularity and
submodularity of the influence spread function in two stages. Then, a series of
adaptive greedy algorithms are proposed with constant approximation ratio.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.02343v1,Provability in BI's Sequent Calculus is Decidable,"The logic of Bunched Implications (BI) combines both additive and
multiplicative connectives, which include two primitive intuitionistic
implications. As a consequence, contexts in the sequent presentation are not
lists, nor multisets, but rather tree-like structures called bunches. This
additional complexity notwithstanding, the logic has a well-behaved metatheory
admitting all the familiar forms of semantics and proof systems. However, the
presentation of an effective proof-search procedure has been elusive since the
logic's debut. We show that one can reduce the proof-search space for any given
sequent to a primitive recursive set, the argument generalizing Gentzen's
decidability argument for classical propositional logic and combining key
features of Dyckhoff's contraction-elimination argument for intuitionistic
logic. An effective proof-search procedure, and hence decidability of
provability, follows as a corollary.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1909.08786v2,DAOC: Stable Clustering of Large Networks,"Clustering is a crucial component of many data mining systems involving the
analysis and exploration of various data. Data diversity calls for clustering
algorithms to be accurate while providing stable (i.e., deterministic and
robust) results on arbitrary input networks. Moreover, modern systems often
operate with large datasets, which implicitly constrains the complexity of the
clustering algorithm. Existing clustering techniques are only partially stable,
however, as they guarantee either determinism or robustness. To address this
issue, we introduce DAOC, a Deterministic and Agglomerative Overlapping
Clustering algorithm. DAOC leverages a new technique called Overlap
Decomposition to identify fine-grained clusters in a deterministic way
capturing multiple optima. In addition, it leverages a novel consensus
approach, Mutual Maximal Gain, to ensure robustness and further improve the
stability of the results while still being capable of identifying micro-scale
clusters. Our empirical results on both synthetic and real-world networks show
that DAOC yields stable clusters while being on average 25% more accurate than
state-of-the-art deterministic algorithms without requiring any tuning. Our
approach has the ambition to greatly simplify and speed up data analysis tasks
involving iterative processing (need for determinism) as well as data
fluctuations (need for robustness) and to provide accurate and reproducible
results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.02520v1,"Resampling and super-resolution of hexagonally sampled images using deep
  learning","Super-resolution (SR) aims to increase the resolution of imagery.
Applications include security, medical imaging, and object recognition. We
propose a deep learning-based SR system that takes a hexagonally sampled
low-resolution image as an input and generates a rectangularly sampled SR image
as an output. For training and testing, we use a realistic observation model
that includes optical degradation from diffraction and sensor degradation from
detector integration. Our SR approach first uses non-uniform interpolation to
partially upsample the observed hexagonal imagery and convert it to a
rectangular grid. We then leverage a state-of-the-art convolutional neural
network (CNN) architecture designed for SR known as Residual Channel Attention
Network (RCAN). In particular, we use RCAN to further upsample and restore the
imagery to produce the final SR image estimate. We demonstrate that this system
is superior to applying RCAN directly to rectangularly sampled LR imagery with
equivalent sample density. The theoretical advantages of hexagonal sampling are
well known. However, to the best of our knowledge, the practical benefit of
hexagonal sampling in light of modern processing techniques such as RCAN SR is
heretofore untested. Our SR system demonstrates a notable advantage of
hexagonally sampled imagery when employing a modified RCAN for hexagonal SR.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0806.3668v1,Approximating Multi-Criteria Max-TSP,"We present randomized approximation algorithms for multi-criteria Max-TSP.
For Max-STSP with k > 1 objective functions, we obtain an approximation ratio
of $1/k - \eps$ for arbitrarily small $\eps > 0$. For Max-ATSP with k objective
functions, we obtain an approximation ratio of $1/(k+1) - \eps$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1208.2771v1,"A Universal Semi-totalistic Cellular Automaton on Kite and Dart Penrose
  Tilings","In this paper we investigate certain properties of semi-totalistic cellular
automata (CA) on the well known quasi-periodic kite and dart two dimensional
tiling of the plane presented by Roger Penrose. We show that, despite the
irregularity of the underlying grid, it is possible to devise a semi-totalistic
CA capable of simulating any boolean circuit on this aperiodic tiling.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.05613v1,"Parallel Streaming Signature EM-tree: A Clustering Algorithm for Web
  Scale Applications","The proliferation of the web presents an unsolved problem of automatically
analyzing billions of pages of natural language. We introduce a scalable
algorithm that clusters hundreds of millions of web pages into hundreds of
thousands of clusters. It does this on a single mid-range machine using
efficient algorithms and compressed document representations. It is applied to
two web-scale crawls covering tens of terabytes. ClueWeb09 and ClueWeb12
contain 500 and 733 million web pages and were clustered into 500,000 to
700,000 clusters. To the best of our knowledge, such fine grained clustering
has not been previously demonstrated. Previous approaches clustered a sample
that limits the maximum number of discoverable clusters. The proposed EM-tree
algorithm uses the entire collection in clustering and produces several orders
of magnitude more clusters than the existing algorithms. Fine grained
clustering is necessary for meaningful clustering in massive collections where
the number of distinct topics grows linearly with collection size. These
fine-grained clusters show an improved cluster quality when assessed with two
novel evaluations using ad hoc search relevance judgments and spam
classifications for external validation. These evaluations solve the problem of
assessing the quality of clusters where categorical labeling is unavailable and
unfeasible.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1802.05629v3,Models of Type Theory Based on Moore Paths,"This paper introduces a new family of models of intensional Martin-L\""of type
theory. We use constructive ordered algebra in toposes. Identity types in the
models are given by a notion of Moore path. By considering a particular gros
topos, we show that there is such a model that is non-truncated, i.e. contains
non-trivial structure at all dimensions. In other words, in this model a type
in a nested sequence of identity types can contain more than one element, no
matter how great the degree of nesting. Although inspired by existing
non-truncated models of type theory based on simplicial and cubical sets, the
notion of model presented here is notable for avoiding any form of Kan filling
condition in the semantics of types.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.00597v1,"COOT: Cooperative Hierarchical Transformer for Video-Text Representation
  Learning","Many real-world video-text tasks involve different levels of granularity,
such as frames and words, clip and sentences or videos and paragraphs, each
with distinct semantics. In this paper, we propose a Cooperative hierarchical
Transformer (COOT) to leverage this hierarchy information and model the
interactions between different levels of granularity and different modalities.
The method consists of three major components: an attention-aware feature
aggregation layer, which leverages the local temporal context (intra-level,
e.g., within a clip), a contextual transformer to learn the interactions
between low-level and high-level semantics (inter-level, e.g. clip-video,
sentence-paragraph), and a cross-modal cycle-consistency loss to connect video
and text. The resulting method compares favorably to the state of the art on
several benchmarks while having few parameters. All code is available
open-source at https://github.com/gingsi/coot-videotext",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1908.04211v4,On Identifiability in Transformers,"In this paper we delve deep in the Transformer architecture by investigating
two of its core components: self-attention and contextual embeddings. In
particular, we study the identifiability of attention weights and token
embeddings, and the aggregation of context into hidden tokens. We show that,
for sequences longer than the attention head dimension, attention weights are
not identifiable. We propose effective attention as a complementary tool for
improving explanatory interpretations based on attention. Furthermore, we show
that input tokens retain to a large degree their identity across the model. We
also find evidence suggesting that identity information is mainly encoded in
the angle of the embeddings and gradually decreases with depth. Finally, we
demonstrate strong mixing of input information in the generation of contextual
embeddings by means of a novel quantification method based on gradient
attribution. Overall, we show that self-attention distributions are not
directly interpretable and present tools to better understand and further
investigate Transformer models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1411.5416v1,"Recommending the Most Encompassing Opposing and Endorsing Arguments in
  Debates","Arguments are essential objects in DirectDemocracyP2P, where they can occur
both in association with signatures for petitions, or in association with other
debated decisions, such as bug sorting by importance. The arguments of a signer
on a given issue are grouped into one single justification, are classified by
the type of signature (e.g., supporting or opposing), and can be subject to
various types of threading.
  Given the available inputs, the two addressed problems are: (i) how to
recommend the best justification, of a given type, to a new voter, (ii) how to
recommend a compact list of justifications subsuming the majority of known
arguments for (or against) an issue.
  We investigate solutions based on weighted bipartite graphs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1803.09609v1,"Modeling power angle spectrum and antenna pattern directions in
  multipath propagation environment","Most propagation models do not consider the influence of antenna patterns on
the parameters and characteristics of received signals. This assumption is
equivalent to the use of isotropic or omnidirectional antennas in these models.
Empirical measurement results indicate that the radiation pattern, gain and
direction of directional antennas significantly influence on properties of the
received signal. This fact shows that consideration the directional antennas in
propagation models is very important especially in the context of emerging
telecommunication technologies such as beamforming or massive MIMO. The purpose
of this paper is to present the modeling method of power angular spectrum and
direction of antenna patterns in a multipath propagation environment.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1102.3029v1,Analysis of multi-stage open shop processing systems,"We study algorithmic problems in multi-stage open shop processing systems
that are centered around reachability and deadlock detection questions. We
characterize safe and unsafe system states. We show that it is easy to
recognize system states that can be reached from the initial state (where the
system is empty), but that in general it is hard to decide whether one given
system state is reachable from another given system state. We show that the
problem of identifying reachable deadlock states is hard in general open shop
systems, but is easy in the special case where no job needs processing on more
than two machines (by linear programming and matching theory), and in the
special case where all machines have capacity one (by graph-theoretic
arguments).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0003061v1,dcs: An Implementation of DATALOG with Constraints,"Answer-set programming (ASP) has emerged recently as a viable programming
paradigm. We describe here an ASP system, DATALOG with constraints or DC, based
on non-monotonic logic. Informally, DC theories consist of propositional
clauses (constraints) and of Horn rules. The semantics is a simple and natural
extension of the semantics of the propositional logic. However, thanks to the
presence of Horn rules in the system, modeling of transitive closure becomes
straightforward. We describe the syntax, use and implementation of DC and
provide experimental results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.09478v1,"A Survey of Text Games for Reinforcement Learning informed by Natural
  Language","Reinforcement Learning has shown success in a number of complex virtual
environments. However, many challenges still exist towards solving problems
with natural language as a core component. Interactive Fiction Games (or Text
Games) are one such problem type that offer a set of partially observable
environments where natural language is required as part of the reinforcement
learning solutions.
  Therefore, this survey's aim is to assist in the development of new Text Game
problem settings and solutions for Reinforcement Learning informed by natural
language. Specifically, this survey summarises: 1) the challenges introduced in
Text Game Reinforcement Learning problems, 2) the generation tools for
evaluating Text Games and the subsequent environments generated and, 3) the
agent architectures currently applied are compared to provide a systematic
review of benchmark methodologies and opportunities for future researchers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1107.4157v1,Linear Differential Equations with Fuzzy Boundary Values,"In this study, we consider a linear differential equation with fuzzy boundary
values. We express the solution of the problem in terms of a fuzzy set of crisp
real functions. Each real function from the solution set satisfies differential
equation, and its boundary values belong to intervals, determined by the
corresponding fuzzy numbers. The least possibility among possibilities of
boundary values in corresponding fuzzy sets is defined as the possibility of
the real function in the fuzzy solution. In order to find the fuzzy solution we
propose a method based on the properties of linear transformations. We show
that, if the corresponding crisp problem has a unique solution then the fuzzy
problem has unique solution too. We also prove that if the boundary values are
triangular fuzzy numbers, then the value of the solution at any time is also a
triangular fuzzy number. We find that the fuzzy solution determined by our
method is the same as the one that is obtained from solution of crisp problem
by the application of the extension principle. We present two examples
describing the proposed method.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1409.4565v1,Improving files availability for BitTorrent using a diffusion model,"The BitTorrent mechanism effectively spreads file fragments by copying the
rarest fragments first. We propose to apply a mathematical model for the
diffusion of fragments on a P2P in order to take into account both the effects
of peer distances and the changing availability of peers while time goes on.
Moreover, we manage to provide a forecast on the availability of a torrent
thanks to a neural network that models the behaviour of peers on the P2P
system. The combination of the mathematical model and the neural network
provides a solution for choosing file fragments that need to be copied first,
in order to ensure their continuous availability, counteracting possible
disconnections by some peers.",0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.12899v1,"BMI: A Behavior Measurement Indicator for Fuel Poverty Using Aggregated
  Load Readings from Smart Meters","Fuel poverty affects between 50 and 125 million households in Europe and is a
significant issue for both developed and developing countries globally. This
means that fuel poor residents are unable to adequately warm their home and run
the necessary energy services needed for lighting, cooking, hot water, and
electrical appliances. The problem is complex but is typically caused by three
factors; low income, high energy costs, and energy inefficient homes. In the
United Kingdom (UK), 4 million families are currently living in fuel poverty.
Those in series financial difficulty are either forced to self-disconnect or
have their services terminated by energy providers. Fuel poverty contributed to
10,000 reported deaths in England in the winter of 2016-2107 due to homes being
cold. While it is recognized by governments as a social, public health and
environmental policy issue, the European Union (EU) has failed to provide a
common definition of fuel poverty or a conventional set of indicators to
measure it. This chapter discusses current fuel poverty strategies across the
EU and proposes a new and foundational behavior measurement indicator designed
to directly assess and monitor fuel poverty risks in households using smart
meters, Consumer Access Device (CAD) data and machine learning. By detecting
Activities of Daily Living (ADLS) through household appliance usage, it is
possible to spot the early signs of financial difficulty and identify when
support packages are required.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1402.5172v1,"Alternation in Quantum Programming: From Superposition of Data to
  Superposition of Programs","We extract a novel quantum programming paradigm - superposition of programs -
from the design idea of a popular class of quantum algorithms, namely quantum
walk-based algorithms. The generality of this paradigm is guaranteed by the
universality of quantum walks as a computational model. A new quantum
programming language QGCL is then proposed to support the paradigm of
superposition of programs. This language can be seen as a quantum extension of
Dijkstra's GCL (Guarded Command Language). Surprisingly, alternation in GCL
splits into two different notions in the quantum setting: classical alternation
(of quantum programs) and quantum alternation, with the latter being introduced
in QGCL for the first time. Quantum alternation is the key program construct
for realizing the paradigm of superposition of programs.
  The denotational semantics of QGCL are defined by introducing a new
mathematical tool called the guarded composition of operator-valued functions.
Then the weakest precondition semantics of QGCL can straightforwardly derived.
Another very useful program construct in realizing the quantum programming
paradigm of superposition of programs, called quantum choice, can be easily
defined in terms of quantum alternation. The relation between quantum choices
and probabilistic choices is clarified through defining the notion of local
variables. We derive a family of algebraic laws for QGCL programs that can be
used in program verification, transformations and compilation. The expressive
power of QGCL is illustrated by several examples where various variants and
generalizations of quantum walks are conveniently expressed using quantum
alternation and quantum choice. We believe that quantum programming with
quantum alternation and choice will play an important role in further
exploiting the power of quantum computing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0806.4899v1,A Dynamic Programming Approach To Length-Limited Huffman Coding,"The ``state-of-the-art'' in Length Limited Huffman Coding algorithms is the
$\Theta(ND)$-time, $\Theta(N)$-space one of Hirschberg and Larmore, where $D\le
N$ is the length restriction on the code. This is a very clever, very problem
specific, technique. In this note we show that there is a simple
Dynamic-Programming (DP) method that solves the problem with the same time and
space bounds. The fact that there was an $\Theta(ND)$ time DP algorithm was
previously known; it is a straightforward DP with the Monge property (which
permits an order of magnitude speedup). It was not interesting, though, because
it also required $\Theta(ND)$ space. The main result of this paper is the
technique developed for reducing the space. It is quite simple and applicable
to many other problems modeled by DPs with the Monge property. We illustrate
this with examples from web-proxy design and wireless mobile paging.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.04209v1,"Towards the Detection of Building Occupancy with Synthetic Environmental
  Data","Information about room-level occupancy is crucial to many building-related
tasks, such as building automation or energy performance simulation. Current
occupancy detection literature focuses on data-driven methods, but is mostly
based on small case studies with few rooms. The necessity to collect
room-specific data for each room of interest impedes applicability of machine
learning, especially data-intensive deep learning approaches, in practice. To
derive accurate predictions from less data, we suggest knowledge transfer from
synthetic data. In this paper, we conduct an experiment with data from a CO$_2$
sensor in an office room, and additional synthetic data obtained from a
simulation. Our contribution includes (a) a simulation method for CO$_2$
dynamics under randomized occupant behavior, (b) a proof of concept for
knowledge transfer from simulated CO$_2$ data, and (c) an outline of future
research implications. From our results, we can conclude that the transfer
approach can effectively reduce the required amount of data for model training.",0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0902.3958v2,Antichains for the Automata-Based Approach to Model-Checking,"We propose and evaluate antichain algorithms to solve the universality and
language inclusion problems for nondeterministic Buechi automata, and the
emptiness problem for alternating Buechi automata. To obtain those algorithms,
we establish the existence of simulation pre-orders that can be exploited to
efficiently evaluate fixed points on the automata defined during the
complementation step (that we keep implicit in our approach). We evaluate the
performance of the algorithm to check the universality of Buechi automata using
the random automaton model recently proposed by Tabakov and Vardi. We show that
on the difficult instances of this probabilistic model, our algorithm
outperforms the standard ones by several orders of magnitude.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.12184v1,Big Ramsey degrees and forbidden cycles,"Using the Carlson-Simpson theorem, we give a new general condition for a
structure in a finite binary relational language to have finite big Ramsey
degrees",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.01516v3,Introducing Self-Attention to Target Attentive Graph Neural Networks,"Session-based recommendation systems suggest relevant items to users by
modeling user behavior and preferences using short-term anonymous sessions.
Existing methods leverage Graph Neural Networks (GNNs) that propagate and
aggregate information from neighboring nodes i.e., local message passing. Such
graph-based architectures have representational limits, as a single sub-graph
is susceptible to overfit the sequential dependencies instead of accounting for
complex transitions between items in different sessions. We propose a new
technique that leverages a Transformer in combination with a target attentive
GNN. This allows richer representations to be learnt, which translates to
empirical performance gains in comparison to a vanilla target attentive GNN.
Our experimental results and ablation show that our proposed method is
competitive with the existing methods on real-world benchmark datasets,
improving on graph-based hypotheses. Code is available at
https://github.com/The-Learning-Machines/SBR",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1207.4747v4,Block-Coordinate Frank-Wolfe Optimization for Structural SVMs,"We propose a randomized block-coordinate variant of the classic Frank-Wolfe
algorithm for convex optimization with block-separable constraints. Despite its
lower iteration cost, we show that it achieves a similar convergence rate in
duality gap as the full Frank-Wolfe algorithm. We also show that, when applied
to the dual structural support vector machine (SVM) objective, this yields an
online algorithm that has the same low iteration complexity as primal
stochastic subgradient methods. However, unlike stochastic subgradient methods,
the block-coordinate Frank-Wolfe algorithm allows us to compute the optimal
step-size and yields a computable duality gap guarantee. Our experiments
indicate that this simple algorithm outperforms competing structural SVM
solvers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1508.03040v7,Syntax Evolution: Problems and Recursion,"To investigate the evolution of syntax, we need to ascertain the evolutionary
r\^ole of syntax and, before that, the very nature of syntax. Here, we will
assume that syntax is computing. And then, since we are computationally Turing
complete, we meet an evolutionary anomaly, the anomaly of sytax: we are
syntactically too competent for syntax. Assuming that problem solving is
computing, and realizing that the evolutionary advantage of Turing completeness
is full problem solving and not syntactic proficiency, we explain the anomaly
of syntax by postulating that syntax and problem solving co-evolved in humans
towards Turing completeness. Examining the requirements that full problem
solving impose on language, we find firstly that semantics is not sufficient
and that syntax is necessary to represent problems. Our final conclusion is
that full problem solving requires a functional semantics on an infinite
tree-structured syntax. Besides these results, the introduction of Turing
completeness and problem solving to explain the evolution of syntax should help
us to fit the evolution of language within the evolution of cognition, giving
us some new clues to understand the elusive relation between language and
thinking.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1005.1684v12,On Macroscopic Complexity and Perceptual Coding,"The theoretical limits of 'lossy' data compression algorithms are considered.
The complexity of an object as seen by a macroscopic observer is the size of
the perceptual code which discards all information that can be lost without
altering the perception of the specified observer. The complexity of this
macroscopically observed state is the simplest description of any microstate
comprising that macrostate. Inference and pattern recognition based on
macrostate rather than microstate complexities will take advantage of the
complexity of the macroscopic observer to ignore irrelevant noise.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0802.4237v3,Safety alternating automata on data words,"A data word is a sequence of pairs of a letter from a finite alphabet and an
element from an infinite set, where the latter can only be compared for
equality. Safety one-way alternating automata with one register on infinite
data words are considered, their nonemptiness is shown EXPSPACE-complete, and
their inclusion decidable but not primitive recursive. The same complexity
bounds are obtained for satisfiability and refinement, respectively, for the
safety fragment of linear temporal logic with freeze quantification. Dropping
the safety restriction, adding past temporal operators, or adding one more
register, each causes undecidability.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0910.5147v1,Sharp Load Thresholds for Cuckoo Hashing,"The paradigm of many choices has influenced significantly the design of
efficient data structures and, most notably, hash tables. Cuckoo hashing is a
technique that extends this concept. There,we are given a table with $n$
locations, and we assume that each location can hold one item. Each item to be
inserted chooses randomly k>1 locations and has to be placed in any one of
them. How much load can cuckoo hashing handle before collisions prevent the
successful assignment of the available items to the chosen locations? Practical
evaluations of this method have shown that one can allocate a number of
elements that is a large proportion of the size of the table, being very close
to 1 even for small values of k such as 4 or 5.
  In this paper we show that there is a critical value for this proportion:
with high probability, when the amount of available items is below this value,
then these can be allocated successfully, but when it exceeds this value, the
allocation becomes impossible. We give explicitly for each k>1 this critical
value. This answers an open question posed by Mitzenmacher (ESA '09) and
underpins theoretically the experimental results. Our proofs are based on the
translation of the question into a hypergraph setting, and the study of the
related typical properties of random k-uniform hypergraphs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1802.07041v1,"Selection from heaps, row-sorted matrices and $X+Y$ using soft heaps","We use soft heaps to obtain simpler optimal algorithms for selecting the
$k$-th smallest item, and the set of~$k$ smallest items, from a heap-ordered
tree, from a collection of sorted lists, and from $X+Y$, where $X$ and $Y$ are
two unsorted sets. Our results match, and in some ways extend and improve,
classical results of Frederickson (1993) and Frederickson and Johnson (1982).
In particular, for selecting the $k$-th smallest item, or the set of~$k$
smallest items, from a collection of~$m$ sorted lists we obtain a new optimal
""output-sensitive"" algorithm that performs only $O(m+\sum_{i=1}^m \log(k_i+1))$
comparisons, where $k_i$ is the number of items of the $i$-th list that belong
to the overall set of~$k$ smallest items.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1902.04121v3,On the Convergence of Network Systems,"The apparent disconnection between the microscopic and the macroscopic is a
major issue in the understanding of complex systems. To this extend, we study
the convergence of repeatedly applying local rules on a network, and touch on
the expressive power of this model. We look at network systems and study their
behavior when different types of local rules are applied on them. For a very
general class of local rules, we prove convergence and provide a certain member
of this class that, when applied on a graph, efficiently computes its k-core
and its (k-1)-crust giving hints on the expressive power of such a model.
Furthermore, we provide guarantees on the speed of convergence for an important
subclass of the aforementioned class. We also study more general rules, and
show that they do not converge. Our counterexamples resolve an open question of
(Zhang, Wang, Wang, Zhou, KDD- 2009) as well, concerning whether a certain
process converges. Finally, we show the universality of our network system, by
providing a local rule under which it is Turing-Complete.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0805.4247v1,Neural network learning of optimal Kalman prediction and control,"Although there are many neural network (NN) algorithms for prediction and for
control, and although methods for optimal estimation (including filtering and
prediction) and for optimal control in linear systems were provided by Kalman
in 1960 (with nonlinear extensions since then), there has been, to my
knowledge, no NN algorithm that learns either Kalman prediction or Kalman
control (apart from the special case of stationary control). Here we show how
optimal Kalman prediction and control (KPC), as well as system identification,
can be learned and executed by a recurrent neural network composed of
linear-response nodes, using as input only a stream of noisy measurement data.
  The requirements of KPC appear to impose significant constraints on the
allowed NN circuitry and signal flows. The NN architecture implied by these
constraints bears certain resemblances to the local-circuit architecture of
mammalian cerebral cortex. We discuss these resemblances, as well as caveats
that limit our current ability to draw inferences for biological function. It
has been suggested that the local cortical circuit (LCC) architecture may
perform core functions (as yet unknown) that underlie sensory, motor,and other
cortical processing. It is reasonable to conjecture that such functions may
include prediction, the estimation or inference of missing or noisy sensory
data, and the goal-driven generation of control signals. The resemblances found
between the KPC NN architecture and that of the LCC are consistent with this
conjecture.",0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0207091v1,"An Almost Classical Logic for Logic Programming and Nonmonotonic
  Reasoning","The model theory of a first-order logic called N^4 is introduced. N^4 does
not eliminate double negations, as classical logic does, but instead reduces
fourfold negations. N^4 is very close to classical logic: N^4 has two truth
values; implications in N^4 are material, like in classical logic; and negation
distributes over compound formulas in N^4 as it does in classical logic.
Results suggest that the semantics of normal logic programs is conveniently
formalized in N^4: Classical logic Herbrand interpretations generalize
straightforwardly to N^4; the classical minimal Herbrand model of a positive
logic program coincides with its unique minimal N^4 Herbrand model; the stable
models of a normal logic program and its so-called complete minimal N^4
Herbrand models coincide.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0302020v1,Analytical formulations of Peer-to-Peer Connection Efficiency,"Use of Peer-to-Peer (P2P) service networks introduces a new communication
paradigm because peers are both clients and servers and so each peer may
provide/request services to/from other peers. Empirical studies of P2P networks
have been undertaken and reveal useful characteristics. However there is to
date little analytical work to describe P2P networks with respect to their
communication paradigm and their interconnections. This paper provides an
analytical formulation and optimisation of peer connection efficiency, in terms
of minimising the fraction of wasted connection time. Peer connection
efficiency is analysed for both a uni- and multi-connected peer. Given this
fundamental optimisation, the paper optimises the number of connections that
peers should make use of as a function of network load, in terms of minimising
the total queue size that requests in the P2P network experience. The results
of this paper provide a basis for engineering high performance P2P
interconnection networks. The optimisations are useful for reducing bandwidth
and power consumption, e.g. in the case of peers being mobile devices with a
limited power supply. Also these results could be used to determine when a
(virtual) circuit should be switched to support a connection.",0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1501.07539v4,"Counting Homomorphisms to Square-Free Graphs, Modulo 2","We study the problem HomsTo$H$ of counting, modulo 2, the homomorphisms from
an input graph to a fixed undirected graph $H$. A characteristic feature of
modular counting is that cancellations make wider classes of instances
tractable than is the case for exact (non-modular) counting, so subtle
dichotomy theorems can arise. We show the following dichotomy: for any $H$ that
contains no 4-cycles, HomsTo$H$ is either in polynomial time or is $\oplus
P$-complete. This confirms a conjecture of Faben and Jerrum that was previously
only known to hold for trees and for a restricted class of treewidth-2 graphs
called cactus graphs. We confirm the conjecture for a rich class of graphs
including graphs of unbounded treewidth. In particular, we focus on square-free
graphs, which are graphs without 4-cycles. These graphs arise frequently in
combinatorics, for example in connection with the strong perfect graph theorem
and in certain graph algorithms. Previous dichotomy theorems required the graph
to be tree-like so that tree-like decompositions could be exploited in the
proof. We prove the conjecture for a much richer class of graphs by adopting a
much more general approach.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1009.4847v1,"Deadline aware virtual machine scheduler for scientific grids and cloud
  computing","Virtualization technology has enabled applications to be decoupled from the
underlying hardware providing the benefits of portability, better control over
execution environment and isolation. It has been widely adopted in scientific
grids and commercial clouds. Since virtualization, despite its benefits incurs
a performance penalty, which could be significant for systems dealing with
uncertainty such as High Performance Computing (HPC) applications where jobs
have tight deadlines and have dependencies on other jobs before they could run.
The major obstacle lies in bridging the gap between performance requirements of
a job and performance offered by the virtualization technology if the jobs were
to be executed in virtual machines. In this paper, we present a novel approach
to optimize job deadlines when run in virtual machines by developing a
deadline-aware algorithm that responds to job execution delays in real time,
and dynamically optimizes jobs to meet their deadline obligations. Our
approaches borrowed concepts both from signal processing and statistical
techniques, and their comparative performance results are presented later in
the paper including the impact on utilization rate of the hardware resources.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1703.07810v2,"New versions of Newton method: step-size choice, convergence domain and
  under-determined equations","Newton method is one of the most powerful methods for finding solutions of
nonlinear equations and for proving their existence. In its ""pure"" form it has
fast convergence near the solution, but small convergence domain. On the other
hand damped Newton method has slower convergence rate, but weaker conditions on
the initial point. We provide new versions of Newton-like algorithms, resulting
in combinations of Newton and damped Newton method with special step-size
choice, and estimate its convergence domain. Under some assumptions the
convergence is global. Explicit complexity results are also addressed. The
adaptive version of the algorithm (with no a priori constants knowledge) is
presented. The method is applicable for under-determined equations (with $m<n$,
$m$ being the number of equations and $n$ being the number of variables). The
results are specified for systems of quadratic equations, for composite
mappings and for one-dimensional equations and inequalities.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1803.06878v4,Parameterized Complexity of Fair Vertex Evaluation Problems,"A prototypical graph problem is centered around a graph-theoretic property
for a set of vertices and a solution to it is a set of vertices for which the
desired property holds. The task is to decide whether, in the given graph,
there exists a solution of a certain quality, where we use size as a quality
measure. In this work, we are changing the measure to the fair measure
[Lin&Sahni: Fair edge deletion problems. IEEE Trans. Comput. 89]. The measure
is k if the number of solution neighbors does not exceed k for any vertex in
the graph. One possible way to study graph problems is by defining the property
in a certain logic. For a given objective an evaluation problem is to find a
set (of vertices) that simultaneously minimizes the assumed measure and
satisfies an appropriate formula.
  In the presented paper we show that there is an FPT algorithm for the MSO
Fair Vertex Evaluation problem for formulas with one free variable
parameterized by the twin cover number of the input graph. Here, the free
variable corresponds to the solution sought. One may define an extended variant
of MSO Fair Vertex Evaluation for formulas with l free variables; here we
measure a maximum number of neighbors in each of the l sets. However, such
variant is W[1]-hard for parameter l even on graphs with twin cover one.
Furthermore, we study the Fair Vertex Cover (Fair VC) problem. Fair VC is among
the simplest problems with respect to the demanded property (i.e., the rest
forms an edgeless graph). On the negative side, Fair VC is W[1]-hard when
parameterized by both treedepth and feedback vertex set of the input graph. On
the positive side, we provide an FPT algorithm for the parameter modular width.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1911.07831v4,"Periodic Spectral Ergodicity: A Complexity Measure for Deep Neural
  Networks and Neural Architecture Search","Establishing associations between the structure and the generalisation
ability of deep neural networks (DNNs) is a challenging task in modern machine
learning. Producing solutions to this challenge will bring progress both in the
theoretical understanding of DNNs and in building new architectures
efficiently. In this work, we address this challenge by developing a new
complexity measure based on the concept of {Periodic Spectral Ergodicity} (PSE)
originating from quantum statistical mechanics. Based on this measure a
technique is devised to quantify the complexity of deep neural networks from
the learned weights and traversing the network connectivity in a sequential
manner, hence the term cascading PSE (cPSE), as an empirical complexity
measure. This measure will capture both topological and internal neural
processing complexity simultaneously. Because of this cascading approach, i.e.,
a symmetric divergence of PSE on the consecutive layers, it is possible to use
this measure for Neural Architecture Search (NAS). We demonstrate the
usefulness of this measure in practice on two sets of vision models, ResNet and
VGG, and sketch the computation of cPSE for more complex network structures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1308.1779v2,"Proving soundness of combinatorial Vickrey auctions and generating
  verified executable code","Using mechanised reasoning we prove that combinatorial Vickrey auctions are
soundly specified in that they associate a unique outcome (allocation and
transfers) to any valid input (bids). Having done so, we auto-generate verified
executable code from the formally defined auction. This removes a source of
error in implementing the auction design. We intend to use formal methods to
verify new auction designs. Here, our contribution is to introduce and
demonstrate the use of formal methods for auction verification in the familiar
setting of a well-known auction.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.02734v2,"Deep learning-based synthetic-CT generation in radiotherapy and PET: a
  review","Recently, deep learning (DL)-based methods for the generation of synthetic
computed tomography (sCT) have received significant research attention as an
alternative to classical ones. We present here a systematic review of these
methods by grouping them into three categories, according to their clinical
applications: I) To replace CT in magnetic resonance (MR)-based treatment
planning. II) Facilitate cone-beam computed tomography (CBCT)-based
image-guided adaptive radiotherapy. III) Derive attenuation maps for the
correction of positron emission tomography (PET). Appropriate database
searching was performed on journal articles published between January 2014 and
December 2020. The DL methods' key characteristics were extracted from each
eligible study, and a comprehensive comparison among network architectures and
metrics was reported. A detailed review of each category was given,
highlighting essential contributions, identifying specific challenges, and
summarising the achievements. Lastly, the statistics of all the cited works
from various aspects were analysed, revealing the popularity and future trends,
and the potential of DL-based sCT generation. The current status of DL-based
sCT generation was evaluated, assessing the clinical readiness of the presented
methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.10920v1,"Cross-layer Navigation Convolutional Neural Network for Fine-grained
  Visual Classification","Fine-grained visual classification (FGVC) aims to classify sub-classes of
objects in the same super-class (e.g., species of birds, models of cars). For
the FGVC tasks, the essential solution is to find discriminative subtle
information of the target from local regions. TraditionalFGVC models preferred
to use the refined features,i.e., high-level semantic information for
recognition and rarely use low-level in-formation. However, it turns out that
low-level information which contains rich detail information also has effect on
improving performance. Therefore, in this paper, we propose cross-layer
navigation convolutional neural network for feature fusion. First, the feature
maps extracted by the backbone network are fed into a convolutional long
short-term memory model sequentially from high-level to low-level to perform
feature aggregation. Then, attention mechanisms are used after feature fusion
to extract spatial and channel information while linking the high-level
semantic information and the low-level texture features, which can better
locate the discriminative regions for the FGVC. In the experiments, three
commonly used FGVC datasets, including CUB-200-2011, Stanford-Cars,
andFGVC-Aircraft datasets, are used for evaluation and we demonstrate the
superiority of the proposed method by comparing it with other referred FGVC
methods to show that this method achieves superior results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1810.10330v1,"Hyper-Process Model: A Zero-Shot Learning algorithm for Regression
  Problems based on Shape Analysis","Zero-shot learning (ZSL) can be defined by correctly solving a task where no
training data is available, based on previous acquired knowledge from
different, but related tasks. So far, this area has mostly drawn the attention
from computer vision community where a new unseen image needs to be correctly
classified, assuming the target class was not used in the training procedure.
Apart from image classification, only a couple of generic methods were proposed
that are applicable to both classification and regression. These learn the
relation among model coefficients so new ones can be predicted according to
provided conditions. So far, up to our knowledge, no methods exist that are
applicable only to regression, and take advantage from such setting. Therefore,
the present work proposes a novel algorithm for regression problems that uses
data drawn from trained models, instead of model coefficients. In this case, a
shape analyses on the data is performed to create a statistical shape model and
generate new shapes to train new models. The proposed algorithm is tested in a
theoretical setting using the beta distribution where main problem to solve is
to estimate a function that predicts curves, based on already learned
different, but related ones.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0110025v2,"Recognizing When Heuristics Can Approximate Minimum Vertex Covers Is
  Complete for Parallel Access to NP","For both the edge deletion heuristic and the maximum-degree greedy heuristic,
we study the problem of recognizing those graphs for which that heuristic can
approximate the size of a minimum vertex cover within a constant factor of r,
where r is a fixed rational number. Our main results are that these problems
are complete for the class of problems solvable via parallel access to NP. To
achieve these main results, we also show that the restriction of the vertex
cover problem to those graphs for which either of these heuristics can find an
optimal solution remains NP-hard.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9909013v1,"Self-stabilizing mutual exclusion on a ring, even if K=N","We show that, contrary to common belief, Dijkstra's self-stabilizing mutual
exclusion algorithm on a ring [Dij74,Dij82] also stabilizes when the number of
states per node is one less than the number of nodes on the ring.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0002002v1,Uniform semantic treatment of default and autoepistemic logics,"We revisit the issue of connections between two leading formalisms in
nonmonotonic reasoning: autoepistemic logic and default logic. For each logic
we develop a comprehensive semantic framework based on the notion of a belief
pair. The set of all belief pairs together with the so called knowledge
ordering forms a complete lattice. For each logic, we introduce several
semantics by means of fixpoints of operators on the lattice of belief pairs.
Our results elucidate an underlying isomorphism of the respective semantic
constructions. In particular, we show that the interpretation of defaults as
modal formulas proposed by Konolige allows us to represent all semantics for
default logic in terms of the corresponding semantics for autoepistemic logic.
Thus, our results conclusively establish that default logic can indeed be
viewed as a fragment of autoepistemic logic. However, as we also demonstrate,
the semantics of Moore and Reiter are given by different operators and occupy
different locations in their corresponding families of semantics. This result
explains the source of the longstanding difficulty to formally relate these two
semantics. In the paper, we also discuss approximating skeptical reasoning with
autoepistemic and default logics and establish constructive principles behind
such approximations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1803.08979v1,"From Shannon's Channel to Semantic Channel via New Bayes' Formulas for
  Machine Learning","A group of transition probability functions form a Shannon's channel whereas
a group of truth functions form a semantic channel. By the third kind of Bayes'
theorem, we can directly convert a Shannon's channel into an optimized semantic
channel. When a sample is not big enough, we can use a truth function with
parameters to produce the likelihood function, then train the truth function by
the conditional sampling distribution. The third kind of Bayes' theorem is
proved. A semantic information theory is simply introduced. The semantic
information measure reflects Popper's hypothesis-testing thought. The Semantic
Information Method (SIM) adheres to maximum semantic information criterion
which is compatible with maximum likelihood criterion and Regularized Least
Squares criterion. It supports Wittgenstein's view: the meaning of a word lies
in its use. Letting the two channels mutually match, we obtain the Channels'
Matching (CM) algorithm for machine learning. The CM algorithm is used to
explain the evolution of the semantic meaning of natural language, such as ""Old
age"". The semantic channel for medical tests and the confirmation measures of
test-positive and test-negative are discussed. The applications of the CM
algorithm to semi-supervised learning and non-supervised learning are simply
introduced. As a predictive model, the semantic channel fits variable sources
and hence can overcome class-imbalance problem. The SIM strictly distinguishes
statistical probability and logical probability and uses both at the same time.
This method is compatible with the thoughts of Bayes, Fisher, Shannon, Zadeh,
Tarski, Davidson, Wittgenstein, and Popper.It is a competitive alternative to
Bayesian inference.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0811.4395v1,List Decoding Tensor Products and Interleaved Codes,"We design the first efficient algorithms and prove new combinatorial bounds
for list decoding tensor products of codes and interleaved codes. We show that
for {\em every} code, the ratio of its list decoding radius to its minimum
distance stays unchanged under the tensor product operation (rather than
squaring, as one might expect). This gives the first efficient list decoders
and new combinatorial bounds for some natural codes including multivariate
polynomials where the degree in each variable is bounded. We show that for {\em
every} code, its list decoding radius remains unchanged under $m$-wise
interleaving for an integer $m$. This generalizes a recent result of Dinur et
al \cite{DGKS}, who proved such a result for interleaved Hadamard codes
(equivalently, linear transformations). Using the notion of generalized Hamming
weights, we give better list size bounds for {\em both} tensoring and
interleaving of binary linear codes. By analyzing the weight distribution of
these codes, we reduce the task of bounding the list size to bounding the
number of close-by low-rank codewords. For decoding linear transformations,
using rank-reduction together with other ideas, we obtain list size bounds that
are tight over small fields.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1805.07368v1,Do Diffusion Protocols Govern Cascade Growth?,"Large cascades can develop in online social networks as people share
information with one another. Though simple reshare cascades have been studied
extensively, the full range of cascading behaviors on social media is much more
diverse. Here we study how diffusion protocols, or the social exchanges that
enable information transmission, affect cascade growth, analogous to the way
communication protocols define how information is transmitted from one point to
another. Studying 98 of the largest information cascades on Facebook, we find a
wide range of diffusion protocols - from cascading reshares of images, which
use a simple protocol of tapping a single button for propagation, to the ALS
Ice Bucket Challenge, whose diffusion protocol involved individuals creating
and posting a video, and then nominating specific others to do the same. We
find recurring classes of diffusion protocols, and identify two key
counterbalancing factors in the construction of these protocols, with
implications for a cascade's growth: the effort required to participate in the
cascade, and the social cost of staying on the sidelines. Protocols requiring
greater individual effort slow down a cascade's propagation, while those
imposing a greater social cost of not participating increase the cascade's
adoption likelihood. The predictability of transmission also varies with
protocol. But regardless of mechanism, the cascades in our analysis all have a
similar reproduction number ($\approx$ 1.8), meaning that lower rates of
exposure can be offset with higher per-exposure rates of adoption. Last, we
show how a cascade's structure can not only differentiate these protocols, but
also be modeled through branching processes. Together, these findings provide a
framework for understanding how a wide variety of information cascades can
achieve substantial adoption across a network.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1107.3658v1,"On Polynomial Kernels for Structural Parameterizations of Odd Cycle
  Transversal","The Odd Cycle Transversal problem (OCT) asks whether a given graph can be
made bipartite (i.e., 2-colorable) by deleting at most l vertices. We study
structural parameterizations of OCT with respect to their polynomial
kernelizability, i.e., whether instances can be efficiently reduced to a size
polynomial in the chosen parameter. It is a major open problem in parameterized
complexity whether Odd Cycle Transversal admits a polynomial kernel when
parameterized by l. On the positive side, we show a polynomial kernel for OCT
when parameterized by the vertex deletion distance to the class of bipartite
graphs of treewidth at most w (for any constant w); this generalizes the
parameter feedback vertex set number (i.e., the distance to a forest).
Complementing this, we exclude polynomial kernels for OCT parameterized by the
distance to outerplanar graphs, conditioned on the assumption that NP \not
\subseteq coNP/poly. Thus the bipartiteness requirement for the treewidth w
graphs is necessary. Further lower bounds are given for parameterization by
distance from cluster and co-cluster graphs respectively, as well as for
Weighted OCT parameterized by the vertex cover number (i.e., the distance from
an independent set).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0205046v2,"On the Number of Iterations for Dantzig-Wolfe Optimization and
  Packing-Covering Approximation Algorithms","We give a lower bound on the iteration complexity of a natural class of
Lagrangean-relaxation algorithms for approximately solving packing/covering
linear programs. We show that, given an input with $m$ random 0/1-constraints
on $n$ variables, with high probability, any such algorithm requires
$\Omega(\rho \log(m)/\epsilon^2)$ iterations to compute a
$(1+\epsilon)$-approximate solution, where $\rho$ is the width of the input.
The bound is tight for a range of the parameters $(m,n,\rho,\epsilon)$.
  The algorithms in the class include Dantzig-Wolfe decomposition, Benders'
decomposition, Lagrangean relaxation as developed by Held and Karp [1971] for
lower-bounding TSP, and many others (e.g. by Plotkin, Shmoys, and Tardos [1988]
and Grigoriadis and Khachiyan [1996]). To prove the bound, we use a discrepancy
argument to show an analogous lower bound on the support size of
$(1+\epsilon)$-approximate mixed strategies for random two-player zero-sum
0/1-matrix games.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.08650v2,"Challenges Porting a C++ Template-Metaprogramming Abstraction Layer to
  Directive-based Offloading","HPC systems employ a growing variety of compute accelerators with different
architectures and from different vendors. Large scientific applications are
required to run efficiently across these systems but need to retain a single
code-base in order to not stifle development. Directive-based offloading
programming models set out to provide the required portability, but, to
existing codes, they themselves represent yet another API to port to. Here, we
present our approach of porting the GPU-accelerated particle-in-cell code
PIConGPU to OpenACC and OpenMP target by adding two new backends to its
existing C++-template metaprogramming-based offloading abstraction layer alpaka
and avoiding other modifications to the application code. We introduce our
approach in the face of conflicts between requirements and available features
in the standards as well as practical hurdles posed by immature compiler
support.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0707.3205v1,Neutrality and Many-Valued Logics,"In this book, we consider various many-valued logics: standard, linear,
hyperbolic, parabolic, non-Archimedean, p-adic, interval, neutrosophic, etc. We
survey also results which show the tree different proof-theoretic frameworks
for many-valued logics, e.g. frameworks of the following deductive calculi:
Hilbert's style, sequent, and hypersequent. We present a general way that
allows to construct systematically analytic calculi for a large family of
non-Archimedean many-valued logics: hyperrational-valued, hyperreal-valued, and
p-adic valued logics characterized by a special format of semantics with an
appropriate rejection of Archimedes' axiom. These logics are built as different
extensions of standard many-valued logics (namely, Lukasiewicz's, Goedel's,
Product, and Post's logics). The informal sense of Archimedes' axiom is that
anything can be measured by a ruler. Also logical multiple-validity without
Archimedes' axiom consists in that the set of truth values is infinite and it
is not well-founded and well-ordered. On the base of non-Archimedean valued
logics, we construct non-Archimedean valued interval neutrosophic logic INL by
which we can describe neutrality phenomena.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.01768v1,Homeostasis: Design and Implementation of a Self-Stabilizing Compiler,"Mainstream compilers perform a multitude of analyses and optimizations on the
given input program. Each analysis pass may generate a program-abstraction.
Each optimization pass is typically composed of multiple alternating phases of
inspection of program-abstractions and transformations of the program. Upon
transformation of a program, the program-abstractions generated by various
analysis passes may become inconsistent with the program's modified state.
Consequently, the downstream transformations may be considered unsafe until the
relevant program-abstractions are stabilized, i.e., the program-abstractions
are made consistent with the modified program. In general, the existing
compiler frameworks do not perform automated stabilization of the
program-abstractions and instead leave it to the optimization writer to deal
with the complex task of identifying the relevant program-abstractions to
stabilize, the points where the stabilization is to be performed, and the exact
procedure of stabilization. Similarly, adding new analyses becomes a challenge
as one has to understand which all existing optimizations may impact the newly
added program-abstractions. In this paper, we address these challenges by
providing the design and implementation of a novel generalized compiler-design
framework called Homeostasis.
  Homeostasis can be used to guarantee the trigger of automated stabilization
of relevant program-abstractions under every possible transformation of the
program. Interestingly, Homeostasis provides such guarantees not only for the
existing optimization passes but also for any future optimizations that may be
added to the framework. We have implemented our proposed ideas in the IMOP
compiler framework, for OpenMP C programs. We present an evaluation which shows
that Homeostasis is efficient and easy to use.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1706.03576v1,Action and perception for spatiotemporal patterns,"This is a contribution to the formalization of the concept of agents in
multivariate Markov chains. Agents are commonly defined as entities that act,
perceive, and are goal-directed. In a multivariate Markov chain (e.g. a
cellular automaton) the transition matrix completely determines the dynamics.
This seems to contradict the possibility of acting entities within such a
system. Here we present definitions of actions and perceptions within
multivariate Markov chains based on entity-sets. Entity-sets represent a
largely independent choice of a set of spatiotemporal patterns that are
considered as all the entities within the Markov chain. For example, the
entity-set can be chosen according to operational closure conditions or
complete specific integration. Importantly, the perception-action loop also
induces an entity-set and is a multivariate Markov chain. We then show that our
definition of actions leads to non-heteronomy and that of perceptions
specialize to the usual concept of perception in the perception-action loop.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9906028v1,On the Power of Positive Turing Reductions,"In the early 1980s, Selman's seminal work on positive Turing reductions
showed that positive Turing reduction to NP yields no greater computational
power than NP itself. Thus, positive Turing and Turing reducibility to NP
differ sharply unless the polynomial hierarchy collapses.
  We show that the situation is quite different for DP, the next level of the
boolean hierarchy. In particular, positive Turing reduction to DP already
yields all (and only) sets Turing reducibility to NP. Thus, positive Turing and
Turing reducibility to DP yield the same class. Additionally, we show that an
even weaker class, P(NP[1]), can be substituted for DP in this context.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.07190v1,On the parameterized complexity of 2-partitions,"We give an FPT algorithm for deciding whether the vertex set a digraph $D$
can be partitioned into two disjoint sets $V_1,V_2$ such that the digraph
$D[V_1]$ induced by $V_1$ has a vertex that can reach all other vertices by
directed paths, the digraph $D[V_2]$ has no vertex of in-degree zero and
$|V_i|\geq k_i$, where $k_1,k_2$ are part of the input. This settles an open
problem from[1,4].",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9906017v1,"Generalization of automatic sequences for numeration systems on a
  regular language","Let L be an infinite regular language on a totally ordered alphabet (A,<).
Feeding a finite deterministic automaton (with output) with the words of L
enumerated lexicographically with respect to < leads to an infinite sequence
over the output alphabet of the automaton. This process generalizes the concept
of k-automatic sequence for abstract numeration systems on a regular language
(instead of systems in base k). Here, I study the first properties of these
sequences and their relations with numeration systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0204056v1,Trading Agents for Roaming Users,"Some roaming users need services to manipulate autonomous processes. Trading
agents running on agent trade servers are used as a case in point. We present a
solution that provides the agent owners with means to upkeeping their desktop
environment, and maintaining their agent trade server processes, via a
briefcase service.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
http://arxiv.org/abs/1101.3589v3,Signature-based algorithms to compute Groebner bases,"This paper describes a Buchberger-style algorithm to compute a Groebner basis
of a polynomial ideal, allowing for a selection strategy based on ""signatures"".
We explain how three recent algorithms can be viewed as different strategies
for the new algorithm, and how other selection strategies can be formulated. We
describe a fourth as an example. We analyze the strategies both theoretically
and empirically, leading to some surprising results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0105007v1,Analysis of Polymorphically Typed Logic Programs Using ACI-Unification,"Analysis of (partial) groundness is an important application of abstract
interpretation. There are several proposals for improving the precision of such
an analysis by exploiting type information, icluding our own work with Hill and
King, where we had shown how the information present in the type declarations
of a program can be used to characterise the degree of instantiation of a term
in a precise and yet inherently finite way. This approach worked for
polymorphically typed programs as in Goedel or HAL. Here, we recast this
approach following works by Codish, Lagoon and Stuckey. To formalise which
properties of terms we want to characterise, we use labelling functions, which
are functions that extract subterms from a term along certain paths. An
abstract term collects the results of all labelling functions of a term. For
the analysis, programs are executed on abstract terms instead of the concrete
ones, and usual unification is replaced by unification modulo an equality
theory which includes the well-known ACI-theory. Thus we generalise the works
by Codish, Lagoon and Stuckey w.r.t. the type systems considered and relate the
works among each other.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1310.5984v2,Multipass greedy coloring of simple uniform hypergraphs,"Let $m^*(n)$ be the minimum number of edges in an $n$-uniform simple
hypergraph that is not two colorable. We prove that
$m^*(n)=\Omega(4^n/\ln^2(n))$. Our result generalizes to $r$-coloring of
$b$-simple uniform hypergraphs. For fixed $r$ and $b$ we prove that a maximum
vertex degree in $b$-simple $n$-uniform hypergraph that is not $r$-colorable
must be $\Omega(r^n /\ln(n))$. By trimming arguments it implies that every such
graph has $\Omega((r^n /\ln(n))^{b+1/b})$ edges. For any fixed $r \geq 2$ our
techniques yield also a lower bound $\Omega(r^n/\ln(n))$ for van der Waerden
numbers $W(n,r)$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0109025v1,Dynamic Global Constraints: A First View,"Global constraints proved themselves to be an efficient tool for modelling
and solving large-scale real-life combinatorial problems. They encapsulate a
set of binary constraints and using global reasoning about this set they filter
the domains of involved variables better than arc consistency among the set of
binary constraints. Moreover, global constraints exploit semantic information
to achieve more efficient filtering than generalised consistency algorithms for
n-ary constraints. Continued expansion of constraint programming (CP) to
various application areas brings new challenges for design of global
constraints. In particular, application of CP to advanced planning and
scheduling (APS) requires dynamic additions of new variables and constraints
during the process of constraint satisfaction and, thus, it would be helpful if
the global constraints could adopt new variables. In the paper, we give a
motivation for such dynamic global constraints and we describe a dynamic
version of the well-known alldifferent constraint.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.01874v2,Proceedings Eighth Workshop on Intersection Types and Related Systems,"This volume contains a final and revised selection of papers presented at the
Eighth Workshop on Intersection Types and Related Systems (ITRS 2016), held on
June 26, 2016 in Porto, in affiliation with FSCD 2016.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.11090v1,Optimistic Temporal Difference Learning for 2048,"Temporal difference (TD) learning and its variants, such as multistage TD
(MS-TD) learning and temporal coherence (TC) learning, have been successfully
applied to 2048. These methods rely on the stochasticity of the environment of
2048 for exploration. In this paper, we propose to employ optimistic
initialization (OI) to encourage exploration for 2048, and empirically show
that the learning quality is significantly improved. This approach
optimistically initializes the feature weights to very large values. Since
weights tend to be reduced once the states are visited, agents tend to explore
those states which are unvisited or visited few times. Our experiments show
that both TD and TC learning with OI significantly improve the performance. As
a result, the network size required to achieve the same performance is
significantly reduced. With additional tunings such as expectimax search,
multistage learning, and tile-downgrading technique, our design achieves the
state-of-the-art performance, namely an average score of 625 377 and a rate of
72% reaching 32768 tiles. In addition, for sufficiently large tests, 65536
tiles are reached at a rate of 0.02%.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.03135v1,Catch Me If You Can: Using Power Analysis to Identify HPC Activity,"Monitoring users on large computing platforms such as high performance
computing (HPC) and cloud computing systems is non-trivial. Utilities such as
process viewers provide limited insight into what users are running, due to
granularity limitation, and other sources of data, such as system call tracing,
can impose significant operational overhead. However, despite technical and
procedural measures, instances of users abusing valuable HPC resources for
personal gains have been documented in the past \cite{hpcbitmine}, and systems
that are open to large numbers of loosely-verified users from around the world
are at risk of abuse. In this paper, we show how electrical power consumption
data from an HPC platform can be used to identify what programs are executed.
The intuition is that during execution, programs exhibit various patterns of
CPU and memory activity. These patterns are reflected in the power consumption
of the system and can be used to identify programs running. We test our
approach on an HPC rack at Lawrence Berkeley National Laboratory using a
variety of scientific benchmarks. Among other interesting observations, our
results show that by monitoring the power consumption of an HPC rack, it is
possible to identify if particular programs are running with precision up to
and recall of 95\% even in noisy scenarios.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/cs/0212042v1,Increasing Evolvability Considered as a Large-Scale Trend in Evolution,"Evolvability is the capacity to evolve. This paper introduces a simple
computational model of evolvability and demonstrates that, under certain
conditions, evolvability can increase indefinitely, even when there is no
direct selection for evolvability. The model shows that increasing evolvability
implies an accelerating evolutionary pace. It is suggested that the conditions
for indefinitely increasing evolvability are satisfied in biological and
cultural evolution. We claim that increasing evolvability is a large-scale
trend in evolution. This hypothesis leads to testable predictions about
biological and cultural evolution.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0002012v1,On The Closest String and Substring Problems,"The problem of finding a center string that is `close' to every given string
arises and has many applications in computational biology and coding theory.
This problem has two versions: the Closest String problem and the Closest
Substring problem. Assume that we are given a set of strings ${\cal S}=\{s_1,
s_2, ..., s_n\}$ of strings, say, each of length $m$. The Closest String
problem asks for the smallest $d$ and a string $s$ of length $m$ which is
within Hamming distance $d$ to each $s_i\in {\cal S}$. This problem comes from
coding theory when we are looking for a code not too far away from a given set
of codes. The problem is NP-hard. Berman et al give a polynomial time algorithm
for constant $d$. For super-logarithmic $d$, Ben-Dor et al give an efficient
approximation algorithm using linear program relaxation technique. The best
polynomial time approximation has ratio 4/3 for all $d$ given by Lanctot et al
and Gasieniec et al. The Closest Substring problem looks for a string $t$ which
is within Hamming distance $d$ away from a substring of each $s_i$. This
problem only has a $2- \frac{2}{2|\Sigma|+1}$ approximation algorithm
previously Lanctot et al and is much more elusive than the Closest String
problem, but it has many applications in finding conserved regions, genetic
drug target identification, and genetic probes in molecular biology. Whether
there are efficient approximation algorithms for both problems are major open
questions in this area. We present two polynomial time approxmation algorithms
with approximation ratio $1+ \epsilon$ for any small $\epsilon$ to settle both
questions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1201.0418v9,"A New Family of Bounded Divergence Measures and Application to Signal
  Detection","We introduce a new one-parameter family of divergence measures, called
bounded Bhattacharyya distance (BBD) measures, for quantifying the
dissimilarity between probability distributions. These measures are bounded,
symmetric and positive semi-definite and do not require absolute continuity. In
the asymptotic limit, BBD measure approaches the squared Hellinger distance. A
generalized BBD measure for multiple distributions is also introduced. We prove
an extension of a theorem of Bradt and Karlin for BBD relating Bayes error
probability and divergence ranking. We show that BBD belongs to the class of
generalized Csiszar f-divergence and derive some properties such as curvature
and relation to Fisher Information. For distributions with vector valued
parameters, the curvature matrix is related to the Fisher-Rao metric. We derive
certain inequalities between BBD and well known measures such as Hellinger and
Jensen-Shannon divergence. We also derive bounds on the Bayesian error
probability. We give an application of these measures to the problem of signal
detection where we compare two monochromatic signals buried in white noise and
differing in frequency and amplitude.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1207.1813v1,Introspective Pushdown Analysis of Higher-Order Programs,"In the static analysis of functional programs, pushdown flow analysis and
abstract garbage collection skirt just inside the boundaries of soundness and
decidability. Alone, each method reduces analysis times and boosts precision by
orders of magnitude. This work illuminates and conquers the theoretical
challenges that stand in the way of combining the power of these techniques.
The challenge in marrying these techniques is not subtle: computing the
reachable control states of a pushdown system relies on limiting access during
transition to the top of the stack; abstract garbage collection, on the other
hand, needs full access to the entire stack to compute a root set, just as
concrete collection does. \emph{Introspective} pushdown systems resolve this
conflict. Introspective pushdown systems provide enough access to the stack to
allow abstract garbage collection, but they remain restricted enough to compute
control-state reachability, thereby enabling the sound and precise product of
pushdown analysis and abstract garbage collection. Experiments reveal
synergistic interplay between the techniques, and the fusion demonstrates
""better-than-both-worlds"" precision.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1203.1525v1,"Constructing subset partition graphs with strong adjacency and end-point
  count properties","Kim defined a very general combinatorial abstraction of the diameter of
polytopes called subset partition graphs to study how certain combinatorial
properties of such graphs may be achieved in lower bound constructions. Using
Lov\'asz' Local Lemma, we give a general randomized construction for subset
partition graphs satisfying strong adjacency and end-point count properties.
This can be used as a building block to conceptually simplify the constructions
given in [Kim11].
  We also use our method to construct abstract spindles, an analogy to the
spindles used by Santos to disprove the Hirsch conjecture, of exponential
length which satisfy the adjacency and end-point count properties.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.10096v1,Emosaic: Visualizing Affective Content of Text at Varying Granularity,"This paper presents Emosaic, a tool for visualizing the emotional tone of
text documents, considering multiple dimensions of emotion and varying levels
of semantic granularity. Emosaic is grounded in psychological research on the
relationship between language, affect, and color perception. We capitalize on
an established three-dimensional model of human emotion: valence (good, nice
vs. bad, awful), arousal (calm, passive vs. exciting, active) and dominance
(weak, controlled vs. strong, in control). Previously, multi-dimensional models
of emotion have been used rarely in visualizations of textual data, due to the
perceptual challenges involved. Furthermore, until recently most text
visualizations remained at a high level, precluding closer engagement with the
deep semantic content of the text. Informed by empirical studies, we introduce
a color mapping that translates any point in three-dimensional affective space
into a unique color. Emosaic uses affective dictionaries of words annotated
with the three emotional parameters of the valence-arousal-dominance model to
extract emotional meanings from texts and then assigns to them corresponding
color parameters of the hue-saturation-brightness color space. This approach of
mapping emotion to color is aimed at helping readers to more easily grasp the
emotional tone of the text. Several features of Emosaic allow readers to
interactively explore the affective content of the text in more detail; e.g.,
in aggregated form as histograms, in sequential form following the order of
text, and in detail embedded into the text display itself. Interaction
techniques have been included to allow for filtering and navigating of text and
visualizations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1502.00367v1,A Solution to Yamakami's Problem on Advised Context-free Languages,"Yamakami [2011, Theoret. Comput. Sci.] studies context-free languages with
advice functions. Here, the length of an advice is assumed to be the same as
that of an input. Let CFL and CFL/n denote the class of all context-free
languages and that with advice functions, respectively. We let CFL(2) denote
the class of intersections of two context-free languages. An interesting
direction of a research is asking how complex CFL(2) is, relative to CFL.
Yamakami raised a problem whether there is a CFL-immune set in CFL(2) - CFL/n.
The best known so far is that LSPACE - CFL/n has a CFL-immune set, where LSPACE
denotes the class of languages recognized in logarithmic-space. We present an
affirmative solution to his problem. Two key concepts of our proof are the
nested palindrome and Yamakami's swapping lemma. The swapping lemma is
applicable to the setting where the pumping lemma (Bar-Hillel's lemma) does not
work. Our proof is an example showing how useful the swapping lemma is.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0710.0871v1,Spreadsheets in Clinical Medicine,"There is overwhelming evidence that the continued and widespread use of
untested spreadsheets in business gives rise to regular, significant and
unexpected financial losses. Whilst this is worrying, it is perhaps a
relatively minor concern compared with the risks arising from the use of poorly
constructed and/or untested spreadsheets in medicine, a practice that is
already occurring. This article is intended as a warning that the use of poorly
constructed and/or untested spreadsheets in clinical medicine cannot be
tolerated. It supports this warning by reporting on potentially serious
weaknesses found while testing a limited number of publicly available clinical
spreadsheets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1
http://arxiv.org/abs/0709.0883v5,"Liquid State Machines in Adbiatic Quantum Computers for General
  Computation",Major mistakes do not read,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.11859v1,"Part of speech and gramset tagging algorithms for unknown words based on
  morphological dictionaries of the Veps and Karelian languages","This research devoted to the low-resource Veps and Karelian languages.
Algorithms for assigning part of speech tags to words and grammatical
properties to words are presented in the article. These algorithms use our
morphological dictionaries, where the lemma, part of speech and a set of
grammatical features (gramset) are known for each word form. The algorithms are
based on the analogy hypothesis that words with the same suffixes are likely to
have the same inflectional models, the same part of speech and gramset. The
accuracy of these algorithms were evaluated and compared. 313 thousand Vepsian
and 66 thousand Karelian words were used to verify the accuracy of these
algorithms. The special functions were designed to assess the quality of
results of the developed algorithms. 92.4% of Vepsian words and 86.8% of
Karelian words were assigned a correct part of speech by the developed
algorithm. 95.3% of Vepsian words and 90.7% of Karelian words were assigned a
correct gramset by our algorithm. Morphological and semantic tagging of texts,
which are closely related and inseparable in our corpus processes, are
described in the paper.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0106041v1,"Computing Complete Graph Isomorphisms and Hamiltonian Cycles from
  Partial Ones","We prove that computing a single pair of vertices that are mapped onto each
other by an isomorphism $\phi$ between two isomorphic graphs is as hard as
computing $\phi$ itself. This result optimally improves upon a result of
G\'{a}l et al. We establish a similar, albeit slightly weaker, result about
computing complete Hamiltonian cycles of a graph from partial Hamiltonian
cycles.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.06482v1,"Extended level set method: a multiphase representation with perfect
  symmetric property, and its application to multi material topology
  optimization","This paper provides an extended level set (X-LS) based topology optimiza-
tion method for multi material design. In the proposed method, each zero level
set of a level set function {\phi}ij represents the boundary between materials
i and j. Each increase or decrease of {\phi}ij corresponds to a material change
between the two materials. This approach reduces the dependence of the initial
configuration in the optimization calculation and simplifies the sensitivity
analysis. First, the topology optimization problem is formulated in the X-LS
representation. Next, the reaction-diffusion equation that updates the level
set function is introduced, and an optimization algorithm that solves the
equilibrium equations and the reaction-diffusion equation using the fi- nite
element method is constructed. Finally, the validity and utility of the
proposed topology optimization method are confirmed using two- and three-
dimensional numerical examples.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.2848v1,Query Rewriting and Optimization for Ontological Databases,"Ontological queries are evaluated against a knowledge base consisting of an
extensional database and an ontology (i.e., a set of logical assertions and
constraints which derive new intensional knowledge from the extensional
database), rather than directly on the extensional database. The evaluation and
optimization of such queries is an intriguing new problem for database
research. In this paper, we discuss two important aspects of this problem:
query rewriting and query optimization. Query rewriting consists of the
compilation of an ontological query into an equivalent first-order query
against the underlying extensional database. We present a novel query rewriting
algorithm for rather general types of ontological constraints which is
well-suited for practical implementations. In particular, we show how a
conjunctive query against a knowledge base, expressed using linear and sticky
existential rules, that is, members of the recently introduced Datalog+/-
family of ontology languages, can be compiled into a union of conjunctive
queries (UCQ) against the underlying database. Ontological query optimization,
in this context, attempts to improve this rewriting process so to produce
possibly small and cost-effective UCQ rewritings for an input query.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.10915v1,"GOALS: Gradient-Only Approximations for Line Searches Towards Robust and
  Consistent Training of Deep Neural Networks","Mini-batch sub-sampling (MBSS) is favored in deep neural network training to
reduce the computational cost. Still, it introduces an inherent sampling error,
making the selection of appropriate learning rates challenging. The sampling
errors can manifest either as a bias or variances in a line search. Dynamic
MBSS re-samples a mini-batch at every function evaluation. Hence, dynamic MBSS
results in point-wise discontinuous loss functions with smaller bias but larger
variance than static sampled loss functions. However, dynamic MBSS has the
advantage of having larger data throughput during training but requires the
complexity regarding discontinuities to be resolved. This study extends the
gradient-only surrogate (GOS), a line search method using quadratic
approximation models built with only directional derivative information, for
dynamic MBSS loss functions. We propose a gradient-only approximation line
search (GOALS) with strong convergence characteristics with defined optimality
criterion. We investigate GOALS's performance by applying it on various
optimizers that include SGD, RMSprop and Adam on ResNet-18 and EfficientNetB0.
We also compare GOALS's against the other existing learning rate methods. We
quantify both the best performing and most robust algorithms. For the latter,
we introduce a relative robust criterion that allows us to quantify the
difference between an algorithm and the best performing algorithm for a given
problem. The results show that training a model with the recommended learning
rate for a class of search directions helps to reduce the model errors in
multimodal cases.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.13455v1,"Fairness Deconstructed: A Sociotechnical View of 'Fair' Algorithms in
  Criminal Justice","Early studies of risk assessment algorithms used in criminal justice revealed
widespread racial biases. In response, machine learning researchers have
developed methods for fairness, many of which rely on equalizing empirical
metrics across protected attributes. Here, I recall sociotechnical perspectives
to delineate the significant gap between fairness in theory and practice,
focusing on criminal justice. I (1) illustrate how social context can undermine
analyses that are restricted to an AI system's outputs, and (2) argue that much
of the fair ML literature fails to account for epistemological issues with
underlying crime data. Instead of building AI that reifies power imbalances,
like risk assessment algorithms, I ask whether data science can be used to
understand the root causes of structural marginalization.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/cs/0107024v1,Enumerating Foldings and Unfoldings between Polygons and Polytopes,"We pose and answer several questions concerning the number of ways to fold a
polygon to a polytope, and how many polytopes can be obtained from one polygon;
and the analogous questions for unfolding polytopes to polygons. Our answers
are, roughly: exponentially many, or nondenumerably infinite.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1612.03937v1,FaaS: Federation-as-a-Service,"This document is the main high-level architecture specification of the
SUNFISH cloud federation solution. Its main objective is to introduce the
concept of Federation-as-a-Service (FaaS) and the SUNFISH platform. FaaS is the
new and innovative cloud federation service proposed by the SUNFISH project.
The document defines the functionalities of FaaS, its governance and precise
objectives. With respect to these objectives, the document proposes the
high-level architecture of the SUNFISH platform: the software architecture that
permits realising a FaaS federation. More specifically, the document describes
all the components forming the platform, the offered functionalities and their
high-level interactions underlying the main FaaS functionalities. The document
concludes by outlining the main implementation strategies towards the actual
implementation of the proposed cloud federation solution.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1207.3718v2,MARFCAT: Transitioning to Binary and Larger Data Sets of SATE IV,"We present a second iteration of a machine learning approach to static code
analysis and fingerprinting for weaknesses related to security, software
engineering, and others using the open-source MARF framework and the MARFCAT
application based on it for the NIST's SATE IV static analysis tool exposition
workshop's data sets that include additional test cases, including new large
synthetic cases. To aid detection of weak or vulnerable code, including source
or binary on different platforms the machine learning approach proved to be
fast and accurate to for such tasks where other tools are either much slower or
have much smaller recall of known vulnerabilities. We use signal and NLP
processing techniques in our approach to accomplish the identification and
classification tasks. MARFCAT's design from the beginning in 2010 made is
independent of the language being analyzed, source code, bytecode, or binary.
In this follow up work with explore some preliminary results in this area. We
evaluated also additional algorithms that were used to process the data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1403.3972v4,"Subset Synchronization and Careful Synchronization of Binary Finite
  Automata","We present a strongly exponential lower bound that applies both to the subset
synchronization threshold for binary deterministic automata and to the careful
synchronization threshold for binary partial automata. In the later form, the
result finishes the research initiated by Martyugin (2013). Moreover, we show
that both the thresholds remain strongly exponential even if restricted to
strongly connected binary automata. In addition, we apply our methods to
computational complexity. Existence of a subset reset word is known to be
PSPACE-complete; we show that this holds even under the restriction to strongly
connected binary automata. The results apply also to the corresponding
thresholds in two more general settings: D1- and D3-directable nondeterministic
automata and composition sequences over finite domains.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0305018v1,"Cluster-based Specification Techniques in Dempster-Shafer Theory for an
  Evidential Intelligence Analysis of MultipleTarget Tracks (Thesis Abstract)","In Intelligence Analysis it is of vital importance to manage uncertainty.
Intelligence data is almost always uncertain and incomplete, making it
necessary to reason and taking decisions under uncertainty. One way to manage
the uncertainty in Intelligence Analysis is Dempster-Shafer Theory. This thesis
contains five results regarding multiple target tracks and intelligence
specification.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.07070v1,Sensitivity-Based Optimization for Blockchain Selfish Mining,"In this paper, we provide a novel dynamic decision method of blockchain
selfish mining by applying the sensitivity-based optimization theory. Our aim
is to find the optimal dynamic blockchain-pegged policy of the dishonest mining
pool. To study the selfish mining attacks, two mining pools is designed by
means of different competitive criterions, where the honest mining pool follows
a two-block leading competitive criterion, while the dishonest mining pool
follows a modification of two-block leading competitive criterion through using
a blockchain-pegged policy. To find the optimal blockchain-pegged policy, we
set up a policy-based continuous-time Markov process and analyze some key
factors. Based on this, we discuss monotonicity and optimality of the long-run
average profit with respect to the blockchain-pegged reward and prove the
structure of the optimal blockchain-pegged policy. We hope the methodology and
results derived in this paper can shed light on the dynamic decision research
on the selfish mining attacks of blockchain selfish mining.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1707.09558v1,"Lessons learnt from the NetIDE project: Taking SDN programming to the
  next level","SDN promises to overcome vendor lock-in by enabling a multi-vendor hardware
and software ecosystem in operator networks. However, we observe that this is
currently not happening. A framework allowing to compose SDN applications
combining different frameworks can help revert the trend. In this paper, we
analyze the challenges in the current SDN landscape and then present the
multi-controller SDN framework developed by the NetIDE project. Our
architecture supports different SDN southbound protocols and we have
implemented a proof of concept using the OpenFlow protocol, which has given us
a greater insight on its shortcomings.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.05260v1,"Sirius: A Mutual Information Tool for Exploratory Visualization of Mixed
  Data","Data scientists across disciplines are increasingly in need of exploratory
analysis tools for data sets with a high volume of features. We expand upon
graph mining approaches for exploratory analysis of high-dimensional data to
introduce Sirius, a visualization package for researchers to explore feature
relationships among mixed data types using mutual information and network
backbone sparsification. Visualizations of feature relationships aid data
scientists in finding meaningful dependence among features, which can engender
further analysis for feature selection, feature extraction, projection,
identification of proxy variables, or insight into temporal variation at the
macro scale. Graph mining approaches for feature analysis exist, such as
association networks of binary features, or correlation networks of
quantitative features, but mixed data types present a unique challenge for
developing comprehensive feature networks for exploratory analysis. Using an
information theoretic approach, Sirius supports heterogeneous data sets
consisting of binary, continuous quantitative, and discrete categorical data
types, and provides a user interface exploring feature pairs with high mutual
information scores. We leverage a backbone sparsification approach from network
theory as a dimensionality reduction technique, which probabilistically trims
edges according to the local network context. Sirius is an open source Python
package and Django web application for exploratory visualization, which can be
deployed in data analysis pipelines. The Sirius codebase and exemplary data
sets can be found at: https://github.com/compstorylab/sirius",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.06582v6,"Implementing 64-bit Maximally Equidistributed $\mathbb{F}_2$-Linear
  Generators with Mersenne Prime Period","CPUs and operating systems are moving from 32 to 64 bits, and hence it is
important to have good pseudorandom number generators designed to fully exploit
these word lengths. However, existing 64-bit very long period generators based
on linear recurrences modulo 2 are not completely optimized in terms of the
equidistribution properties. Here we develop 64-bit maximally equidistributed
pseudorandom number generators that are optimal in this respect and have speeds
equivalent to 64-bit Mersenne Twisters. We provide a table of specific
parameters with period lengths from $2^{607}-1$ to $2^{44497}-1$. (An online
appendix is available at http://www.ritsumei.ac.jp/~harase/melg-64-app.pdf)",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1011.0014v2,Galois Theory of Algorithms,"Many different programs are the implementation of the same algorithm. The
collection of programs can be partitioned into different classes corresponding
to the algorithms they implement. This makes the collection of algorithms a
quotient of the collection of programs. Similarly, there are many different
algorithms that implement the same computable function. The collection of
algorithms can be partitioned into different classes corresponding to what
computable function they implement. This makes the collection of computable
functions into a quotient of the collection of algorithms. Algorithms are
intermediate between programs and functions:
  Programs $\twoheadrightarrow$ Algorithms $\twoheadrightarrow$ Functions.
  \noindent Galois theory investigates the way that a subobject sits inside an
object. We investigate how a quotient object sits inside an object. By looking
at the Galois group of programs, we study the intermediate types of algorithms
possible and the types of structures these algorithms can have.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1905.06228v1,"Significance of parallel computing on the performance of Digital Image
  Correlation algorithms in MATLAB","Digital Image Correlation (DIC) is a powerful tool used to evaluate
displacements and deformations in a non-intrusive manner. By comparing two
images, one of the undeformed reference state of a specimen and another of the
deformed target state, the relative displacement between those two states is
determined. DIC is well known and often used for post-processing analysis of
in-plane displacements and deformation of specimen. Increasing the analysis
speed to enable real-time DIC analysis will be beneficial and extend the field
of use of this technique. Here we tested several combinations of the most
common DIC methods in combination with different parallelization approaches in
MATLAB and evaluated their performance to determine whether real-time analysis
is possible with these methods. To reflect improvements in computing technology
different hardware settings were also analysed. We found that implementation
problems can reduce the efficiency of a theoretically superior algorithm such
that it becomes practically slower than a sub-optimal algorithm. The
Newton-Raphson algorithm in combination with a modified Particle Swarm
algorithm in parallel image computation was found to be most effective. This is
contrary to theory, suggesting that the inverse-compositional Gauss-Newton
algorithm is superior. As expected, the Brute Force Search algorithm is the
least effective method. We also found that the correct choice of
parallelization tasks is crucial to achieve improvements in computing speed. A
poorly chosen parallelisation approach with high parallel overhead leads to
inferior performance. Finally, irrespective of the computing mode the correct
choice of combinations of integer-pixel and sub-pixel search algorithms is
decisive for an efficient analysis. Using currently available hardware
real-time analysis at high framerates remains an aspiration.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.06645v3,The Complexity of Bicriteria Tree-Depth,"The tree-depth problem can be seen as finding an elimination tree of minimum
height for a given input graph $G$. We introduce a bicriteria generalization in
which additionally the width of the elimination tree needs to be bounded by
some input integer $b$. We are interested in the case when $G$ is the line
graph of a tree, proving that the problem is NP-hard and obtaining a
polynomial-time additive $2b$-approximation algorithm. This particular class of
graphs received significant attention in the past, mainly due to a number of
potential applications, e.g. in parallel assembly of modular products, or
parallel query processing in relational databases, as well as purely
combinatorial applications, including searching in tree-like partial orders
(which in turn generalizes binary search on sorted data).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1206.0238v1,Rapid Feature Extraction for Optical Character Recognition,"Feature extraction is one of the fundamental problems of character
recognition. The performance of character recognition system is depends on
proper feature extraction and correct classifier selection. In this article, a
rapid feature extraction method is proposed and named as Celled Projection (CP)
that compute the projection of each section formed through partitioning an
image. The recognition performance of the proposed method is compared with
other widely used feature extraction methods that are intensively studied for
many different scripts in literature. The experiments have been conducted using
Bangla handwritten numerals along with three different well known classifiers
which demonstrate comparable results including 94.12% recognition accuracy
using celled projection.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1202.6601v2,Multiple spreaders affect the indirect influence on Twitter,"Most studies on social influence have focused on direct influence, while
another interesting question can be raised as whether indirect influence exists
between two users who're not directly connected in the network and what affects
such influence. In addition, the theory of \emph{complex contagion} tells us
that more spreaders will enhance the indirect influence between two users. Our
observation of intensity of indirect influence, propagated by $n$ parallel
spreaders and quantified by retweeting probability on Twitter, shows that
complex contagion is validated globally but is violated locally. In other
words, the retweeting probability increases non-monotonically with some local
drops.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1910.11848v3,Finite Boolean Algebras for Solid Geometry using Julia's Sparse Arrays,"The goal of this paper is to introduce a new method in computer-aided
geometry of solid modeling. We put forth a novel algebraic technique to
evaluate any variadic expression between polyhedral d-solids (d = 2, 3) with
regularized operators of union, intersection, and difference, i.e., any CSG
tree. The result is obtained in three steps: first, by computing an independent
set of generators for the d-space partition induced by the input; then, by
reducing the solid expression to an equivalent logical formula between Boolean
terms made by zeros and ones; and, finally, by evaluating this expression using
bitwise operators. This method is implemented in Julia using sparse arrays. The
computational evaluation of every possible solid expression, usually denoted as
CSG (Constructive Solid Geometry), is reduced to an equivalent logical
expression of a finite set algebra over the cells of a space partition, and
solved by native bitwise operators.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1911.00143v2,Multivariate Medians for Image and Shape Analysis,"Having been studied since long by statisticians, multivariate median concepts
found their way into the image processing literature in the course of the last
decades, being used to construct robust and efficient denoising filters for
multivariate images such as colour images but also matrix-valued images. Based
on the similarities between image and geometric data as results of the sampling
of continuous physical quantities, it can be expected that the understanding of
multivariate median filters for images provides a starting point for the
development of shape processing techniques. This paper presents an overview of
multivariate median concepts relevant for image and shape processing. It
focusses on their mathematical principles and discusses important properties
especially in the context of image processing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.06839v2,The algebra of row monomial matrices,"We consider an algebra with non-standard operations on the class of row
monomial matrices (having one unit and rest of zeros in every row). The class
of row monomial matrices is closed under multiplication, but not closed under
ordinary matrix addition. The paper considers a kind of summation operation on
row monomial matrices and the necessary conditions to be closed under the
operation in this class.
  The most significant difference between the algebra of row monomial matrices
and linear algebra is the summation operation, with respect to which the class
of row monomial matrices is closed.
  The operation of summation in the algebra can be considered also as an
algebra of subsets of any set. The class of subsets of given set is closed
under considered operation of summation.
  The deterministic finite automaton (DFA) can be presented by a complete
underlying graph of the automaton with edges labelled by letters of an
alphabet. Row monomial matrices can be induced by words in the alphabet of
labels on edges of underlying graph of DFA and present a mapping of the set of
states.
  A word $s$ of letters on edges of underlying graph $\Gamma$ of deterministic
finite automaton (DFA) is called synchronizing if $s$ sends all states of the
automaton to a unique state.
  J. \v{C}erny discovered in 1964 a sequence of $n$-state complete DFA
possessing a minimal synchronizing word of length $(n-1)^2$. The hypothesis,
mostly known today as \v{C}erny conjecture, claims that $(n-1)^2$ is a precise
upper bound on the length of such a word over alphabet $\Sigma$ of letters on
edges of $\Gamma$ for every complete $n$-state DFA. The hypothesis was
formulated in 1966 by Starke.
  The following proof of the conjecture uses the algebra of row monomial
matrices.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0306110v1,Run Control and Monitor System for the CMS Experiment,"The Run Control and Monitor System (RCMS) of the CMS experiment is the set of
hardware and software components responsible for controlling and monitoring the
experiment during data-taking. It provides users with a ""virtual counting
room"", enabling them to operate the experiment and to monitor detector status
and data quality from any point in the world. This paper describes the
architecture of the RCMS with particular emphasis on its scalability through a
distributed collection of nodes arranged in a tree-based hierarchy. The current
implementation of the architecture in a prototype RCMS used in test beam
setups, detector validations and DAQ demonstrators is documented. A discussion
of the key technologies used, including Web Services, and the results of tests
performed with a 128-node system are presented.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1105.1982v1,Secure Data Processing in a Hybrid Cloud,"Cloud computing has made it possible for a user to be able to select a
computing service precisely when needed. However, certain factors such as
security of data and regulatory issues will impact a user's choice of using
such a service. A solution to these problems is the use of a hybrid cloud that
combines a user's local computing capabilities (for mission- or
organization-critical tasks) with a public cloud (for less influential tasks).
We foresee three challenges that must be overcome before the adoption of a
hybrid cloud approach: 1) data design: How to partition relations in a hybrid
cloud? The solution to this problem must account for the sensitivity of
attributes in a relation as well as the workload of a user; 2) data security:
How to protect a user's data in a public cloud with encryption while enabling
query processing over this encrypted data? and 3) query processing: How to
execute queries efficiently over both, encrypted and unencrypted data? This
paper addresses these challenges and incorporates their solutions into an
add-on tool for a Hadoop and Hive based cloud computing infrastructure.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0602079v3,SISO APP Searches in Lattices with Tanner Graphs,"An efficient, low-complexity, soft-output detector for general lattices is
presented, based on their Tanner graph (TG) representations. Closest-point
searches in lattices can be performed as non-binary belief propagation on
associated TGs; soft-information output is naturally generated in the process;
the algorithm requires no backtrack (cf. classic sphere decoding), and extracts
extrinsic information. A lattice's coding gain enables equivalence relations
between lattice points, which can be thereby partitioned in cosets. Total and
extrinsic a posteriori probabilities at the detector's output further enable
the use of soft detection information in iterative schemes. The algorithm is
illustrated via two scenarios that transmit a 32-point, uncoded
super-orthogonal (SO) constellation for multiple-input multiple-output (MIMO)
channels, carved from an 8-dimensional non-orthogonal lattice (a direct sum of
two 4-dimensional checkerboard lattice): it achieves maximum likelihood
performance in quasistatic fading; and, performs close to interference-free
transmission, and identically to list sphere decoding, in independent fading
with coordinate interleaving and iterative equalization and detection. Latter
scenario outperforms former despite the absence of forward error correction
coding---because the inherent lattice coding gain allows for the refining of
extrinsic information. The lattice constellation is the same as the one
employed in the SO space-time trellis codes first introduced for 2-by-2 MIMO by
Ionescu et al., then independently by Jafarkhani and Seshadri. Complexity is
log-linear in lattice dimensionality, vs. cubic in sphere decoders.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1509.09174v4,Model-independent comparison of simulation output,"Computational models of complex systems are usually elaborate and sensitive
to implementation details, characteristics which often affect their
verification and validation. Model replication is a possible solution to this
issue. It avoids biases associated with the language or toolkit used to develop
the original model, not only promoting its verification and validation, but
also fostering the credibility of the underlying conceptual model. However,
different model implementations must be compared to assess their equivalence.
The problem is, given two or more implementations of a stochastic model, how to
prove that they display similar behavior? In this paper, we present a model
comparison technique, which uses principal component analysis to convert
simulation output into a set of linearly uncorrelated statistical measures,
analyzable in a consistent, model-independent fashion. It is appropriate for
ascertaining distributional equivalence of a model replication with its
original implementation. Besides model-independence, this technique has three
other desirable properties: a) it automatically selects output features that
best explain implementation differences; b) it does not depend on the
distributional properties of simulation output; and, c) it simplifies the
modelers' work, as it can be used directly on simulation outputs. The proposed
technique is shown to produce similar results to the manual or empirical
selection of output features when applied to a well-studied reference model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1701.05954v1,Learning Policies for Markov Decision Processes from Data,"We consider the problem of learning a policy for a Markov decision process
consistent with data captured on the state-actions pairs followed by the
policy. We assume that the policy belongs to a class of parameterized policies
which are defined using features associated with the state-action pairs. The
features are known a priori, however, only an unknown subset of them could be
relevant. The policy parameters that correspond to an observed target policy
are recovered using $\ell_1$-regularized logistic regression that best fits the
observed state-action samples. We establish bounds on the difference between
the average reward of the estimated and the original policy (regret) in terms
of the generalization error and the ergodic coefficient of the underlying
Markov chain. To that end, we combine sample complexity theory and sensitivity
analysis of the stationary distribution of Markov chains. Our analysis suggests
that to achieve regret within order $O(\sqrt{\epsilon})$, it suffices to use
training sample size on the order of $\Omega(\log n \cdot poly(1/\epsilon))$,
where $n$ is the number of the features. We demonstrate the effectiveness of
our method on a synthetic robot navigation example.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1905.12104v2,Heuristics in Multi-Winner Approval Voting,"In many real world situations, collective decisions are made using voting.
Moreover, scenarios such as committee or board elections require voting rules
that return multiple winners. In multi-winner approval voting (AV), an agent
may vote for as many candidates as they wish. Winners are chosen by tallying up
the votes and choosing the top-$k$ candidates receiving the most votes. An
agent may manipulate the vote to achieve a better outcome by voting in a way
that does not reflect their true preferences. In complex and uncertain
situations, agents may use heuristics to strategize, instead of incurring the
additional effort required to compute the manipulation which most favors them.
In this paper, we examine voting behavior in multi-winner approval voting
scenarios with complete information. We show that people generally manipulate
their vote to obtain a better outcome, but often do not identify the optimal
manipulation. Instead, voters tend to prioritize the candidates with the
highest utilities. Using simulations, we demonstrate the effectiveness of these
heuristics in situations where agents only have access to partial information.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.10966v1,A Composable Glitch-Aware Delay Model,"We introduce the Composable Involution Delay Model (CIDM) for fast and
accurate digital simulation. It is based on the Involution Delay Model (IDM)
[F\""ugger et al., IEEE TCAD 2020], which has been shown to be the only existing
candidate for faithful glitch propagation known so far. In its present form,
however, it has shortcomings that limit its practical applicability and
utility. First, IDM delay predictions are conceptually based on discretizing
the analog signal waveforms using specific matching input and output
discretization threshold voltages. Unfortunately, they are difficult to
determine and typically different for interconnected gates. Second,
metastability and high-frequency oscillations in a real circuit could be
invisible in the IDM signal predictions. Our CIDM reduces the characterization
effort by allowing independent discretization thresholds, improves
composability and increases the modeling power by exposing canceled pulse
trains at the gate interconnect. We formally show that, despite these
improvements, the CIDM still retains the IDM's faithfulness, which is a
consequence of the mathematical properties of involution delay functions.",0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1808.02365v2,"Pricing Financial Derivatives using Radial Basis Function generated
  Finite Differences with Polyharmonic Splines on Smoothly Varying Node Layouts","In this paper, we study the benefits of using polyharmonic splines and node
layouts with smoothly varying density for developing robust and efficient
radial basis function generated finite difference (RBF-FD) methods for pricing
of financial derivatives. We present a significantly improved RBF-FD scheme and
successfully apply it to two types of multidimensional partial differential
equations in finance: a two-asset European call basket option under the
Black--Scholes--Merton model, and a European call option under the Heston
model. We also show that the performance of the improved method is equally high
when it comes to pricing American options. By studying convergence,
computational performance, and conditioning of the discrete systems, we show
the superiority of the introduced approaches over previously used versions of
the RBF-FD method in financial applications.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.02215v1,"Math Programming based Reinforcement Learning for Multi-Echelon
  Inventory Management","Reinforcement learning has lead to considerable break-throughs in diverse
areas such as robotics, games and many others. But the application to RL in
complex real-world decision making problems remains limited. Many problems in
operations management (inventory and revenue management, for example) are
characterized by large action spaces and stochastic system dynamics. These
characteristics make the problem considerably harder to solve for existing RL
methods that rely on enumeration techniques to solve per step action problems.
To resolve these issues, we develop Programmable Actor Reinforcement Learning
(PARL), a policy iteration method that uses techniques from integer programming
and sample average approximation. Analytically, we show that the for a given
critic, the learned policy in each iteration converges to the optimal policy as
the underlying samples of the uncertainty go to infinity. Practically, we show
that a properly selected discretization of the underlying uncertain
distribution can yield near optimal actor policy even with very few samples
from the underlying uncertainty. We then apply our algorithm to real-world
inventory management problems with complex supply chain structures and show
that PARL outperforms state-of-the-art RL and inventory optimization methods in
these settings. We find that PARL outperforms commonly used base stock
heuristic by 44.7% and the best performing RL method by up to 12.1% on average
across different supply chain environments.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0205016v1,From Alife Agents to a Kingdom of N Queens,"This paper presents a new approach to solving N-queen problems, which
involves a model of distributed autonomous agents with artificial life (ALife)
and a method of representing N-queen constraints in an agent environment. The
distributed agents locally interact with their living environment, i.e., a
chessboard, and execute their reactive behaviors by applying their behavioral
rules for randomized motion, least-conflict position searching, and cooperating
with other agents etc. The agent-based N-queen problem solving system evolves
through selection and contest according to the rule of Survival of the Fittest,
in which some agents will die or be eaten if their moving strategies are less
efficient than others. The experimental results have shown that this system is
capable of solving large-scale N-queen problems. This paper also provides a
model of ALife agents for solving general CSPs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1905.01018v1,Fractal Time Series Analysis of Social Network Activities,"In the work, a comparative correlation and fractal analysis of time series of
Bitcoin crypto currency rate and community activities in social networks
associated with Bitcoin was conducted. A significant correlation between the
Bitcoin rate and the community activities was detected. Time series fractal
analysis indicated the presence of self-similar and multifractal properties.
The results of researches showed that the series having a strong correlation
dependence have a similar multifractal structure.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1712.06131v1,Super-sparse Learning in Similarity Spaces,"In several applications, input samples are more naturally represented in
terms of similarities between each other, rather than in terms of feature
vectors. In these settings, machine-learning algorithms can become very
computationally demanding, as they may require matching the test samples
against a very large set of reference prototypes. To mitigate this issue,
different approaches have been developed to reduce the number of required
reference prototypes. Current reduction approaches select a small subset of
representative prototypes in the space induced by the similarity measure, and
then separately train the classification function on the reduced subset.
However, decoupling these two steps may not allow reducing the number of
prototypes effectively without compromising accuracy. We overcome this
limitation by jointly learning the classification function along with an
optimal set of virtual prototypes, whose number can be either fixed a priori or
optimized according to application-specific criteria. Creating a super-sparse
set of virtual prototypes provides much sparser solutions, drastically reducing
complexity at test time, at the expense of a slightly increased complexity
during training. A much smaller set of prototypes also results in
easier-to-interpret decisions. We empirically show that our approach can reduce
up to ten times the complexity of Support Vector Machines, LASSO and ridge
regression at test time, without almost affecting their classification
accuracy.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1307.2852v2,Competitive Equilibrium Relaxations in General Auctions,"The goal of an auction is to determine commodity prices such that all
participants are perfectly happy. Such a solution is called a competitive
equilibrium and does not exist in general. For this reason we are interested in
solutions which are similar to a competitive equilibrium. The article
introduces two relaxations of a competitive equilibrium for general auctions.
Both relaxations determine one price per commodity by solving a difficult
non-convex optimization problem. The first model is a mathematical program with
equilibrium constraints (MPEC), which ensures that each participant is either
perfectly happy or his bid is rejected. An exact algorithm and a heuristic are
provided for this model. The second model is a relaxation of the first one and
only ensures that no participant incurs a loss. In an optimal solution to the
second model, no participant can be made better off without making another one
worse off.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/0912.0685v3,"Uniform sampling of undirected and directed graphs with a fixed degree
  sequence","Many applications in network analysis require algorithms to sample uniformly
at random from the set of all graphs with a prescribed degree sequence. We
present a Markov chain based approach which converges to the uniform
distribution of all realizations for both the directed and undirected case. It
remains an open challenge whether these Markov chains are rapidly mixing.
  For the case of directed graphs, we also explain in this paper that a popular
switching algorithm fails in general to sample uniformly at random because the
state graph of the Markov chain decomposes into different isomorphic
components. We call degree sequences for which the state graph is strongly
connected arc swap sequences. To handle arbitrary degree sequences, we develop
two different solutions. The first uses an additional operation (a
reorientation of induced directed 3-cycles) which makes the state graph
strongly connected, the second selects randomly one of the isomorphic
components and samples inside it. Our main contribution is a precise
characterization of arc swap sequences, leading to an efficient recognition
algorithm. Finally, we point out some interesting consequences for network
analysis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1007.4993v1,"Proceedings Ninth International Workshop on the Foundations of
  Coordination Languages and Software Architectures","This volume contains the proceedings of FOCLASA 2010, the 9th International
Workshop on the Foundations of Coordination Languages and Software
Architectures. FOCLASA 2010 was held in Paris, France on July 30th, 2010 as a
satellite event of the 21st International Conference on Concurrency Theory,
CONCUR 2010. The papers presented in this proceedings tackle different issues
that are currently central to our community, namely software adaptation, sensor
networks, distributed control, non-functional aspects of coordination such as
resources, timing and stochastics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0609147v2,Identifying Crosscutting Concerns Using Fan-in Analysis,"Aspect mining is a reverse engineering process that aims at finding
crosscutting concerns in existing systems. This paper proposes an aspect mining
approach based on determining methods that are called from many different
places, and hence have a high fan-in, which can be seen as a symptom of
crosscutting functionality. The approach is semi-automatic, and consists of
three steps: metric calculation, method filtering, and call site analysis.
Carrying out these steps is an interactive process supported by an Eclipse
plug-in called FINT. Fan-in analysis has been applied to three open source Java
systems, totaling around 200,000 lines of code. The most interesting concerns
identified are discussed in detail, which includes several concerns not
previously discussed in the aspect-oriented literature. The results show that a
significant number of crosscutting concerns can be recognized using fan-in
analysis, and each of the three steps can be supported by tools.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9909014v1,Reasoning About Common Knowledge with Infinitely Many Agents,"Complete axiomatizations and exponential-time decision procedures are
provided for reasoning about knowledge and common knowledge when there are
infinitely many agents. The results show that reasoning about knowledge and
common knowledge with infinitely many agents is no harder than when there are
finitely many agents, provided that we can check the cardinality of certain set
differences G - G', where G and G' are sets of agents. Since our complexity
results are independent of the cardinality of the sets G involved, they
represent improvements over the previous results even with the sets of agents
involved are finite. Moreover, our results make clear the extent to which
issues of complexity and completeness depend on how the sets of agents involved
are represented.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0410045v4,"Analysis of and workarounds for element reversal for a finite
  element-based algorithm for warping triangular and tetrahedral meshes","We consider an algorithm called FEMWARP for warping triangular and
tetrahedral finite element meshes that computes the warping using the finite
element method itself. The algorithm takes as input a two- or three-dimensional
domain defined by a boundary mesh (segments in one dimension or triangles in
two dimensions) that has a volume mesh (triangles in two dimensions or
tetrahedra in three dimensions) in its interior. It also takes as input a
prescribed movement of the boundary mesh. It computes as output updated
positions of the vertices of the volume mesh. The first step of the algorithm
is to determine from the initial mesh a set of local weights for each interior
vertex that describes each interior vertex in terms of the positions of its
neighbors. These weights are computed using a finite element stiffness matrix.
After a boundary transformation is applied, a linear system of equations based
upon the weights is solved to determine the final positions of the interior
vertices. The FEMWARP algorithm has been considered in the previous literature
(e.g., in a 2001 paper by Baker). FEMWARP has been succesful in computing
deformed meshes for certain applications. However, sometimes FEMWARP reverses
elements; this is our main concern in this paper. We analyze the causes for
this undesirable behavior and propose several techniques to make the method
more robust against reversals. The most successful of the proposed methods
includes combining FEMWARP with an optimization-based untangler.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0207095v4,Eternity variables to prove simulation of specifications,"Simulations of specifications are introduced as a unification and
generalization of refinement mappings, history variables, forward simulations,
prophecy variables, and backward simulations. A specification implements
another specification if and only if there is a simulation from the first one
to the second one that satisfies a certain condition. By adding stutterings,
the formalism allows that the concrete behaviours take more (or possibly less)
steps than the abstract ones.
  Eternity variables are introduced as a more powerful alternative for prophecy
variables and backward simulations. This formalism is semantically complete:
every simulation that preserves quiescence is a composition of a forward
simulation, an extension with eternity variables, and a refinement mapping.
This result does not need finite invisible nondeterminism and machine closure
as in the Abadi-Lamport Theorem. Internal continuity is weakened to
preservation of quiescence.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.5071v3,"Probabilistic analysis of Wiedemann's algorithm for minimal polynomial
  computation","Blackbox algorithms for linear algebra problems start with projection of the
sequence of powers of a matrix to a sequence of vectors (Lanczos), a sequence
of scalars (Wiedemann) or a sequence of smaller matrices (block methods). Such
algorithms usually depend on the minimal polynomial of the resulting sequence
being that of the given matrix. Here exact formulas are given for the
probability that this occurs. They are based on the generalized Jordan normal
form (direct sum of companion matrices of the elementary divisors) of the
matrix. Sharp bounds follow from this for matrices of unknown elementary
divisors. The bounds are valid for all finite field sizes and show that a small
blocking factor can give high probability of success for all cardinalities and
matrix dimensions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.03057v2,"PEMNET: A Transfer Learning-based Modeling Approach of High-Temperature
  Polymer Electrolyte Membrane Electrochemical Systems","Widespread adoption of high-temperature polymer electrolyte membrane fuel
cells (HT-PEMFCs) and HT-PEM electrochemical hydrogen pumps (HT-PEM ECHPs)
requires models and computational tools that provide accurate scale-up and
optimization. Knowledge-based modeling has limitations as it is time consuming
and requires information about the system that is not always available (e.g.,
material properties and interfacial behavior between different materials).
Data-driven modeling on the other hand, is easier to implement, but often
necessitates large datasets that could be difficult to obtain. In this
contribution, knowledge-based modeling and data-driven modeling are uniquely
combined by implementing a Few-Shot Learning (FSL) approach. A knowledge-based
model originally developed for a HT-PEMFC was used to generate simulated data
(887,735 points) and used to pretrain a neural network source model.
Furthermore, the source model developed for HT-PEMFCs was successfully applied
to HT-PEM ECHPs - a different electrochemical system that utilizes similar
materials to the fuel cell. Experimental datasets from both HT-PEMFCs and
HT-PEM ECHPs with different materials and operating conditions (~50 points
each) were used to train 8 target models via FSL. Models for the unseen data
reached high accuracies in all cases (rRMSE between 1.04 and 3.73% for HT-PEMCs
and between 6.38 and 8.46% for HT-PEM ECHPs).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0803.0316v1,Staged Self-Assembly:Nanomanufacture of Arbitrary Shapes with O(1) Glues,"We introduce staged self-assembly of Wang tiles, where tiles can be added
dynamically in sequence and where intermediate constructions can be stored for
later mixing. This model and its various constraints and performance measures
are motivated by a practical nanofabrication scenario through protein-based
bioengineering. Staging allows us to break through the traditional lower bounds
in tile self-assembly by encoding the shape in the staging algorithm instead of
the tiles. All of our results are based on the practical assumption that only a
constant number of glues, and thus only a constant number of tiles, can be
engineered, as each new glue type requires significant biochemical research and
experiments. Under this assumption, traditional tile self-assembly cannot even
manufacture an n*n square; in contrast, we show how staged assembly enables
manufacture of arbitrary orthogonal shapes in a variety of precise formulations
of the model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9902002v1,"Automatic Identification of Subjects for Textual Documents in Digital
  Libraries","The amount of electronic documents in the Internet grows very quickly. How to
effectively identify subjects for documents becomes an important issue. In
past, the researches focus on the behavior of nouns in documents. Although
subjects are composed of nouns, the constituents that determine which nouns are
subjects are not only nouns. Based on the assumption that texts are
well-organized and event-driven, nouns and verbs together contribute the
process of subject identification. This paper considers four factors: 1) word
importance, 2) word frequency, 3) word co-occurrence, and 4) word distance and
proposes a model to identify subjects for textual documents. The preliminary
experiments show that the performance of the proposed model is close to that of
human beings.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1005.2280v1,"Modelling Nonlinear Sequence Generators in terms of Linear Cellular
  Automata","In this work, a wide family of LFSR-based sequence generators, the so-called
Clock-Controlled Shrinking Generators (CCSGs), has been analyzed and identified
with a subset of linear Cellular Automata (CA). In fact, a pair of linear
models describing the behavior of the CCSGs can be derived. The algorithm that
converts a given CCSG into a CA-based linear model is very simple and can be
applied to CCSGs in a range of practical interest. The linearity of these
cellular models can be advantageously used in two different ways: (a) for the
analysis and/or cryptanalysis of the CCSGs and (b) for the reconstruction of
the output sequence obtained from this kind of generators.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0604023v1,Communication Bottlenecks in Scale-Free Networks,"We consider the effects of network topology on the optimality of packet
routing quantified by $\gamma_c$, the rate of packet insertion beyond which
congestion and queue growth occurs. The key result of this paper is to show
that for any network, there exists an absolute upper bound, expressed in terms
of vertex separators, for the scaling of $\gamma_c$ with network size $N$,
irrespective of the routing algorithm used. We then derive an estimate to this
upper bound for scale-free networks, and introduce a novel static routing
protocol which is superior to shortest path routing under intense packet
insertion rates.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0308026v1,Improvements to time bracketed authentication,"We describe a collection of techniques whereby audiovisual or other
recordings of significant events can be made in a way that hinders
falsification, pre-dating, or post-dating by interested parties, even by the
makers and operators of the recording equipment. A central feature of these
techniques is the interplay between private information, which by its nature is
untrustworthy and susceptible to suppression or manipulation by interested
parties, and public information, which is too widely known to be manipulated by
anyone. While authenticated recordings may be infeasible to falsify, they can
be abused in other ways, such as being used for blackmail or harassment; but
susceptibility to these abuses can be reduced by encryption and secret sharing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1505.03015v1,"Power, Energy and Speed of Embedded and Server Multi-Cores applied to
  Distributed Simulation of Spiking Neural Networks: ARM in NVIDIA Tegra vs
  Intel Xeon quad-cores","This short note regards a comparison of instantaneous power, total energy
consumption, execution time and energetic cost per synaptic event of a spiking
neural network simulator (DPSNN-STDP) distributed on MPI processes when
executed either on an embedded platform (based on a dual socket quad-core ARM
platform) or a server platform (INTEL-based quad-core dual socket platform). We
also compare the measure with those reported by leading custom and semi-custom
designs: TrueNorth and SpiNNaker. In summary, we observed that: 1- we spent 2.2
micro-Joule per simulated event on the ""embedded platform"", approx. 4.4 times
lower than what was spent by the ""server platform""; 2- the instantaneous power
consumption of the ""embedded platform"" was 14.4 times better than the ""server""
one; 3- the server platform is a factor 3.3 faster. The ""embedded platform"" is
made of NVIDIA Jetson TK1 boards, interconnected by Ethernet, each mounting a
Tegra K1 chip including a quad-core ARM Cortex-A15 at 2.3GHz. The ""server
platform"" is based on dual-socket quad-core Intel Xeon CPUs (E5620 at 2.4GHz).
The measures were obtained with the DPSNN-STDP simulator (Distributed Simulator
of Polychronous Spiking Neural Network with synaptic Spike Timing Dependent
Plasticity) developed by INFN, that already proved its efficient scalability
and execution speed-up on hundreds of similar ""server"" cores and MPI processes,
applied to neural nets composed of several billions of synapses.",0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1202.6444v3,"Tensor Rank and Strong Quantum Nondeterminism in Multiparty
  Communication","In this paper we study quantum nondeterminism in multiparty communication.
There are three (possibly) different types of nondeterminism in quantum
computation: i) strong, ii) weak with classical proofs, and iii) weak with
quantum proofs. Here we focus on the first one. A strong quantum
nondeterministic protocol accepts a correct input with positive probability,
and rejects an incorrect input with probability 1. In this work we relate
strong quantum nondeterministic multiparty communication complexity to the rank
of the communication tensor in the Number-On-Forehead and Number-In-Hand
models. In particular, by extending the definition proposed by de Wolf to {\it
nondeterministic tensor-rank} ($nrank$), we show that for any boolean function
$f$ when there is no prior shared entanglement between the players, 1) in the
Number-On-Forehead model, the cost is upper-bounded by the logarithm of
$nrank(f)$; 2) in the Number-In-Hand model, the cost is lower-bounded by the
logarithm of $nrank(f)$. Furthermore, we show that when the number of players
is $o(\log\log n)$ we have that $NQP\nsubseteq BQP$ for Number-On-Forehead
communication.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.07823v1,Polynomial-delay Enumeration Algorithms in Set Systems,"We consider a set system $(V, {\mathcal C}\subseteq 2^V)$ on a finite set $V$
of elements, where we call a set $C\in {\mathcal C}$ a component. We assume
that two oracles $\mathrm{L}_1$ and $\mathrm{L}_2$ are available, where given
two subsets $X,Y\subseteq V$, $\mathrm{L}_1$ returns a maximal component $C\in
{\mathcal C}$ with $X\subseteq C\subseteq Y$; and given a set $Y\subseteq V$,
$\mathrm{L}_2$ returns all maximal components $C\in {\mathcal C}$ with
$C\subseteq Y$. Given a set $I$ of attributes and a function $\sigma:V\to 2^I$
in a transitive system, a component $C\in {\mathcal C}$ is called a solution if
the set of common attributes in $C$ is inclusively maximal; i.e.,
$\bigcap_{v\in C}\sigma(v)\supsetneq \bigcap_{v\in X}\sigma(v)$ for any
component $X\in{\mathcal C}$ with $C\subsetneq X$. We prove that there exists
an algorithm of enumerating all solutions (or all components) in delay bounded
by a polynomial with respect to the input size and the running times of the
oracles.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.03806v2,Flipping the Perspective in Contact Tracing,"We introduce a fundamentally different paradigm for contact tracing: for each
positive case, do not only ask direct contacts to quarantine; instead, tell
everyone how many relationships away the disease just struck (so, ""2"" is a
close physical contact of a close physical contact). This new approach, which
has already been deployed in a publicly downloadable app, brings a new tool to
bear on pandemic control, powered by network theory. Like a weather satellite
providing early warning of incoming hurricanes, it empowers individuals to see
transmission approaching from far away, and incites behavior change to directly
avoid exposure. This flipped perspective engages natural self-interested
instincts of self-preservation, reducing reliance on altruism, and the
resulting caution reduces pandemic spread in the social vicinity of each
infection. Consequently, our new system solves the behavior coordination
problem which has hampered many other app-based interventions to date. We also
provide a heuristic mathematical analysis that shows how our system already
achieves critical mass from the user perspective at very low adoption
thresholds (likely below 10% in some common types of communities as indicated
empirically in the first practical deployment); after that point, the design of
our system naturally accelerates further adoption, while also alerting even
non-users of the app. This article seeks to lay the theoretical foundation for
our approach, and to open the area for further research along many dimensions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/0812.4460v1,"Emergence of Spontaneous Order Through Neighborhood Formation in
  Peer-to-Peer Recommender Systems","The advent of the Semantic Web necessitates paradigm shifts away from
centralized client/server architectures towards decentralization and
peer-to-peer computation, making the existence of central authorities
superfluous and even impossible. At the same time, recommender systems are
gaining considerable impact in e-commerce, providing people with
recommendations that are personalized and tailored to their very needs. These
recommender systems have traditionally been deployed with stark centralized
scenarios in mind, operating in closed communities detached from their host
network's outer perimeter. We aim at marrying these two worlds, i.e.,
decentralized peer-to-peer computing and recommender systems, in one
agent-based framework. Our architecture features an epidemic-style protocol
maintaining neighborhoods of like-minded peers in a robust, selforganizing
fashion. In order to demonstrate our architecture's ability to retain
scalability, robustness and to allow for convergence towards high-quality
recommendations, we conduct offline experiments on top of the popular MovieLens
dataset.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9907034v1,Polynomial-Time Multi-Selectivity,"We introduce a generalization of Selman's P-selectivity that yields a more
flexible notion of selectivity, called (polynomial-time) multi-selectivity, in
which the selector is allowed to operate on multiple input strings. Since our
introduction of this class, it has been used to prove the first known (and
optimal) lower bounds for generalized selectivity-like classes in terms of
EL_2, the second level of the extended low hierarchy. We study the resulting
selectivity hierarchy, denoted by SH, which we prove does not collapse. In
particular, we study the internal structure and the properties of SH and
completely establish, in terms of incomparability and strict inclusion, the
relations between our generalized selectivity classes and Ogihara's P-mc
(polynomial-time membership-comparable) classes. Although SH is a strictly
increasing infinite hierarchy, we show that the core results that hold for the
P-selective sets and that prove them structurally simple also hold for SH. In
particular, all sets in SH have small circuits; the NP sets in SH are in Low_2,
the second level of the low hierarchy within NP; and SAT cannot be in SH unless
P = NP. Finally, it is known that P-Sel, the class of P-selective sets, is not
closed under union or intersection. We provide an extended selectivity
hierarchy that is based on SH and that is large enough to capture those
closures of the P-selective sets, and yet, in contrast with the P-mc classes,
is refined enough to distinguish them.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1410.6863v1,Editing to Eulerian Graphs,"We investigate the problem of modifying a graph into a connected graph in
which the degree of each vertex satisfies a prescribed parity constraint. Let
$ea$, $ed$ and $vd$ denote the operations edge addition, edge deletion and
vertex deletion respectively. For any $S\subseteq \{ea,ed,vd\}$, we define
Connected Degree Parity Editing$(S)$ (CDPE($S$)) to be the problem that takes
as input a graph $G$, an integer $k$ and a function $\delta\colon
V(G)\rightarrow\{0,1\}$, and asks whether $G$ can be modified into a connected
graph $H$ with $d_{H}(v)\equiv\delta(v)~(\bmod~2)$ for each $v\in V(H)$, using
at most $k$ operations from $S$. We prove that
  1. if $S=\{ea\}$ or $S=\{ea,ed\}$, then CDPE($S$) can be solved in polynomial
time;
  2. if $\{vd\} \subseteq S\subseteq \{ea,ed,vd\}$, then CDPE($S$) is
NP-complete and W[1]-hard when parameterized by $k$, even if $\delta\equiv 0$.
  Together with known results by Cai and Yang and by Cygan, Marx, Pilipczuk,
Pilipczuk and Schlotter, our results completely classify the classical and
parameterized complexity of the CDPE($S$) problem for all $S\subseteq
\{ea,ed,vd\}$. We obtain the same classification for a natural variant of the
CDPE($S$) problem on directed graphs, where the target is a weakly connected
digraph in which the difference between the in- and out-degree of every vertex
equals a prescribed value. As an important implication of our results, we
obtain polynomial-time algorithms for the Eulerian Editing problem and its
directed variant.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.04450v2,Scene Editing as Teleoperation: A Case Study in 6DoF Kit Assembly,"Studies in robot teleoperation have been centered around action
specifications -- from continuous joint control to discrete end-effector pose
control. However, these robot-centric interfaces often require skilled
operators with extensive robotics expertise. To make teleoperation accessible
to non-expert users, we propose the framework ""Scene Editing as Teleoperation""
(SEaT), where the key idea is to transform the traditional ""robot-centric""
interface into a ""scene-centric"" interface -- instead of controlling the robot,
users focus on specifying the task's goal by manipulating digital twins of the
real-world objects. As a result, a user can perform teleoperation without any
expert knowledge of the robot hardware. To achieve this goal, we utilize a
category-agnostic scene-completion algorithm that translates the real-world
workspace (with unknown objects) into a manipulable virtual scene
representation and an action-snapping algorithm that refines the user input
before generating the robot's action plan. To train the algorithms, we
procedurally generated a large-scale, diverse kit-assembly dataset that
contains object-kit pairs that mimic real-world object-kitting tasks. Our
experiments in simulation and on a real-world system demonstrate that our
framework improves both the efficiency and success rate for 6DoF kit-assembly
tasks. A user study demonstrates that SEaT framework participants achieve a
higher task success rate and report a lower subjective workload compared to an
alternative robot-centric interface. Video can be found at
https://www.youtube.com/watch?v=-NdR3mkPbQQ .",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1309.6144v2,Playing with parameters: structural parameterization in graphs,"When considering a graph problem from a parameterized point of view, the
parameter chosen is often the size of an optimal solution of this problem (the
""standard"" parameter). A natural subject for investigation is what happens when
we parameterize such a problem by various other parameters, some of which may
be the values of optimal solutions to different problems. Such research is
known as parameterized ecology. In this paper, we investigate seven natural
vertex problems, along with their respective parameters: the size of a maximum
independent set, the size of a minimum vertex cover, the size of a maximum
clique, the chromatic number, the size of a minimum dominating set, the size of
a minimum independent dominating set and the size of a minimum feedback vertex
set. We study the parameterized complexity of each of these problems with
respect to the standard parameter of the others.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0506040v1,A Fixed-Length Coding Algorithm for DNA Sequence Compression,"While achieving a compression ratio of 2.0 bits/base, the new algorithm codes
non-N bases in fixed length. It dramatically reduces the time of coding and
decoding than previous DNA compression algorithms and some universal
compression programs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1501.04186v1,"New Directions in Anonymization: Permutation Paradigm, Verifiability by
  Subjects and Intruders, Transparency to Users","There are currently two approaches to anonymization: ""utility first"" (use an
anonymization method with suitable utility features, then empirically evaluate
the disclosure risk and, if necessary, reduce the risk by possibly sacrificing
some utility) or ""privacy first"" (enforce a target privacy level via a privacy
model, e.g., k-anonymity or epsilon-differential privacy, without regard to
utility). To get formal privacy guarantees, the second approach must be
followed, but then data releases with no utility guarantees are obtained. Also,
in general it is unclear how verifiable is anonymization by the data subject
(how safely released is the record she has contributed?), what type of intruder
is being considered (what does he know and want?) and how transparent is
anonymization towards the data user (what is the user told about methods and
parameters used?).
  We show that, using a generally applicable reverse mapping transformation,
any anonymization for microdata can be viewed as a permutation plus (perhaps) a
small amount of noise; permutation is thus shown to be the essential principle
underlying any anonymization of microdata, which allows giving simple utility
and privacy metrics. From this permutation paradigm, a new privacy model
naturally follows, which we call (d,v)-permuted privacy. The privacy ensured by
this method can be verified by each subject contributing an original record
(subject-verifiability) and also at the data set level by the data protector.
We then proceed to define a maximum-knowledge intruder model, which we argue
should be the one considered in anonymization. Finally, we make the case for
anonymization transparent to the data user, that is, compliant with Kerckhoff's
assumption (only the randomness used, if any, must stay secret).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1804.08032v1,A Channel-based Exact Inference Algorithm for Bayesian Networks,"This paper describes a new algorithm for exact Bayesian inference that is
based on a recently proposed compositional semantics of Bayesian networks in
terms of channels. The paper concentrates on the ideas behind this algorithm,
involving a linearisation (`stretching') of the Bayesian network, followed by a
combination of forward state transformation and backward predicate
transformation, while evidence is accumulated along the way. The performance of
a prototype implementation of the algorithm in Python is briefly compared to a
standard implementation (pgmpy): first results show competitive performance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1404.3660v1,"Constant-factor approximations for Capacitated Arc Routing without
  triangle inequality","Given an undirected graph with edge costs and edge demands, the Capacitated
Arc Routing problem (CARP) asks for minimum-cost routes for equal-capacity
vehicles so as to satisfy all demands. Constant-factor polynomial-time
approximation algorithms were proposed for CARP with triangle inequality, while
CARP was claimed to be NP-hard to approximate within any constant factor in
general. Correcting this claim, we show that any factor {\alpha} approximation
for CARP with triangle inequality yields a factor {\alpha} approximation for
the general CARP.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1502.02417v1,"Semantics-based services for a low carbon society: An application on
  emissions trading system data and scenarios management","A low carbon society aims at fighting global warming by stimulating synergic
efforts from governments, industry and scientific communities. Decision support
systems should be adopted to provide policy makers with possible scenarios,
options for prompt countermeasures in case of side effects on environment,
economy and society due to low carbon society policies, and also options for
information management. A necessary precondition to fulfill this agenda is to
face the complexity of this multi-disciplinary domain and to reach a common
understanding on it as a formal specification. Ontologies are widely accepted
means to share knowledge. Together with semantic rules, they enable advanced
semantic services to manage knowledge in a smarter way. Here we address the
European Emissions Trading System (EU-ETS) and we present a knowledge base
consisting of the EREON ontology and a catalogue of rules. Then we describe two
innovative semantic services to manage ETS data and information on ETS
scenarios.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9809051v1,"Spoken Language Dialogue Systems and Components: Best practice in
  development and evaluation (DISC 24823) - Periodic Progress Report 1: Basic
  Details of the Action","The DISC project aims to (a) build an in-depth understanding of the
state-of-the-art in spoken language dialogue systems (SLDSs) and components
development and evaluation with the purpose of (b) developing a first best
practice methodology in the field. The methodology will be accompanied by (c) a
series of development and evaluation support tools. To the limited extent
possible within the duration of the project, the draft versions of the
methodology and the tools will be (d) tested by SLDS developers from industry
and research, and will be (e) packaged to best suit their needs. In the first
year of DISC, (a) has been accomplished, and (b) and (c) have started. A
proposal to complete the work proposed above by adding 12 months to the 18
months of the present project, has been submitted to Esprit Long-Term Research
in March 1998.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0812.0852v3,Hierarchy and equivalence of multi-letter quantum finite automata,"Multi-letter {\it quantum finite automata} (QFAs) were a new one-way QFA
model proposed recently by Belovs, Rosmanis, and Smotrovs (LNCS, Vol. 4588,
Springer, Berlin, 2007, pp. 60-71), and they showed that multi-letter QFAs can
accept with no error some regular languages ($(a+b)^{*}b$) that are
unacceptable by the one-way QFAs. In this paper, we continue to study
multi-letter QFAs. We mainly focus on two issues: (1) we show that
$(k+1)$-letter QFAs are computationally more powerful than $k$-letter QFAs,
that is, $(k+1)$-letter QFAs can accept some regular languages that are
unacceptable by any $k$-letter QFA. A comparison with the one-way QFAs is made
by some examples; (2) we prove that a $k_{1}$-letter QFA ${\cal A}_1$ and
another $k_{2}$-letter QFA ${\cal A}_2$ are equivalent if and only if they are
$(n_{1}+n_{2})^{4}+k-1$-equivalent, and the time complexity of determining the
equivalence of two multi-letter QFAs using this method is
$O(n^{12}+k^{2}n^{4}+kn^{8})$, where $n_{1}$ and $n_{2}$ are the numbers of
states of ${\cal A}_{1}$ and ${\cal A}_{2}$, respectively, and
$k=\max(k_{1},k_{2})$. Some other issues are addressed for further
consideration.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1910.13313v1,"Multi-material Topology Optimization of Lattice Structures using
  Geometry Projection","This work presents a computational method for the design of architected truss
lattice materials where each strut can be made of one of a set of available
materials. We design the lattices to extremize effective properties. As
customary in topology optimization, we design a periodic unit cell of the
lattice and obtain the effective properties via numerical homogenization. Each
bar is represented as a cylindrical offset surface of a medial axis
parameterized by the positions of the endpoints of the medial axis. These
parameters are smoothly mapped onto a continuous density field for the primal
and sensitivity analysis via the geometry projection method. A size variable
per material is ascribed to each bar and penalized as in density-based topology
optimization to facilitate the entire removal of bars from the design. During
the optimization, we allow bars to be made of a mixture of the available
materials. However, to ensure each bar is either exclusively made of one
material or removed altogether from the optimal design, we impose optimization
constraints that ensure each size variable is 0 or 1, and that at most one
material size variable is 1. The proposed material interpolation scheme readily
accommodates any number of materials. To obtain lattices with desired material
symmetries, we design only a reference region of the unit cell and reflect its
geometry projection with respect to the appropriate planes of symmetry. Also,
to ensure bars remain whole upon reflection inside the unit cell or with
respect to the periodic boundaries, we impose a no-cut constraint on the bars.
We demonstrate the efficacy of our method via numerical examples of bulk and
shear moduli maximization and Poisson's ratio minimization for two- and
three-material lattices with cubic symmetry.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.07936v1,"Birkhoff's Completeness Theorem for Multi-Sorted Algebras Formalized in
  Agda","This document provides a formal proof of Birkhoff's completeness theorem for
multi-sorted algebras which states that any equational entailment valid in all
models is also provable in the equational theory.
  More precisely, if a certain equation is valid in all models that validate a
fixed set of equations, then this equation is derivable from that set using the
proof rules for a congruence.
  The proof has been formalized in Agda version 2.6.2 with the Agda Standard
Library version 1.7 and this document reproduces the commented Agda code.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.05704v1,"Automatic code generation from sketches of mobile applications in
  end-user development using Deep Learning","A common need for mobile application development by end-users or in computing
education is to transform a sketch of a user interface into wireframe code
using App Inventor, a popular block-based programming environment. As this task
is challenging and time-consuming, we present the Sketch2aia approach that
automates this process. Sketch2aia employs deep learning to detect the most
frequent user interface components and their position on a hand-drawn sketch
creating an intermediate representation of the user interface and then
automatically generates the App Inventor code of the wireframe. The approach
achieves an average user interface component classification accuracy of 87,72%
and results of a preliminary user evaluation indicate that it generates
wireframes that closely mirror the sketches in terms of visual similarity. The
approach has been implemented as a web tool and can be used to support the
end-user development of mobile applications effectively and efficiently as well
as the teaching of user interface design in K-12.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.08948v2,"A New Citation Recommendation Strategy Based on Term Functions in
  Related Studies Section","Purpose: Researchers frequently encounter the following problems when writing
scientific articles: (1) Selecting appropriate citations to support the
research idea is challenging. (2) The literature review is not conducted
extensively, which leads to working on a research problem that others have well
addressed. This study focuses on citation recommendation in the related studies
section by applying the term function of a citation context, potentially
improving the efficiency of writing a literature review.
Design/methodology/approach: We present nine term functions with three newly
created and six identified from existing literature. Using these term functions
as labels, we annotate 531 research papers in three topics to evaluate our
proposed recommendation strategy. BM25 and Word2vec with VSM are implemented as
the baseline models for the recommendation. Then the term function information
is applied to enhance the performance. Findings: The experiments show that the
term function-based methods outperform the baseline methods regarding the
recall, precision, and F1-score measurement, demonstrating that term functions
are useful in identifying valuable citations. Research limitations: The dataset
is insufficient due to the complexity of annotating citation functions for
paragraphs in the related studies section. More recent deep learning models
should be performed to future validate the proposed approach. Practical
implications: The citation recommendation strategy can be helpful for valuable
citation discovery, semantic scientific retrieval, and automatic literature
review generation. Originality/value: The proposed citation function-based
citation recommendation can generate intuitive explanations of the results for
users, improving the transparency, persuasiveness, and effectiveness of
recommender systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1708.06233v1,Fake News in Social Networks,"We model the spread of news as a social learning game on a network. Agents
can either endorse or oppose a claim made in a piece of news, which itself may
be either true or false. Agents base their decision on a private signal and
their neighbors' past actions. Given these inputs, agents follow strategies
derived via multi-agent deep reinforcement learning and receive utility from
acting in accordance with the veracity of claims. Our framework yields
strategies with agent utility close to a theoretical, Bayes optimal benchmark,
while remaining flexible to model re-specification. Optimized strategies allow
agents to correctly identify most false claims, when all agents receive
unbiased private signals. However, an adversary's attempt to spread fake news
by targeting a subset of agents with a biased private signal can be successful.
Even more so when the adversary has information about agents' network position
or private signal. When agents are aware of the presence of an adversary they
re-optimize their strategies in the training stage and the adversary's attack
is less effective. Hence, exposing agents to the possibility of fake news can
be an effective way to curtail the spread of fake news in social networks. Our
results also highlight that information about the users' private beliefs and
their social network structure can be extremely valuable to adversaries and
should be well protected.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1909.10227v3,Deep Convolutions for In-Depth Automated Rock Typing,"The description of rocks is one of the most time-consuming tasks in the
everyday work of a geologist, especially when very accurate description is
required. We here present a method that reduces the time needed for accurate
description of rocks, enabling the geologist to work more efficiently. We
describe the application of methods based on color distribution analysis and
feature extraction. Then we focus on a new approach, used by us, which is based
on convolutional neural networks. We used several well-known neural network
architectures (AlexNet, VGG, GoogLeNet, ResNet) and made a comparison of their
performance. The precision of the algorithms is up to 95% on the validation set
with GoogLeNet architecture. The best of the proposed algorithms can describe
50 m of full-size core in one minute.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1310.5469v1,Parameterized Algorithms for Finding Square Roots,"We show that the following two problems are fixed-parameter tractable with
parameter k: testing whether a connected n-vertex graph with m edges has a
square root with at most n-1+k edges and testing whether such a graph has a
square root with at least m-k edges. Our first result implies that squares of
graphs obtained from trees by adding at most k edges can be recognized in
polynomial time for every fixed k>=0; previously this result was known only for
k=0. Our second result is equivalent to stating that deciding whether a graph
can be modified into a square root of itself by at most k edge deletions is
fixed-parameter tractable with parameter k.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0003072v1,"MOO: A Methodology for Online Optimization through Mining the Offline
  Optimum","Ports, warehouses and courier services have to decide online how an arriving
task is to be served in order that cost is minimized (or profit maximized).
These operators have a wealth of historical data on task assignments; can these
data be mined for knowledge or rules that can help the decision-making?
  MOO is a novel application of data mining to online optimization. The idea is
to mine (logged) expert decisions or the offline optimum for rules that can be
used for online decisions. It requires little knowledge about the task
distribution and cost structure, and is applicable to a wide range of problems.
  This paper presents a feasibility study of the methodology for the well-known
k-server problem. Experiments with synthetic data show that optimization can be
recast as classification of the optimum decisions; the resulting heuristic can
achieve the optimum for strong request patterns, consistently outperforms other
heuristics for weak patterns, and is robust despite changes in cost model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.04553v1,"LP WAN Gateway Location Selection Using Modified K-Dominating Set
  Algorithm","The LP WAN networks use gateways or base stations to communicate with devices
distributed on large distances, up to tens of kilometres. The selection of
optimal gateway locations in wireless networks should allow providing the
complete coverage for a given set of nodes, taking into account the
limitations, such as the number of nodes served per access point or required
redundancy. In this paper, we describe the problem of selecting the base
stations in a network using the concept of $k$-dominating set. In our model, we
include information about the required redundancy and spectral efficiency. We
consider the additional requirements on the resulting connections and provide
the greedy algorithm for solving the problem. The algorithm is evaluated in
randomly generated network topologies and using the coordinates of sample real
smart metering networks.",0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0906.2315v2,Symbolic Script Programming for Java,"Computer algebra in Java is a promising field of development. It has not yet
reached an industrial strength, in part because of a lack of good user
interfaces. Using a general purpose scripting language can bring a natural
mathematical notation, akin to the one of specialized interfaces included in
most computer algebra systems. We present such an interface for Java computer
algebra libraries, using scripts available in the JSR 223 framework. We
introduce the concept of `symbolic programming' and show its usefulness by
prototypes of symbolic polynomials and polynomial rings.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1509.03335v1,Decomposing Digital Paintings into Layers via RGB-space Geometry,"In digital painting software, layers organize paintings. However, layers are
not explicitly represented, transmitted, or published with the final digital
painting. We propose a technique to decompose a digital painting into layers.
In our decomposition, each layer represents a coat of paint of a single paint
color applied with varying opacity throughout the image. Our decomposition is
based on the painting's RGB-space geometry. In RGB-space, a geometric structure
is revealed due to the linear nature of the standard Porter-Duff ""over"" pixel
compositing operation. The vertices of the convex hull of pixels in RGB-space
suggest paint colors. Users choose the degree of simplification to perform on
the convex hull, as well as a layer order for the colors. We solve a
constrained optimization problem to find maximally translucent, spatially
coherent opacity for each layer, such that the composition of the layers
reproduces the original image. We demonstrate the utility of the resulting
decompositions for re-editing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1706.08312v2,$Œº$Nap: Practical Micro-Sleeps for 802.11 WLANs,"In this paper, we revisit the idea of putting interfaces to sleep during
'packet overhearing' (i.e., when there are ongoing transmissions addressed to
other stations) from a practical standpoint. To this aim, we perform a robust
experimental characterisation of the timing and consumption behaviour of a
commercial 802.11 card. We design $\mu$Nap, a local standard-compliant
energy-saving mechanism that leverages micro-sleep opportunities inherent to
the CSMA operation of 802.11 WLANs. This mechanism is backwards compatible and
incrementally deployable, and takes into account the timing limitations of
existing hardware, as well as practical CSMA-related issues (e.g., capture
effect). According to the performance assessment carried out through
trace-based simulation, the use of our scheme would result in a 57% reduction
in the time spent in overhearing, thus leading to an energy saving of 15.8% of
the activity time.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1610.09900v2,Inference Compilation and Universal Probabilistic Programming,"We introduce a method for using deep neural networks to amortize the cost of
inference in models from the family induced by universal probabilistic
programming languages, establishing a framework that combines the strengths of
probabilistic programming and deep learning methods. We call what we do
""compilation of inference"" because our method transforms a denotational
specification of an inference problem in the form of a probabilistic program
written in a universal programming language into a trained neural network
denoted in a neural network specification language. When at test time this
neural network is fed observational data and executed, it performs approximate
inference in the original model specified by the probabilistic program. Our
training objective and learning procedure are designed to allow the trained
neural network to be used as a proposal distribution in a sequential importance
sampling inference engine. We illustrate our method on mixture models and
Captcha solving and show significant speedups in the efficiency of inference.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.01174v3,"LiDARNet: A Boundary-Aware Domain Adaptation Model for Point Cloud
  Semantic Segmentation","We present a boundary-aware domain adaptation model for LiDAR scan full-scene
semantic segmentation (LiDARNet). Our model can extract both the domain private
features and the domain shared features with a two-branch structure. We
embedded Gated-SCNN into the segmentor component of LiDARNet to learn boundary
information while learning to predict full-scene semantic segmentation labels.
Moreover, we further reduce the domain gap by inducing the model to learn a
mapping between two domains using the domain shared and private features.
Additionally, we introduce a new dataset (SemanticUSL\footnote{The access
address of
SemanticUSL:\url{https://unmannedlab.github.io/research/SemanticUSL}}) for
domain adaptation for LiDAR point cloud semantic segmentation. The dataset has
the same data format and ontology as SemanticKITTI. We conducted experiments on
real-world datasets SemanticKITTI, SemanticPOSS, and SemanticUSL, which have
differences in channel distributions, reflectivity distributions, diversity of
scenes, and sensors setup. Using our approach, we can get a single
projection-based LiDAR full-scene semantic segmentation model working on both
domains. Our model can keep almost the same performance on the source domain
after adaptation and get an 8\%-22\% mIoU performance increase in the target
domain.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.10094v2,Finding All Bounded-Length Simple Cycles in a Directed Graph,"A new efficient algorithm is presented for finding all simple cycles that
satisfy a length constraint in a directed graph. When the number of vertices is
non-trivial, most cycle-finding problems are of practical interest for sparse
graphs only. We show that for a class of sparse graphs in which the vertex
degrees are almost uniform, our algorithm can find all cycles of length less
than or equal to $k$ in $O((c+n)(k-1)d^k)$ steps, where $n$ is the number of
vertices, $c$ is the total number of cycles discovered, $d$ is the average
degree of the graph's vertices, and $k > 1$. While our analysis for the running
time addresses only a class of sparse graphs, we provide empirical and
experimental evidence of the efficiency of the algorithm for general sparse
graphs. This algorithm is a significant improvement over the only other
deterministic algorithm for this problem known to us; it also lends itself to
massive parallelism. Experimental results of a serial implementation on some
large real-world graphs are presented.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.05829v1,"NekRS, a GPU-Accelerated Spectral Element Navier-Stokes Solver","The development of NekRS, a GPU-oriented thermal-fluids simulation code based
on the spectral element method (SEM) is described. For performance portability,
the code is based on the open concurrent compute abstraction and leverages
scalable developments in the SEM code Nek5000 and in libParanumal, which is a
library of high-performance kernels for high-order discretizations and
PDE-based miniapps. Critical performance sections of the Navier-Stokes time
advancement are addressed. Performance results on several platforms are
presented, including scaling to 27,648 V100s on OLCF Summit, for calculations
of up to 60B gridpoints.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9908005v3,Polygonal Chains Cannot Lock in 4D,"We prove that, in all dimensions d>=4, every simple open polygonal chain and
every tree may be straightened, and every simple closed polygonal chain may be
convexified. These reconfigurations can be achieved by algorithms that use
polynomial time in the number of vertices, and result in a polynomial number of
``moves.'' These results contrast to those known for d=2, where trees can
``lock,'' and for d=3, where open and closed chains can lock.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1603.07392v2,"A Comment on the Averseness of Random Serial Dictatorship to Stochastic
  Dominance Efficiency","Random Serial Dictatorship (RSD) is arguably the most well-known and widely
used assignment rule. Although it returns an ex post efficient assignment,
Bogomolnaia and Moulin (A new solution to the random assignment problem, J.
Econ. Theory 100, 295--328) proved that RSD may not be SD-efficient (efficient
with respect stochastic dominance). The result raises the following question:
under what conditions is RSD not SD-efficient? In this comment, we give a
detailed argument that the RSD assignment is not SD-efficient if and only if an
ex post assignment exists that is not SD-efficient. Hence RSD can be viewed as
being inherently averse to SD-efficiency. The characterization was proved by
Manea (2009).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1101.0057v1,PERSEUS Technology: New Trends in Information and Communication Security,"Using cryptography to protect information and communication has bacically two
major drawbacks. First, the specific entropy profile of encrypted data makes
their detection very easy. Second, the use of cryptography can be more or less
regulated, not to say forbidden, according to the countries. If the right to
freely protect our personal and private data is a fundamental right, it must
not hinder the action of Nation States with respect to National security.
Allowing encryption to citizens holds for bad guys as well.
  In this paper we propose a new approach in information and communication
security that may solve all these issues, thus representing a rather
interesting trade-off between apparently opposite security needs. We introduce
the concept of scalable security based on computationnally hard problem of
coding theory with the PERSEUS technology.
  The core idea is to encode date with variable punctured convolutional codes
in such a way that any cryptanalytic attempt will require a time-consuming
encoder reconstruction in order to decode. By adding noise in a suitable way,
that reconstruction becomes untractable in practice except for Intelligence
services that however must use supercomputers during a significant, scalable
amount of time. Hence it limits naturally any will to unduly performs such
attacks (eg. against citizens' privacy).
  On the users' side, encoder and noise parameters are first exchanged through
an initial, short HTTPS session. The principles behind that approach have been
mathematically validated in 1997 and 2007. We present the PERSEUS library we
have developed under the triple GPL/LGPL/MPL licences. This library can be used
to protect any kind of data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0
http://arxiv.org/abs/1309.6650v1,An Inter-lingual Reference Approach For Multi-Lingual Ontology Matching,"Ontologies are considered as the backbone of the Semantic Web. With the
rising success of the Semantic Web, the number of participating communities
from different countries is constantly increasing. The growing number of
ontologies available in different natural languages leads to an
interoperability problem. In this paper, we discuss several approaches for
ontology matching; examine similarities and differences, identify weaknesses,
and compare the existing automated approaches with the manual approaches for
integrating multilingual ontologies. In addition to that, we propose a new
architecture for a multilingual ontology matching service. As a case study we
used an example of two multilingual enterprise ontologies - the university
ontology of Freie Universitaet Berlin and the ontology for Fayoum University in
Egypt.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.4062v1,Permutation Reconstruction from MinMax-Betweenness Constraints,"In this paper, we investigate the reconstruction of permutations on {1, 2,
..., n} from betweenness constraints involving the minimum and the maximum
element located between t and t+1, for all t=1, 2, ..., n-1. We propose two
variants of the problem (directed and undirected), and focus first on the
directed version, for which we draw up general features and design a polynomial
algorithm in a particular case. Then, we investigate necessary and sufficient
conditions for the uniqueness of the reconstruction in both directed and
undirected versions, using a parameter k whose variation controls the
stringency of the betweenness constraints. We finally point out open problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.02291v1,"Framework for Inferring Leadership Dynamics of Complex Movement from
  Time Series","Leadership plays a key role in social animals, including humans,
decision-making and coalescence in coordinated activities such as hunting,
migration, sport, diplomatic negotiation etc. In these coordinated activities,
leadership is a process that organizes interactions among members to make a
group achieve collective goals. Understanding initiation of coordinated
activities allows scientists to gain more insight into social species
behaviors. However, by using only time series of activities data, inferring
leadership as manifested by the initiation of coordinated activities faces many
challenging issues. First, coordinated activities are dynamic and are changing
over time. Second, several different coordinated activities might occur
simultaneously among subgroups. Third, there is no fundamental concept to
describe these activities computationally. In this paper, we formalize Faction
Initiator Inference Problem and propose a leadership inference framework as a
solution of this problem. The framework makes no assumption about the
characteristics of a leader or the parameters of the coordination process. The
framework performs better than our non-trivial baseline in both simulated and
biological datasets (schools of fish). Moreover, we demonstrate the application
of our framework as a tool to study group merging and splitting dynamics on
another biological dataset of trajectories of wild baboons. In addition, our
problem formalization and framework enable opportunities for scientists to
analyze coordinated activities and generate scientific hypotheses about
collective behaviors that can be tested statistically and in the field.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.14606v1,"Improving Generalization of Deep Fault Detection Models in the Presence
  of Mislabeled Data","Mislabeled samples are ubiquitous in real-world datasets as rule-based or
expert labeling is usually based on incorrect assumptions or subject to biased
opinions. Neural networks can ""memorize"" these mislabeled samples and, as a
result, exhibit poor generalization. This poses a critical issue in fault
detection applications, where not only the training but also the validation
datasets are prone to contain mislabeled samples. In this work, we propose a
novel two-step framework for robust training with label noise. In the first
step, we identify outliers (including the mislabeled samples) based on the
update in the hypothesis space. In the second step, we propose different
approaches to modifying the training data based on the identified outliers and
a data augmentation technique. Contrary to previous approaches, we aim at
finding a robust solution that is suitable for real-world applications, such as
fault detection, where no clean, ""noise-free"" validation dataset is available.
Under an approximate assumption about the upper limit of the label noise, we
significantly improve the generalization ability of the model trained under
massive label noise.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1108.2359v1,"An Abstract Semantics for Inference of Types and Effects in a Multi-Tier
  Web Language","Types-and-effects are type systems, which allow one to express general
semantic properties and to statically reason about program's execution. They
have been widely exploited to specify static analyses, for example to track
computational side effects, exceptions and communications in concurrent
programs. In this paper we adopt abstract interpretation techniques to
reconstruct (following the Cousot's methodology) a types-and-effects system
developed to handle security problems of a multi-tier web language. Our
reconstruction allows us to show that this types-and-effects system is not
sound with respect to the semantics of the language. In addition, we correct
the soundness issues in the analysis and systematically construct a correct
analyser.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0109001v1,"Abstract Computability, Algebraic Specification and Initiality","computable functions are defined by abstract finite deterministic algorithms
on many-sorted algebras. We show that there exist finite universal algebraic
specifications that specify uniquely (up to isomorphism) (i) all abstract
computable functions on any many-sorted algebra; and (ii) all functions
effectively approximable by abstract computable functions on any metric
algebra.
  We show that there exist universal algebraic specifications for all the
classically computable functions on the set R of real numbers. The algebraic
specifications used are mainly bounded universal equations and conditional
equations. We investigate the initial algebra semantics of these
specifications, and derive situations where algebraic specifications define
precisely the computable functions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.06283v1,Analog Computing for Molecular Dynamics,"Modern analog computers are ideally suited to solving large systems of
ordinary differential equations at high speed with low energy consumtion and
limited accuracy. In this article, we survey N-body physics, applied to a
simple water model inspired by force fields which are popular in molecular
dynamics. We demonstrate a setup which simulate a single water molecule in
time. To the best of our knowledge such a simulation has never been done on
analog computers before. Important implementation aspects of the model, such as
scaling, data range and circuit design, are highlighted. We also analyze the
performance and compare the solution with a numerical approach.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1807.00940v1,"Automated Proofs of Unique Normal Forms w.r.t. Conversion for Term
  Rewriting Systems","The notion of normal forms is ubiquitous in various equivalent
transformations. Confluence (CR), one of the central properties of term
rewriting systems (TRSs), concerns uniqueness of normal forms. Yet another such
property, which is weaker than confluence, is the property of unique normal
forms w.r.t. conversion (UNC). Famous examples having UNC but not CR include
the TRSs consisting of S,K,I-rules for the combinatory logic supplemented with
various pairing rules (de Vrijer, 1999). Recently, automated confluence proof
of TRSs has caught attentions leading to investigations of automatable methods
for (dis)proving CR of TRSs; some powerful confluence tools have been developed
as well. In contrast, there have been little efforts on (dis)proving UNC
automatically yet. Indeed, there are few tools that are capable of (dis)proving
UNC; furthermore, only few UNC criteria have been elaborated in these tools. In
this paper, we address automated methods to prove or disprove UNC of given
TRSs. We report automation of some criteria known so far, and also present some
new criteria and methods for proving or disproving UNC. Presented methods are
implemented over the confluence prover ACP (Aoto et al., 2009) and an
experimental evaluation is reported.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1401.5752v1,"Towards a Software Architecture Maturity Model for Improving
  Ultra-Large-Scale Systems Interoperability","For the last two decades, software architecture has been adopted as one of
the main viable solutions to address the ever-increasing demands in the design
and development of software systems. Nevertheless, the rapidly growing
utilization of communication networks and interconnections among software
systems have introduced some critical challenges, which need to be handled in
order to fully unleash the potential of these systems. In this respect,
Ultra-Large-Scale (ULS) systems, generally considered as a system of systems,
have gained considerable attention, since their scale is incomparable to the
traditional systems. The scale of ULS systems makes drastic changes in various
aspects of system development. As a result, it requires that we broaden our
understanding of software architectures and the ways we structure them. In this
paper, we investigate the lack of an architectural maturity model framework for
ULS system interoperability, and propose an architectural maturity model
framework to improve ULS system interoperability.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0302026v1,"Recursive function templates as a solution of linear algebra expressions
  in C++","The article deals with a kind of recursive function templates in C++, where
the recursion is realized corresponding template parameters to achieve better
computational performance. Some specialization of these template functions ends
the recursion and can be implemented using optimized hardware dependent or
independent routines. The method is applied in addition to the known expression
templates technique to solve linear algebra expressions with the help of the
BLAS library. The whole implementation produces a new library, which keeps
object-oriented benefits and has a higher computational speed represented in
the tests.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.00793v1,"Results and Tools for Evaluating the Effectiveness of Focusing Systems
  to Improve Accessibility in Wireless Networks","The widespread use of wireless technologies leads to an ever-increasing
number of users and permanently functioning devices. However, the growth of the
number of wireless users in a limited space and a limited frequency range leads
to an increase in their mutual influence, which ultimately affects the
throughput of wireless channels and even the performance of the system as a
whole. The article presents the statistics and tendencies of the distribution
of wireless networks of the IEEE 802.11 standard systems, as well as analyzes
the main problems that arise during the expansion of their use. Substantiation
and choice of ways to overcome these difficulties largely depends on the
objective control of radiation parameters of access points and subscriber funds
in a particular environment. The review of the state control facilities
provided by the developers of the equipment is presented, and author's variants
of experimental measuring complexes are offered, allowing to control signal and
information parameters of Wi-Fi systems. The experimental results obtained with
the use of the indicated means, obtained using the accelerating metal-plate
lens as an additional autonomous element for focusing the field, including for
MIMO systems, the effect of the accelerating metal-plate lens on the spatial
distribution of the field, on the spectral structure of the signal are
presented. In addition, polarization effects were investigated. Possible ways
to further increase the availability, integrity of information and energy
efficiency of wireless access systems are discussed. The authors propose
simpler and less costly options for increasing the direction of radiation on
the basis of an accelerating metal-plate lens, experimentally tested, as well
as the use of zone zoning on the path of the computer.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.04318v1,"Computer-Aided Data Mining: Automating a Novel Knowledge Discovery and
  Data Mining Process Model for Metabolomics","This work presents MeKDDaM-SAGA, computer-aided automation software for
implementing a novel knowledge discovery and data mining process model that was
designed for performing justifiable, traceable and reproducible metabolomics
data analysis. The process model focuses on achieving metabolomics analytical
objectives and on considering the nature of its involved data. MeKDDaM-SAGA was
successfully used for guiding the process model execution in a number of
metabolomics applications. It satisfies the requirements of the proposed
process model design and execution. The software realises the process model
layout, structure and flow and it enables its execution externally using
various data mining and machine learning tools or internally using a number of
embedded facilities that were built for performing a number of automated
activities such as data preprocessing, data exploration, data acclimatization,
modelling, evaluation and visualization. MeKDDaM-SAGA was developed using
object-oriented software engineering methodology and was constructed in Java.
It consists of 241 design classes that were designed to implement 27 use-cases.
The software uses an XML database to guarantee portability and uses a GUI
interface to ensure its user-friendliness. It implements an internal embedded
version control system that is used to realise and manage the process flow,
feedback and iterations and to enable undoing and redoing the execution of the
process phases, activities, and the internal tasks within its phases.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2111.11859v1,Longitudinal Speech Biomarkers for Automated Alzheimer's Detection,"We introduce a novel audio processing architecture, the Open Voice Brain
Model (OVBM), improving detection accuracy for Alzheimer's (AD) longitudinal
discrimination from spontaneous speech. We also outline the OVBM design
methodology leading us to such architecture, which in general can incorporate
multimodal biomarkers and target simultaneously several diseases and other AI
tasks. Key in our methodology is the use of multiple biomarkers complementing
each other, and when two of them uniquely identify different subjects in a
target disease we say they are orthogonal. We illustrate the methodology by
introducing 16 biomarkers, three of which are orthogonal, demonstrating
simultaneous above state-of-the-art discrimination for apparently unrelated
diseases such as AD and COVID-19. Inspired by research conducted at the MIT
Center for Brain Minds and Machines, OVBM combines biomarker implementations of
the four modules of intelligence: The brain OS chunks and overlaps audio
samples and aggregates biomarker features from the sensory stream and cognitive
core creating a multi-modal graph neural network of symbolic compositional
models for the target task. We apply it to AD, achieving above state-of-the-art
accuracy of 93.8% on raw audio, while extracting a subject saliency map that
longitudinally tracks relative disease progression using multiple biomarkers,
16 in the reported AD task. The ultimate aim is to help medical practice by
detecting onset and treatment impact so that intervention options can be
longitudinally tested. Using the OBVM design methodology, we introduce a novel
lung and respiratory tract biomarker created using 200,000+ cough samples to
pre-train a model discriminating cough cultural origin. This cough dataset sets
a new benchmark as the largest audio health dataset with 30,000+ subjects
participating in April 2020, demonstrating for the first-time cough cultural
bias.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1611.09541v2,"On the Complexity of the Word Problem for Automaton Semigroups and
  Automaton Groups","In this paper, we study the word problem for automaton semigroups and
automaton groups from a complexity point of view. As an intermediate concept
between automaton semigroups and automaton groups, we introduce
automaton-inverse semigroups, which are generated by partial, yet invertible
automata. We show that there is an automaton-inverse semigroup and, thus, an
automaton semigroup with a PSPACE-complete word problem. We also show that
there is an automaton group for which the word problem with a single rational
constraint is PSPACE-complete. Additionally, we provide simpler constructions
for the uniform word problems of these classes. For the uniform word problem
for automaton groups (without rational constraints), we show NL-hardness.
Finally, we investigate a question asked by Cain about a better upper bound for
the length of a word on which two distinct elements of an automaton semigroup
must act differently.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0112010v1,A Straightforward Approach to Morphological Analysis and Synthesis,"In this paper we present a lexicon-based approach to the problem of
morphological processing. Full-form words, lemmas and grammatical tags are
interconnected in a DAWG. Thus, the process of analysis/synthesis is reduced to
a search in the graph, which is very fast and can be performed even if several
pieces of information are missing from the input. The contents of the DAWG are
updated using an on-line incremental process. The proposed approach is language
independent and it does not utilize any morphophonetic rules or any other
special linguistic information.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.11817v1,Scalable Source Code Similarity Detection in Large Code Repositories,"Source code similarity are increasingly used in application development to
identify clones, isolate bugs, and find copy-rights violations. Similar code
fragments can be very problematic due to the fact that errors in the original
code must be fixed in every copy. Other maintenance changes, such as extensions
or patches, must be applied multiple times. Furthermore, the diversity of
coding styles and flexibility of modern languages makes it difficult and cost
ineffective to manually inspect large code repositories. Therefore, detection
is only feasible by automatic techniques. We present an efficient and scalable
approach for similar code fragment identification based on source code control
flow graphs fingerprinting. The source code is processed to generate control
flow graphs that are then hashed to create a unique fingerprint of the code
capturing semantics as well as syntax similarity. The fingerprints can then be
efficiently stored and retrieved to perform similarity search between code
fragments. Experimental results from our prototype implementation supports the
validity of our approach and show its effectiveness and efficiency in
comparison with other solutions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.4754v1,Will This Paper Increase Your h-index? Scientific Impact Prediction,"Scientific impact plays a central role in the evaluation of the output of
scholars, departments, and institutions. A widely used measure of scientific
impact is citations, with a growing body of literature focused on predicting
the number of citations obtained by any given publication. The effectiveness of
such predictions, however, is fundamentally limited by the power-law
distribution of citations, whereby publications with few citations are
extremely common and publications with many citations are relatively rare.
Given this limitation, in this work we instead address a related question asked
by many academic researchers in the course of writing a paper, namely: ""Will
this paper increase my h-index?"" Using a real academic dataset with over 1.7
million authors, 2 million papers, and 8 million citation relationships from
the premier online academic service ArnetMiner, we formalize a novel scientific
impact prediction problem to examine several factors that can drive a paper to
increase the primary author's h-index. We find that the researcher's authority
on the publication topic and the venue in which the paper is published are
crucial factors to the increase of the primary author's h-index, while the
topic popularity and the co-authors' h-indices are of surprisingly little
relevance. By leveraging relevant factors, we find a greater than 87.5%
potential predictability for whether a paper will contribute to an author's
h-index within five years. As a further experiment, we generate a
self-prediction for this paper, estimating that there is a 76% probability that
it will contribute to the h-index of the co-author with the highest current
h-index in five years. We conclude that our findings on the quantification of
scientific impact can help researchers to expand their influence and more
effectively leverage their position of ""standing on the shoulders of giants.""",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1002.4286v2,"Redundancy, Deduction Schemes, and Minimum-Size Bases for Association
  Rules","Association rules are among the most widely employed data analysis methods in
the field of Data Mining. An association rule is a form of partial implication
between two sets of binary variables. In the most common approach, association
rules are parameterized by a lower bound on their confidence, which is the
empirical conditional probability of their consequent given the antecedent,
and/or by some other parameter bounds such as ""support"" or deviation from
independence. We study here notions of redundancy among association rules from
a fundamental perspective. We see each transaction in a dataset as an
interpretation (or model) in the propositional logic sense, and consider
existing notions of redundancy, that is, of logical entailment, among
association rules, of the form ""any dataset in which this first rule holds must
obey also that second rule, therefore the second is redundant"". We discuss
several existing alternative definitions of redundancy between association
rules and provide new characterizations and relationships among them. We show
that the main alternatives we discuss correspond actually to just two variants,
which differ in the treatment of full-confidence implications. For each of
these two notions of redundancy, we provide a sound and complete deduction
calculus, and we show how to construct complete bases (that is,
axiomatizations) of absolutely minimum size in terms of the number of rules. We
explore finally an approach to redundancy with respect to several association
rules, and fully characterize its simplest case of two partial premises.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1308.1262v1,"Pattern recognition issues on anisotropic smoothed particle
  hydrodynamics","This is a preliminary theoretical discussion on the computational
requirements of the state of the art smoothed particle hydrodynamics (SPH) from
the optics of pattern recognition and artificial intelligence. It is pointed
out in the present paper that, when including anisotropy detection to improve
resolution on shock layer, SPH is a very peculiar case of unsupervised machine
learning. On the other hand, the free particle nature of SPH opens an
opportunity for artificial intelligence to study particles as agents acting in
a collaborative framework in which the timed outcomes of a fluid simulation
forms a large knowledge base, which might be very attractive in computational
astrophysics phenomenological problems like self-propagating star formation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1111.5885v2,Type inference in mathematics,"In the theory of programming languages, type inference is the process of
inferring the type of an expression automatically, often making use of
information from the context in which the expression appears. Such mechanisms
turn out to be extremely useful in the practice of interactive theorem proving,
whereby users interact with a computational proof assistant to construct formal
axiomatic derivations of mathematical theorems. This article explains some of
the mechanisms for type inference used by the Mathematical Components project,
which is working towards a verification of the Feit-Thompson theorem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9511102v1,Set Theory for Verification: II. Induction and Recursion,"A theory of recursive definitions has been mechanized in Isabelle's
Zermelo-Fraenkel (ZF) set theory. The objective is to support the formalization
of particular recursive definitions for use in verification, semantics proofs
and other computational reasoning. Inductively defined sets are expressed as
least fixedpoints, applying the Knaster-Tarski Theorem over a suitable set.
Recursive functions are defined by well-founded recursion and its derivatives,
such as transfinite recursion. Recursive data structures are expressed by
applying the Knaster-Tarski Theorem to a set, such as V[omega], that is closed
under Cartesian product and disjoint sum. Worked examples include the
transitive closure of a relation, lists, variable-branching trees and mutually
recursive trees and forests. The Schr\""oder-Bernstein Theorem and the soundness
of propositional logic are proved in Isabelle sessions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0205047v2,"K-Medians, Facility Location, and the Chernoff-Wald Bound","The paper gives approximation algorithms for the k-medians and
facility-location problems (both NP-hard). For k-medians, the algorithm returns
a solution using at most ln(n+n/epsilon)k medians and having cost at most
(1+epsilon) times the cost of the best solution that uses at most k medians.
Here epsilon > 0 is an input to the algorithm. In comparison, the best previous
algorithm (Jyh-Han Lin and Jeff Vitter, 1992) had a (1+1/epsilon)ln(n) term
instead of the ln(n+n/epsilon) term in the performance guarantee. For facility
location, the algorithm returns a solution of cost at most d+ln(n) k, provided
there exists a solution of cost d+k where d is the assignment cost and k is the
facility cost. In comparison, the best previous algorithm (Dorit Hochbaum,
1982) returned a solution of cost at most ln(n)(d+k). For both problems, the
algorithms currently provide the best performance guarantee known for the
general (non-metric) problems.
  The paper also introduces a new probabilistic bound (called ""Chernoff-Wald
bound"") for bounding the expectation of the maximum of a collection of sums of
random variables, when each sum contains a random number of terms. The bound is
used to analyze the randomized rounding scheme that underlies the algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.00874v3,"Weakly-supervised Object Localization for Few-shot Learning and
  Fine-grained Few-shot Learning","Few-shot learning (FSL) aims to learn novel visual categories from very few
samples, which is a challenging problem in real-world applications. Many
methods of few-shot classification work well on general images to learn global
representation. However, they can not deal with fine-grained categories well at
the same time due to a lack of subtle and local information. We argue that
localization is an efficient approach because it directly provides the
discriminative regions, which is critical for both general classification and
fine-grained classification in a low data regime. In this paper, we propose a
Self-Attention Based Complementary Module (SAC Module) to fulfill the
weakly-supervised object localization, and more importantly produce the
activated masks for selecting discriminative deep descriptors for few-shot
classification. Based on each selected deep descriptor, Semantic Alignment
Module (SAM) calculates the semantic alignment distance between the query and
support images to boost classification performance. Extensive experiments show
our method outperforms the state-of-the-art methods on benchmark datasets under
various settings, especially on the fine-grained few-shot tasks. Besides, our
method achieves superior performance over previous methods when training the
model on miniImageNet and evaluating it on the different datasets,
demonstrating its superior generalization capacity. Extra visualization shows
the proposed method can localize the key objects more interval.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.14946v1,"Towards Large-Scale Rendering of Simulated Crops for Synthetic Ground
  Truth Generation on Modular Supercomputers","Computer Vision problems deal with the semantic extraction of information
from camera images. Especially for field crop images, the underlying problems
are hard to label and even harder to learn, and the availability of
high-quality training data is low. Deep neural networks do a good job of
extracting the necessary models from training examples. However, they rely on
an abundance of training data that is not feasible to generate or label by
expert annotation. To address this challenge, we make use of the Unreal Engine
to render large and complex virtual scenes. We rely on the performance of
individual nodes by distributing plant simulations across nodes and both
generate scenes as well as train neural networks on GPUs, restricting node
communication to parallel learning.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.15118v1,"Overview of the CLEF-2019 CheckThat!: Automatic Identification and
  Verification of Claims","We present an overview of the second edition of the CheckThat! Lab at CLEF
2019. The lab featured two tasks in two different languages: English and
Arabic. Task 1 (English) challenged the participating systems to predict which
claims in a political debate or speech should be prioritized for fact-checking.
Task 2 (Arabic) asked to (A) rank a given set of Web pages with respect to a
check-worthy claim based on their usefulness for fact-checking that claim, (B)
classify these same Web pages according to their degree of usefulness for
fact-checking the target claim, (C) identify useful passages from these pages,
and (D) use the useful pages to predict the claim's factuality. CheckThat!
provided a full evaluation framework, consisting of data in English (derived
from fact-checking sources) and Arabic (gathered and annotated from scratch)
and evaluation based on mean average precision (MAP) and normalized discounted
cumulative gain (nDCG) for ranking, and F1 for classification. A total of 47
teams registered to participate in this lab, and fourteen of them actually
submitted runs (compared to nine last year). The evaluation results show that
the most successful approaches to Task 1 used various neural networks and
logistic regression. As for Task 2, learning-to-rank was used by the highest
scoring runs for subtask A, while different classifiers were used in the other
subtasks. We release to the research community all datasets from the lab as
well as the evaluation scripts, which should enable further research in the
important tasks of check-worthiness estimation and automatic claim
verification.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0306122v1,The Best Trail Algorithm for Assisted Navigation of Web Sites,"We present an algorithm called the Best Trail Algorithm, which helps solve
the hypertext navigation problem by automating the construction of memex-like
trails through the corpus. The algorithm performs a probabilistic best-first
expansion of a set of navigation trees to find relevant and compact trails. We
describe the implementation of the algorithm, scoring methods for trails,
filtering algorithms and a new metric called \emph{potential gain} which
measures the potential of a page for future navigation opportunities.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1512.06941v1,Automatic Inference of Specifications in the K Framework,"Despite its many unquestionable benefits, formal specifications are not
widely used in industrial software development. In order to reduce the time and
effort required to write formal specifications, in this paper we propose a
technique for automatically discovering specifications from real code. The
proposed methodology relies on the symbolic execution capabilities recently
provided by the K framework that we exploit to automatically infer formal
specifications from programs that are written in a non-trivial fragment of C,
called KernelC. Roughly speaking, our symbolic analysis of KernelC programs
explains the execution of a (modifier) function by using other (observer)
routines in the program. We implemented our technique in the automated tool
Kindspec 2.0, which generates axioms that describe the precise input/output
behavior of C routines that handle pointer-based structures (i.e., result
values and state change). We describe the implementation of our system and
discuss the differences w.r.t. our previous work on inferring specifications
from C code.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1403.3304v1,A Spatial Data Model for Moving Object Databases,"Moving Object Databases will have significant role in Geospatial Information
Systems as they allow users to model continuous movements of entities in the
databases and perform spatio-temporal analysis. For representing and querying
moving objects, and algebra with a comprehensive framework of User Defined
Types together with a set of functions on those types is needed. Moreover,
concerning real world applications, moving objects move along constrained
environments like transportation networks so that an extra algebra for modeling
networks is demanded, too. These algebras can be inserted in any data model if
their designs are based on available standards such as Open Geospatial
Consortium that provides a common model for existing DBMS's. In this paper, we
focus on extending a spatial data model for constrained moving objects. Static
and moving geometries in our model are based on Open Geospatial Consortium
standards. We also extend Structured Query Language for retrieving, querying,
and manipulating spatio-temporal data related to moving objects as a simple and
expressive query language. Finally as a proof of concept, we implement a
generator to generate data for moving objects constrained by a transportation
network. Such a generator primarily aims at traffic planning applications.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1203.3376v1,"Learning, Social Intelligence and the Turing Test - why an
  ""out-of-the-box"" Turing Machine will not pass the Turing Test","The Turing Test (TT) checks for human intelligence, rather than any putative
general intelligence. It involves repeated interaction requiring learning in
the form of adaption to the human conversation partner. It is a macro-level
post-hoc test in contrast to the definition of a Turing Machine (TM), which is
a prior micro-level definition. This raises the question of whether learning is
just another computational process, i.e. can be implemented as a TM. Here we
argue that learning or adaption is fundamentally different from computation,
though it does involve processes that can be seen as computations. To
illustrate this difference we compare (a) designing a TM and (b) learning a TM,
defining them for the purpose of the argument. We show that there is a
well-defined sequence of problems which are not effectively designable but are
learnable, in the form of the bounded halting problem. Some characteristics of
human intelligence are reviewed including it's: interactive nature, learning
abilities, imitative tendencies, linguistic ability and context-dependency. A
story that explains some of these is the Social Intelligence Hypothesis. If
this is broadly correct, this points to the necessity of a considerable period
of acculturation (social learning in context) if an artificial intelligence is
to pass the TT. Whilst it is always possible to 'compile' the results of
learning into a TM, this would not be a designed TM and would not be able to
continually adapt (pass future TTs). We conclude three things, namely that: a
purely ""designed"" TM will never pass the TT; that there is no such thing as a
general intelligence since it necessary involves learning; and that
learning/adaption and computation should be clearly distinguished.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0106029v1,"Building Views with Description Logics in ADE: Application Development
  Environment","Any of views is formally defined within description logics that were
established as a family of logics for modeling complex hereditary structures
and have a suitable expressive power. This paper considers the Application
Development Environment (ADE) over generalized variable concepts that are used
to build database applications involving the supporting views. The front-end
user interacts the database via separate ADE access mechanism intermediated by
view support. The variety of applications may be generated that communicate
with different and distinct desktop databases in a data warehouse. The advanced
techniques allows to involve remote or stored procedures retrieval and call.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1510.06507v1,"Modelling, Measuring and Compensating Color Weak Vision","We use methods from Riemann geometry to investigate transformations between
the color spaces of color-normal and color weak observers. The two main
applications are the simulation of the perception of a color weak observer for
a color normal observer and the compensation of color images in a way that a
color weak observer has approximately the same perception as a color normal
observer. The metrics in the color spaces of interest are characterized with
the help of ellipsoids defined by the just-noticable-differences between color
which are measured with the help of color-matching experiments. The constructed
mappings are isometries of Riemann spaces that preserve the perceived
color-differences for both observers. Among the two approaches to build such an
isometry, we introduce normal coordinates in Riemann spaces as a tool to
construct a global color-weak compensation map. Compared to previously used
methods this method is free from approximation errors due to local
linearizations and it avoids the problem of shifting locations of the origin of
the local coordinate system. We analyse the variations of the Riemann metrics
for different observers obtained from new color matching experiments and
describe three variations of the basic method. The performance of the methods
is evaluated with the help of semantic differential (SD) tests.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.05408v1,"Robust deep learning-based semantic organ segmentation in hyperspectral
  images","Semantic image segmentation is an important prerequisite for
context-awareness and autonomous robotics in surgery. The state of the art has
focused on conventional RGB video data acquired during minimally invasive
surgery, but full-scene semantic segmentation based on spectral imaging data
and obtained during open surgery has received almost no attention to date. To
address this gap in the literature, we are investigating the following research
questions based on hyperspectral imaging (HSI) data of pigs acquired in an open
surgery setting: (1) What is an adequate representation of HSI data for neural
network-based fully automated organ segmentation, especially with respect to
the spatial granularity of the data (pixels vs. superpixels vs. patches vs.
full images)? (2) Is there a benefit of using HSI data compared to other
modalities, namely RGB data and processed HSI data (e.g. tissue parameters like
oxygenation), when performing semantic organ segmentation? According to a
comprehensive validation study based on 506 HSI images from 20 pigs, annotated
with a total of 19 classes, deep learning-based segmentation performance
increases - consistently across modalities - with the spatial context of the
input data. Unprocessed HSI data offers an advantage over RGB data or processed
data from the camera provider, with the advantage increasing with decreasing
size of the input to the neural network. Maximum performance (HSI applied to
whole images) yielded a mean dice similarity coefficient (DSC) of 0.89
(standard deviation (SD) 0.04), which is in the range of the inter-rater
variability (DSC of 0.89 (SD 0.07)). We conclude that HSI could become a
powerful image modality for fully-automatic surgical scene understanding with
many advantages over traditional imaging, including the ability to recover
additional functional tissue information.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1707.04016v1,Dependency Injection for Programming by Optimization,"Programming by Optimization tools perform automatic software configuration
according to the specification supplied by a software developer. Developers
specify design spaces for program components, and the onerous task of
determining which configuration best suits a given use case is determined using
automated analysis tools and optimization heuristics. However, in current
approaches to Programming by Optimization, design space specification and
exploration relies on external configuration algorithms, executable wrappers
and fragile, preprocessed programming language extensions.
  Here we show that the architectural pattern of Dependency Injection provides
a superior alternative to the traditional Programming by Optimization pipeline.
We demonstrate that configuration tools based on Dependency Injection fit
naturally into the software development process, while requiring less overhead
than current wrapper-based mechanisms. Furthermore, the structural
correspondence between Dependency Injection and context-free grammars yields a
new class of evolutionary metaheuristics for automated algorithm configuration.
We found that the new heuristics significantly outperform existing
configuration algorithms on many problems of interest (in one case by two
orders of magnitude). We anticipate that these developments will make
Programming by Optimization immediately applicable to a large number of
enterprise software projects.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.05346v1,Extending Nearly Complete 1-Planar Drawings in Polynomial Time,"The problem of extending partial geometric graph representations such as
plane graphs has received considerable attention in recent years. In
particular, given a graph $G$, a connected subgraph $H$ of $G$ and a drawing
$\mathcal{H}$ of $H$, the extension problem asks whether $\mathcal{H}$ can be
extended into a drawing of $G$ while maintaining some desired property of the
drawing (e.g., planarity).
  In their breakthrough result, Angelini et al. [ACM TALG 2015] showed that the
extension problem is polynomial-time solvable when the aim is to preserve
planarity. Very recently we considered this problem for partial 1-planar
drawings [ICALP 2020], which are drawings in the plane that allow each edge to
have at most one crossing. The most important question identified and left open
in that work is whether the problem can be solved in polynomial time when $H$
can be obtained from $G$ by deleting a bounded number of vertices and edges. In
this work, we answer this question positively by providing a constructive
polynomial-time decision algorithm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0809.1836v1,"The complexity of counting solutions to Generalised Satisfiability
  Problems modulo k","Generalised Satisfiability Problems (or Boolean Constraint Satisfaction
Problems), introduced by Schaefer in 1978, are a general class of problem which
allow the systematic study of the complexity of satisfiability problems with
different types of constraints. In 1979, Valiant introduced the complexity
class parity P, the problem of counting the number of solutions to NP problems
modulo two. Others have since considered the question of counting modulo other
integers.
  We give a dichotomy theorem for the complexity of counting the number of
solutions to Generalised Satisfiability Problems modulo integers. This follows
from an earlier result of Creignou and Hermann which gave a counting dichotomy
for these types of problem, and the dichotomy itself is almost identical.
Specifically, counting the number of solutions to a Generalised Satisfiability
Problem can be done in polynomial time if all the relations are affine.
Otherwise, except for one special case with k = 2, it is #_kP-complete.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1012.2648v1,Distributed XML Design,"A distributed XML document is an XML document that spans several machines. We
assume that a distribution design of the document tree is given, consisting of
an XML kernel-document T[f1,...,fn] where some leaves are ""docking points"" for
external resources providing XML subtrees (f1,...,fn, standing, e.g., for Web
services or peers at remote locations). The top-down design problem consists
in, given a type (a schema document that may vary from a DTD to a tree
automaton) for the distributed document, ""propagating"" locally this type into a
collection of types, that we call typing, while preserving desirable
properties. We also consider the bottom-up design which consists in, given a
type for each external resource, exhibiting a global type that is enforced by
the local types, again with natural desirable properties. In the article, we
lay out the fundamentals of a theory of distributed XML design, analyze
problems concerning typing issues in this setting, and study their complexity.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.12314v1,"CharacterChat: Supporting the Creation of Fictional Characters through
  Conversation and Progressive Manifestation with a Chatbot","We present CharacterChat, a concept and chatbot to support writers in
creating fictional characters. Concretely, writers progressively turn the bot
into their imagined character through conversation. We iteratively developed
CharacterChat in a user-centred approach, starting with a survey on character
creation with writers (N=30), followed by two qualitative user studies (N=7 and
N=8). Our prototype combines two modes: (1) Guided prompts help writers define
character attributes (e.g. User: ""Your name is Jane.""), including suggestions
for attributes (e.g. Bot: ""What is my main motivation?"") and values, realised
as a rule-based system with a concept network. (2) Open conversation with the
chatbot helps writers explore their character and get inspiration, realised
with a language model that takes into account the defined character attributes.
Our user studies reveal benefits particularly for early stages of character
creation, and challenges due to limited conversational capabilities. We
conclude with lessons learned and ideas for future work.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.01495v1,"On Positional and Structural Node Features for Graph Neural Networks on
  Non-attributed Graphs","Graph neural networks (GNNs) have been widely used in various graph-related
problems such as node classification and graph classification, where the
superior performance is mainly established when natural node features are
available. However, it is not well understood how GNNs work without natural
node features, especially regarding the various ways to construct artificial
ones. In this paper, we point out the two types of artificial node
features,i.e., positional and structural node features, and provide insights on
why each of them is more appropriate for certain tasks,i.e., positional node
classification, structural node classification, and graph classification.
Extensive experimental results on 10 benchmark datasets validate our insights,
thus leading to a practical guideline on the choices between different
artificial node features for GNNs on non-attributed graphs. The code is
available at https://github.com/zjzijielu/gnn-exp/.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0702123v1,Tree automata and separable sets of input variables,"We consider the computational complexity of tree transducers, depending on
their separable sets of input variables.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0610035v2,Positional Determinacy of Games with Infinitely Many Priorities,"We study two-player games of infinite duration that are played on finite or
infinite game graphs. A winning strategy for such a game is positional if it
only depends on the current position, and not on the history of the play. A
game is positionally determined if, from each position, one of the two players
has a positional winning strategy.
  The theory of such games is well studied for winning conditions that are
defined in terms of a mapping that assigns to each position a priority from a
finite set. Specifically, in Muller games the winner of a play is determined by
the set of those priorities that have been seen infinitely often; an important
special case are parity games where the least (or greatest) priority occurring
infinitely often determines the winner. It is well-known that parity games are
positionally determined whereas Muller games are determined via finite-memory
strategies.
  In this paper, we extend this theory to the case of games with infinitely
many priorities. Such games arise in several application areas, for instance in
pushdown games with winning conditions depending on stack contents.
  For parity games there are several generalisations to the case of infinitely
many priorities. While max-parity games over omega or min-parity games over
larger ordinals than omega require strategies with infinite memory, we can
prove that min-parity games with priorities in omega are positionally
determined. Indeed, it turns out that the min-parity condition over omega is
the only infinitary Muller condition that guarantees positional determinacy on
all game graphs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.04565v2,"The Locus Algorithm III: A Grid Computing system to generate catalogues
  of optimised pointings for Differential Photometry","This paper discusses the hardware and software components of the Grid
Computing system used to implement the Locus Algorithm to identify optimum
pointings for differential photometry of 61,662,376 stars and 23,799 quasars.
The scale of the data, together with initial operational assessments demanded a
High Performance Computing (HPC) system to complete the data analysis. Grid
computing was chosen as the HPC solution as the optimum choice available within
this project. The physical and logical structure of the National Grid computing
Infrastructure informed the approach that was taken. That approach was one of
layered separation of the different project components to enable maximum
flexibility and extensibility.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.02104v2,"Discriminatory Expressions to Produce Interpretable Models in Short
  Documents","Social Networking Sites (SNS) are one of the most important ways of
communication. In particular, microblogging sites are being used as analysis
avenues due to their peculiarities (promptness, short texts...). There are
countless researches that use SNS in novel manners, but machine learning has
focused mainly in classification performance rather than interpretability
and/or other goodness metrics. Thus, state-of-the-art models are black boxes
that should not be used to solve problems that may have a social impact. When
the problem requires transparency, it is necessary to build interpretable
pipelines. Although the classifier may be interpretable, resulting models are
too complex to be considered comprehensible, making it impossible for humans to
understand the actual decisions. This paper presents a feature selection
mechanism that is able to improve comprehensibility by using less but more
meaningful features while achieving good performance in microblogging contexts
where interpretability is mandatory. Moreover, we present a ranking method to
evaluate features in terms of statistical relevance and bias. We conducted
exhaustive tests with five different datasets in order to evaluate
classification performance, generalisation capacity and complexity of the
model. Results show that our proposal is better and the most stable one in
terms of accuracy, generalisation and comprehensibility.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.08470v2,"Efficient $\ell^0$ gradient-based Super Resolution for simplified image
  segmentation","We consider a variational model for single-image super-resolution based on
the assumption that the gradient of the target image is sparse. We enforce this
assumption by considering both an isotropic and an anisotropic $\ell^0$
regularisation on the image gradient combined with a quadratic data fidelity,
similarly as studied in [1] for general signal recovery problems. For the
numerical realisation of the model, we propose a novel efficient ADMM splitting
algorithm whose substeps solutions are computed efficiently by means of
hard-thresholding and standard conjugate-gradient solvers. We test our model on
highly-degraded synthetic and real-world data and quantitatively compare our
results with several variational approaches as well as with state-of-the-art
deep-learning techniques. Our experiments show that $\ell^0$
gradient-regularised super-resolved images can be effectively used to improve
the accuracy of standard segmentation algorithms when applied to QR and cell
detection, and landcover classification problems, in comparison to the results
achieved by other approaches.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0511080v1,A dissemination strategy for immunizing scale-free networks,"We consider the problem of distributing a vaccine for immunizing a scale-free
network against a given virus or worm. We introduce a new method, based on
vaccine dissemination, that seems to reflect more accurately what is expected
to occur in real-world networks. Also, since the dissemination is performed
using only local information, the method can be easily employed in practice.
Using a random-graph framework, we analyze our method both mathematically and
by means of simulations. We demonstrate its efficacy regarding the trade-off
between the expected number of nodes that receive the vaccine and the network's
resulting vulnerability to develop an epidemic as the virus or worm attempts to
infect one of its nodes. For some scenarios, the new method is seen to render
the network practically invulnerable to attacks while requiring only a small
fraction of the nodes to receive the vaccine.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1502.00231v1,Feature Selection with Redundancy-complementariness Dispersion,"Feature selection has attracted significant attention in data mining and
machine learning in the past decades. Many existing feature selection methods
eliminate redundancy by measuring pairwise inter-correlation of features,
whereas the complementariness of features and higher inter-correlation among
more than two features are ignored. In this study, a modification item
concerning the complementariness of features is introduced in the evaluation
criterion of features. Additionally, in order to identify the interference
effect of already-selected False Positives (FPs), the
redundancy-complementariness dispersion is also taken into account to adjust
the measurement of pairwise inter-correlation of features. To illustrate the
effectiveness of proposed method, classification experiments are applied with
four frequently used classifiers on ten datasets. Classification results verify
the superiority of proposed method compared with five representative feature
selection methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1003.1572v1,Expressiveness and Extensions of an Instruction Sequence Semigroup,"PGA, short for ProGram Algebra, describes sequential programs as finite or
infinite (repeating) sequences of instructions. The semigroup C of finite
instruction sequences was introduced as an equally expressive alternative to
PGA. PGA instructions are executed from left to right; most C instructions come
in a left-to-right as well as a right-to-left flavor. This thesis builds on C
by introducing an alternative semigroup Cg which employs label and goto
instructions instead of relative jump instructions as control structures. Cg
can be translated to C and vice versa (and is thus equally expressive). It is
shown that restricting the instruction sets of C and Cg to contain only
finitely many distinct jump, goto or label instructions in either or both
directions reduces their expressiveness. Instruction sets with an infinite
number of these instructions in both directions (not necessarily all such
instructions) do not suffer a loss of expressiveness.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1010.2438v2,On Designing Multicore-aware Simulators for Biological Systems,"The stochastic simulation of biological systems is an increasingly popular
technique in bioinformatics. It often is an enlightening technique, which may
however result in being computational expensive. We discuss the main
opportunities to speed it up on multi-core platforms, which pose new challenges
for parallelisation techniques. These opportunities are developed in two
general families of solutions involving both the single simulation and a bulk
of independent simulations (either replicas of derived from parameter sweep).
Proposed solutions are tested on the parallelisation of the CWC simulator
(Calculus of Wrapped Compartments) that is carried out according to proposed
solutions by way of the FastFlow programming framework making possible fast
development and efficient execution on multi-cores.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1202.4665v3,"Algorithms and Almost Tight Results for 3-Colorability of Small Diameter
  Graphs","In spite of the extensive studies of the 3-coloring problem with respect to
several basic parameters, the complexity status of the 3-coloring problem on
graphs with small diameter, i.e. with diameter 2 or 3, has been a longstanding
and challenging open question. For graphs with diameter 2 we provide the first
subexponential algorithm with complexity $2^{O(\sqrt{n\log n})}$, which is
asymptotically the same as the currently best known time complexity for the
graph isomorphism (GI) problem. Moreover, we prove that the graph isomorphism
problem on 3-colorable graphs with diameter 2 is GI-complete. Furthermore we
present a subclass of graphs with diameter 2 that admits a polynomial algorithm
for 3-coloring. For graphs with diameter 3 we establish the complexity of
3-coloring by proving that for every $\varepsilon \in [0,1)$, 3-coloring is
NP-complete on triangle-free graphs of diameter 3 and radius 2 with $n$
vertices and minimum degree $\delta=\Theta(n^{\varepsilon})$. Moreover,
assuming ETH, we provide three different amplifications of our hardness results
to obtain for every $\varepsilon \in [0,1)$ subexponential lower bounds for the
complexity of 3-coloring on triangle-free graphs with diameter 3 and minimum
degree $\delta=\Theta(n^{\varepsilon})$. Finally, we provide a 3-coloring
algorithm with running time
$2^{O(\min\{\delta\Delta,\frac{n}{\delta}\log\delta\})}$ for graphs with
diameter 3, where $\delta$ (resp. $\Delta $) is the minimum (resp. maximum)
degree of the input graph. To the best of our knowledge, this algorithm is the
first subexponential algorithm for graphs with $\delta=\omega(1)$ and for
graphs with $\delta=O(1)$ and $\Delta=o(n)$. Due to the above lower bounds of
the complexity of 3-coloring, the running time of this algorithm is
asymptotically almost tight when the minimum degree if the input graph is
$\delta=\Theta(n^{\varepsilon})$, where $\varepsilon \in [1/2,1)$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1312.4628v1,"Counting Triangulations and other Crossing-free Structures via Onion
  Layers","Let $P$ be a set of $n$ points in the plane. A crossing-free structure on $P$
is a plane graph with vertex set $P$. Examples of crossing-free structures
include triangulations of $P$, spanning cycles of $P$, also known as
polygonalizations of $P$, among others. In this paper we develop a general
technique for computing the number of crossing-free structures of an input set
$P$. We apply the technique to obtain algorithms for computing the number of
triangulations, matchings, and spanning cycles of $P$. The running time of our
algorithms is upper bounded by $n^{O(k)}$, where $k$ is the number of onion
layers of $P$. In particular, for $k = O(1)$ our algorithms run in polynomial
time. In addition, we show that our algorithm for counting triangulations is
never slower than $O^{*}(3.1414^{n})$, even when $k = \Theta(n)$. Given that
there are several well-studied configurations of points with at least
$\Omega(3.464^{n})$ triangulations, and some even with $\Omega(8^{n})$
triangulations, our algorithm asymptotically outperforms any enumeration
algorithm for such instances. In fact, it is widely believed that any set of
$n$ points must have at least $\Omega(3.464^{n})$ triangulations. If this is
true, then our algorithm is strictly sub-linear in the number of triangulations
counted. We also show that our techniques are general enough to solve the
""Restricted-Triangulation-Counting-Problem"", which we prove to be $W[2]$-hard
in the parameter $k$. This implies a ""no free lunch"" result: In order to be
fixed-parameter tractable, our general algorithm must rely on additional
properties that are specific to the considered class of structures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.04137v5,On Robust Mean Estimation under Coordinate-level Corruption,"We study the problem of robust mean estimation and introduce a novel Hamming
distance-based measure of distribution shift for coordinate-level corruptions.
We show that this measure yields adversary models that capture more realistic
corruptions than those used in prior works, and present an
information-theoretic analysis of robust mean estimation in these settings. We
show that for structured distributions, methods that leverage the structure
yield information theoretically more accurate mean estimation. We also focus on
practical algorithms for robust mean estimation and study when data
cleaning-inspired approaches that first fix corruptions in the input data and
then perform robust mean estimation can match the information theoretic bounds
of our analysis. We finally demonstrate experimentally that this two-step
approach outperforms structure-agnostic robust estimation and provides accurate
mean estimation even for high-magnitude corruption.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1603.08182v3,3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions,"Matching local geometric features on real-world depth images is a challenging
task due to the noisy, low-resolution, and incomplete nature of 3D scan data.
These difficulties limit the performance of current state-of-art methods, which
are typically based on histograms over geometric properties. In this paper, we
present 3DMatch, a data-driven model that learns a local volumetric patch
descriptor for establishing correspondences between partial 3D data. To amass
training data for our model, we propose a self-supervised feature learning
method that leverages the millions of correspondence labels found in existing
RGB-D reconstructions. Experiments show that our descriptor is not only able to
match local geometry in new scenes for reconstruction, but also generalize to
different tasks and spatial scales (e.g. instance-level object model alignment
for the Amazon Picking Challenge, and mesh surface correspondence). Results
show that 3DMatch consistently outperforms other state-of-the-art approaches by
a significant margin. Code, data, benchmarks, and pre-trained models are
available online at http://3dmatch.cs.princeton.edu",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1407.1422v1,Spectrum of mixed bi-uniform hypergraphs,"A mixed hypergraph is a triple $H=(V,\mathcal{C},\mathcal{D})$, where $V$ is
a set of vertices, $\mathcal{C}$ and $\mathcal{D}$ are sets of hyperedges. A
vertex-coloring of $H$ is proper if $C$-edges are not totally multicolored and
$D$-edges are not monochromatic. The feasible set $S(H)$ of $H$ is the set of
all integers, $s$, such that $H$ has a proper coloring with $s$ colors.
  Bujt\'as and Tuza [Graphs and Combinatorics 24 (2008), 1--12] gave a
characterization of feasible sets for mixed hypergraphs with all $C$- and
$D$-edges of the same size $r$, $r\geq 3$.
  In this note, we give a short proof of a complete characterization of all
possible feasible sets for mixed hypergraphs with all $C$-edges of size $\ell$
and all $D$-edges of size $m$, where $\ell, m \geq 2$. Moreover, we show that
for every sequence $(r(s))_{s=\ell}^n$, $n \geq \ell$, of natural numbers there
exists such a hypergraph with exactly $r(s)$ proper colorings using $s$ colors,
$s = \ell,\ldots,n$, and no proper coloring with more than $n$ colors. Choosing
$\ell = m=r$ this answers a question of Bujt\'as and Tuza, and generalizes
their result with a shorter proof.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.02394v2,Automating Distributed Tiered Storage Management in Cluster Computing,"Data-intensive platforms such as Hadoop and Spark are routinely used to
process massive amounts of data residing on distributed file systems like HDFS.
Increasing memory sizes and new hardware technologies (e.g., NVRAM, SSDs) have
recently led to the introduction of storage tiering in such settings. However,
users are now burdened with the additional complexity of managing the multiple
storage tiers and the data residing on them while trying to optimize their
workloads. In this paper, we develop a general framework for automatically
moving data across the available storage tiers in distributed file systems.
Moreover, we employ machine learning for tracking and predicting file access
patterns, which we use to decide when and which data to move up or down the
storage tiers for increasing system performance. Our approach uses incremental
learning to dynamically refine the models with new file accesses, allowing them
to naturally adjust and adapt to workload changes over time. Our extensive
evaluation using realistic workloads derived from Facebook and CMU traces
compares our approach with several other policies and showcases significant
benefits in terms of both workload performance and cluster efficiency.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1912.12712v2,"Haptic communication optimises joint decisions and affords implicit
  confidence sharing","Group decisions can outperform the choices of the best individual group
members. Previous research suggested that optimal group decisions require
individuals to communicate explicitly (e.g., verbally) their confidence levels.
Our study addresses the untested hypothesis that implicit communication using a
sensorimotor channel -- haptic coupling -- may afford optimal group decisions,
too. We report that haptically coupled dyads solve a perceptual discrimination
task more accurately than their best individual members; and five times faster
than dyads using explicit communication. Furthermore, our computational
analyses indicate that the haptic channel affords implicit confidence sharing.
We found that dyads take leadership over the choice and communicate their
confidence in it by modulating both the timing and the force of their
movements. Our findings may pave the way to negotiation technologies using fast
sensorimotor communication to solve problems in groups.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1411.3292v3,"Bayesian M-ary Hypothesis Testing: The Meta-Converse and Verd√∫-Han
  Bounds are Tight","Two alternative exact characterizations of the minimum error probability of
Bayesian M-ary hypothesis testing are derived. The first expression corresponds
to the error probability of an induced binary hypothesis test and implies the
tightness of the meta-converse bound by Polyanskiy, Poor and Verd\'u; the
second expression is function of an information-spectrum measure and implies
the tightness of a generalized Verd\'u-Han lower bound. The formulas
characterize the minimum error probability of several problems in information
theory and help to identify the steps where existing converse bounds are loose.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0607105v5,"Nearly-Linear Time Algorithms for Preconditioning and Solving Symmetric,
  Diagonally Dominant Linear Systems","We present a randomized algorithm that, on input a symmetric, weakly
diagonally dominant n-by-n matrix A with m nonzero entries and an n-vector b,
produces a y such that $\norm{y - \pinv{A} b}_{A} \leq \epsilon \norm{\pinv{A}
b}_{A}$ in expected time $O (m \log^{c}n \log (1/\epsilon)),$ for some constant
c. By applying this algorithm inside the inverse power method, we compute
approximate Fiedler vectors in a similar amount of time. The algorithm applies
subgraph preconditioners in a recursive fashion. These preconditioners improve
upon the subgraph preconditioners first introduced by Vaidya (1990).
  For any symmetric, weakly diagonally-dominant matrix A with non-positive
off-diagonal entries and $k \geq 1$, we construct in time $O (m \log^{c} n)$ a
preconditioner B of A with at most $2 (n - 1) + O ((m/k) \log^{39} n)$ nonzero
off-diagonal entries such that the finite generalized condition number
$\kappa_{f} (A,B)$ is at most k, for some other constant c.
  In the special case when the nonzero structure of the matrix is planar the
corresponding linear system solver runs in expected time $ O (n \log^{2} n + n
\log n \ \log \log n \ \log (1/\epsilon))$.
  We hope that our introduction of algorithms of low asymptotic complexity will
lead to the development of algorithms that are also fast in practice.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0611037v2,On Conditional Branches in Optimal Decision Trees,"The decision tree is one of the most fundamental programming abstractions. A
commonly used type of decision tree is the alphabetic binary tree, which uses
(without loss of generality) ``less than'' versus ''greater than or equal to''
tests in order to determine one of $n$ outcome events. The process of finding
an optimal alphabetic binary tree for a known probability distribution on
outcome events usually has the underlying assumption that the cost (time) per
decision is uniform and thus independent of the outcome of the decision. This
assumption, however, is incorrect in the case of software to be optimized for a
given microprocessor, e.g., in compiling switch statements or in fine-tuning
program bottlenecks. The operation of the microprocessor generally means that
the cost for the more likely decision outcome can or will be less -- often far
less -- than the less likely decision outcome. Here we formulate a variety of
$O(n^3)$-time $O(n^2)$-space dynamic programming algorithms to solve such
optimal binary decision tree problems, optimizing for the behavior of
processors with predictive branch capabilities, both static and dynamic. In the
static case, we use existing results to arrive at entropy-based performance
bounds. Solutions to this formulation are often faster in practice than
``optimal'' decision trees as formulated in the literature, and, for small
problems, are easily worth the extra complexity in finding the better solution.
This can be applied in fast implementation of decoding Huffman codes.",0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.04388v2,"GRIMGEP: Learning Progress for Robust Goal Sampling in Visual Deep
  Reinforcement Learning","Designing agents, capable of learning autonomously a wide range of skills is
critical in order to increase the scope of reinforcement learning. It will both
increase the diversity of learned skills and reduce the burden of manually
designing reward functions for each skill. Self-supervised agents, setting
their own goals, and trying to maximize the diversity of those goals have shown
great promise towards this end. However, a currently known limitation of agents
trying to maximize the diversity of sampled goals is that they tend to get
attracted to noise or more generally to parts of the environments that cannot
be controlled (distractors). When agents have access to predefined goal
features or expert knowledge, absolute Learning Progress (ALP) provides a way
to distinguish between regions that can be controlled and those that cannot.
However, those methods often fall short when the agents are only provided with
raw sensory inputs such as images. In this work we extend those concepts to
unsupervised image-based goal exploration. We propose a framework that allows
agents to autonomously identify and ignore noisy distracting regions while
searching for novelty in the learnable regions to both improve overall
performance and avoid catastrophic forgetting. Our framework can be combined
with any state-of-the-art novelty seeking goal exploration approaches. We
construct a rich 3D image based environment with distractors. Experiments on
this environment show that agents using our framework successfully identify
interesting regions of the environment, resulting in drastically improved
performances. The source code is available at
https://sites.google.com/view/grimgep.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.11834v2,Harmless Sets in Sparse Classes,"In the classic TARGET SAT SELECTION problem, we are asked to minimise the
number of nodes to activate so that, after the application of a certain
propagation process, all nodes of the graph are active. Bazgan and Chopin
[Discrete Optimization}, 14:170--182, 2014] introduced the opposite problem,
named HARMLESS SET, in which they ask to maximise the number of nodes to
activate such that not a single additional node is activated. In this paper we
investigate how sparsity impacts the tractability of HARMLESS SET.
Specifically, we answer two open questions posed by the aforementioned authors,
namely a) whether the problem is FPT on planar graphs and b) whether it is FPT
parametrised by treewidth. The first question can be answered in the positive
using existing meta-theorems on sparse classes, and we further show that
HARMLESS SET not only admits a polynomial kernel, but that it can be solved in
subexponential time.
  We then answer the second question in the negative by showing that the
problem is W[1]-hard when parametrised by a parameter that upper bounds
treewidth.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0912.4947v3,"Infinitary Combinatory Reduction Systems: Normalising Reduction
  Strategies","We study normalising reduction strategies for infinitary Combinatory
Reduction Systems (iCRSs). We prove that all fair, outermost-fair, and
needed-fair strategies are normalising for orthogonal, fully-extended iCRSs.
These facts properly generalise a number of results on normalising strategies
in first-order infinitary rewriting and provide the first examples of
normalising strategies for infinitary lambda calculus.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0801.4775v1,"Spreadsheet Assurance by ""Control Around"" is a Viable Alternative to the
  Traditional Approach","The traditional approach to spreadsheet auditing generally consists of
auditing every distinct formula within a spreadsheet. Although tools are
developed to support auditors during this process, the approach is still very
time consuming and therefore relatively expensive. As an alternative to the
traditional ""control through"" approach, this paper discusses a ""control around""
approach. Within the proposed approach not all distinct formulas are audited
separately, but the relationship between input data and output data of a
spreadsheet is audited through comparison with a shadow model developed in a
modelling language. Differences between the two models then imply possible
errors in the spreadsheet. This paper describes relevant issues regarding the
""control around"" approach and the circumstances in which this approach is
preferred above a traditional spreadsheet audit approach.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1807.07856v4,"A Novel Method for Extrinsic Calibration of Multiple RGB-D Cameras Using
  Descriptor-Based Patterns","This letter presents a novel method to estimate the relative poses between
RGB-D cameras with minimal overlapping fields of view in a panoramic RGB-D
camera system. This calibration problem is relevant to applications such as
indoor 3D mapping and robot navigation that can benefit from a 360$^\circ$
field of view using RGB-D cameras. The proposed approach relies on
descriptor-based patterns to provide well-matched 2D keypoints in the case of a
minimal overlapping field of view between cameras. Integrating the matched 2D
keypoints with corresponding depth values, a set of 3D matched keypoints are
constructed to calibrate multiple RGB-D cameras. Experiments validated the
accuracy and efficiency of the proposed calibration approach, both superior to
those of existing methods (800 ms vs. 5 seconds; rotation error of 0.56 degrees
vs. 1.6 degrees; and translation error of 1.80 cm vs. 2.5 cm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1306.4635v1,Towards Multistage Design of Modular Systems,"The paper describes multistage design of composite (modular) systems (i.e.,
design of a system trajectory). This design process consists of the following:
(i) definition of a set of time/logical points; (ii) modular design of the
system for each time/logical point (e.g., on the basis of combinatorial
synthesis as hierarchical morphological design or multiple choice problem) to
obtain several system solutions; (iii) selection of the system solution for
each time/logical point while taking into account their quality and the quality
of compatibility between neighbor selected system solutions (here,
combinatorial synthesis is used as well). Mainly, the examined time/logical
points are based on a time chain. In addition, two complicated cases are
considered: (a) the examined logical points are based on a tree-like structure,
(b) the examined logical points are based on a digraph. Numerical examples
illustrate the approach.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0
http://arxiv.org/abs/cs/0610027v3,LTL with the Freeze Quantifier and Register Automata,"A data word is a sequence of pairs of a letter from a finite alphabet and an
element from an infinite set, where the latter can only be compared for
equality. To reason about data words, linear temporal logic is extended by the
freeze quantifier, which stores the element at the current word position into a
register, for equality comparisons deeper in the formula. By translations from
the logic to alternating automata with registers and then to faulty counter
automata whose counters may erroneously increase at any time, and from faulty
and error-free counter automata to the logic, we obtain a complete complexity
table for logical fragments defined by varying the set of temporal operators
and the number of registers. In particular, the logic with future-time
operators and 1 register is decidable but not primitive recursive over finite
data words. Adding past-time operators or 1 more register, or switching to
infinite data words, cause undecidability.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1407.7930v1,Entity-Linking via Graph-Distance Minimization,"Entity-linking is a natural-language-processing task that consists in
identifying the entities mentioned in a piece of text, linking each to an
appropriate item in some knowledge base; when the knowledge base is Wikipedia,
the problem comes to be known as wikification (in this case, items are
wikipedia articles). One instance of entity-linking can be formalized as an
optimization problem on the underlying concept graph, where the quantity to be
optimized is the average distance between chosen items. Inspired by this
application, we define a new graph problem which is a natural variant of the
Maximum Capacity Representative Set. We prove that our problem is NP-hard for
general graphs; nonetheless, under some restrictive assumptions, it turns out
to be solvable in linear time. For the general case, we propose two heuristics:
one tries to enforce the above assumptions and another one is based on the
notion of hitting distance; we show experimentally how these approaches perform
with respect to some baselines on a real-world dataset.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1812.06145v2,"Improving the Performance of Unimodal Dynamic Hand-Gesture Recognition
  with Multimodal Training","We present an efficient approach for leveraging the knowledge from multiple
modalities in training unimodal 3D convolutional neural networks (3D-CNNs) for
the task of dynamic hand gesture recognition. Instead of explicitly combining
multimodal information, which is commonplace in many state-of-the-art methods,
we propose a different framework in which we embed the knowledge of multiple
modalities in individual networks so that each unimodal network can achieve an
improved performance. In particular, we dedicate separate networks per
available modality and enforce them to collaborate and learn to develop
networks with common semantics and better representations. We introduce a
""spatiotemporal semantic alignment"" loss (SSA) to align the content of the
features from different networks. In addition, we regularize this loss with our
proposed ""focal regularization parameter"" to avoid negative knowledge transfer.
Experimental results show that our framework improves the test time recognition
accuracy of unimodal networks, and provides the state-of-the-art performance on
various dynamic hand gesture recognition datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1803.04488v3,"Concept2vec: Metrics for Evaluating Quality of Embeddings for
  Ontological Concepts","Although there is an emerging trend towards generating embeddings for
primarily unstructured data and, recently, for structured data, no systematic
suite for measuring the quality of embeddings has been proposed yet. This
deficiency is further sensed with respect to embeddings generated for
structured data because there are no concrete evaluation metrics measuring the
quality of the encoded structure as well as semantic patterns in the embedding
space. In this paper, we introduce a framework containing three distinct tasks
concerned with the individual aspects of ontological concepts: (i) the
categorization aspect, (ii) the hierarchical aspect, and (iii) the relational
aspect. Then, in the scope of each task, a number of intrinsic metrics are
proposed for evaluating the quality of the embeddings. Furthermore, w.r.t. this
framework, multiple experimental studies were run to compare the quality of the
available embedding models. Employing this framework in future research can
reduce misjudgment and provide greater insight about quality comparisons of
embeddings for ontological concepts. We positioned our sampled data and code at
https://github.com/alshargi/Concept2vec under GNU General Public License v3.0.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1302.4765v1,Design Features for the Social Web: The Architecture of Deme,"We characterize the ""social Web"" and argue for several features that are
desirable for users of socially oriented web applications. We describe the
architecture of Deme, a web content management system (WCMS) and extensible
framework, and show how it implements these desired features. We then compare
Deme on our desiderata with other web technologies: traditional HTML, previous
open source WCMSs (illustrated by Drupal), commercial Web 2.0 applications, and
open-source, object-oriented web application frameworks. The analysis suggests
that a WCMS can be well suited to building social websites if it makes more of
the features of object-oriented programming, such as polymorphism, and class
inheritance, available to non-programmers in an accessible vocabulary.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1111.5357v1,Digraph Complexity Measures and Applications in Formal Language Theory,"We investigate structural complexity measures on digraphs, in particular the
cycle rank. This concept is intimately related to a classical topic in formal
language theory, namely the star height of regular languages. We explore this
connection, and obtain several new algorithmic insights regarding both cycle
rank and star height. Among other results, we show that computing the cycle
rank is NP-complete, even for sparse digraphs of maximum outdegree 2.
Notwithstanding, we provide both a polynomial-time approximation algorithm and
an exponential-time exact algorithm for this problem. The former algorithm
yields an O((log n)^(3/2))- approximation in polynomial time, whereas the
latter yields the optimum solution, and runs in time and space O*(1.9129^n) on
digraphs of maximum outdegree at most two. Regarding the star height problem,
we identify a subclass of the regular languages for which we can precisely
determine the computational complexity of the star height problem. Namely, the
star height problem for bideterministic languages is NP-complete, and this
holds already for binary alphabets. Then we translate the algorithmic results
concerning cycle rank to the bideterministic star height problem, thus giving a
polynomial-time approximation as well as a reasonably fast exact exponential
algorithm for bideterministic star height.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0405066v1,A Logic for Reasoning about Digital Rights,"We present a logic for reasoning about licenses, which are ``terms of use''
for digital resources. The logic provides a language for writing both
properties of licenses and specifications that govern a client's actions. We
discuss the complexity of checking properties and specifications written in our
logic and propose a technique for verification. A key feature of our approach
is that it is essentially parameterized by the language in which the licenses
are written, provided that this language can be given a trace-based semantics.
We consider two license languages to illustrate this flexibility.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/cs/0103013v1,CRL at Ntcir2,"We have developed systems of two types for NTCIR2. One is an enhenced version
of the system we developed for NTCIR1 and IREX. It submitted retrieval results
for JJ and CC tasks. A variety of parameters were tried with the system. It
used such characteristics of newspapers as locational information in the CC
tasks. The system got good results for both of the tasks. The other system is a
portable system which avoids free parameters as much as possible. The system
submitted retrieval results for JJ, JE, EE, EJ, and CC tasks. The system
automatically determined the number of top documents and the weight of the
original query used in automatic-feedback retrieval. It also determined
relevant terms quite robustly. For EJ and JE tasks, it used document expansion
to augment the initial queries. It achieved good results, except on the CC
tasks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1605.06368v1,Modeling Evolutionary Dynamics of Lurking in Social Networks,"Lurking is a complex user-behavioral phenomenon that occurs in all
large-scale online communities and social networks. It generally refers to the
behavior characterizing users that benefit from the information produced by
others in the community without actively contributing back to the production of
social content. The amount and evolution of lurkers may strongly affect an
online social environment, therefore understanding the lurking dynamics and
identifying strategies to curb this trend are relevant problems. In this
regard, we introduce the Lurker Game, i.e., a model for analyzing the
transitions from a lurking to a non-lurking (i.e., active) user role, and vice
versa, in terms of evolutionary game theory. We evaluate the proposed Lurker
Game by arranging agents on complex networks and analyzing the system
evolution, seeking relations between the network topology and the final
equilibrium of the game. Results suggest that the Lurker Game is suitable to
model the lurking dynamics, showing how the adoption of rewarding mechanisms
combined with the modeling of hypothetical heterogeneity of users' interests
may lead users in an online community towards a cooperative behavior.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0809.3565v2,On fractionality of the path packing problem,"In this paper, we study fractional multiflows in undirected graphs. A
fractional multiflow in a graph G with a node subset T, called terminals, is a
collection of weighted paths with ends in T such that the total weights of
paths traversing each edge does not exceed 1. Well-known fractional path
packing problem consists of maximizing the total weight of paths with ends in a
subset S of TxT over all fractional multiflows. Together, G,T and S form a
network. A network is an Eulerian network if all nodes in N\T have even
degrees.
  A term ""fractionality"" was defined for the fractional path packing problem by
A. Karzanov as the smallest natural number D so that there exists a solution to
the problem that becomes integer-valued when multiplied by D. A. Karzanov has
defined the class of Eulerian networks in terms of T and S, outside which D is
infinite and proved that whithin this class D can be 1,2 or 4. He conjectured
that D should be 1 or 2 for this class of networks. In this paper we prove this
conjecture.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.05548v1,"Segmentation with Residual Attention U-Net and an Edge-Enhancement
  Approach Preserves Cell Shape Features","The ability to extrapolate gene expression dynamics in living single cells
requires robust cell segmentation, and one of the challenges is the amorphous
or irregularly shaped cell boundaries. To address this issue, we modified the
U-Net architecture to segment cells in fluorescence widefield microscopy images
and quantitatively evaluated its performance. We also proposed a novel loss
function approach that emphasizes the segmentation accuracy on cell boundaries
and encourages shape feature preservation. With a 97% sensitivity, 93%
specificity, 91% Jaccard similarity, and 95% Dice coefficient, our proposed
method called Residual Attention U-Net with edge-enhancement surpassed the
state-of-the-art U-Net in segmentation performance as evaluated by the
traditional metrics. More remarkably, the same proposed candidate also
performed the best in terms of the preservation of valuable shape features,
namely area, eccentricity, major axis length, solidity and orientation. These
improvements on shape feature preservation can serve as useful assets for
downstream cell tracking and quantification of changes in cell statistics or
features over time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0011030v1,"Logic Programming Approaches for Representing and Solving Constraint
  Satisfaction Problems: A Comparison","Many logic programming based approaches can be used to describe and solve
combinatorial search problems. On the one hand there is constraint logic
programming which computes a solution as an answer substitution to a query
containing the variables of the constraint satisfaction problem. On the other
hand there are systems based on stable model semantics, abductive systems, and
first order logic model generators which compute solutions as models of some
theory. This paper compares these different approaches from the point of view
of knowledge representation (how declarative are the programs) and from the
point of view of performance (how good are they at solving typical problems).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.08334v1,"A homogenized damping model for the propagation of elastic wave in a
  porous solid","This paper develops an averaging technique based on the combination of the
eigenfunction expansion method and the collaboration method to investigate the
multiple scattering effect of the SH wave propagation in a porous medium. The
semi-analytical averaging technique is conducted using Monto Carlo method to
understand the macroscopic dispersion and attenuation phenomena of the stress
wave propagation in a porous solid caused by the multiple scattering effects.
The averaging technique is verified by finite element analysis. Finally, a
simple homogenized elastic model with damping is proposed to describe the
macroscopic dispersion and attenuation effects of SH waves in porous media.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0910.5046v1,Temporal Debugging using URDB,"A new style of temporal debugging is proposed. The new URDB debugger can
employ such techniques as temporal search for finding an underlying fault that
is causing a bug. This improves on the standard iterative debugging style,
which iteratively re-executes a program under debugger control in the search
for the underlying fault. URDB acts as a meta-debugger, with current support
for four widely used debuggers: gdb, MATLAB, python, and perl. Support for a
new debugger can be added in a few hours. Among its points of novelty are: (i)
the first reversible debuggers for MATLAB, python, and perl; (ii) support for
today's multi-core architectures; (iii) reversible debugging of multi-process
and distributed computations; and (iv) temporal search on changes in program
expressions. URDB gains its reversibility and temporal abilities through the
fast checkpoint-restart capability of DMTCP (Distributed MultiThreaded
CheckPointing). The recently enhanced DMTCP also adds ptrace support, enabling
one to freeze, migrate, and replicate debugging sessions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.09162v2,Abnormal Road Surface Detection Using Wheel Sensor Data,"In this manuscript we demonstrate that accurate road abnormality detection
based on signals from a 3D force measuring sensor implanted into the tires of a
vehicle is possible. We discuss approximating the sensor's output using
adaptive Hermite-functions [4] and present an experiment that shows the
connection between abnormal road conditions and the level of noise in the
residual signal. Finally, we experiment with different classification schemes
and conclude that a model-based neural network architecture (VP-NET [7])
outperforms the other candidates in both accuracy and simplicity for surface
abnormality detection tasks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1402.2930v2,"An Algorithm to Compute the Topological Euler Characteristic,
  Chern-Schwartz-MacPherson Class and Segre Class of Projective Varieties","Let $V$ be a closed subscheme of a projective space $\mathbb{P}^n$. We give
an algorithm to compute the Chern-Schwartz-MacPherson class, Euler
characteristic and Segre class of $ V$. The algorithm can be implemented using
either symbolic or numerical methods. The algorithm is based on a new method
for calculating the projective degrees of a rational map defined by a
homogeneous ideal. Using this result and known formulas for the
Chern-Schwartz-MacPherson class of a projective hypersurface and the Segre
class of a projective variety in terms of the projective degrees of certain
rational maps we give algorithms to compute the Chern-Schwartz-MacPherson class
and Segre class of a projective variety. Since the Euler characteristic of $V$
is the degree of the zero dimensional component of the
Chern-Schwartz-MacPherson class of $V$ our algorithm also computes the Euler
characteristic $\chi(V)$. Relationships between the algorithm developed here
and other existing algorithms are discussed. The algorithm is tested on several
examples and performs favourably compared to current algorithms for computing
Chern-Schwartz-MacPherson classes, Segre classes and Euler characteristics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.04160v2,"The Crossover Process: Learnability and Data Protection from Inference
  Attacks","It is usual to consider data protection and learnability as conflicting
objectives. This is not always the case: we show how to jointly control
inference --- seen as the attack --- and learnability by a noise-free process
that mixes training examples, the Crossover Process (cp). One key point is that
the cp~is typically able to alter joint distributions without touching on
marginals, nor altering the sufficient statistic for the class. In other words,
it saves (and sometimes improves) generalization for supervised learning, but
can alter the relationship between covariates --- and therefore fool measures
of nonlinear independence and causal inference into misleading ad-hoc
conclusions. For example, a cp~can increase / decrease odds ratios, bring
fairness or break fairness, tamper with disparate impact, strengthen, weaken or
reverse causal directions, change observed statistical measures of dependence.
For each of these, we quantify changes brought by a cp, as well as its
statistical impact on generalization abilities via a new complexity measure
that we call the Rademacher cp~complexity. Experiments on a dozen readily
available domains validate the theory.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/cs/0207051v2,Exporting Prolog source code,"In this paper we present a simple source code configuration tool. ExLibris
operates on libraries and can be used to extract from local libraries all code
relevant to a particular project. Our approach is not designed to address
problems arising in code production lines, but rather, to support the needs of
individual or small teams of researchers who wish to communicate their Prolog
programs. In the process, we also wish to accommodate and encourage the writing
of reusable code. Moreover, we support and propose ways of dealing with issues
arising in the development of code that can be run on a variety of like-minded
Prolog systems. With consideration to these aims we have made the following
decisions: (i) support file-based source development, (ii) require minimal
program transformation, (iii) target simplicity of usage, and (iv) introduce
minimum number of new primitives.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1408.1675v3,Database Queries that Explain their Work,"Provenance for database queries or scientific workflows is often motivated as
providing explanation, increasing understanding of the underlying data sources
and processes used to compute the query, and reproducibility, the capability to
recompute the results on different inputs, possibly specialized to a part of
the output. Many provenance systems claim to provide such capabilities;
however, most lack formal definitions or guarantees of these properties, while
others provide formal guarantees only for relatively limited classes of
changes. Building on recent work on provenance traces and slicing for
functional programming languages, we introduce a detailed tracing model of
provenance for multiset-valued Nested Relational Calculus, define trace slicing
algorithms that extract subtraces needed to explain or recompute specific parts
of the output, and define query slicing and differencing techniques that
support explanation. We state and prove correctness properties for these
techniques and present a proof-of-concept implementation in Haskell.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.05690v1,"Candidate Software Process Flaws for the Boeing 737 Max MCAS Algorithm
  and Risks for a Proposed Upgrade","By reasoning about the claims and speculations promised as part of the public
discourse, we analyze the hypothesis that flaws in software engineering played
a critical role in the Boeing 737 MCAS incidents. We use promise-based
reasoning to discuss how, from an outsider's perspective, one may assemble
clues about what went wrong. Rather than looking for a Rational Alternative
Design (RAD), as suggested by Wendel, we look for candidate flaws in the
software process. We describe four such potential flaws. Recently, Boeing has
circulated information on its envisaged MCAS algorithm upgrade. We cast this as
a promise to resolve the flaws, i.e. to provide a RAD for the B737 Max. We
offer an assessment of B-Max-New based on the public discourse.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2012.02197v2,"Addressing machine learning concept drift reveals declining vaccine
  sentiment during the COVID-19 pandemic","Social media analysis has become a common approach to assess public opinion
on various topics, including those about health, in near real-time. The growing
volume of social media posts has led to an increased usage of modern machine
learning methods in natural language processing. While the rapid dynamics of
social media can capture underlying trends quickly, it also poses a technical
problem: algorithms trained on annotated data in the past may underperform when
applied to contemporary data. This phenomenon, known as concept drift, can be
particularly problematic when rapid shifts occur either in the topic of
interest itself, or in the way the topic is discussed. Here, we explore the
effect of machine learning concept drift by focussing on vaccine sentiments
expressed on Twitter, a topic of central importance especially during the
COVID-19 pandemic. We show that while vaccine sentiment has declined
considerably during the COVID-19 pandemic in 2020, algorithms trained on
pre-pandemic data would have largely missed this decline due to concept drift.
Our results suggest that social media analysis systems must address concept
drift in a continuous fashion in order to avoid the risk of systematic
misclassification of data, which is particularly likely during a crisis when
the underlying data can change suddenly and rapidly.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.03109v1,"Laplace Matching for fast Approximate Inference in Generalized Linear
  Models","Bayesian inference in generalized linear models (GLMs), i.e.~Gaussian
regression with non-Gaussian likelihoods, is generally non-analytic and
requires computationally expensive approximations, such as sampling or
variational inference. We propose an approximate inference framework primarily
designed to be computationally cheap while still achieving high approximation
quality. The concept, which we call \emph{Laplace Matching}, involves
closed-form, approximate, bi-directional transformations between the parameter
spaces of exponential families. These are constructed from Laplace
approximations under custom-designed basis transformations. The mappings can
then be leveraged to effectively turn a latent Gaussian distribution into a
conjugate prior for a rich class of observable variables. This effectively
turns inference in GLMs into conjugate inference (with small approximation
errors). We empirically evaluate the method in two different GLMs, showing
approximation quality comparable to state-of-the-art approximate inference
techniques at a drastic reduction in computational cost. More specifically, our
method has a cost comparable to the \emph{very first} step of the iterative
optimization usually employed in standard GLM inference.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.01524v1,Multi-Level Evolution Strategies for High-Resolution Black-Box Control,"This paper introduces a multi-level (m-lev) mechanism into Evolution
Strategies (ESs) in order to address a class of global optimization problems
that could benefit from fine discretization of their decision variables. Such
problems arise in engineering and scientific applications, which possess a
multi-resolution control nature, and thus may be formulated either by means of
low-resolution variants (providing coarser approximations with presumably lower
accuracy for the general problem) or by high-resolution controls. A particular
scientific application concerns practical Quantum Control (QC) problems, whose
targeted optimal controls may be discretized to increasingly higher resolution,
which in turn carries the potential to obtain better control yields. However,
state-of-the-art derivative-free optimization heuristics for high-resolution
formulations nominally call for an impractically large number of objective
function calls. Therefore, an effective algorithmic treatment for such problems
is needed. We introduce a framework with an automated scheme to facilitate
guided-search over increasingly finer levels of control resolution for the
optimization problem, whose on-the-fly learned parameters require careful
adaptation. We instantiate the proposed m-lev self-adaptive ES framework by two
specific strategies, namely the classical elitist single-child (1+1)-ES and the
non-elitist multi-child derandomized $(\mu_W,\lambda)$-sep-CMA-ES. We first
show that the approach is suitable by simulation-based optimization of QC
systems which were heretofore viewed as too complex to address. We also present
a laboratory proof-of-concept for the proposed approach on a basic experimental
QC system objective.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9909006v1,Motion Planning of Legged Robots,"We study the problem of computing the free space F of a simple legged robot
called the spider robot. The body of this robot is a single point and the legs
are attached to the body. The robot is subject to two constraints: each leg has
a maximal extension R (accessibility constraint) and the body of the robot must
lie above the convex hull of its feet (stability constraint). Moreover, the
robot can only put its feet on some regions, called the foothold regions. The
free space F is the set of positions of the body of the robot such that there
exists a set of accessible footholds for which the robot is stable. We present
an efficient algorithm that computes F in O(n2 log n) time using O(n2 alpha(n))
space for n discrete point footholds where alpha(n) is an extremely slowly
growing function (alpha(n) <= 3 for any practical value of n). We also present
an algorithm for computing F when the foothold regions are pairwise disjoint
polygons with n edges in total. This algorithm computes F in O(n2 alpha8(n) log
n) time using O(n2 alpha8(n)) space (alpha8(n) is also an extremely slowly
growing function). These results are close to optimal since Omega(n2) is a
lower bound for the size of F.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0202003v3,Simple Optimal Wait-free Multireader Registers,"Multireader shared registers are basic objects used as communication medium
in asynchronous concurrent computation. We propose a surprisingly simple and
natural scheme to obtain several wait-free constructions of bounded 1-writer
multireader registers from atomic 1-writer 1-reader registers, that is easier
to prove correct than any previous construction. Our main construction is the
first symmetric pure timestamp one that is optimal with respect to the
worst-case local use of control bits; the other one is optimal with respect to
global use of control bits; both are optimal in time.",0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.00587v1,A Tensor SVD-based Classification Algorithm Applied to fMRI Data,"To analyze the abundance of multidimensional data, tensor-based frameworks
have been developed. Traditionally, the matrix singular value decomposition
(SVD) is used to extract the most dominant features from a matrix containing
the vectorized data. While the SVD is highly useful for data that can be
appropriately represented as a matrix, this step of vectorization causes us to
lose the high-dimensional relationships intrinsic to the data. To facilitate
efficient multidimensional feature extraction, we utilize a projection-based
classification algorithm using the t-SVDM, a tensor analog of the matrix SVD.
Our work extends the t-SVDM framework and the classification algorithm, both
initially proposed for tensors of order 3, to any number of dimensions. We then
apply this algorithm to a classification task using the StarPlus fMRI dataset.
Our numerical experiments demonstrate that there exists a superior tensor-based
approach to fMRI classification than the best possible equivalent matrix-based
approach. Our results illustrate the advantages of our chosen tensor framework,
provide insight into beneficial choices of parameters, and could be further
developed for classification of more complex imaging data. We provide our
Python implementation at https://github.com/elizabethnewman/tensor-fmri.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0209029v1,A generalization of Amdahl's law and relative conditions of parallelism,"In this work I present a generalization of Amdahl's law on the limits of a
parallel implementation with many processors. In particular I establish some
mathematical relations involving the number of processors and the dimension of
the treated problem, and with these conditions I define, on the ground of the
reachable speedup, some classes of parallelism for the implementations. I also
derive a condition for obtaining superlinear speedup. The used mathematical
technics are those of differential calculus. I describe some examples from
classical problems offered by the specialized literature on the subject.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1803.11437v1,A Rule for Committee Selection with Soft Diversity Constraints,"Committee selection with diversity or distributional constraints is a
ubiquitous problem. However, many of the formal approaches proposed so far have
certain drawbacks including (1) computationally intractability in general, and
(2) inability to suggest a solution for certain instances where the hard
constraints cannot be met. We propose a practical and polynomial-time algorithm
for diverse committee selection that draws on the idea of using soft bounds and
satisfies natural axioms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.12243v3,"An analysis of the SIGMOD 2014 Programming Contest: Complex queries on
  the LDBC social network graph","This report contains an analysis of the queries defined in the SIGMOD 2014
Programming Contest. We first describe the data set, then present the queries,
providing graphical illustrations for them and pointing out their caveats. Our
intention is to document our lessons learnt and simplify the work of those who
will attempt to create a solution to this contest. We also demonstrate the
influence of this contest by listing followup works which used these queries as
inspiration to design better algorithms or to define interesting graph queries.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1501.02686v1,"Watermarking PDF Documents using Various Representations of
  Self-inverting Permutations","This work provides to web users copyright protection of their Portable
Document Format (PDF) documents by proposing efficient and easily implementable
techniques for PDF watermarking; our techniques are based on the ideas of our
recently proposed watermarking techniques for software, image, and audio,
expanding thus the digital objects that can be efficiently watermarked through
the use of self-inverting permutations. In particular, we present various
representations of a self-inverting permutation $\pi^*$ namely
1D-representation, 2D-representation, and RPG-representation, and show that
theses representations can be efficiently applied to PDF watermarking. Indeed,
we first present an audio-based technique for marking a PDF document $T$ by
exploiting the 1D-representation of a permutation $\pi^*$, and then, since
pages of a PDF document $T$ are 2D objects, we present an image-based algorithm
for encoding $\pi^*$ into $T$ by first mapping the elements of $\pi^*$ into a
matrix $A^*$ and then using the information stored in $A^*$ to mark invisibly
specific areas of PDF document $T$. Finally, we describe a graph-based
watermarking algorithm for embedding a self-inverting permutation $\pi^*$ into
the document structure of a PDF file $T$ by exploiting the RPG-representation
of $\pi^*$ and the structure of a PDF document. We have evaluated the embedding
and extracting algorithms by testing them on various and different in
characteristics PDF documents.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1510.01920v1,"Encouraging Diversity- and Representation-Awareness in Geographically
  Centralized Content","In centralized countries, not only population, media and economic power are
concentrated, but people give more attention to central locations. While this
is not inherently bad, this behavior extends to micro-blogging platforms:
central locations get more attention in terms of information flow. In this
paper we study the effects of an information filtering algorithm that
decentralizes content in such platforms. Particularly, we find that users from
non-central locations were not able to identify the geographical diversity on
timelines generated by the algorithm, which were diverse by construction. To
make users see the inherent diversity, we define a design rationale to approach
this problem, focused on an already known visualization technique: treemaps.
Using interaction data from an ""in the wild"" deployment of our proposed system,
we find that, even though there are effects of centralization in exploratory
user behavior, the treemap was able to make users see the inherent geographical
diversity of timelines, and engage with user generated content. With these
results in mind, we propose practical actions for micro-blogging platforms to
account for the differences and biased behavior induced by centralization.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.06681v6,Weighted Regular Tree Grammars with Storage,"We introduce weighted regular tree grammars with storage as combination of
(a) regular tree grammars with storage and (b) weighted tree automata over
multioperator monoids. Each weighted regular tree grammar with storage
generates a weighted tree language, which is a mapping from the set of trees to
the multioperator monoid. We prove that, for multioperator monoids canonically
associated to particular strong bi-monoids, the support of the generated
weighted tree languages can be generated by (unweighted) regular tree grammars
with storage. We characterize the class of all generated weighted tree
languages by the composition of three basic concepts. Moreover, we prove
results on the elimination of chain rules and of finite storage types, and we
characterize weighted regular tree grammars with storage by a new weighted
MSO-logic.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.04739v1,"MIPT-NSU-UTMN at SemEval-2021 Task 5: Ensembling Learning with
  Pre-trained Language Models for Toxic Spans Detection","This paper describes our system for SemEval-2021 Task 5 on Toxic Spans
Detection. We developed ensemble models using BERT-based neural architectures
and post-processing to combine tokens into spans. We evaluated several
pre-trained language models using various ensemble techniques for toxic span
identification and achieved sizable improvements over our baseline fine-tuned
BERT models. Finally, our system obtained a F1-score of 67.55% on test data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0309002v2,El informe NERA analizado,"This is a review of the article ""Government Preferences for Promoting
Open-Source Software: A Solution in Search of A Problem"" by David Evans and
Bernard J. Reddy. This report was paid for by Microsoft and put together at its
request. Now Microsoft is using it as part of their lobbying campaign in Europe
against governments' promotion of Open Source Software. As expected, this
article is strongly biased and most of the conclusions are based upon false
hypotheses and evidence.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/cs/9811031v1,Speech Synthesis with Neural Networks,"Text-to-speech conversion has traditionally been performed either by
concatenating short samples of speech or by using rule-based systems to convert
a phonetic representation of speech into an acoustic representation, which is
then converted into speech. This paper describes a system that uses a
time-delay neural network (TDNN) to perform this phonetic-to-acoustic mapping,
with another neural network to control the timing of the generated speech. The
neural network system requires less memory than a concatenation system, and
performed well in tests comparing it to commercial systems using other
technologies.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/cs/0307028v1,Issues in Communication Game,"As interaction between autonomous agents, communication can be analyzed in
game-theoretic terms. Meaning game is proposed to formalize the core of
intended communication in which the sender sends a message and the receiver
attempts to infer its meaning intended by the sender. Basic issues involved in
the game of natural language communication are discussed, such as salience,
grammaticality, common sense, and common belief, together with some
demonstration of the feasibility of game-theoretic account of language.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.07909v1,"Surveying the Research on Fake News in Social Media: a Tale of Networks
  and Language","The history of journalism and news diffusion is tightly coupled with the
effort to dispel hoaxes, misinformation, propaganda, unverified rumours, poor
reporting, and messages containing hate and divisions. With the explosive
growth of online social media and billions of individuals engaged with
consuming, creating, and sharing news, this ancient problem has surfaced with a
renewed intensity threatening our democracies, public health, and news outlets
credibility. This has triggered many researchers to develop new methods for
studying, understanding, detecting, and preventing fake-news diffusion; as a
consequence, thousands of scientific papers have been published in a relatively
short period, making researchers of different disciplines to struggle in search
of open problems and most relevant trends. The aim of this survey is threefold:
first, we want to provide the researchers interested in this multidisciplinary
and challenging area with a network-based analysis of the existing literature
to assist them with a visual exploration of papers that can be of interest;
second, we present a selection of the main results achieved so far adopting the
network as an unifying framework to represent and make sense of data, to model
diffusion processes, and to evaluate different debunking strategies. Finally,
we present an outline of the most relevant research trends focusing on the
moving target of fake-news, bots, and trolls identification by means of data
mining and text technologies; despite scholars working on computational
linguistics and networks traditionally belong to different scientific
communities, we expect that forthcoming computational approaches to prevent
fake news from polluting the social media must be developed using hybrid and
up-to-date methodologies.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1811.00989v1,"CMI: An Online Multi-objective Genetic Autoscaler for Scientific and
  Engineering Workflows in Cloud Infrastructures with Unreliable Virtual
  Machines","Cloud Computing is becoming the leading paradigm for executing scientific and
engineering workflows. The large-scale nature of the experiments they model and
their variable workloads make clouds the ideal execution environment due to
prompt and elastic access to huge amounts of computing resources. Autoscalers
are middleware-level software components that allow scaling up and down the
computing platform by acquiring or terminating virtual machines (VM) at the
time that workflow's tasks are being scheduled. In this work we propose a novel
online multi-objective autoscaler for workflows denominated Cloud
Multi-objective Intelligence (CMI), that aims at the minimization of makespan,
monetary cost and the potential impact of errors derived from unreliable VMs.
In addition, this problem is subject to monetary budget constraints. CMI is
responsible for periodically solving the autoscaling problems encountered along
the execution of a workflow. Simulation experiments on four well-known
workflows exhibit that CMI significantly outperforms a state-of-the-art
autoscaler of similar characteristics called Spot Instances Aware Autoscaling
(SIAA). These results convey a solid base for deepening in the study of other
meta-heuristic methods for autoscaling workflow applications using cheap but
unreliable infrastructures.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.03100v2,"Reasoning on Multi-Relational Contextual Hierarchies via Answer Set
  Programming with Algebraic Measures","Dealing with context dependent knowledge has led to different formalizations
of the notion of context. Among them is the Contextualized Knowledge Repository
(CKR) framework, which is rooted in description logics but links on the
reasoning side strongly to logic programs and Answer Set Programming (ASP) in
particular. The CKR framework caters for reasoning with defeasible axioms and
exceptions in contexts, which was extended to knowledge inheritance across
contexts in a coverage (specificity) hierarchy. However, the approach supports
only this single type of contextual relation and the reasoning procedures work
only for restricted hierarchies, due to non-trivial issues with model
preference under exceptions. In this paper, we overcome these limitations and
present a generalization of CKR hierarchies to multiple contextual relations,
along with their interpretation of defeasible axioms and preference. To support
reasoning, we use ASP with algebraic measures, which is a recent extension of
ASP with weighted formulas over semirings that allows one to associate
quantities with interpretations depending on the truth values of propositional
atoms. Notably, we show that for a relevant fragment of CKR hierarchies with
multiple contextual relations, query answering can be realized with the popular
asprin framework. The algebraic measures approach is more powerful and enables
e.g. reasoning with epistemic queries over CKRs, which opens interesting
perspectives for the use of quantitative ASP extensions in other applications.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.11178v1,Overview of the IBM Neural Computer Architecture,"The IBM Neural Computer (INC) is a highly flexible, re-configurable parallel
processing system that is intended as a research and development platform for
emerging machine intelligence algorithms and computational neuroscience. It
consists of hundreds of programmable nodes, primarily based on Xilinx's Field
Programmable Gate Array (FPGA) technology. The nodes are interconnected in a
scalable 3d mesh topology. We overview INC, emphasizing unique features such as
flexibility and scalability both in the types of computations performed and in
the available modes of communication, enabling new machine intelligence
approaches and learning strategies not well suited to the matrix
manipulation/SIMD libraries that GPUs are optimized for. This paper describes
the architecture of the machine and applications are to be described in detail
elsewhere.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0703069v3,Portlet Wrappers using JavaScript,"In this paper we extend the classical portal (with static portlets) design
with HTML DOM Web clipping on the client browser using dynamic JavaScript
portlets: the portal server supplies the user/passwords for all services
through https and the client browser retrieves web pages and
cuts/selects/changes the desired parts using paths (XPath) in the Web page
structure. This operation brings along a set of advantages: dynamic wrapping of
existing legacy websites in the client browser, the reloading of only changed
portlets instead of whole portal, low bandwidth on the server, the elimination
of re-writing the URL links in the portal, and last but not least, a support
for Java applets in portlets by putting the login cookies on the client
browser. Our solution is compliant with JSR168 Portlet Specification allowing
portability across all vendor platforms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0106037v1,Using the No-Search Easy-Hard Technique for Downward Collapse,"The top part of the preceding figure [figure appears in actual paper] shows
some classes from the (truth-table) bounded-query and boolean hierarchies. It
is well-known that if either of these hierarchies collapses at a given level,
then all higher levels of that hierarchy collapse to that same level. This is a
standard ``upward translation of equality'' that has been known for over a
decade. The issue of whether these hierarchies can translate equality {\em
downwards\/} has proven vastly more challenging. In particular, with regard to
the figure above, consider the following claim:
  $$P_{m-tt}^{\Sigma_k^p} = P_{m+1-tt}^{\Sigma_k^p} \implies
  DIFF_m(\Sigma_k^p) coDIFF_m(\Sigma_k^p) = BH(\Sigma_k^p). (*) $$
  This claim, if true, says that equality translates downwards between levels
of the bounded-query hierarchy and the boolean hierarchy levels that (before
the fact) are immediately below them.
  Until recently, it was not known whether (*) {\em ever\/} held, except for
the degenerate cases $m=0$ and $k=0$. Then Hemaspaandra, Hemaspaandra, and
Hempel \cite{hem-hem-hem:j:downward-translation} proved that (*) holds for all
$m$, for $k > 2$. Buhrman and Fortnow~\cite{buh-for:j:two-queries} then showed
that, when $k=2$, (*) holds for the case $m = 1$. In this paper, we prove that
for the case $k=2$, (*) holds for all values of $m$. Since there is an oracle
relative to which ``for $k=1$, (*) holds for all $m$'' fails
\cite{buh-for:j:two-queries}, our achievement of the $k=2$ case cannot to be
strengthened to $k=1$ by any relativizable proof technique. The new downward
translation we obtain also tightens the collapse in the polynomial hierarchy
implied by a collapse in the bounded-query hierarchy of the second level of the
polynomial hierarchy.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1102.5471v1,An Implicit Cover Problem in Wild Population Study,"In an implicit combinatorial optimization problem, the constraints are not
enumerated explicitly but rather stated implicitly through equations, other
constraints or auxiliary algorithms. An important subclass of such problems is
the implicit set cover (or, equivalently, hitting set) problem in which the
sets are not given explicitly but rather defined implicitly For example, the
well-known minimum feedback arc set problem is such a problem. In this paper,
we consider such a cover problem that arises in the study of wild populations
in biology in which the sets are defined implicitly via the Mendelian
constraints and prove approximability results for this problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0107013v2,The Logic Programming Paradigm and Prolog,"This is a tutorial on logic programming and Prolog appropriate for a course
on programming languages for students familiar with imperative programming.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1401.0102v3,A DDoS-Aware IDS Model Based on Danger Theory and Mobile Agents,"We propose an artificial immune model for intrusion detection in distributed
systems based on a relatively recent theory in immunology called Danger theory.
Based on Danger theory, immune response in natural systems is a result of
sensing corruption as well as sensing unknown substances. In contrast,
traditional self-nonself discrimination theory states that immune response is
only initiated by sensing nonself (unknown) patterns. Danger theory solves many
problems that could only be partially explained by the traditional model.
Although the traditional model is simpler, such problems result in high false
positive rates in immune-inspired intrusion detection systems. We believe using
danger theory in a multi-agent environment that computationally emulates the
behavior of natural immune systems is effective in reducing false positive
rates. We first describe a simplified scenario of immune response in natural
systems based on danger theory and then, convert it to a computational model as
a network protocol. In our protocol, we define several immune signals and model
cell signaling via message passing between agents that emulate cells. Most
messages include application-specific patterns that must be meaningfully
extracted from various system properties. We show how to model these messages
in practice by performing a case study on the problem of detecting distributed
denial-of-service attacks in wireless sensor networks. We conduct a set of
systematic experiments to find a set of performance metrics that can accurately
distinguish malicious patterns. The results indicate that the system can be
efficiently used to detect malicious patterns with a high level of accuracy.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1110.4159v1,A Logic for Choreographies,"We explore logical reasoning for the global calculus, a coordination model
based on the notion of choreography, with the aim to provide a methodology for
specification and verification of structured communications. Starting with an
extension of Hennessy-Milner logic, we present the global logic (GL), a modal
logic describing possible interactions among participants in a choreography. We
illustrate its use by giving examples of properties on service specifications.
Finally, we show that, despite GL is undecidable, there is a significant
decidable fragment which we provide with a sound and complete proof system for
checking validity of formulae.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1904.05980v1,Parallel algorithms development for programmable logic devices,"Programmable Logic Devices (PLDs) continue to grow in size and currently
contain several millions of gates. At the same time, research effort is going
into higher-level hardware synthesis methodologies for reconfigurable computing
that can exploit PLD technology. In this paper, we explore the effectiveness
and extend one such formal methodology in the design of massively parallel
algorithms. We take a step-wise refinement approach to the development of
correct reconfigurable hardware circuits from formal specifications. A
functional programming notation is used for specifying algorithms and for
reasoning about them. The specifications are realised through the use of a
combination of function decomposition strategies, data refinement techniques,
and off-the-shelf refinements based upon higher-order functions. The
off-the-shelf refinements are inspired by the operators of Communicating
Sequential Processes (CSP) and map easily to programs in Handel-C (a hardware
description language). The Handel-C descriptions are directly compiled into
reconfigurable hardware. The practical realisation of this methodology is
evidenced by a case studying the matrix multiplication algorithm as it is
relatively simple and well known. In this paper, we obtain several hardware
implementations with different performance characteristics by applying
different refinements to the algorithm. The developed designs are compiled and
tested under Celoxica's RC-1000 reconfigurable computer with its 2 million
gates Virtex-E FPGA. Performance analysis and evaluation of these
implementations are included.",0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0102006v3,Orderly Spanning Trees with Applications,"We introduce and study the {\em orderly spanning trees} of plane graphs. This
algorithmic tool generalizes {\em canonical orderings}, which exist only for
triconnected plane graphs. Although not every plane graph admits an orderly
spanning tree, we provide an algorithm to compute an {\em orderly pair} for any
connected planar graph $G$, consisting of a plane graph $H$ of $G$, and an
orderly spanning tree of $H$. We also present several applications of orderly
spanning trees: (1) a new constructive proof for Schnyder's Realizer Theorem,
(2) the first area-optimal 2-visibility drawing of $G$, and (3) the best known
encodings of $G$ with O(1)-time query support. All algorithms in this paper run
in linear time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0808.3651v3,Flow Faster: Efficient Decision Algorithms for Probabilistic Simulations,"Strong and weak simulation relations have been proposed for Markov chains,
while strong simulation and strong probabilistic simulation relations have been
proposed for probabilistic automata. However, decision algorithms for strong
and weak simulation over Markov chains, and for strong simulation over
probabilistic automata are not efficient, which makes it as yet unclear whether
they can be used as effectively as their non-probabilistic counterparts. This
paper presents drastically improved algorithms to decide whether some
(discrete- or continuous-time) Markov chain strongly or weakly simulates
another, or whether a probabilistic automaton strongly simulates another. The
key innovation is the use of parametric maximum flow techniques to amortize
computations. We also present a novel algorithm for deciding strong
probabilistic simulation preorders on probabilistic automata, which has
polynomial complexity via a reduction to an LP problem. When extending the
algorithms for probabilistic automata to their continuous-time counterpart, we
retain the same complexity for both strong and strong probabilistic
simulations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1501.06313v1,"Matching or Crashing? Personality-based Team Formation in Crowdsourcing
  Environments","""Does placing workers together based on their personality give better
performance results in cooperative crowdsourcing settings, compared to
non-personality based crowd team formation?"" In this work we examine the impact
of personality compatibility on the effectiveness of crowdsourced team work.
Using a personality-based group dynamics approach, we examine two main types of
personality combinations (matching and crashing) on two main types of tasks
(collaborative and competitive). Our experimental results show that personality
compatibility significantly affects the quality of the team's final outcome,
the quality of interactions and the emotions experienced by the team members.
The present study is the first to examine the effect of personality over team
result in crowdsourcing settings, and it has practical implications for the
better design of crowdsourced team work.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1610.04758v1,"Sensing Emotions in Text Messages: An Application and Deployment Study
  of EmotionPush","Instant messaging and push notifications play important roles in modern
digital life. To enable robust sense-making and rich context awareness in
computer mediated communications, we introduce EmotionPush, a system that
automatically conveys the emotion of received text with a colored push
notification on mobile devices. EmotionPush is powered by state-of-the-art
emotion classifiers and is deployed for Facebook Messenger clients on Android.
The study showed that the system is able to help users prioritize interactions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1903.08588v2,"A Polynomial-time Solution for Robust Registration with Extreme Outlier
  Rates","We propose a robust approach for the registration of two sets of 3D points in
the presence of a large amount of outliers. Our first contribution is to
reformulate the registration problem using a Truncated Least Squares (TLS) cost
that makes the estimation insensitive to a large fraction of spurious
point-to-point correspondences. The second contribution is a general framework
to decouple rotation, translation, and scale estimation, which allows solving
in cascade for the three transformations. Since each subproblem (scale,
rotation, and translation estimation) is still non-convex and combinatorial in
nature, out third contribution is to show that (i) TLS scale and
(component-wise) translation estimation can be solved exactly and in polynomial
time via an adaptive voting scheme, (ii) TLS rotation estimation can be relaxed
to a semidefinite program and the relaxation is tight in practice, even in the
presence of an extreme amount of outliers. We validate the proposed algorithm,
named TEASER (Truncated least squares Estimation And SEmidefinite Relaxation),
in standard registration benchmarks showing that the algorithm outperforms
RANSAC and robust local optimization techniques, and favorably compares with
Branch-and-Bound methods, while being a polynomial-time algorithm. TEASER can
tolerate up to 99% outliers and returns highly-accurate solutions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0811.1885v1,The Expressive Power of Binary Submodular Functions,"It has previously been an open problem whether all Boolean submodular
functions can be decomposed into a sum of binary submodular functions over a
possibly larger set of variables. This problem has been considered within
several different contexts in computer science, including computer vision,
artificial intelligence, and pseudo-Boolean optimisation. Using a connection
between the expressive power of valued constraints and certain algebraic
properties of functions, we answer this question negatively.
  Our results have several corollaries. First, we characterise precisely which
submodular functions of arity 4 can be expressed by binary submodular
functions. Next, we identify a novel class of submodular functions of arbitrary
arities which can be expressed by binary submodular functions, and therefore
minimised efficiently using a so-called expressibility reduction to the Min-Cut
problem. More importantly, our results imply limitations on this kind of
reduction and establish for the first time that it cannot be used in general to
minimise arbitrary submodular functions. Finally, we refute a conjecture of
Promislow and Young on the structure of the extreme rays of the cone of Boolean
submodular functions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1305.3102v1,FPT is Characterized by Useful Obstruction Sets,"Many graph problems were first shown to be fixed-parameter tractable using
the results of Robertson and Seymour on graph minors. We show that the
combination of finite, computable, obstruction sets and efficient order tests
is not just one way of obtaining strongly uniform FPT algorithms, but that all
of FPT may be captured in this way. Our new characterization of FPT has a
strong connection to the theory of kernelization, as we prove that problems
with polynomial kernels can be characterized by obstruction sets whose elements
have polynomial size. Consequently we investigate the interplay between the
sizes of problem kernels and the sizes of the elements of such obstruction
sets, obtaining several examples of how results in one area yield new insights
in the other. We show how exponential-size minor-minimal obstructions for
pathwidth k form the crucial ingredient in a novel OR-cross-composition for
k-Pathwidth, complementing the trivial AND-composition that is known for this
problem. In the other direction, we show that OR-cross-compositions into a
parameterized problem can be used to rule out the existence of efficiently
generated quasi-orders on its instances that characterize the NO-instances by
polynomial-size obstructions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.4070v1,"Self-referencing cellular automata: A model of the evolution of
  information control in biological systems","Cellular automata have been useful artificial models for exploring how
relatively simple rules combined with spatial memory can give rise to complex
emergent patterns. Moreover, studying the dynamics of how rules emerge under
artificial selection for function has recently become a powerful tool for
understanding how evolution can innovate within its genetic rule space.
However, conventional cellular automata lack the kind of state feedback that is
surely present in natural evolving systems. Each new generation of a population
leaves an indelible mark on its environment and thus affects the selective
pressures that shape future generations of that population. To model this
phenomenon, we have augmented traditional cellular automata with
state-dependent feedback. Rather than generating automata executions from an
initial condition and a static rule, we introduce mappings which generate
iteration rules from the cellular automaton itself. We show that these new
automata contain disconnected regions which locally act like conventional
automata, thus encapsulating multiple functions into one structure.
Consequently, we have provided a new model for processes like cell
differentiation. Finally, by studying the size of these regions, we provide
additional evidence that the dynamics of self-reference may be critical to
understanding the evolution of natural language. In particular, the rules of
elementary cellular automata appear to be distributed in the same way as words
in the corpus of a natural language.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1804.02251v2,This One Simple Trick Disrupts Digital Communities,"This paper describes an agent based simulation used to model human actions in
belief space, a high-dimensional subset of information space associated with
opinions. Using insights from animal collective behavior, we are able to
simulate and identify behavior patterns that are similar to nomadic, flocking
and stampeding patterns of animal groups. These behaviors have analogous
manifestations in human interaction, emerging as solitary explorers, the
fashion-conscious, and members of polarized echo chambers. We demonstrate that
a small portion of nomadic agents that widely traverse belief space can disrupt
a larger population of stampeding agents. Extending the model, we introduce the
concept of Adversarial Herding, where bad actors can exploit properties of
technologically mediated communication to artificially create self sustaining
runaway polarization. We call this condition the Pishkin Effect as it recalls
the large scale buffalo stampedes that could be created by native Americans
hunters. We then discuss opportunities for system design that could leverage
the ability to recognize these negative patterns, and discuss affordances that
may disrupt the formation of natural and deliberate echo chambers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0903.2543v1,"Multi-Agent Crisis Response systems - Design Requirements and Analysis
  of Current Systems","Crisis response is a critical area of research, with encouraging progress in
the past view yeas. The aim of the research is to contribute to building future
crisis environment where software agents, robots, responders, crisis managers,
and crisis organizations interact to provide advice, protection and aid. This
paper discusses the crisis response domain requirements, and provides analysis
of five crisis response systems namely: DrillSim [2], DEFACTO [15], ALADDIN
[1], RoboCup Rescue [18], and FireGrid [3]. Analysis of systems includes
systems architecture and methodology. In addition, we identified features and
limitations of systems based on crisis response domain requirements.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
http://arxiv.org/abs/1807.02911v3,A Combined CNN and LSTM Model for Arabic Sentiment Analysis,"Deep neural networks have shown good data modelling capabilities when dealing
with challenging and large datasets from a wide range of application areas.
Convolutional Neural Networks (CNNs) offer advantages in selecting good
features and Long Short-Term Memory (LSTM) networks have proven good abilities
of learning sequential data. Both approaches have been reported to provide
improved results in areas such image processing, voice recognition, language
translation and other Natural Language Processing (NLP) tasks. Sentiment
classification for short text messages from Twitter is a challenging task, and
the complexity increases for Arabic language sentiment classification tasks
because Arabic is a rich language in morphology. In addition, the availability
of accurate pre-processing tools for Arabic is another current limitation,
along with limited research available in this area. In this paper, we
investigate the benefits of integrating CNNs and LSTMs and report obtained
improved accuracy for Arabic sentiment analysis on different datasets.
Additionally, we seek to consider the morphological diversity of particular
Arabic words by using different sentiment classification levels.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1012.5240v1,Exploring Grid Polygons Online,"We investigate the exploration problem of a short-sighted mobile robot moving
in an unknown cellular room. To explore a cell, the robot must enter it. Once
inside, the robot knows which of the 4 adjacent cells exist and which are
boundary edges. The robot starts from a specified cell adjacent to the room's
outer wall; it visits each cell, and returns to the start. Our interest is in a
short exploration tour; that is, in keeping the number of multiple cell visits
small. For abitrary environments containing no obstacles we provide a strategy
producing tours of length S <= C + 1/2 E - 3, and for environments containing
obstacles we provide a strategy, that is bound by S <= C + 1/2 E + 3H + WCW -
2, where C denotes the number of cells-the area-, E denotes the number of
boundary edges-the perimeter-, and H is the number of obstacles, and WCW is a
measure for the sinuosity of the given environment.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1506.07631v1,Dynamic Mechanism Design with Interdependent Valuations,"We consider an infinite horizon dynamic mechanism design problem with
interdependent valuations. In this setting the type of each agent is assumed to
be evolving according to a first order Markov process and is independent of the
types of other agents. However, the valuation of an agent can depend on the
types of other agents, which makes the problem fall into an interdependent
valuation setting. Designing truthful mechanisms in this setting is non-trivial
in view of an impossibility result which says that for interdependent
valuations, any efficient and ex-post incentive compatible mechanism must be a
constant mechanism, even in a static setting. Mezzetti (2004) circumvents this
problem by splitting the decisions of allocation and payment into two stages.
However, Mezzetti's result is limited to a static setting and moreover in the
second stage of that mechanism, agents are weakly indifferent about reporting
their valuations truthfully. This paper provides a first attempt at designing a
dynamic mechanism which is efficient, `strict' ex-post incentive compatible and
ex-post individually rational in a setting with interdependent values and
Markovian type evolution.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1905.01960v1,"The Methods to Improve Quality of Service by Accounting Secure
  Parameters","A solution to the problem of ensuring quality of service, providing a greater
number of services with higher efficiency taking into account network security
is proposed. In this paper, experiments were conducted to analyze the effect of
self-similarity and attacks on the quality of service parameters. Method of
buffering and control of channel capacity and calculating of routing cost
method in the network, which take into account the parameters of traffic
multifractality and the probability of detecting attacks in telecommunications
networks were proposed. The both proposed methods accounting the given
restrictions on the delay time and the number of lost packets for every type
quality of service traffic. During simulation the parameters of transmitted
traffic (self-similarity, intensity) and the parameters of network (current
channel load, node buffer size) were changed and the maximum allowable load of
network was determined. The results of analysis show that occurrence of
overload when transmitting traffic over a switched channel associated with
multifractal traffic characteristics and presence of attack. It was shown that
proposed methods can reduce the lost data and improve the efficiency of network
resources.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1201.3900v1,Elasticity on Ontology Matching of Folksodriven Structure Network,"Nowadays folksonomy tags are used not just for personal organization, but for
communication and sharing between people sharing their own local interests. In
this paper is considered the new concept structure called ""Folksodriven"" to
represent folksonomies. The Folksodriven Structure Network (FSN) was thought as
folksonomy tags suggestions for the user on a dataset built on chosen websites
- based on Natural Language Processing (NLP). Morphological changes, such as
changes in folksonomy tags chose have direct impact on network connectivity
(structural plasticity) of the folksonomy tags considered. The goal of this
paper is on defining a base for a FSN plasticity theory to analyze. To perform
such goal it is necessary a systematic mathematical analysis on deformation and
fracture for the ontology matching on the FSN. The advantages of that approach
could be used on a new interesting method to be employed by a knowledge
management system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1101.5957v1,"A Kind of Representation of Common Knowledge and its Application in
  Requirements Analysis","Since the birth of software engineering, it always are recognized as one pure
engineering subject, therefore, the foundational scientific problems are not
paid much attention. This paper proposes that Requirements Analysis, the kernel
process of software engineering, can be modeled based on the concept of ""common
knowledge"". Such a model would make us understand the nature of this process.
This paper utilizes the formal language as the tool to characterize the ""common
knowledge""-based Requirements Analysis model, and theoretically proves that :
1) the precondition of success of software projects regardless of cost would be
that the participants in a software project have fully known the requirement
specification, if the participants do not understand the meaning of the other
participants; 2) the precondition of success of software projects regardless of
cost would be that the union set of knowledge of basic facts of the
participants in a software project can fully cover the requirement
specification, if the participants can always understand the meaning of the
other participants. These two theorems may have potential meanings to propose
new software engineering methodology.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1301.6843v1,ChaMAILeon: Simplified email sharing like never before!,"While passwords, by definition, are meant to be secret, recent trends in the
Internet usage have witnessed an increasing number of people sharing their
email passwords for both personal and professional purposes. As sharing
passwords increases the chances of your passwords being compromised, leading
websites like Google strongly advise their users not to share their passwords
with anyone. To cater to this conflict of usability versus security and
privacy, we introduce ChaMAILeon, an experimental service, which allows users
to share their email passwords while maintaining their privacy and not
compromising their security. In this report, we discuss the technical details
of the implementation of ChaMAILeon.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0509069v3,Fast and Compact Regular Expression Matching,"We study 4 problems in string matching, namely, regular expression matching,
approximate regular expression matching, string edit distance, and subsequence
indexing, on a standard word RAM model of computation that allows
logarithmic-sized words to be manipulated in constant time. We show how to
improve the space and/or remove a dependency on the alphabet size for each
problem using either an improved tabulation technique of an existing algorithm
or by combining known algorithms in a new way.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.11467v2,Probabilistic Rainfall Estimation from Automotive Lidar,"Robust sensing and perception in adverse weather conditions remain one of the
biggest challenges for realizing reliable autonomous vehicle mobility services.
Prior work has established that rainfall rate is a useful measure for the
adversity of atmospheric weather conditions. This work presents a probabilistic
hierarchical Bayesian model that infers rainfall rate from automotive lidar
point cloud sequences with high accuracy and reliability. The model is a
hierarchical mixture of experts model, or a probabilistic decision tree, with
gating and expert nodes consisting of variational logistic and linear
regression models. Experimental data used to train and evaluate the model is
collected in a large-scale rainfall experiment facility from both stationary
and moving vehicle platforms. The results show prediction accuracy comparable
to the measurement resolution of a disdrometer, and the soundness and
usefulness of the uncertainty estimation. The model achieves RMSE 2.42\,mm/h
after filtering out uncertain predictions. The error is comparable to the mean
rainfall rate change of 3.5\,mm/h between measurements. Model parameter studies
show how predictive performance changes with tree depth, sampling duration, and
crop box dimension. A second experiment demonstrates the predictability of
higher rainfall above 300\,mm/h using a different lidar sensor, demonstrating
sensor independence.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1707.01032v2,The City Pulse of Buenos Aires,"Cell phone technology generates massive amounts of data. Although this data
has been gathered for billing and logging purposes, today it has a much higher
value, because its volume makes it very useful for big data analyses. In this
project, we analyze the viability of using cell phone records to lower the cost
of urban and transportation planning, in particular, to find out how people
travel in a specific city (in this case, Buenos Aires, in Argentina). We use
anonymized cell phone data to estimate the distribution of the population in
the city using different periods of time. We compare those results with
traditional methods (urban polling) using data from Buenos Aires
origin-destination surveys. Traditional polling methods have a much smaller
sample, in the order of tens of thousands (or even less for smaller cities), to
maintain reasonable costs. Furthermore, these studies are performed at most
once per decade, in the best cases, in Argentina and many other countries. Our
objective is to prove that new methods based on cell phone data are reliable,
and can be used indirectly to keep a real-time track of the flow of people
among different parts of a city. We also go further to explore new
possibilities opened by these methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1610.07390v3,A Practical Approach to Interval Refinement for math.h/cmath Functions,"Verification of C++ programs has seen considerable progress in several areas,
but not for programs that use these languages' mathematical libraries. The
reason is that all libraries in widespread use come with no guarantees about
the computed results. This would seem to prevent any attempt at formal
verification of programs that use them: without a specification for the
functions, no conclusion can be drawn statically about the behavior of the
program. We propose an alternative to surrender. We introduce a pragmatic
approach that leverages the fact that most math.h/cmath functions are almost
piecewise monotonic: as we discovered through exhaustive testing, they may have
glitches, often of very small size and in small numbers. We develop interval
refinement techniques for such functions based on a modified dichotomic search,
that enable verification via symbolic execution based model checking, abstract
interpretation, and test data generation. Our refinement algorithms are the
first in the literature to be able to handle non-correctly rounded function
implementations, enabling verification in the presence of the most common
implementations. We experimentally evaluate our approach on real-world code,
showing its ability to detect or rule out anomalous behaviors.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0906.0380v3,"Observational Equivalence and Full Abstraction in the Symmetric
  Interaction Combinators","The symmetric interaction combinators are an equally expressive variant of
Lafont's interaction combinators. They are a graph-rewriting model of
deterministic computation. We define two notions of observational equivalence
for them, analogous to normal form and head normal form equivalence in the
lambda-calculus. Then, we prove a full abstraction result for each of the two
equivalences. This is obtained by interpreting nets as certain subsets of the
Cantor space, called edifices, which play the same role as Boehm trees in the
theory of the lambda-calculus.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1102.5728v1,Named Entity Recognition Using Web Document Corpus,"This paper introduces a named entity recognition approach in textual corpus.
This Named Entity (NE) can be a named: location, person, organization, date,
time, etc., characterized by instances. A NE is found in texts accompanied by
contexts: words that are left or right of the NE. The work mainly aims at
identifying contexts inducing the NE's nature. As such, The occurrence of the
word ""President"" in a text, means that this word or context may be followed by
the name of a president as President ""Obama"". Likewise, a word preceded by the
string ""footballer"" induces that this is the name of a footballer. NE
recognition may be viewed as a classification method, where every word is
assigned to a NE class, regarding the context. The aim of this study is then to
identify and classify the contexts that are most relevant to recognize a NE,
those which are frequently found with the NE. A learning approach using
training corpus: web documents, constructed from learning examples is then
suggested. Frequency representations and modified tf-idf representations are
used to calculate the context weights associated to context frequency, learning
example frequency, and document frequency in the corpus.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1402.2700v5,Bowtie-free graphs have a Ramsey lift,"A bowtie is a graph consisting of two triangles with one vertex identified.
We show that the class of all (countable) graphs not containing a bowtie as a
subgraph has a Ramsey lift (expansion). This solves one of the old problems in
the area and it is the first non-trivial Ramsey class with a non-trivial
algebraic closure.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.01366v2,Recent Advances on the Graph Isomorphism Problem,"We give an overview of recent advances on the graph isomorphism problem. Our
main focus will be on Babai's quasi-polynomial time isomorphism test and
subsequent developments that led to the design of isomorphism algorithms with a
quasi-polynomial parameterized running time of the from
$n^{\text{polylog}(k)}$, where $k$ is a graph parameter such as the maximum
degree. A second focus will be the combinatorial Weisfeiler-Leman algorithm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0308023v1,On the complexity of curve fitting algorithms,"We study a popular algorithm for fitting polynomial curves to scattered data
based on the least squares with gradient weights. We show that sometimes this
algorithm admits a substantial reduction of complexity, and, furthermore, find
precise conditions under which this is possible. It turns out that this is,
indeed, possible when one fits circles but not ellipses or hyperbolas.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.05274v1,Stability of SGD: Tightness Analysis and Improved Bounds,"Stochastic Gradient Descent (SGD) based methods have been widely used for
training large-scale machine learning models that also generalize well in
practice. Several explanations have been offered for this generalization
performance, a prominent one being algorithmic stability [18]. However, there
are no known examples of smooth loss functions for which the analysis can be
shown to be tight. Furthermore, apart from the properties of the loss function,
data distribution has also been shown to be an important factor in
generalization performance. This raises the question: is the stability analysis
of [18] tight for smooth functions, and if not, for what kind of loss functions
and data distributions can the stability analysis be improved? In this paper we
first settle open questions regarding tightness of bounds in the
data-independent setting: we show that for general datasets, the existing
analysis for convex and strongly-convex loss functions is tight, but it can be
improved for non-convex loss functions. Next, we give a novel and improved
data-dependent bounds: we show stability upper bounds for a large class of
convex regularized loss functions, with negligible regularization parameters,
and improve existing data-dependent bounds in the non-convex setting. We hope
that our results will initiate further efforts to better understand the
data-dependent setting under non-convex loss functions, leading to an improved
understanding of the generalization abilities of deep networks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.09391v1,"Real-time Lane detection and Motion Planning in Raspberry Pi and Arduino
  for an Autonomous Vehicle Prototype","This paper discusses a vehicle prototype that recognizes streets' lanes and
plans its motion accordingly without any human input. Pi Camera 1.3 captures
real-time video, which is then processed by Raspberry-Pi 3.0 Model B. The image
processing algorithms are written in Python 3.7.4 with OpenCV 4.2. Arduino Uno
is utilized to control the PID algorithm that controls the motor controller,
which in turn controls the wheels. Algorithms that are used to detect the lanes
are the Canny edge detection algorithm and Hough transformation. Elementary
algebra is used to draw the detected lanes. After detection, the lanes are
tracked using the Kalman filter prediction method. Then the midpoint of the two
lanes is found, which is the initial steering direction. This initial steering
direction is further smoothed by using the Past Accumulation Average Method and
Kalman Filter Prediction Method. The prototype was tested in a controlled
environment in real-time. Results from comprehensive testing suggest that this
prototype can detect road lanes and plan its motion successfully.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1010.4934v1,Design-on-demand or how to create a target-oriented social web-site,"We describe an informal methodology for developing on-line applications,
which is, to some extent, complementary to the Web 2.0 aspects of web
development. The presented methodology is suitable for developing low-cost and
non-cost web sites targeted at medium-sized communities. We present basic
building blocks used in the described strategy. To achieve a better
understanding of the discussed concepts we comment on their application during
the realization of two web projects. We focus on the role of community-driven
development, which is crucial for projects of the discussed type.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2002.07672v2,"Structural Invariants for the Verification of Systems with Parameterized
  Architectures","We consider parameterized concurrent systems consisting of a finite but
unknown number of components, obtained by replicating a given set of finite
state automata. Components communicate by executing atomic interactions whose
participants update their states simultaneously. We introduce an interaction
logic to specify both the type of interactions (e.g.\ rendez-vous, broadcast)
and the topology of the system (e.g.\ pipeline, ring). The logic can be easily
embedded in monadic second order logic of finitely many successors, and is
therefore decidable.
  Proving safety properties of such a parameterized system, like deadlock
freedom or mutual exclusion, requires to infer an inductive invariant that
contains all reachable states of all system instances, and no unsafe state. We
present a method to automatically synthesize inductive invariants directly from
the formula describing the interactions, without costly fixed point iterations.
We experimentally prove that this invariant is strong enough to verify safety
properties of a large number of systems including textbook examples (dining
philosophers, synchronization schemes), classical mutual exclusion algorithms,
cache-coherence protocols and self-stabilization algorithms, for an arbitrary
number of components.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0610165v1,Decentralized Failure Diagnosis of Stochastic Discrete Event Systems,"Recently, the diagnosability of {\it stochastic discrete event systems}
(SDESs) was investigated in the literature, and, the failure diagnosis
considered was {\it centralized}. In this paper, we propose an approach to {\it
decentralized} failure diagnosis of SDESs, where the stochastic system uses
multiple local diagnosers to detect failures and each local diagnoser possesses
its own information. In a way, the centralized failure diagnosis of SDESs can
be viewed as a special case of the decentralized failure diagnosis presented in
this paper with only one projection. The main contributions are as follows: (1)
We formalize the notion of codiagnosability for stochastic automata, which
means that a failure can be detected by at least one local stochastic diagnoser
within a finite delay. (2) We construct a codiagnoser from a given stochastic
automaton with multiple projections, and the codiagnoser associated with the
local diagnosers is used to test codiagnosability condition of SDESs. (3) We
deal with a number of basic properties of the codiagnoser. In particular, a
necessary and sufficient condition for the codiagnosability of SDESs is
presented. (4) We give a computing method in detail to check whether
codiagnosability is violated. And (5) some examples are described to illustrate
the applications of the codiagnosability and its computing method.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.06735v2,"SpotTheFake: An Initial Report on a New CNN-Enhanced Platform for
  Counterfeit Goods Detection","The counterfeit goods trade represents nowadays more than 3.3% of the whole
world trade and thus it's a problem that needs now more than ever a lot of
attention and a reliable solution that would reduce the negative impact it has
over the modern society. This paper presents the design and early stage
development of a novel counterfeit goods detection platform that makes use of
the outstsanding learning capabilities of the classical VGG16 convolutional
model trained through the process of ""transfer learning"" and a multi-stage fake
detection procedure that proved to be not only reliable but also very robust in
the experiments we have conducted so far using an image dataset of various
goods which we gathered ourselves.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.11828v1,Speeding Up OPFython with Numba,"A graph-inspired classifier, known as Optimum-Path Forest (OPF), has proven
to be a state-of-the-art algorithm comparable to Logistic Regressors, Support
Vector Machines in a wide variety of tasks. Recently, its Python-based version,
denoted as OPFython, has been proposed to provide a more friendly framework and
a faster prototyping environment. Nevertheless, Python-based algorithms are
slower than their counterpart C-based algorithms, impacting their performance
when confronted with large amounts of data. Therefore, this paper proposed a
simple yet highly efficient speed up using the Numba package, which accelerates
Numpy-based calculations and attempts to increase the algorithm's overall
performance. Experimental results showed that the proposed approach achieved
better results than the na\""ive Python-based OPF and speeded up its distance
measurement calculation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1602.07667v3,Game-Theoretic Semantics for Alternating-Time Temporal Logic,"We introduce versions of game-theoretic semantics (GTS) for Alternating-Time
Temporal Logic (ATL). In GTS, truth is defined in terms of existence of a
winning strategy in a semantic evaluation game, and thus the game-theoretic
perspective appears in the framework of ATL on two semantic levels: on the
object level in the standard semantics of the strategic operators, and on the
meta-level where game-theoretic logical semantics is applied to ATL. We unify
these two perspectives into semantic evaluation games specially designed for
ATL. The game-theoretic perspective enables us to identify new variants of the
semantics of ATL based on limiting the time resources available to the verifier
and falsifier in the semantic evaluation game. We introduce and analyse an
unbounded and (ordinal) bounded GTS and prove these to be equivalent to the
standard (Tarski-style) compositional semantics. We show that in these both
versions of GTS, truth of ATL formulae can always be determined in finite time,
i.e., without constructing infinite paths. We also introduce a non-equivalent
finitely bounded semantics and argue that it is natural from both logical and
game-theoretic perspectives.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1204.5853v3,Simultaneous Embedding of Planar Graphs,"Simultaneous embedding is concerned with simultaneously representing a series
of graphs sharing some or all vertices. This forms the basis for the
visualization of dynamic graphs and thus is an important field of research.
Recently there has been a great deal of work investigating simultaneous
embedding problems both from a theoretical and a practical point of view. We
survey recent work on this topic.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0307033v1,Excellence in Computer Simulation,"Excellent computer simulations are done for a purpose. The most valid
purposes are to explore uncharted territory, to resolve a well-posed scientific
or technical question, or to make a design choice. Stand-alone modeling can
serve the first purpose. The other two goals need a full integration of the
modeling effort into a scientific or engineering program.
  Some excellent work, much of it related to the Department of Energy
Laboratories, is reviewed. Some less happy stories are recounted.
  In the past, some of the most impressive work has involved complexity and
chaos. Prediction in a complex world requires a first principles understanding
based upon the intersection of theory, experiment and simulation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0
http://arxiv.org/abs/1006.5040v1,"The comparison of Wiktionary thesauri transformed into the
  machine-readable format","Wiktionary is a unique, peculiar, valuable and original resource for natural
language processing (NLP). The paper describes an open-source Wiktionary
parser: its architecture and requirements followed by a description of
Wiktionary features to be taken into account, some open problems of Wiktionary
and the parser. The current implementation of the parser extracts the
definitions, semantic relations, and translations from English and Russian
Wiktionaries. The paper's goal is to interest researchers (1) in using the
constructed machine-readable dictionary for different NLP tasks, (2) in
extending the software to parse 170 still unused Wiktionaries. The comparison
of a number and types of semantic relations, a number of definitions, and a
number of translations in the English Wiktionary and the Russian Wiktionary has
been carried out. It was found that the number of semantic relations in the
English Wiktionary is larger by 1.57 times than in Russian (157 and 100
thousands). But the Russian Wiktionary has more ""rich"" entries (with a big
number of semantic relations), e.g. the number of entries with three or more
semantic relations is larger by 1.63 times than in the English Wiktionary. Upon
comparison, it was found out the methodological shortcomings of the Wiktionary.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.00756v2,"Multigrid preconditioners for the mixed finite element dynamical core of
  the LFRic atmospheric model","Due to the wide separation of time scales in geophysical fluid dynamics,
semi-implicit time integrators are commonly used in operational atmospheric
forecast models. They guarantee the stable treatment of fast (acoustic and
gravity) waves, while not suffering from severe restrictions on the timestep
size. To propagate the state of the atmosphere forward in time, a non-linear
equation for the prognostic variables has to be solved at every timestep. Since
the nonlinearity is typically weak, this is done with a small number of Newton-
or Picard- iterations, which in turn require the efficient solution of a large
system on linear equations with $\mathcal{O}(10^6-10^9)$ unknowns. This linear
solve is often the computationally most costly part of the model. In this paper
an efficient linear solver for the LFRic next-generation model, currently
developed by the Met Office, is described. The model uses an advanced mimetic
finite element discretisation which makes the construction of efficient solvers
challenging compared to models using standard finite-difference and
finite-volume methods. The linear solver hinges on a bespoke multigrid
preconditioner of the Schur-complement system for the pressure correction. By
comparing to Krylov-subspace methods, the superior performance and robustness
of the multigrid algorithm is demonstrated for standard test cases and
realistic model setups. In production mode, the model will have to run in
parallel on 100,000s of processing elements. As confirmed by numerical
experiments, one particular advantage of the multigrid solver is its excellent
parallel scalability due to avoiding expensive global reduction operations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.10529v1,"A Lightweight Concept Drift Detection and Adaptation Framework for IoT
  Data Streams","In recent years, with the increasing popularity of ""Smart Technology"", the
number of Internet of Things (IoT) devices and systems have surged
significantly. Various IoT services and functionalities are based on the
analytics of IoT streaming data. However, IoT data analytics faces concept
drift challenges due to the dynamic nature of IoT systems and the ever-changing
patterns of IoT data streams. In this article, we propose an adaptive IoT
streaming data analytics framework for anomaly detection use cases based on
optimized LightGBM and concept drift adaptation. A novel drift adaptation
method named Optimized Adaptive and Sliding Windowing (OASW) is proposed to
adapt to the pattern changes of online IoT data streams. Experiments on two
public datasets show the high accuracy and efficiency of our proposed adaptive
LightGBM model compared against other state-of-the-art approaches. The proposed
adaptive LightGBM model can perform continuous learning and drift adaptation on
IoT data streams without human intervention.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.05561v1,From Multiview Image Curves to 3D Drawings,"Reconstructing 3D scenes from multiple views has made impressive strides in
recent years, chiefly by correlating isolated feature points, intensity
patterns, or curvilinear structures. In the general setting - without
controlled acquisition, abundant texture, curves and surfaces following
specific models or limiting scene complexity - most methods produce unorganized
point clouds, meshes, or voxel representations, with some exceptions producing
unorganized clouds of 3D curve fragments. Ideally, many applications require
structured representations of curves, surfaces and their spatial relationships.
This paper presents a step in this direction by formulating an approach that
combines 2D image curves into a collection of 3D curves, with topological
connectivity between them represented as a 3D graph. This results in a 3D
drawing, which is complementary to surface representations in the same sense as
a 3D scaffold complements a tent taut over it. We evaluate our results against
truth on synthetic and real datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1506.07849v2,"Gradient-based Constrained Optimization Using a Database of Linear
  Reduced-Order Models","A methodology grounded in model reduction is presented for accelerating the
gradient-based solution of a family of linear or nonlinear constrained
optimization problems where the constraints include at least one linear Partial
Differential Equation (PDE). A key component of this methodology is the
construction, during an offline phase, of a database of pointwise, linear,
Projection-based Reduced-Order Models (PROM)s associated with a design
parameter space and the linear PDE(s). A parameter sampling procedure based on
an appropriate saturation assumption is proposed to maximize the efficiency of
such a database of PROMs. A real-time method is also presented for
interpolating at any queried but unsampled parameter vector in the design
parameter space the relevant sensitivities of a PROM. The practical
feasibility, computational advantages, and performance of the proposed
methodology are demonstrated for several realistic, nonlinear, aerodynamic
shape optimization problems governed by linear aeroelastic constraints.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1503.02859v2,"Dimension of the Lisbon voting rules in the EU Council: a challenge and
  new world record","The new voting system of the Council of the European Union cannot be
represented as the intersection of six or fewer weighted games, i.e., its
dimension is at least 7. This sets a new record for real-world voting bodies. A
heuristic combination of different discrete optimization methods yields a
representation as the intersection of 13368 weighted games. Determination of
the exact dimension is posed as a challenge to the community. The system's
Boolean dimension is proven to be 3.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1202.2407v1,"Proceedings Fourth Workshop on Mathematically Structured Functional
  Programming","This volume contains the proceedings of the Fourth Workshop on Mathematically
Structured Functional Programming (MSFP 2012), taking place on 25 March, 2012
in Tallinn, Estonia, as a satellite event of the European Joint Conferences on
Theory and Practice of Software, ETAPS 2012.
  MSFP is devoted to the derivation of functionality from structure. It
highlights concepts from algebra, semantics and type theory as they are
increasingly reflected in programming practice, especially functional
programming. The workshop consists of two invited presentations and eight
contributed papers on a range of topics at that interface.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0503043v3,"Complexity Issues in Finding Succinct Solutions of PSPACE-Complete
  Problems","We study the problem of deciding whether some PSPACE-complete problems have
models of bounded size. Contrary to problems in NP, models of PSPACE-complete
problems may be exponentially large. However, such models may take polynomial
space in a succinct representation. For example, the models of a QBF are
explicitely represented by and-or trees (which are always of exponential size)
but can be succinctely represented by circuits (which can be polynomial or
exponential). We investigate the complexity of deciding the existence of such
succinct models when a bound on size is given.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1107.4684v1,Introducing Sourcements,"Sourcing processes are discussed at a high abstraction level. A dedicated
terminology is developed concerning general aspects of sourcing. The term
sourcement is coined to denote a building block for sourcing. No- tions of
allocation, functional architecture and allocational architecture, equilibrium,
and configuration are discussed. Limitations of the concept of outsourcing are
outlined. This theoretical work is meant to serve as a point of departure for
the subsequent development of a detailed theory of sourcing and sourcing
transformations, which can be a tool for dealing with practical applica- tions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1708.01135v1,Long range forces in a performance portable Molecular Dynamics framework,"Molecular Dynamics (MD) codes predict the fundamental properties of matter by
following the trajectories of a collection of interacting model particles. To
exploit diverse modern manycore hardware, efficient codes must use all
available parallelism. At the same time they need to be portable and easily
extendible by the domain specialist (physicist/chemist) without detailed
knowledge of this hardware. To address this challenge, we recently described a
new Domain Specific Language (DSL) for the development of performance portable
MD codes based on a ""Separation of Concerns"": a Python framework automatically
generates efficient parallel code for a range of target architectures.
  Electrostatic interactions between charged particles are important in many
physical systems and often dominate the runtime. Here we discuss the inclusion
of long-range interaction algorithms in our code generation framework. These
algorithms require global communications and careful consideration has to be
given to any impact on parallel scalability. We implemented an Ewald summation
algorithm for electrostatic forces, present scaling comparisons for different
system sizes and compare to the performance of existing codes. We also report
on further performance optimisations delivered with OpenMP shared memory
parallelism.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1504.08013v1,Differential Calculus on Cayley Graphs,"We conservatively extend classical elementary differential calculus to the
Cartesian closed category of convergence spaces. By specializing results about
the convergence space representation of directed graphs, we use Cayley graphs
to obtain a differential calculus on groups, from which we then extract a
Boolean differential calculus, in which both linearity and the product rule,
also called the Leibniz identity, are satisfied.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1707.08753v1,Categories for Dynamic Epistemic Logic,"The primary goal of this paper is to recast the semantics of modal logic, and
dynamic epistemic logic (DEL) in particular, in category-theoretic terms. We
first review the category of relations and categories of Kripke frames, with
particular emphasis on the duality between relations and adjoint homomorphisms.
Using these categories, we then reformulate the semantics of DEL in a more
categorical and algebraic form. Several virtues of the new formulation will be
demonstrated: The DEL idea of updating a model into another is captured
naturally by the categorical perspective -- which emphasizes a family of
objects and structural relationships among them, as opposed to a single object
and structure on it. Also, the categorical semantics of DEL can be merged
straightforwardly with a standard categorical semantics for first-order logic,
providing a semantics for first-order DEL.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1605.08905v4,A Study of $k$-dipath Colourings of Oriented Graphs,"We examine $t$-colourings of oriented graphs in which, for a fixed integer $k
\geq 1$, vertices joined by a directed path of length at most $k$ must be
assigned different colours. A homomorphism model that extends the ideas of
Sherk for the case $k=2$ is described. Dichotomy theorems for the complexity of
the problem of deciding, for fixed $k$ and $t$, whether there exists such a
$t$-colouring are proved.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1507.00177v1,"An exponential lower bound for homogeneous depth-5 circuits over finite
  fields","In this paper, we show exponential lower bounds for the class of homogeneous
depth-$5$ circuits over all small finite fields. More formally, we show that
there is an explicit family $\{P_d : d \in \mathbb{N}\}$ of polynomials in
$\mathsf{VNP}$, where $P_d$ is of degree $d$ in $n = d^{O(1)}$ variables, such
that over all finite fields $\mathbb{F}_q$, any homogeneous depth-$5$ circuit
which computes $P_d$ must have size at least $\exp(\Omega_q(\sqrt{d}))$.
  To the best of our knowledge, this is the first super-polynomial lower bound
for this class for any field $\mathbb{F}_q \neq \mathbb{F}_2$.
  Our proof builds up on the ideas developed on the way to proving lower bounds
for homogeneous depth-$4$ circuits [GKKS13, FLMS13, KLSS14, KS14] and for
non-homogeneous depth-$3$ circuits over finite fields [GK98, GR00]. Our key
insight is to look at the space of shifted partial derivatives of a polynomial
as a space of functions from $\mathbb{F}_q^n \rightarrow \mathbb{F}_q$ as
opposed to looking at them as a space of formal polynomials and builds over a
tighter analysis of the lower bound of Kumar and Saraf [KS14].",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0302028v1,"The Boolean Functions Computed by Random Boolean Formulas OR How to Grow
  the Right Function","Among their many uses, growth processes (probabilistic amplification), were
used for constructing reliable networks from unreliable components, and
deriving complexity bounds of various classes of functions. Hence, determining
the initial conditions for such processes is an important and challenging
problem. In this paper we characterize growth processes by their initial
conditions and derive conditions under which results such as Valiant's (1984)
hold. First, we completely characterize growth processes that use linear
connectives. Second, by extending Savick\'y's (1990) analysis, via
``Restriction Lemmas'', we characterize growth processes that use monotone
connectives, and show that our technique is applicable to growth processes that
use other connectives as well. Additionally, we obtain explicit bounds on the
convergence rates of several growth processes, including the growth process
studied by Savick\'y (1990).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.01309v1,Nesting negations in FO2 over infinite words,"We consider two-variable first-order logic FO2 over infinite words.
Restricting the number of nested negations defines an infinite hierarchy; its
levels are often called the half-levels of the FO2 quantifier alternation
hierarchy. For every level of this hierarchy, we give an effective
characterization. For the lower levels, this characterization is a combination
of an algebraic and a topological property. For the higher levels, algebraic
properties turn out to be sufficient. Within two-variable first-order logic,
each algebraic property is a single ordered identity of omega-terms. The
topological properties are the same as for the lower half-levels of the
quantifier alternation hierarchy without the two-variable restriction (i.e.,
the Cantor topology and the alphabetic topology).
  Our result generalizes the corresponding result for finite words. The proof
uses novel techniques and is based on a refinement of Mal'cev products for
ordered monoids.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.11241v2,"Artificial intelligence for detection and quantification of rust and
  leaf miner in coffee crop","Pest and disease control plays a key role in agriculture since the damage
caused by these agents are responsible for a huge economic loss every year.
Based on this assumption, we create an algorithm capable of detecting rust
(Hemileia vastatrix) and leaf miner (Leucoptera coffeella) in coffee leaves
(Coffea arabica) and quantify disease severity using a mobile application as a
high-level interface for the model inferences. We used different convolutional
neural network architectures to create the object detector, besides the OpenCV
library, k-means, and three treatments: the RGB and value to quantification,
and the AFSoft software, in addition to the analysis of variance, where we
compare the three methods. The results show an average precision of 81,5% in
the detection and that there was no significant statistical difference between
treatments to quantify the severity of coffee leaves, proposing a
computationally less costly method. The application, together with the trained
model, can detect the pest and disease over different image conditions and
infection stages and also estimate the disease infection stage.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2104.02564v1,"H√∂lder Gradient Descent and Adaptive Regularization Methods in Banach
  Spaces for First-Order Points","This paper considers optimization of smooth nonconvex functionals in smooth
infinite dimensional spaces. A H\""older gradient descent algorithm is first
proposed for finding approximate first-order points of regularized polynomial
functionals. This method is then applied to analyze the evaluation complexity
of an adaptive regularization method which searches for approximate first-order
points of functionals with $\beta$-H\""older continuous derivatives. It is shown
that finding an $\epsilon$-approximate first-order point requires at most
$O(\epsilon^{-\frac{p+\beta}{p+\beta-1}})$ evaluations of the functional and
its first $p$ derivatives.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.01078v1,"On the Hardness of Almost All Subset Sum Problems by Ordinary
  Branch-and-Bound","Given $n$ positive integers $a_1,a_2,\dots,a_n$, and a positive integer right
hand side $\beta$, we consider the feasibility version of the subset sum
problem which is the problem of determining whether a subset of
$a_1,a_2,\dots,a_n$ adds up to $\beta$. We show that if the right hand side
$\beta$ is chosen as $\lfloor r\sum_{j=1}^n a_j \rfloor$ for a constant $0 < r
< 1$ and if the $a_j$'s are independentand identically distributed from a
discrete uniform distribution taking values ${1,2,\dots,\lfloor 10^{n/2}
\rfloor }$, then the probability that the instance of the subset sum problem
generated requires the creation of an exponential number of branch-and-bound
nodes when one branches on the individual variables in any order goes to $1$ as
$n$ goes to infinity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1112.4993v1,"Online Proceedings of the 11th International Colloquium on
  Implementation of Constraint LOgic Programming Systems (CICLOPS 2011),
  Lexington, KY, U.S.A., July 10, 2011","These are the revised versions of the papers presented at CICLOPS 2011, a
workshop colocated with ICLP 2011.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0003076v2,Constraint Programming viewed as Rule-based Programming,"We study here a natural situation when constraint programming can be entirely
reduced to rule-based programming. To this end we explain first how one can
compute on constraint satisfaction problems using rules represented by simple
first-order formulas. Then we consider constraint satisfaction problems that
are based on predefined, explicitly given constraints. To solve them we first
derive rules from these explicitly given constraints and limit the computation
process to a repeated application of these rules, combined with labeling.We
consider here two types of rules. The first type, that we call equality rules,
leads to a new notion of local consistency, called {\em rule consistency} that
turns out to be weaker than arc consistency for constraints of arbitrary arity
(called hyper-arc consistency in \cite{MS98b}). For Boolean constraints rule
consistency coincides with the closure under the well-known propagation rules
for Boolean constraints. The second type of rules, that we call membership
rules, yields a rule-based characterization of arc consistency. To show
feasibility of this rule-based approach to constraint programming we show how
both types of rules can be automatically generated, as {\tt CHR} rules of
\cite{fruhwirth-constraint-95}. This yields an implementation of this approach
to programming by means of constraint logic programming. We illustrate the
usefulness of this approach to constraint programming by discussing various
examples, including Boolean constraints, two typical examples of many valued
logics, constraints dealing with Waltz's language for describing polyhedral
scenes, and Allen's qualitative approach to temporal logic.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.02318v2,Nearly Tight Lower Bounds for Succinct Range Minimum Query,"Given an array of distinct integers $A[1\ldots n]$, the Range Minimum Query
(RMQ) problem requires us to construct a data structure from $A$, supporting
the RMQ query: given an interval $[a,b]\subseteq[1,n]$, return the index of the
minimum element in subarray $A[a\ldots b]$, i.e. return
$\text{argmin}_{i\in[a,b]}A[i]$. The fundamental problem has a long history.
The textbook solution which uses $O(n)$ words of space and $O(1)$ time by
Gabow, Bentley, Tarjan (STOC 1984) and Harel, Tarjan (SICOMP 1984) dates back
to 1980s. The state-of-the-art solution is presented by Fischer, Heun (SICOMP
2011) and Navarro, Sadakane (TALG 2014). The solution uses $2n-1.5\log
n+n/\left(\frac{\log n}{t}\right)^t+\tilde{O}(n^{3/4})$ bits of space and
$O(t)$ query time, where the additive $\tilde{O}(n^{3/4})$ is a pre-computed
lookup table used in the RAM model, assuming the word-size is $\Theta(\log n)$
bits. On the other hand, the only known lower bound is proved by Liu and Yu
(STOC 2020). They show that any data structure which solves RMQ in $t$ query
time must use $2n-1.5\log n+n/(\log n)^{O(t^2\log^2t)}$ bits of space, assuming
the word-size is $\Theta(\log n)$ bits.
  In this paper, we prove nearly tight lower bound for this problem. We show
that, for any data structure which solves RMQ in $t$ query time, $2n-1.5\log
n+n/(\log n)^{O(t\log^2t)}$ bits of space is necessary in the cell-probe model
with word-size $\Theta(\log n)$ bits. We emphasize that, in terms of time
complexity, our lower bound is tight up to a polylogarithmic factor.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0908.2588v1,Wild Card Queries for Searching Resources on the Web,"We propose a domain-independent framework for searching and retrieving facts
and relationships within natural language text sources. In this framework, an
extraction task over a text collection is expressed as a query that combines
text fragments with wild cards, and the query result is a set of facts in the
form of unary, binary and general $n$-ary tuples. A significance of our
querying mechanism is that, despite being both simple and declarative, it can
be applied to a wide range of extraction tasks. A problem in querying natural
language text though is that a user-specified query may not retrieve enough
exact matches. Unlike term queries which can be relaxed by removing some of the
terms (as is done in search engines), removing terms from a wild card query
without ruining its meaning is more challenging. Also, any query expansion has
the potential to introduce false positives. In this paper, we address the
problem of query expansion, and also analyze a few ranking alternatives to
score the results and to remove false positives. We conduct experiments and
report an evaluation of the effectiveness of our querying and scoring
functions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1205.0852v3,"On the Parameterized Complexity and Kernelization of the Workflow
  Satisfiability Problem","A workflow specification defines a set of steps and the order in which those
steps must be executed. Security requirements may impose constraints on which
groups of users are permitted to perform subsets of those steps. A workflow
specification is said to be satisfiable if there exists an assignment of users
to workflow steps that satisfies all the constraints. An algorithm for
determining whether such an assignment exists is important, both as a static
analysis tool for workflow specifications, and for the construction of run-time
reference monitors for workflow management systems. Finding such an assignment
is a hard problem in general, but work by Wang and Li in 2010 using the theory
of parameterized complexity suggests that efficient algorithms exist under
reasonable assumptions about workflow specifications. In this paper, we improve
the complexity bounds for the workflow satisfiability problem. We also
generalize and extend the types of constraints that may be defined in a
workflow specification and prove that the satisfiability problem remains
fixed-parameter tractable for such constraints. Finally, we consider
preprocessing for the problem and prove that in an important special case, in
polynomial time, we can reduce the given input into an equivalent one, where
the number of users is at most the number of steps. We also show that no such
reduction exists for two natural extensions of this case, which bounds the
number of users by a polynomial in the number of steps, provided a
widely-accepted complexity-theoretical assumption holds.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1006.2204v1,MDPs with Unawareness,"Markov decision processes (MDPs) are widely used for modeling decision-making
problems in robotics, automated control, and economics. Traditional MDPs assume
that the decision maker (DM) knows all states and actions. However, this may
not be true in many situations of interest. We define a new framework, MDPs
with unawareness (MDPUs) to deal with the possibilities that a DM may not be
aware of all possible actions. We provide a complete characterization of when a
DM can learn to play near-optimally in an MDPU, and give an algorithm that
learns to play near-optimally when it is possible to do so, as efficiently as
possible. In particular, we characterize when a near-optimal solution can be
found in polynomial time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.07445v1,Challenges in Detoxifying Language Models,"Large language models (LM) generate remarkably fluent text and can be
efficiently adapted across NLP tasks. Measuring and guaranteeing the quality of
generated text in terms of safety is imperative for deploying LMs in the real
world; to this end, prior work often relies on automatic evaluation of LM
toxicity. We critically discuss this approach, evaluate several toxicity
mitigation strategies with respect to both automatic and human evaluation, and
analyze consequences of toxicity mitigation in terms of model bias and LM
quality. We demonstrate that while basic intervention strategies can
effectively optimize previously established automatic metrics on the
RealToxicityPrompts dataset, this comes at the cost of reduced LM coverage for
both texts about, and dialects of, marginalized groups. Additionally, we find
that human raters often disagree with high automatic toxicity scores after
strong toxicity reduction interventions -- highlighting further the nuances
involved in careful evaluation of LM toxicity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1503.02675v2,Global 6DOF Pose Estimation from Untextured 2D City Models,"We propose a method for estimating the 3D pose for the camera of a mobile
device in outdoor conditions, using only an untextured 2D model. Previous
methods compute only a relative pose using a SLAM algorithm, or require many
registered images, which are cumbersome to acquire. By contrast, our method
returns an accurate, absolute camera pose in an absolute referential using
simple 2D+height maps, which are broadly available, to refine a first estimate
of the pose provided by the device's sensors. We show how to first estimate the
camera absolute orientation from straight line segments, and then how to
estimate the translation by aligning the 2D map with a semantic segmentation of
the input image. We demonstrate the robustness and accuracy of our approach on
a challenging dataset.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.10185v3,"About subordinated generalizations of 3 classical models of option
  pricing","In this paper, we investigate the relation between Bachelier and
Black-Scholes models driven by the infinitely divisible inverse subordinators.
Such models, in contrast to their classical equivalents, can be used in markets
where periods of stagnation are observed. We introduce the subordinated
Cox-Ross-Rubinstein model and prove that the price of the underlying in that
model converges in distribution to the price of underlying in the subordinated
Black-Scholes model defined in [26]. Motivated by this fact we price the
selected option contracts using the binomial trees. The results are compared to
other numerical methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.03835v1,"Attention-Based Model and Deep Reinforcement Learning for Distribution
  of Event Processing Tasks","Event processing is the cornerstone of the dynamic and responsive Internet of
Things (IoT). Recent approaches in this area are based on representational
state transfer (REST) principles, which allow event processing tasks to be
placed at any device that follows the same principles. However, the tasks
should be properly distributed among edge devices to ensure fair resources
utilization and guarantee seamless execution. This article investigates the use
of deep learning to fairly distribute the tasks. An attention-based neural
network model is proposed to generate efficient load balancing solutions under
different scenarios. The proposed model is based on the Transformer and Pointer
Network architectures, and is trained by an advantage actor-critic
reinforcement learning algorithm. The model is designed to scale to the number
of event processing tasks and the number of edge devices, with no need for
hyperparameters re-tuning or even retraining. Extensive experimental results
show that the proposed model outperforms conventional heuristics in many key
performance indicators. The generic design and the obtained results show that
the proposed model can potentially be applied to several other load balancing
problem variations, which makes the proposal an attractive option to be used in
real-world scenarios due to its scalability and efficiency.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.02099v1,"GeoGebra e situa√ß√µes que envolvem modela√ß√£o numa abordagem
  STEAM","In order to implement a STEAM approach including the use of technology,
namely the use of interactive mathematics software GeoGebra, in mathematics
classes, in the lusophone space, the materials presented here were conceived,
to be implemented in a first phase among teachers. Later, with the necessary
adaptations, these tasks will be applied to the students. The tasks deal with
modeling situations, in two- and three-dimensional geometric problems, in order
to apply GeoGebra software in its analysis to illustrate its capabilities. The
different windows of this software are used, namely the 2D and 3D windows, CAS
window, spreadsheet and extra two dimensional windows in order to study cutting
planes in solids and some surfaces. The tasks are presented so that any user,
regardless of the degree of knowledge they have of the software, can follow
them, being supported in scripts with some indications of the tools and
commands to use. Designed for the teaching and learning of Mathematics, from a
STEAM approach, these tasks allow connections with other Sciences and the Arts,
and allow the development of projects using and consolidating relevant
mathematical contents. These tasks are part of the proposals of activities of
the participants of the Training Courses for Trainers in GeoGebra for
Portuguese Speaking Countries, which from 2019 have an impact on the STEAM
approach. These courses are carried out with the high sponsorship of the
Organization of Ibero-American States for Education, Science and Culture (OEI).
Given the interest that the tasks have for the users of the Iberian space, as
well as their dissemination at a global level, the materials initially
developed in Portuguese language will be adapted for Spanish and English
speakers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/2112.07525v1,On Improving Resource Allocations by Sharing,"Given an initial resource allocation, where some agents may envy others or
where a different distribution of resources might lead to higher social
welfare, our goal is to improve the allocation without reassigning resources.
We consider a sharing concept allowing resources being shared with social
network neighbors of the resource owners. To this end, we introduce a formal
model that allows a central authority to compute an optimal sharing between
neighbors based on an initial allocation. Advocating this point of view, we
focus on the most basic scenario where a resource may be shared by two
neighbors in a social network and each agent can participate in a bounded
number of sharings. We present algorithms for optimizing utilitarian and
egalitarian social welfare of allocations and for reducing the number of
envious agents. In particular, we examine the computational complexity with
respect to several natural parameters. Furthermore, we study cases with
restricted social network structures and, among others, devise polynomial-time
algorithms in path- and tree-like (hierarchical) social networks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.6885v2,On the design of an expert help system for computer algebra systems,"It is our intention here only to discuss the nature, complexity and tools
concerning the design of Smart Help, an expert help facility for aiding users
of Computer Algebra Systems. Although the expert help system presented here has
been particularly oriented to REDUCE (as a consequence of our former experience
with this system), we point out that the concept of Smart Help can be extended
to other Computer Algebra Systems. Technically, Smart Help is a Production
System on the top of a particular implementation of MANTRA, a hybrid knowledge
representation system, which has REDUCE integrated as an additional knowledge
representation module. Since the heuristic level of MANTRA has not yet been
implemented, being presently represented by the Lisp language itself, Smart
Help is coded in Lisp and resides in the same Lisp session of MANTRA. A
prototype of Smart Help is now running on a SUN work-station on an experimental
basis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1711.00275v1,Fast Dynamic Arrays,"We present a highly optimized implementation of tiered vectors, a data
structure for maintaining a sequence of $n$ elements supporting access in time
$O(1)$ and insertion and deletion in time $O(n^\epsilon)$ for $\epsilon > 0$
while using $o(n)$ extra space. We consider several different implementation
optimizations in C++ and compare their performance to that of vector and
multiset from the standard library on sequences with up to $10^8$ elements. Our
fastest implementation uses much less space than multiset while providing
speedups of $40\times$ for access operations compared to multiset and speedups
of $10.000\times$ compared to vector for insertion and deletion operations
while being competitive with both data structures for all other operations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1905.06946v2,To Warn or Not to Warn: Online Signaling in Audit Games,"Routine operational use of sensitive data is often governed by law and
regulation. For instance, in the medical domain, there are various statues at
the state and federal level that dictate who is permitted to work with
patients' records and under what conditions. To screen for potential privacy
breaches, logging systems are usually deployed to trigger alerts whenever
suspicious access is detected. However, such mechanisms are often inefficient
because 1) the vast majority of triggered alerts are false positives, 2) small
budgets make it unlikely that a real attack will be detected, and 3) attackers
can behave strategically, such that traditional auditing mechanisms cannot
easily catch them. To improve efficiency, information systems may invoke
signaling, so that whenever a suspicious access request occurs, the system can,
in real time, warn the user that the access may be audited. Then, at the close
of a finite period, a selected subset of suspicious accesses are audited. This
gives rise to an online problem in which one needs to determine 1) whether a
warning should be triggered and 2) the likelihood that the data request event
will be audited. In this paper, we formalize this auditing problem as a
Signaling Audit Game (SAG), in which we model the interactions between an
auditor and an attacker in the context of signaling and the usability cost is
represented as a factor of the auditor's payoff. We study the properties of its
Stackelberg equilibria and develop a scalable approach to compute its solution.
We show that a strategic presentation of warnings adds value in that SAGs
realize significantly higher utility for the auditor than systems without
signaling. We illustrate the value of the proposed auditing model and the
consistency of its advantages over existing baseline methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1806.08040v1,"An empirical study on the names of points of interest and their changes
  with geographic distance","While Points Of Interest (POIs), such as restaurants, hotels, and barber
shops, are part of urban areas irrespective of their specific locations, the
names of these POIs often reveal valuable information related to local culture,
landmarks, influential families, figures, events, and so on. Place names have
long been studied by geographers, e.g., to understand their origins and
relations to family names. However, there is a lack of large-scale empirical
studies that examine the localness of place names and their changes with
geographic distance. In addition to enhancing our understanding of the
coherence of geographic regions, such empirical studies are also significant
for geographic information retrieval where they can inform computational models
and improve the accuracy of place name disambiguation. In this work, we conduct
an empirical study based on 112,071 POIs in seven US metropolitan areas
extracted from an open Yelp dataset. We propose to adopt term frequency and
inverse document frequency in geographic contexts to identify local terms used
in POI names and to analyze their usages across different POI types. Our
results show an uneven usage of local terms across POI types, which is highly
consistent among different geographic regions. We also examine the decaying
effect of POI name similarity with the increase of distance among POIs. While
our analysis focuses on urban POI names, the presented methods can be
generalized to other place types as well, such as mountain peaks and streets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1507.01697v1,Making Digital Artifacts on the Web Verifiable and Reliable,"The current Web has no general mechanisms to make digital artifacts --- such
as datasets, code, texts, and images --- verifiable and permanent. For digital
artifacts that are supposed to be immutable, there is moreover no commonly
accepted method to enforce this immutability. These shortcomings have a serious
negative impact on the ability to reproduce the results of processes that rely
on Web resources, which in turn heavily impacts areas such as science where
reproducibility is important. To solve this problem, we propose trusty URIs
containing cryptographic hash values. We show how trusty URIs can be used for
the verification of digital artifacts, in a manner that is independent of the
serialization format in the case of structured data files such as
nanopublications. We demonstrate how the contents of these files become
immutable, including dependencies to external digital artifacts and thereby
extending the range of verifiability to the entire reference tree. Our approach
sticks to the core principles of the Web, namely openness and decentralized
architecture, and is fully compatible with existing standards and protocols.
Evaluation of our reference implementations shows that these design goals are
indeed accomplished by our approach, and that it remains practical even for
very large files.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.01413v2,Out-of-Distribution Detection for Automotive Perception,"Neural networks (NNs) are widely used for object classification in autonomous
driving. However, NNs can fail on input data not well represented by the
training dataset, known as out-of-distribution (OOD) data. A mechanism to
detect OOD samples is important for safety-critical applications, such as
automotive perception, to trigger a safe fallback mode. NNs often rely on
softmax normalization for confidence estimation, which can lead to high
confidences being assigned to OOD samples, thus hindering the detection of
failures. This paper presents a method for determining whether inputs are OOD,
which does not require OOD data during training and does not increase the
computational cost of inference. The latter property is especially important in
automotive applications with limited computational resources and real-time
constraints. Our proposed approach outperforms state-of-the-art methods on
real-world automotive datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.00864v1,"A fast method for evaluating Volume potentials in the Galerkin boundary
  element method","Three algorithm are proposed to evaluate volume potentials that arise in
boundary element methods for elliptic PDEs. The approach is to apply a modified
fast multipole method for a boundary concentrated volume mesh. If $h$ is the
meshwidth of the boundary, then the volume is discretized using nearly
$O(h^{-2})$ degrees of freedom, and the algorithm computes potentials in nearly
$O(h^{-2})$ complexity. Here nearly means that logarithmic terms of $h$ may
appear. Thus the complexity of volume potentials calculations is of the same
asymptotic order as boundary potentials. For sources and potentials with
sufficient regularity the parameters of the algorithm can be designed such that
the error of the approximated potential converges at any specified rate
$O(h^p)$. The accuracy and effectiveness of the proposed algorithms are
demonstrated for potentials of the Poisson equation in three dimensions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.06221v1,"An Efficient Method of Partitioning High Volumes of Multidimensional
  Data for Parallel Clustering Algorithms","An optimal data partitioning in parallel & distributed implementation of
clustering algorithms is a necessary computation as it ensures independent task
completion, fair distribution, less number of affected points and better &
faster merging. Though partitioning using Kd Tree is being conventionally used
in academia, it suffers from performance drenches and bias (non equal
distribution) as dimensionality of data increases and hence is not suitable for
practical use in industry where dimensionality can be of order of 100s to
1000s. To address these issues we propose two new partitioning techniques using
existing mathematical models & study their feasibility, performance (bias and
partitioning speed) & possible variants in choosing initial seeds. First method
uses an n dimensional hashed grid based approach which is based on mapping the
points in space to a set of cubes which hashes the points. Second method uses a
tree of voronoi planes where each plane corresponds to a partition. We found
that grid based approach was computationally impractical, while using a tree of
voronoi planes (using scalable K-Means++ initial seeds) drastically
outperformed the Kd-tree tree method as dimensionality increased.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1411.7753v1,On Low Discrepancy Samplings in Product Spaces of Motion Groups,"Deterministically generating near-uniform point samplings of the motion
groups like SO(3), SE(3) and their n-wise products SO(3)^n, SE(3)^n is
fundamental to numerous applications in computational and data sciences. The
natural measure of sampling quality is discrepancy. In this work, our main goal
is construct low discrepancy deterministic samplings in product spaces of the
motion groups. To this end, we develop a novel strategy (using a two-step
discrepancy construction) that leads to an almost exponential improvement in
size (from the trivial direct product). To the best of our knowledge, this is
the first nontrivial construction for SO(3)^n, SE(3)^n and the hypertorus T^n.
  We also construct new low discrepancy samplings of S^2 and SO(3). The central
component in our construction for SO(3) is an explicit construction of N points
in S^2 with discrepancy \tilde{\O}(1/\sqrt{N}) with respect to convex sets,
matching the bound achieved for the special case of spherical caps in
\cite{ABD_12}. We also generalize the discrepancy of Cartesian product sets
\cite{Chazelle04thediscrepancy} to the discrepancy of local Cartesian product
sets.
  The tools we develop should be useful in generating low discrepancy samplings
of other complicated geometric spaces.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1602.02445v2,"A Logspace Solution to the Word and Conjugacy problem of Generalized
  Baumslag-Solitar Groups","Baumslag-Solitar groups were introduced in 1962 by Baumslag and Solitar as
examples for finitely presented non-Hopfian two-generator groups. Since then,
they served as examples for a wide range of purposes. As Baumslag-Solitar
groups are HNN extensions, there is a natural generalization in terms of graph
of groups.
  Concerning algorithmic aspects of generalized Baumslag-Solitar groups,
several decidability results are known. Indeed, a straightforward application
of standard algorithms leads to a polynomial time solution of the word problem
(the question whether some word over the generators represents the identity of
the group). The conjugacy problem (the question whether two given words
represent conjugate group elements) is more complicated; still decidability has
been established by Anshel and Stebe for ordinary Baumslag-Solitar groups and
for generalized Baumslag-Solitar groups independently by Lockhart and Beeker.
However, up to now no precise complexity estimates have been given.
  In this work, we give a LOGSPACE algorithm for both problems. More precisely,
we describe a uniform TC^0 many-one reduction of the word problem to the word
problem of the free group. Then we refine the known techniques for the
conjugacy problem and show that it can be solved in LOGSPACE. Moreover, for
ordinary Baumslag-Solitar groups also conjugacy is AC^0-Turing-reducible to the
word problem of the free group.
  Finally, we consider uniform versions (where also the graph of groups is part
of the input) of both word and conjugacy problem: while the word problem still
is solvable in LOGSPACE, the conjugacy problem becomes EXPSPACE-complete.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0806.4341v1,On Sequences with Non-Learnable Subsequences,"The remarkable results of Foster and Vohra was a starting point for a series
of papers which show that any sequence of outcomes can be learned (with no
prior knowledge) using some universal randomized forecasting algorithm and
forecast-dependent checking rules. We show that for the class of all
computationally efficient outcome-forecast-based checking rules, this property
is violated. Moreover, we present a probabilistic algorithm generating with
probability close to one a sequence with a subsequence which simultaneously
miscalibrates all partially weakly computable randomized forecasting
algorithms. %subsequences non-learnable by each randomized algorithm.
  According to the Dawid's prequential framework we consider partial recursive
randomized algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.09701v1,"Visual design intuition: Predicting dynamic properties of beams from raw
  cross-section images","In this work we aim to mimic the human ability to acquire the intuition to
estimate the performance of a design from visual inspection and experience
alone. We study the ability of convolutional neural networks to predict static
and dynamic properties of cantilever beams directly from their raw
cross-section images. Using pixels as the only input, the resulting models
learn to predict beam properties such as volume maximum deflection and
eigenfrequencies with 4.54% and 1.43% Mean Average Percentage Error (MAPE)
respectively, compared to the Finite Element Analysis (FEA) approach. Training
these models doesn't require prior knowledge of theory or relevant geometric
properties, but rather relies solely on simulated or empirical data, thereby
making predictions based on ""experience"" as opposed to theoretical knowledge.
Since this approach is over 1000 times faster than FEA, it can be adopted to
create surrogate models that could speed up the preliminary optimization
studies where numerous consecutive evaluations of similar geometries are
required. We suggest that this modeling approach would aid in addressing
challenging optimization problems involving complex structures and physical
phenomena for which theoretical models are unavailable.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.03458v1,Deep Neural Network Training without Multiplications,"Is multiplication really necessary for deep neural networks? Here we propose
just adding two IEEE754 floating-point numbers with an integer-add instruction
in place of a floating-point multiplication instruction. We show that ResNet
can be trained using this operation with competitive classification accuracy.
Our proposal did not require any methods to solve instability and decrease in
accuracy, which is common in low-precision training. In some settings, we may
obtain equal accuracy to the baseline FP32 result. This method will enable
eliminating the multiplications in deep neural-network training and inference.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0204022v1,"Annotation Graphs and Servers and Multi-Modal Resources: Infrastructure
  for Interdisciplinary Education, Research and Development","Annotation graphs and annotation servers offer infrastructure to support the
analysis of human language resources in the form of time-series data such as
text, audio and video. This paper outlines areas of common need among empirical
linguists and computational linguists. After reviewing examples of data and
tools used or under development for each of several areas, it proposes a common
framework for future tool development, data annotation and resource sharing
based upon annotation graphs and servers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1011.3096v1,A Trust Model Based on Service Classification in Mobile Services,"Internet of Things (IoT) and B3G/4G communication are promoting the pervasive
mobile services with its advanced features. However, security problems are also
baffled the development. This paper proposes a trust model to protect the
user's security. The billing or trust operator works as an agent to provide a
trust authentication for all the service providers. The services are classified
by sensitive value calculation. With the value, the user's trustiness for
corresponding service can be obtained. For decision, three trust regions are
divided, which is referred to three ranks: high, medium and low. The trust
region tells the customer, with his calculated trust value, which rank he has
got and which authentication methods should be used for access. Authentication
history and penalty are also involved with reasons.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/cs/0205005v5,"PSPACE-Completeness of Sliding-Block Puzzles and Other Problems through
  the Nondeterministic Constraint Logic Model of Computation","We present a nondeterministic model of computation based on reversing edge
directions in weighted directed graphs with minimum in-flow constraints on
vertices. Deciding whether this simple graph model can be manipulated in order
to reverse the direction of a particular edge is shown to be PSPACE-complete by
a reduction from Quantified Boolean Formulas. We prove this result in a variety
of special cases including planar graphs and highly restricted vertex
configurations, some of which correspond to a kind of passive constraint logic.
Our framework is inspired by (and indeed a generalization of) the ``Generalized
Rush Hour Logic'' developed by Flake and Baum.
  We illustrate the importance of our model of computation by giving simple
reductions to show that several motion-planning problems are PSPACE-hard. Our
main result along these lines is that classic unrestricted sliding-block
puzzles are PSPACE-hard, even if the pieces are restricted to be all dominoes
(1x2 blocks) and the goal is simply to move a particular piece. No prior
complexity results were known about these puzzles. This result can be seen as a
strengthening of the existing result that the restricted Rush Hour puzzles are
PSPACE-complete, of which we also give a simpler proof. Finally, we strengthen
the existing result that the pushing-blocks puzzle Sokoban is PSPACE-complete,
by showing that it is PSPACE-complete even if no barriers are allowed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.06974v1,"Algorithmic Auditing and Social Justice: Lessons from the History of
  Audit Studies","Algorithmic audits have been embraced as tools to investigate the functioning
and consequences of sociotechnical systems. Though the term is used somewhat
loosely in the algorithmic context and encompasses a variety of methods, it
maintains a close connection to audit studies in the social sciences--which
have, for decades, used experimental methods to measure the prevalence of
discrimination across domains like housing and employment. In the social
sciences, audit studies originated in a strong tradition of social justice and
participatory action, often involving collaboration between researchers and
communities; but scholars have argued that, over time, social science audits
have become somewhat distanced from these original goals and priorities. We
draw from this history in order to highlight difficult tensions that have
shaped the development of social science audits, and to assess their
implications in the context of algorithmic auditing. In doing so, we put forth
considerations to assist in the development of robust and engaged assessments
of sociotechnical systems that draw from auditing's roots in racial equity and
social justice.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2005.08211v1,"Quantifying the Impact on Software Complexity of Composable Inductive
  Programming using Zoea","Composable inductive programming as implemented in the Zoea programming
language is a simple declarative approach to software development. At the
language level it is evident that Zoea is significantly simpler than all
mainstream languages. However, until now we have only had anecdotal evidence
that software produced with Zoea is also simpler than equivalent software
produced with conventional languages. This paper presents the results of a
quantitative comparison of the software complexity of equivalent code
implemented in Zoea and also in a conventional programming language. The study
uses a varied set of programming tasks from a popular programming language
chrestomathy. Results are presented for relative program complexity using two
established metrics and also for relative program size. It was found that Zoea
programs are approximately 50% the complexity of equivalent programs in a
conventional language and on average equal in size. The results suggest that
current programming languages (as opposed to software requirements) are the
largest contributor to software complexity and that significant complexity
could be avoided through an inductive programming approach.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0306087v1,"OO Model of the STAR offline production ""Event Display"" and its
  implementation based on Qt-ROOT","The paper presents the ""Event Display"" package for the STAR offline
production as a special visualization tool to debug the reconstruction code.
This can be achieved if an author of the algorithm / code may build his/her own
custom Event Display alone from the base software blocks and re-used some
well-designed, easy to learn user-friendly patterns. For STAR offline
production Event Display ROOT with Qt lower level interface was chosen as the
base tools.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1709.03450v1,"UI-Net: Interactive Artificial Neural Networks for Iterative Image
  Segmentation Based on a User Model","For complex segmentation tasks, fully automatic systems are inherently
limited in their achievable accuracy for extracting relevant objects.
Especially in cases where only few data sets need to be processed for a highly
accurate result, semi-automatic segmentation techniques exhibit a clear benefit
for the user. One area of application is medical image processing during an
intervention for a single patient. We propose a learning-based cooperative
segmentation approach which includes the computing entity as well as the user
into the task. Our system builds upon a state-of-the-art fully convolutional
artificial neural network (FCN) as well as an active user model for training.
During the segmentation process, a user of the trained system can iteratively
add additional hints in form of pictorial scribbles as seed points into the FCN
system to achieve an interactive and precise segmentation result. The
segmentation quality of interactive FCNs is evaluated. Iterative FCN approaches
can yield superior results compared to networks without the user input channel
component, due to a consistent improvement in segmentation quality after each
interaction.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.08081v1,"Control Flow Versus Data Flow in Distributed Systems Integration:
  Revival of Flow-Based Programming for the Industrial Internet of Things","When we consider the application layer of networked infrastructures, data and
control flow are important concerns in distributed systems integration.
Modularity is a fundamental principle in software design, in particular for
distributed system architectures. Modularity emphasizes high cohesion of
individual modules and low coupling between modules. Microservices are a recent
modularization approach with the specific requirements of independent
deployability and, in particular, decentralized data management. Cohesiveness
of microservices goes hand-in-hand with loose coupling, making the development,
deployment, and evolution of microservice architectures flexible and scalable.
However, in our experience with microservice architectures, interactions and
flows among microservices are usually more complex than in traditional,
monolithic enterprise systems, since services tend to be smaller and only have
one responsibility, causing collaboration needs. We suggest that for loose
coupling among microservices, explicit control-flow modeling and execution with
central workflow engines should be avoided on the application integration
level. On the level of integrating microservices, data-flow modeling should be
dominant. Control-flow should be secondary and preferably delegated to the
microservices. We discuss coupling in distributed systems integration and
reflect the history of business process modeling with respect to data and
control flow. To illustrate our recommendations, we present some results for
flow-based programming in our Industrial DevOps project Titan, where we employ
flow-based programming for the Industrial Internet of Things.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.09233v1,"Case Study: Predictive Fairness to Reduce Misdemeanor Recidivism Through
  Social Service Interventions","The criminal justice system is currently ill-equipped to improve outcomes of
individuals who cycle in and out of the system with a series of misdemeanor
offenses. Often due to constraints of caseload and poor record linkage, prior
interactions with an individual may not be considered when an individual comes
back into the system, let alone in a proactive manner through the application
of diversion programs. The Los Angeles City Attorney's Office recently created
a new Recidivism Reduction and Drug Diversion unit (R2D2) tasked with reducing
recidivism in this population. Here we describe a collaboration with this new
unit as a case study for the incorporation of predictive equity into machine
learning based decision making in a resource-constrained setting. The program
seeks to improve outcomes by developing individually-tailored social service
interventions (i.e., diversions, conditional plea agreements, stayed
sentencing, or other favorable case disposition based on appropriate social
service linkage rather than traditional sentencing methods) for individuals
likely to experience subsequent interactions with the criminal justice system,
a time and resource-intensive undertaking that necessitates an ability to focus
resources on individuals most likely to be involved in a future case. Seeking
to achieve both efficiency (through predictive accuracy) and equity (improving
outcomes in traditionally under-served communities and working to mitigate
existing disparities in criminal justice outcomes), we discuss the equity
outcomes we seek to achieve, describe the corresponding choice of a metric for
measuring predictive fairness in this context, and explore a set of options for
balancing equity and efficiency when building and selecting machine learning
models in an operational public policy setting.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0
http://arxiv.org/abs/0909.2891v1,Going Off-road: Transversal Complexity in Road Networks,"A geometric graph is a graph embedded in the plane with vertices at points
and edges drawn as curves (which are usually straight line segments) between
those points. The average transversal complexity of a geometric graph is the
number of edges of that graph that are crossed by random line or line segment.
In this paper, we study the average transversal complexity of road networks. By
viewing road networks as multiscale-dispersed graphs, we show that a random
line will cross the edges of such a graph O(sqrt(n)) times on average. In
addition, we provide by empirical evidence from experiments on the road
networks of the fifty states of United States and the District of Columbia that
this bound holds in practice and has a small constant factor. Combining this
result with data structuring techniques from computational geometry, allows us
to show that we can then do point location and ray-shooting navigational
queries with respect to road networks in O(sqrt(n) log n) expected time.
Finally, we provide empirical justification for this claim as well.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1205.0963v1,Source Unfoldings of Convex Polyhedra via Certain Closed Curves,"We extend the notion of a source unfolding of a convex polyhedron P to be
based on a closed polygonal curve Q in a particular class rather than based on
a point. The class requires that Q ""lives on a cone"" to both sides; it includes
simple, closed quasigeodesics. Cutting a particular subset of the cut locus of
Q (in P) leads to a non-overlapping unfolding of the polyhedron. This gives a
new general method to unfold the surface of any convex polyhedron to a simple,
planar polygon.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.11338v1,Mining GIS Data to Predict Urban Sprawl,"This paper addresses the interesting problem of processing and analyzing data
in geographic information systems (GIS) to achieve a clear perspective on urban
sprawl. The term urban sprawl refers to overgrowth and expansion of low-density
areas with issues such as car dependency and segregation between residential
versus commercial use. Sprawl has impacts on the environment and public health.
In our work, spatiotemporal features related to real GIS data on urban sprawl
such as population growth and demographics are mined to discover knowledge for
decision support. We adapt data mining algorithms, Apriori for association rule
mining and J4.8 for decision tree classification to geospatial analysis,
deploying the ArcGIS tool for mapping. Knowledge discovered by mining this
spatiotemporal data is used to implement a prototype spatial decision support
system (SDSS). This SDSS predicts whether urban sprawl is likely to occur.
Further, it estimates the values of pertinent variables to understand how the
variables impact each other. The SDSS can help decision-makers identify
problems and create solutions for avoiding future sprawl occurrence and
conducting urban planning where sprawl already occurs, thus aiding sustainable
development. This work falls in the broad realm of geospatial intelligence and
sets the stage for designing a large scale SDSS to process big data in complex
environments, which constitutes part of our future work.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1409.6510v1,Linearizable special cases of the QAP,"We consider special cases of the quadratic assignment problem (QAP) that are
linearizable in the sense of Bookhold. We provide combinatorial
characterizations of the linearizable instances of the weighted feedback arc
set QAP, and of the linearizable instances of the traveling salesman QAP. As a
by-product, this yields a new well-solvable special case of the weighted
feedback arc set problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.05343v2,Sharp Waiting-Time Bounds for Multiserver Jobs,"Multiserver jobs, which are jobs that occupy multiple servers simultaneously
during service, are prevalent in today's computing clusters. But little is
known about the delay performance of systems with multiserver jobs. We consider
queueing models for multiserver jobs in a scaling regime where the total number
of servers in the system becomes large and meanwhile both the system load and
the number of servers that a job needs scale with the total number of servers.
Prior work has derived upper bounds on the queueing probability in this scaling
regime. However, without proper lower bounds, the existing results cannot be
used to differentiate between policies. In this paper, we study the delay
performance by establishing sharp bounds on the mean waiting time of
multiserver jobs, where the waiting time of a job is the time spent in queueing
rather than in service. We first consider the commonly used
First-Come-First-Serve (FCFS) policy and characterize the exact order of its
mean waiting time. We then prove a lower bound on the mean waiting time of all
policies, and demonstrate that there is an order gap between this lower bound
and the mean waiting time under FCFS. We finally complement the lower bound
with an achievability result: we show that under a priority policy that we call
P-Priority, the mean waiting time achieves the order of the lower bound. This
achievability result implies the tightness of the lower bound, the asymptotic
optimality of P-Priority, and the strict suboptimality of FCFS.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1302.2757v1,Lock-free Concurrent Data Structures,"Concurrent data structures are the data sharing side of parallel programming.
Data structures give the means to the program to store data, but also provide
operations to the program to access and manipulate these data. These operations
are implemented through algorithms that have to be efficient. In the sequential
setting, data structures are crucially important for the performance of the
respective computation. In the parallel programming setting, their importance
becomes more crucial because of the increased use of data and resource sharing
for utilizing parallelism.
  The first and main goal of this chapter is to provide a sufficient background
and intuition to help the interested reader to navigate in the complex research
area of lock-free data structures. The second goal is to offer the programmer
familiarity to the subject that will allow her to use truly concurrent methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.13176v1,Action similarity judgment based on kinematic primitives,"Understanding which features humans rely on -- in visually recognizing action
similarity is a crucial step towards a clearer picture of human action
perception from a learning and developmental perspective. In the present work,
we investigate to which extent a computational model based on kinematics can
determine action similarity and how its performance relates to human similarity
judgments of the same actions. To this aim, twelve participants perform an
action similarity task, and their performances are compared to that of a
computational model solving the same task. The chosen model has its roots in
developmental robotics and performs action classification based on learned
kinematic primitives. The comparative experiment results show that both the
model and human participants can reliably identify whether two actions are the
same or not. However, the model produces more false hits and has a greater
selection bias than human participants. A possible reason for this is the
particular sensitivity of the model towards kinematic primitives of the
presented actions. In a second experiment, human participants' performance on
an action identification task indicated that they relied solely on kinematic
information rather than on action semantics. The results show that both the
model and human performance are highly accurate in an action similarity task
based on kinematic-level features, which can provide an essential basis for
classifying human actions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1612.00989v1,Online Page Migration on Ring Networks in Uniform Model,"This paper explores the problem of page migration in ring networks. A ring
network is a connected graph, in which each node is connected with exactly two
other nodes. In this problem, one of the nodes in a given network holds a page
of size D. This node is called the server and the page is a non-duplicable data
in the network. Requests are issued by nodes to access the page one after
another. Every time a new request is issued, the server must serve the request
and may migrate to another node before the next request arrives. A service
costs the distance between the server and the requesting node, and the
migration costs the distance of the migration multiplied by D. The problem is
to minimize the total costs of services and migrations. We study this problem
in uniform model, for which the page has a unit size, i.e. D=1. A
3.326-competitive algorithm improving the current best upper bound is designed.
We show that this ratio is tight for our algorithm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1806.00810v2,"A New Style of Proof for Mathematics Organized as a Network of Axiomatic
  Theories","A theory graph is a network of axiomatic theories connected with
meaning-preserving mappings called theory morphisms. Theory graphs are well
suited for organizing large bodies of mathematical knowledge. Traditional and
formal proofs do not adequately fulfill all the purposes that mathematical
proofs have, and they do not exploit the structure inherent in a theory graph.
We propose a new style of proof that fulfills the principal purposes of a
mathematical proof as well as capitalizes on the connections provided by the
theory morphisms in a theory graph. This new style of proof combines the
strengths of traditional proofs with the strengths of formal proofs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0601032v2,Efficient Open World Reasoning for Planning,"We consider the problem of reasoning and planning with incomplete knowledge
and deterministic actions. We introduce a knowledge representation scheme
called PSIPLAN that can effectively represent incompleteness of an agent's
knowledge while allowing for sound, complete and tractable entailment in
domains where the set of all objects is either unknown or infinite. We present
a procedure for state update resulting from taking an action in PSIPLAN that is
correct, complete and has only polynomial complexity. State update is performed
without considering the set of all possible worlds corresponding to the
knowledge state. As a result, planning with PSIPLAN is done without direct
manipulation of possible worlds. PSIPLAN representation underlies the PSIPOP
planning algorithm that handles quantified goals with or without exceptions
that no other domain independent planner has been shown to achieve. PSIPLAN has
been implemented in Common Lisp and used in an application on planning in a
collaborative interface.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.03602v2,"Case-based similar image retrieval for weakly annotated large
  histopathological images of malignant lymphoma using deep metric learning","In the present study, we propose a novel case-based similar image retrieval
(SIR) method for hematoxylin and eosin (H&E)-stained histopathological images
of malignant lymphoma. When a whole slide image (WSI) is used as an input
query, it is desirable to be able to retrieve similar cases by focusing on
image patches in pathologically important regions such as tumor cells. To
address this problem, we employ attention-based multiple instance learning,
which enables us to focus on tumor-specific regions when the similarity between
cases is computed. Moreover, we employ contrastive distance metric learning to
incorporate immunohistochemical (IHC) staining patterns as useful supervised
information for defining appropriate similarity between heterogeneous malignant
lymphoma cases. In the experiment with 249 malignant lymphoma patients, we
confirmed that the proposed method exhibited higher evaluation measures than
the baseline case-based SIR methods. Furthermore, the subjective evaluation by
pathologists revealed that our similarity measure using IHC staining patterns
is appropriate for representing the similarity of H&E-stained tissue images for
malignant lymphoma.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.11547v1,TLGAN: document Text Localization using Generative Adversarial Nets,"Text localization from the digital image is the first step for the optical
character recognition task. Conventional image processing based text
localization performs adequately for specific examples. Yet, a general text
localization are only archived by recent deep-learning based modalities. Here
we present document Text Localization Generative Adversarial Nets (TLGAN) which
are deep neural networks to perform the text localization from digital image.
TLGAN is an versatile and easy-train text localization model requiring a small
amount of data. Training only ten labeled receipt images from Robust Reading
Challenge on Scanned Receipts OCR and Information Extraction (SROIE), TLGAN
achieved 99.83% precision and 99.64% recall for SROIE test data. Our TLGAN is a
practical text localization solution requiring minimal effort for data labeling
and model training and producing a state-of-art performance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0211024v1,Narses: A Scalable Flow-Based Network Simulator,"Most popular, modern network simulators, such as ns, are targeted towards
simulating low-level protocol details. These existing simulators are not
intended for simulating large distributed applications with many hosts and many
concurrent connections over long periods of simulated time. We introduce a new
simulator, Narses, targeted towards large distributed applications. The goal of
Narses is to simulate and validate large applications efficiently using network
models of varying levels of detail. We introduce several simplifying
assumptions that allow our simulator to scale to the needs of large distributed
applications while maintaining a reasonable degree of accuracy. Initial results
show up to a 45 times speedup while consuming 28% of the memory used by ns.
Narses maintains a reasonable degree of accuracy -- within 8% on average.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0203011v1,"Capturing Knowledge of User Preferences: ontologies on recommender
  systems","Tools for filtering the World Wide Web exist, but they are hampered by the
difficulty of capturing user preferences in such a dynamic environment. We
explore the acquisition of user profiles by unobtrusive monitoring of browsing
behaviour and application of supervised machine-learning techniques coupled
with an ontological representation to extract user preferences. A multi-class
approach to paper classification is used, allowing the paper topic taxonomy to
be utilised during profile construction. The Quickstep recommender system is
presented and two empirical studies evaluate it in a real work setting,
measuring the effectiveness of using a hierarchical topic ontology compared
with an extendable flat list.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.07468v1,"Computationally-Efficient Climate Predictions using Multi-Fidelity
  Surrogate Modelling","Accurately modelling the Earth's climate has widespread applications ranging
from forecasting local weather to understanding global climate change.
Low-fidelity simulations of climate phenomena are readily available, but
high-fidelity simulations are expensive to obtain. We therefore investigate the
potential of Gaussian process-based multi-fidelity surrogate modelling as a way
to produce high-fidelity climate predictions at low cost. Specifically, our
model combines the predictions of a low-fidelity Global Climate Model (GCM) and
those of a high-fidelity Regional Climate Model (RCM) to produce high-fidelity
temperature predictions for a mountainous region on the coastline of Peru. We
are able to produce high-fidelity temperature predictions at significantly
lower computational cost compared to the high-fidelity model alone: our
predictions have an average error of $15.62^\circ\text{C}^2$ yet our approach
only evaluates the high-fidelity model on 6% of the region of interest.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.12065v1,On the Problem of $p_1^{-1}$ in Locality-Sensitive Hashing,"A Locality-Sensitive Hash (LSH) function is called
$(r,cr,p_1,p_2)$-sensitive, if two data-points with a distance less than $r$
collide with probability at least $p_1$ while data points with a distance
greater than $cr$ collide with probability at most $p_2$. These functions form
the basis of the successful Indyk-Motwani algorithm (STOC 1998) for nearest
neighbour problems. In particular one may build a $c$-approximate nearest
neighbour data structure with query time $\tilde O(n^\rho/p_1)$ where
$\rho=\frac{\log1/p_1}{\log1/p_2}\in(0,1)$. That is, sub-linear time, as long
as $p_1$ is not too small. This is significant since most high dimensional
nearest neighbour problems suffer from the curse of dimensionality, and can't
be solved exact, faster than a brute force linear-time scan of the database.
  Unfortunately, the best LSH functions tend to have very low collision
probabilities, $p_1$ and $p_2$. Including the best functions for Cosine and
Jaccard Similarity. This means that the $n^\rho/p_1$ query time of LSH is often
not sub-linear after all, even for approximate nearest neighbours!
  In this paper, we improve the general Indyk-Motwani algorithm to reduce the
query time of LSH to $\tilde O(n^\rho/p_1^{1-\rho})$ (and the space usage
correspondingly.) Since $n^\rho p_1^{\rho-1} < n \Leftrightarrow p_1 > n^{-1}$,
our algorithm always obtains sublinear query time, for any collision
probabilities at least $1/n$. For $p_1$ and $p_2$ small enough, our improvement
over all previous methods can be \emph{up to a factor $n$} in both query time
and space.
  The improvement comes from a simple change to the Indyk-Motwani algorithm,
which can easily be implemented in existing software packages.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0403027v2,An approach to membrane computing under inexactitude,"In this paper we introduce a fuzzy version of symport/antiport membrane
systems. Our fuzzy membrane systems handle possibly inexact copies of reactives
and their rules are endowed with threshold functions that determine whether a
rule can be applied or not to a given set of objects, depending of the degree
of accuracy of these objects to the reactives specified in the rule. We prove
that these fuzzy membrane systems generate exactly the recursively enumerable
finite-valued fuzzy sets of natural numbers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1006.1126v1,Body-and-cad Geometric Constraint Systems,"Motivated by constraint-based CAD software, we develop the foundation for the
rigidity theory of a very general model: the body-and-cad structure, composed
of rigid bodies in 3D constrained by pairwise coincidence, angular and distance
constraints. We identify 21 relevant geometric constraints and develop the
corresponding infinitesimal rigidity theory for these structures. The classical
body-and-bar rigidity model can be viewed as a body-and-cad structure that uses
only one constraint from this new class. As a consequence, we identify a new,
necessary, but not sufficient, counting condition for minimal rigidity of
body-and-cad structures: nested sparsity. This is a slight generalization of
the well-known sparsity condition of Maxwell.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1908.06091v1,"Development of Atlas, a flexible data structure framework","This document is one of the deliverable reports created for the ESCAPE
project. ESCAPE stands for Energy-efficient Scalable Algorithms for Weather
Prediction at Exascale. The project develops world-class, extreme-scale
computing capabilities for European operational numerical weather prediction
and future climate models. This is done by identifying Weather & Climate dwarfs
which are key patterns in terms of computation and communication (in the spirit
of the Berkeley dwarfs). These dwarfs are then optimised for different hardware
architectures (single and multi-node) and alternative algorithms are explored.
Performance portability is addressed through the use of domain specific
languages.
  In this deliverable report, we present Atlas, a new software library that is
currently being developed at the European Centre for Medium-Range Weather
Forecasts (ECMWF), with the scope of handling data structures required for NWP
applications in a flexible and massively parallel way. Atlas provides a
versatile framework for the future development of efficient NWP and climate
applications on emerging HPC architectures. The applications range from full
Earth system models, to specific tools required for post-processing weather
forecast products.
  Atlas provides data structures for building various numerical strategies to
solve equations on the sphere or limited area's on the sphere. These data
structures may contain a distribution of points (grid) and, possibly, a
composition of elements (mesh), required to implement the numerical operations
required. Atlas can also represent a given field within a specific spatial
projection. Atlas is capable of mapping fields between different grids as part
of pre- and post-processing stages or as part of coupling processes whose
respective fields are discretised on different grids or meshes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.03511v1,Ethics of Technology needs more Political Philosophy,"The ongoing debate on the ethics of self-driving cars typically focuses on
two approaches to answering ethical questions: moral philosophy and social
science. I argue that these two approaches are both lacking. We should neither
deduce answers from individual moral theories nor should we expect social
science to give us complete answers. To supplement these approaches, we should
turn to political philosophy. The issues we face are collective decisions that
we make together rather than individual decisions we make in light of what we
each have reason to value. Political philosophy adds three basic concerns to
our conceptual toolkit: reasonable pluralism, human agency, and legitimacy.
These three concerns have so far been largely overlooked in the debate on the
ethics of self-driving cars.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1606.02193v2,"Adapting Sampling Interval of Sensor Networks Using On-Line
  Reinforcement Learning","Monitoring Wireless Sensor Networks (WSNs) are composed of sensor nodes that
report temperature, relative humidity, and other environmental parameters. The
time between two successive measurements is a critical parameter to set during
the WSN configuration because it can impact the WSN's lifetime, the wireless
medium contention and the quality of the reported data. As trends in monitored
parameters can significantly vary between scenarios and within time,
identifying a sampling interval suitable for several cases is also challenging.
In this work, we propose a dynamic sampling rate adaptation scheme based on
reinforcement learning, able to tune sensors' sampling interval on-the-fly,
according to environmental conditions and application requirements. The primary
goal is to set the sampling interval to the best value possible so as to avoid
oversampling and save energy, while not missing environmental changes that can
be relevant for the application. In simulations, our mechanism could reduce up
to 73% the total number of transmissions compared to a fixed strategy and,
simultaneously, keep the average quality of information provided by the WSN.
The inherent flexibility of the reinforcement learning algorithm facilitates
its use in several scenarios, so as to exploit the broad scope of the Internet
of Things.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1407.0904v2,COFFEE: an Optimizing Compiler for Finite Element Local Assembly,"The numerical solution of partial differential equations using the finite
element method is one of the key applications of high performance computing.
Local assembly is its characteristic operation. This entails the execution of a
problem-specific kernel to numerically evaluate an integral for each element in
the discretized problem domain. Since the domain size can be huge, executing
efficient kernels is fundamental. Their op- timization is, however, a
challenging issue. Even though affine loop nests are generally present, the
short trip counts and the complexity of mathematical expressions make it hard
to determine a single or unique sequence of successful transformations.
Therefore, we present the design and systematic evaluation of COF- FEE, a
domain-specific compiler for local assembly kernels. COFFEE manipulates
abstract syntax trees generated from a high-level domain-specific language for
PDEs by introducing domain-aware composable optimizations aimed at improving
instruction-level parallelism, especially SIMD vectorization, and register
locality. It then generates C code including vector intrinsics. Experiments
using a range of finite-element forms of increasing complexity show that
significant performance improvement is achieved.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0701185v3,Graph Operations on Clique-Width Bounded Graphs,"Clique-width is a well-known graph parameter. Many NP-hard graph problems
admit polynomial-time solutions when restricted to graphs of bounded
clique-width. The same holds for NLC-width. In this paper we study the behavior
of clique-width and NLC-width under various graph operations and graph
transformations. We give upper and lower bounds for the clique-width and
NLC-width of the modified graphs in terms of the clique-width and NLC-width of
the involved graphs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.07283v5,Calibrating sufficiently,"When probabilistic classifiers are trained and calibrated, the so-called
grouping loss component of the calibration loss can easily be overlooked.
Grouping loss refers to the gap between observable information and information
actually exploited in the calibration exercise. We investigate the relation
between grouping loss and the concept of sufficiency, identifying
comonotonicity as a useful criterion for sufficiency. We revisit the probing
reduction approach of Langford & Zadrozny (2005) and find that it produces an
estimator of probabilistic classifiers that reduces grouping loss. Finally, we
discuss Brier curves as tools to support training and 'sufficient' calibration
of probabilistic classifiers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0701130v1,"On the Correlation of Geographic and Network Proximity at Internet Edges
  and its Implications for Mobile Unicast and Multicast Routing","Significant effort has been invested recently to accelerate handover
operations in a next generation mobile Internet. Corresponding works for
developing efficient mobile multicast management are emergent. Both problems
simultaneously expose routing complexity between subsequent points of
attachment as a characteristic parameter for handover performance in access
networks.
  As continuous mobility handovers necessarily occur between access routers
located in geographic vicinity, this paper investigates on the hypothesis that
geographically adjacent edge networks attain a reduced network distances as
compared to arbitrary Internet nodes. We therefore evaluate and analyze edge
distance distributions in various regions for clustered IP ranges on their
geographic location such as a city. We use traceroute to collect packet
forwarding path and round-trip-time of each intermediate node to scan-wise
derive an upper bound of the node distances. Results of different scanning
origins are compared to obtain the best estimation of network distance of each
pair. Our results are compared with corresponding analysis of CAIDA Skitter
data, overall leading to fairly stable, reproducible edge distance
distributions. As a first conclusion on expected impact on handover performance
measures, our results indicate a general optimum for handover anticipation time
in 802.11 networks of 25 ms.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1407.7592v2,On multiply-exponential write-once Turing machines,"In this work we analyze the multiply-exponential complexity classes for
write-once Turing machines, i.e. machines that can write to a given tape cell
at most once. We show that $k$-DExpWOSpace = $k$-DExpWOTime = $k$-ExpTime and
the nondeterministic counterpart. For alternating machines we show that
$k$-AExpWOTime = $k$-AExpTime = $k-1$-ExpSpace.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1905.08621v1,Shortest-Path-Preserving Rounding,"Various applications of graphs, in particular applications related to finding
shortest paths, naturally get inputs with real weights on the edges. However,
for algorithmic or visualization reasons, inputs with integer weights would
often be preferable or even required. This raises the following question: given
an undirected graph with non-negative real weights on the edges and an error
threshold $\varepsilon$, how efficiently can we decide whether we can round all
weights such that shortest paths are maintained, and the change of weight of
each shortest path is less than $\varepsilon$? So far, only for path-shaped
graphs a polynomial-time algorithm was known. In this paper we prove, by
reduction from 3-SAT, that, in general, the problem is NP-hard. However, if the
graph is a tree with $n$ vertices, the problem can be solved in $O(n^2)$ time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.00309v1,"Isomorphism problems for tensors, groups, and cubic forms: completeness
  and reductions","In this paper we consider the problems of testing isomorphism of tensors,
$p$-groups, cubic forms, algebras, and more, which arise from a variety of
areas, including machine learning, group theory, and cryptography. These
problems can all be cast as orbit problems on multi-way arrays under different
group actions. Our first two main results are:
  1. All the aforementioned isomorphism problems are equivalent under
polynomial-time reductions, in conjunction with the recent results of
Futorny-Grochow-Sergeichuk (Lin. Alg. Appl., 2019).
  2. Isomorphism of $d$-tensors reduces to isomorphism of 3-tensors, for any $d
\geq 3$.
  Our results suggest that these isomorphism problems form a rich and robust
equivalence class, which we call Tensor Isomorphism-complete, or TI-complete.
We then leverage the techniques used in the above results to prove two
first-of-their-kind results for Group Isomorphism (GpI):
  3. We give a reduction from GpI for $p$-groups of exponent $p$ and small
class ($c < p$) to GpI for $p$-groups of exponent $p$ and class 2. The latter
are widely believed to be the hardest cases of GpI, but as far as we know, this
is the first reduction from any more general class of groups to this class.
  4. We give a search-to-decision reduction for isomorphism of $p$-groups of
exponent $p$ and class 2 in time $|G|^{O(\log \log |G|)}$. While
search-to-decision reductions for Graph Isomorphism (GI) have been known for
more than 40 years, as far as we know this is the first non-trivial
search-to-decision reduction in the context of GpI.
  Our main technique for (1), (3), and (4) is a linear-algebraic analogue of
the classical graph coloring gadget, which was used to obtain the
search-to-decision reduction for GI. This gadget construction may be of
independent interest and utility. The technique for (2) gives a method for
encoding an arbitrary tensor into an algebra.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0105008v1,Applying Slicing Technique to Software Architectures,"Software architecture is receiving increasingly attention as a critical
design level for software systems. As software architecture design resources
(in the form of architectural specifications) are going to be accumulated, the
development of techniques and tools to support architectural understanding,
testing, reengineering, maintenance, and reuse will become an important issue.
This paper introduces a new form of slicing, named architectural slicing, to
aid architectural understanding and reuse. In contrast to traditional slicing,
architectural slicing is designed to operate on the architectural specification
of a software system, rather than the source code of a program. Architectural
slicing provides knowledge about the high-level structure of a software system,
rather than the low-level implementation details of a program. In order to
compute an architectural slice, we present the architecture information flow
graph which can be used to represent information flows in a software
architecture. Based on the graph, we give a two-phase algorithm to compute an
architectural slice.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1106.0814v1,"Proceedings of Second International Symposium on Games, Automata, Logics
  and Formal Verification","This volume contains the Proceedings of the Second International Symposium on
Games, Automata, Languages, and Formal Verification (GandALF 2011). The
conference was held in Minori (Amalfi Coast, Italy), from the 15th to the 17th
of June 2011. The aim of the GandALF Symposium is to provide a forum for
researchers from different areas and with different background, that share a
common interest in game theory, mathematical logic, automata theory, and their
applications to the specification, design, and verification of complex systems.
This proceedings contain the abstracts of three invited talks and nineteen
regular papers that have been selected through a rigorous reviewing process
according to originality, quality, and relevance to the topics of the
symposium.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.04985v1,Energy-Based Models for Code Generation under Compilability Constraints,"Neural language models can be successfully trained on source code, leading to
applications such as code completion. However, their versatile autoregressive
self-supervision objective overlooks important global sequence-level features
that are present in the data such as syntactic correctness or compilability. In
this work, we pose the problem of learning to generate compilable code as
constraint satisfaction. We define an Energy-Based Model (EBM) representing a
pre-trained generative model with an imposed constraint of generating only
compilable sequences. We then use the KL-Adaptive Distributional Policy
Gradient algorithm (Khalifa et al., 2021) to train a generative model
approximating the EBM. We conduct experiments showing that our proposed
approach is able to improve compilability rates without sacrificing diversity
and complexity of the generated samples.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1109.1517v1,Dynamic Maintenance of Half-Space Depth for Points and Contours,"Half-space depth (also called Tukey depth or location depth) is one of the
most commonly studied data depth measures because it possesses many desirable
properties for data depth functions. The data depth contours bound regions of
increasing depth. For the sample case, there are two competing definitions of
contours: the rank-based contours and the cover-based contours.
  In this paper, we present three dynamic algorithms for maintaining the
half-space depth of points and contours: The first maintains the half-space
depth of a single point in a data set in $O(\log n)$ time per update
(insertion/deletion) and overall linear space. By maintaining such a data
structure for each data point, we present an algorithm for dynamically
maintaining the rank-based contours in $O(n\cdot\log n)$ time per update and
overall quadratic space. The third dynamic algorithm maintains the cover-based
contours in $O(n\cdot \log^2 n)$ time per update and overall quadratic space.
  We also augment our first algorithm to maintain the local cover-based
contours at data points while maintaining the same complexities. A corollary of
this discussion is a strong structural result of independent interest
describing the behavior of dynamic cover-based contours near data points.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.10535v1,"Time-based Handover Skipping in Cellular Networks: Spatially Stochastic
  Modeling and Analysis","Handover (HO) management has attracted attention of research in the context
of wireless cellular communication networks. One crucial problem of HO
management is to deal with increasing HOs experienced by a mobile user. To
address this problem, HO skipping techniques have been studied in recent years.
In this paper, we propose a novel HO skipping scheme, namely, time-based HO
skipping. In the proposed scheme, HOs of a user are controlled by a certain
fixed period of time, which we call skipping time. The skipping time can be
managed as a system parameter, thereby enabling flexible operation of HO
skipping. We analyze the transmission performance of the proposed scheme on the
basis of a stochastic geometry approach. In the scenario where a user performs
the time-based HO skipping, we derive the analytical expressions for two
performance metrics: the HO rate and the expected data rate. The analysis
results demonstrate that the scenario with the time-based HO skipping
outperforms the scenario without HO skipping particularly when the user moves
fast. Furthermore, we reveal that there is a unique optimal skipping time
maximizing the transmission performance, which we obtain approximately.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1503.05951v1,Rank Subspace Learning for Compact Hash Codes,"The era of Big Data has spawned unprecedented interests in developing hashing
algorithms for efficient storage and fast nearest neighbor search. Most
existing work learn hash functions that are numeric quantizations of feature
values in projected feature space. In this work, we propose a novel hash
learning framework that encodes feature's rank orders instead of numeric values
in a number of optimal low-dimensional ranking subspaces. We formulate the
ranking subspace learning problem as the optimization of a piece-wise linear
convex-concave function and present two versions of our algorithm: one with
independent optimization of each hash bit and the other exploiting a sequential
learning framework. Our work is a generalization of the Winner-Take-All (WTA)
hash family and naturally enjoys all the numeric stability benefits of rank
correlation measures while being optimized to achieve high precision at very
short code length. We compare with several state-of-the-art hashing algorithms
in both supervised and unsupervised domain, showing superior performance in a
number of data sets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.05137v1,"Energy Predictive Models for Convolutional Neural Networks on Mobile
  Platforms","Energy use is a key concern when deploying deep learning models on mobile and
embedded platforms. Current studies develop energy predictive models based on
application-level features to provide researchers a way to estimate the energy
consumption of their deep learning models. This information is useful for
building resource-aware models that can make efficient use of the hard-ware
resources. However, previous works on predictive modelling provide little
insight into the trade-offs involved in the choice of features on the final
predictive model accuracy and model complexity. To address this issue, we
provide a comprehensive analysis of building regression-based predictive models
for deep learning on mobile devices, based on empirical measurements gathered
from the SyNERGY framework.Our predictive modelling strategy is based on two
types of predictive models used in the literature:individual layers and
layer-type. Our analysis of predictive models show that simple layer-type
features achieve a model complexity of 4 to 32 times less for convolutional
layer predictions for a similar accuracy compared to predictive models using
more complex features adopted by previous approaches. To obtain an overall
energy estimate of the inference phase, we build layer-type predictive models
for the fully-connected and pooling layers using 12 representative
Convolutional NeuralNetworks (ConvNets) on the Jetson TX1 and the Snapdragon
820using software backends such as OpenBLAS, Eigen and CuDNN. We obtain an
accuracy between 76% to 85% and a model complexity of 1 for the overall energy
prediction of the test ConvNets across different hardware-software
combinations.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1806.03687v1,Complex network representation through multi-dimensional node projection,"Complex network topology might get pretty complicated challenging many
network analysis objectives, such as community detection for example. This
however makes common emergent network phenomena such as scale-free topology or
small-world property even more intriguing. In the present proof-of-concept
paper we propose a simple model of network representation inspired by a signal
transmission physical analogy, which is apparently capable of reproducing both
of the above phenomena. The model appears to be general enough to represent
and/or approximate arbitrary complex networks. We propose an approach
constructing such a representation by projecting each node into a
multi-dimensional space of signal spectrum vectors, where network topology is
induced by their overlaps. As one of the implications this enables reducing
community detection in complex networks to a straightforward clustering over
the projection space, for which multiple efficient approaches are available. We
believe such a network representation could turn out to be a useful tool for
multiple network analysis objectives.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1510.08779v4,"Effect of Gromov-hyperbolicity Parameter on Cuts and Expansions in
  Graphs and Some Algorithmic Implications","$\delta$-hyperbolic graphs, originally conceived by Gromov in 1987, occur
often in many network applications; for fixed $\delta$, such graphs are simply
called hyperbolic graphs and include non-trivial interesting classes of
""non-expander"" graphs. The main motivation of this paper is to investigate the
effect of the hyperbolicity measure $\delta$ on expansion and cut-size bounds
on graphs (here $\delta$ need not be a constant), and the asymptotic ranges of
$\delta$ for which these results may provide improved approximation algorithms
for related combinatorial problems. To this effect, we provide constructive
bounds on node expansions for $\delta$-hyperbolic graphs as a function of
$\delta$, and show that many witnesses (subsets of nodes) for such expansions
can be computed efficiently even if the witnesses are required to be nested or
sufficiently distinct from each other. To the best of our knowledge, these are
the first such constructive bounds proven. We also show how to find a large
family of s-t cuts with relatively small number of cut-edges when s and t are
sufficiently far apart. We then provide algorithmic consequences of these
bounds and their related proof techniques for two problems for
$\delta$-hyperbolic graphs (where $\delta$ is a function $f$ of the number of
nodes, the exact nature of growth of $f$ being dependent on the particular
problem considered).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1602.02244v1,"Fast Multipole Method as a Matrix-Free Hierarchical Low-Rank
  Approximation","There has been a large increase in the amount of work on hierarchical
low-rank approximation methods, where the interest is shared by multiple
communities that previously did not intersect. This objective of this article
is two-fold; to provide a thorough review of the recent advancements in this
field from both analytical and algebraic perspectives, and to present a
comparative benchmark of two highly optimized implementations of contrasting
methods for some simple yet representative test cases. We categorize the recent
advances in this field from the perspective of compute-memory tradeoff, which
has not been considered in much detail in this area. Benchmark tests reveal
that there is a large difference in the memory consumption and performance
between the different methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1807.07957v1,"Decentralized Task Allocation in Multi-Robot Systems via Bipartite Graph
  Matching Augmented with Fuzzy Clustering","Robotic systems, working together as a team, are becoming valuable players in
different real-world applications, from disaster response to warehouse
fulfillment services. Centralized solutions for coordinating multi-robot teams
often suffer from poor scalability and vulnerability to communication
disruptions. This paper develops a decentralized multi-agent task allocation
(Dec-MATA) algorithm for multi-robot applications. The task planning problem is
posed as a maximum-weighted matching of a bipartite graph, the solution of
which using the blossom algorithm allows each robot to autonomously identify
the optimal sequence of tasks it should undertake. The graph weights are
determined based on a soft clustering process, which also plays a problem
decomposition role seeking to reduce the complexity of the individual-agents'
task assignment problems. To evaluate the new Dec-MATA algorithm, a series of
case studies (of varying complexity) are performed, with tasks being
distributed randomly over an observable 2D environment. A centralized approach,
based on a state-of-the-art MILP formulation of the multi-Traveling Salesman
problem is used for comparative analysis. While getting within 7-28% of the
optimal cost obtained by the centralized algorithm, the Dec-MATA algorithm is
found to be 1-3 orders of magnitude faster and minimally sensitive to
task-to-robot ratios, unlike the centralized algorithm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1307.1370v2,Matching Known Patients to Health Records in Washington State Data,"The State of Washington sells patient-level health data for $50. This
publicly available dataset has virtually all hospitalizations occurring in the
State in a given year, including patient demographics, diagnoses, procedures,
attending physician, hospital, a summary of charges, and how the bill was paid.
It does not contain patient names or addresses (only ZIPs). Newspaper stories
printed in the State for the same year that contain the word ""hospitalized""
often include a patient's name and residential information and explain why the
person was hospitalized, such as vehicle accident or assault. News information
uniquely and exactly matched medical records in the State database for 35 of
the 81 cases (or 43 percent) found in 2011, thereby putting names to patient
records. A news reporter verified matches by contacting patients. Employers,
financial organizations and others know the same kind of information as
reported in news stories making it just as easy for them to identify the
medical records of employees, debtors, and others.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,1,1,0,0
http://arxiv.org/abs/cs/9912018v1,Computation in an algebra of test selection criteria,"One of the key concepts in testing is that of adequate test sets. A test
selection criterion decides which test sets are adequate. In this paper, a
language schema for specifying a large class of test selection criteria is
developed; the schema is based on two operations for building complex criteria
from simple ones. Basic algebraic properties of the two operations are derived.
  In the second part of the paper, a simple language-an instance of the general
schema-is studied in detail, with the goal of generating small adequate test
sets automatically. It is shown that one version of the problem is intractable,
while another is solvable by an efficient algorithm. An implementation of the
algorithm is described.",0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1701.04777v5,The fast parallel algorithm for CNF SAT without algebra,"A novel parallel algorithm for solving the classical Decision Boolean
Satisfiability problem with clauses in conjunctive normal form is depicted. My
approach for solving SAT is without using algebra or other computational search
strategies such as branch and bound, back-forward, tree representation, etc.
The method is based on the special class of SAT problems, Simple SAT (SSAT).
The algorithm's design includes parallel execution, object oriented, and short
termination as my previous versions but it keep track of the tested
unsatisfactory binary values to improve the efficiency and to favor short
termination. The resulting algorithm is linear with respect to the number of
clauses plus a process data on the partial solutions of the subproblems SSAT of
an arbitrary SAT and it is bounded by $2^{n}$ iterations where $n$ is the
number of logical variables. The novelty for the solution of arbitrary SAT
problems is a linear algorithm, such its complexity is less or equal than the
algorithms of the state of the art for solving SAT.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1909.12755v4,On the Approximation Ratio of the $k$-Opt and Lin-Kernighan Algorithm,"The $k$-Opt and Lin-Kernighan algorithm are two of the most important local
search approaches for the Metric TSP. Both start with an arbitrary tour and
make local improvements in each step to get a shorter tour. We show that for
any fixed $k\geq 3$ the approximation ratio of the $k$-Opt algorithm for Metric
TSP is $O(\sqrt[k]{n})$. Assuming the Erd\H{o}s girth conjecture, we prove a
matching lower bound of $\Omega(\sqrt[k]{n})$. Unconditionally, we obtain
matching bounds for $k=3,4,6$ and a lower bound of
$\Omega(n^{\frac{2}{3k-3}})$. Our most general bounds depend on the values of a
function from extremal graph theory and are tight up to a factor logarithmic in
the number of vertices unconditionally. Moreover, all the upper bounds also
apply to a parameterized version of the Lin-Kernighan algorithm with
appropriate parameters. We also show that the approximation ratio of $k$-Opt
for Graph TSP is $\Omega\left(\frac{\log(n)}{\log\log(n)}\right)$ and
$O\left(\left(\frac{\log(n)}{\log\log(n)}\right)^{\log_2(9)+\epsilon}\right)$
for all $\epsilon>0$. For the (1,2)-TSP we give a lower bound of
$\frac{11}{10}$ on the approximation ratio of the $k$-improv and $k$-Opt
algorithm for arbitrary fixed $k$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0305052v1,On the Existence and Convergence Computable Universal Priors,"Solomonoff unified Occam's razor and Epicurus' principle of multiple
explanations to one elegant, formal, universal theory of inductive inference,
which initiated the field of algorithmic information theory. His central result
is that the posterior of his universal semimeasure M converges rapidly to the
true sequence generating posterior mu, if the latter is computable. Hence, M is
eligible as a universal predictor in case of unknown mu. We investigate the
existence and convergence of computable universal (semi)measures for a
hierarchy of computability classes: finitely computable, estimable, enumerable,
and approximable. For instance, M is known to be enumerable, but not finitely
computable, and to dominate all enumerable semimeasures. We define seven
classes of (semi)measures based on these four computability concepts. Each
class may or may not contain a (semi)measure which dominates all elements of
another class. The analysis of these 49 cases can be reduced to four basic
cases, two of them being new. The results hold for discrete and continuous
semimeasures. We also investigate more closely the types of convergence,
possibly implied by universality: in difference and in ratio, with probability
1, in mean sum, and for Martin-Loef random sequences. We introduce a
generalized concept of randomness for individual sequences and use it to
exhibit difficulties regarding these issues.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.07812v2,Recovering Data Permutations from Noisy Observations: The Linear Regime,"This paper considers a noisy data structure recovery problem. The goal is to
investigate the following question: Given a noisy observation of a permuted
data set, according to which permutation was the original data sorted? The
focus is on scenarios where data is generated according to an isotropic
Gaussian distribution, and the noise is additive Gaussian with an arbitrary
covariance matrix. This problem is posed within a hypothesis testing framework.
The objective is to study the linear regime in which the optimal decoder has a
polynomial complexity in the data size, and it declares the permutation by
simply computing a permutation-independent linear function of the noisy
observations. The main result of the paper is a complete characterization of
the linear regime in terms of the noise covariance matrix. Specifically, it is
shown that this matrix must have a very flat spectrum with at most three
distinct eigenvalues to induce the linear regime. Several practically relevant
implications of this result are discussed, and the error probability incurred
by the decision criterion in the linear regime is also characterized. A core
technical component consists of using linear algebraic and geometric tools,
such as Steiner symmetrization.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.06264v1,"Parameterization of Forced Isotropic Turbulent Flow using Autoencoders
  and Generative Adversarial Networks","Autoencoders and generative neural network models have recently gained
popularity in fluid mechanics due to their spontaneity and low processing time
instead of high fidelity CFD simulations. Auto encoders are used as model order
reduction tools in applications of fluid mechanics by compressing input
high-dimensional data using an encoder to map the input space into a
lower-dimensional latent space. Whereas, generative models such as Variational
Auto-encoders (VAEs) and Generative Adversarial Networks (GANs) are proving to
be effective in generating solutions to chaotic models with high 'randomness'
such as turbulent flows. In this study, forced isotropic turbulence flow is
generated by parameterizing into some basic statistical characteristics. The
models trained on pre-simulated data from dependencies on these characteristics
and the flow generation is then affected by varying these parameters. The
latent vectors pushed along the generator models like the decoders and
generators contain independent entries which can be used to create different
outputs with similar properties. The use of neural network-based architecture
removes the need for dependency on the classical mesh-based Navier-Stoke
equation estimation which is prominent in many CFD softwares.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.04033v1,An Examination of the Effectiveness of Teaching Data Modelling Concepts,"The effective teaching of data modelling concepts is very important; it
constitutes the fundament of database planning methods and the handling of
databases with the help of database management lan-guages, typically SQL. We
examined three courses. The students of two courses prepared for the exam by
solving tests, while the students of the third course prepared by solving tasks
from a printed exercise book. The number of task for the second course was 2.5
times more than the number of task for the first course. The main purpose of
our examination was to determine the effectiveness of the teaching of data
modelling concepts, and to decide if there is a significant difference between
the results of the three courses. According to our examination, with increasing
the number of test tasks and with the use of exercise book, the results became
significantly better.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1401.5197v3,"A user-friendly nano-CT image alignment and 3D reconstruction platform
  based on LabVIEW","X-ray computed tomography at the nanometer scale (nano-CT) offers a wide
range of applications in scientific and industrial areas. Here we describe a
reliable, user-friendly and fast software package based on LabVIEW that may
allow to perform all procedures after the acquisition of raw projection images
in order to obtain the inner structure of the investigated sample. A suitable
image alignment process to address misalignment problems among image series due
to mechanical manufacturing errors, thermal expansion and other external
factors has been considered together with a novel fast parallel beam 3D
reconstruction procedure, developed ad hoc to perform the tomographic
reconstruction. Remarkably improved reconstruction results obtained at the
Beijing Synchrotron Radiation Facility after the image calibration confirmed
the fundamental role of this image alignment procedure that minimizes unwanted
blurs and additional streaking artifacts always present in reconstructed
slices. Moreover, this nano-CT image alignment and its associated 3D
reconstruction procedure fully based on LabVIEW routines, significantly reduce
the data post-processing cycle, thus making faster and easier the activity of
the users during experimental runs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.02906v1,Multirobot Coverage of Linear Modular Environments,"Multirobot systems for covering environments are increasingly used in
applications like cleaning, industrial inspection, patrolling, and precision
agriculture. The problem of covering a given environment using multiple robots
can be naturally formulated and studied as a multi-Traveling Salesperson
Problem (mTSP). In a mTSP, the environment is represented as a graph and the
goal is to find tours (starting and ending at the same depot) for the robots in
order to visit all the vertices with minimum global cost, namely the length of
the longest tour. The mTSP is an NP-hard problem for which several
approximation algorithms have been proposed. These algorithms usually assume
generic environments, but tighter approximation bounds can be reached focusing
on specific environments. In this paper, we address the case of environments
composed of sub-parts, called modules, that can be reached from each other only
through some linking structures. Examples are multi-floor buildings, in which
the modules are the floors and the linking structures are the staircases or the
elevators, and floors of large hotels or hospitals, in which the modules are
the rooms and the linking structures are the corridors. We focus on linear
modular environments, with the modules organized sequentially, presenting an
efficient (with polynomial worst-case time complexity) algorithm that finds a
solution for the mTSP whose cost is within a bounded distance from the cost of
the optimal solution. The main idea of our algorithm is to allocate disjoint
""blocks"" of adjacent modules to the robots, in such a way that each module is
covered by only one robot. We experimentally compare our algorithm against some
state-of-the-art algorithms for solving mTSPs in generic environments and show
that it is able to provide solutions with lower makespan and spending a
computing time several orders of magnitude shorter.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.13600v1,"Eye Gaze Controlled Interfaces for Head Mounted and Multi-Functional
  Displays in Military Aviation Environment","Eye gaze controlled interfaces allow us to directly manipulate a graphical
user interface just by looking at it. This technology has great potential in
military aviation, in particular, operating different displays in situations
where pilots hands are occupied with flying the aircraft. This paper reports
studies on analyzing accuracy of eye gaze controlled interface inside aircraft
undertaking representative flying missions. We reported that pilots can
undertake representative pointing and selection tasks at less than 2 secs on
average. Further, we evaluated the accuracy of eye gaze tracking glass under
various G-conditions and analyzed its failure modes. We observed that the
accuracy of an eye tracker is less than 5 degree of visual angle up to +3G,
although it is less accurate at minus 1G and plus 5G. We observed that eye
tracker may fail to track under higher external illumination. We also infer
that an eye tracker to be used in military aviation need to have larger
vertical field of view than the present available systems. We used this
analysis to develop eye gaze trackers for Multi-Functional displays and Head
Mounted Display System. We obtained significant reduction in pointing and
selection times using our proposed HMDS system compared to traditional TDS.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.10067v1,"Towards Automatic Sizing for PPE with a Point Cloud Based Variational
  Autoencoder","Sizing and fitting of Personal Protective Equipment (PPE) is a critical part
of the product creation process; however, traditional methods to do this type
of work can be labor intensive and based on limited or non-representative
anthropomorphic data. In the case of PPE, a poor fit can jeopardize an
individual's health and safety. In this paper we present an unsupervised
machine learning algorithm that can identify a representative set of exemplars,
individuals that can be utilized by designers as idealized sizing models. The
algorithm is based around a Variational Autoencoder (VAE) with a Point-Net
inspired encoder and decoder architecture trained on Human point-cloud data
obtained from the CEASAR dataset. The learned latent space is then clustered to
identify a specified number of sizing groups. We demonstrate this technique on
scans of human faces to provide designers of masks and facial coverings a
reference set of individuals to test existing mask styles.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1510.08345v2,"A polynomial expansion line search for large-scale unconstrained
  minimization of smooth L2-regularized loss functions, with implementation in
  Apache Spark","In large-scale unconstrained optimization algorithms such as limited memory
BFGS (LBFGS), a common subproblem is a line search minimizing the loss function
along a descent direction. Commonly used line searches iteratively find an
approximate solution for which the Wolfe conditions are satisfied, typically
requiring multiple function and gradient evaluations per line search, which is
expensive in parallel due to communication requirements. In this paper we
propose a new line search approach for cases where the loss function is
analytic, as in least squares regression, logistic regression, or low rank
matrix factorization. We approximate the loss function by a truncated Taylor
polynomial, whose coefficients may be computed efficiently in parallel with
less communication than evaluating the gradient, after which this polynomial
may be minimized with high accuracy in a neighbourhood of the expansion point.
Our Polynomial Expansion Line Search (PELS) was implemented in the Apache Spark
framework and used to accelerate the training of a logistic regression model on
binary classification datasets from the LIBSVM repository with LBFGS and the
Nonlinear Conjugate Gradient (NCG) method. In large-scale numerical experiments
in parallel on a 16-node cluster with 256 cores using the URL, KDDA, and KDDB
datasets, the PELS approach produced significant convergence improvements
compared to the use of classical Wolfe line searches. For example, to reach the
final training label prediction accuracies, LBFGS using PELS had speedup
factors of 1.8--2 over LBFGS using a Wolfe line search, measured by both the
number of iterations and the time required, due to the better accuracy of step
sizes computed in the line search. PELS has the potential to significantly
accelerate large-scale regression and factorization computations, and is
applicable to continuous optimization problems with smooth loss functions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.01033v5,"Characteristic Logics for Behavioural Hemimetrics via Fuzzy Lax
  Extensions","In systems involving quantitative data, such as probabilistic, fuzzy, or
metric systems, behavioural distances provide a more fine-grained comparison of
states than two-valued notions of behavioural equivalence or behaviour
inclusion. Like in the two-valued case, the wide variation found in system
types creates a need for generic methods that apply to many system types at
once. Approaches of this kind are emerging within the paradigm of universal
coalgebra, based either on lifting pseudometrics along set functors or on
lifting general real-valued (fuzzy) relations along functors by means of fuzzy
lax extensions. An immediate benefit of the latter is that they allow bounding
behavioural distance by means of fuzzy (bi-)simulations that need not
themselves be hemi- or pseudometrics; this is analogous to classical
simulations and bisimulations, which need not be preorders or equivalence
relations, respectively. The known generic pseudometric liftings, specifically
the generic Kantorovich and Wasserstein liftings, both can be extended to yield
fuzzy lax extensions, using the fact that both are effectively given by a
choice of quantitative modalities. Our central result then shows that in fact
all fuzzy lax extensions are Kantorovich extensions for a suitable set of
quantitative modalities, the so-called Moss modalities. For nonexpansive fuzzy
lax extensions, this allows for the extraction of quantitative modal logics
that characterize behavioural distance, i.e. satisfy a quantitative version of
the Hennessy-Milner theorem; equivalently, we obtain expressiveness of a
quantitative version of Moss' coalgebraic logic. All our results explicitly
hold also for asymmetric distances (hemimetrics), i.e. notions of quantitative
simulation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1608.03327v1,A Step Towards Checking Security in IoT,"The Internet of Things (IoT) is smartifying our everyday life. Our starting
point is IoT-LySa, a calculus for describing IoT systems, and its static
analysis, which will be presented at Coordination 2016. We extend the mentioned
proposal in order to begin an investigation about security issues, in
particular for the static verification of secrecy and some other security
properties.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1212.6110v1,Hyperplane Arrangements and Locality-Sensitive Hashing with Lift,"Locality-sensitive hashing converts high-dimensional feature vectors, such as
image and speech, into bit arrays and allows high-speed similarity calculation
with the Hamming distance. There is a hashing scheme that maps feature vectors
to bit arrays depending on the signs of the inner products between feature
vectors and the normal vectors of hyperplanes placed in the feature space. This
hashing can be seen as a discretization of the feature space by hyperplanes. If
labels for data are given, one can determine the hyperplanes by using learning
algorithms. However, many proposed learning methods do not consider the
hyperplanes' offsets. Not doing so decreases the number of partitioned regions,
and the correlation between Hamming distances and Euclidean distances becomes
small. In this paper, we propose a lift map that converts learning algorithms
without the offsets to the ones that take into account the offsets. With this
method, the learning methods without the offsets give the discretizations of
spaces as if it takes into account the offsets. For the proposed method, we
input several high-dimensional feature data sets and studied the relationship
between the statistical characteristics of data, the number of hyperplanes, and
the effect of the proposed method.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1710.06396v1,On the bit-size of non-radical triangular sets,"We present upper bounds on the bit-size of coefficients of non-radical
lexicographical Groebner bases in purely triangular form (triangular sets) of
dimension zero. This extends a previous work [Dahan-Schost, Issac'2004],
constrained to radical triangular sets; it follows the same technical steps,
based on interpolation. However, key notion of height of varieties is not
available for points with multiplicities; therefore the bounds obtained are
less universal and depend on some input data. We also introduce a related
family of non- monic polynomials that have smaller coefficients, and smaller
bounds. It is not obvious to compute them from the initial triangular set
though.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0006034v1,Type Classes and Constraint Handling Rules,"Type classes are an elegant extension to traditional, Hindley-Milner based
typing systems. They are used in modern, typed languages such as Haskell to
support controlled overloading of symbols. Haskell 98 supports only
single-parameter and constructor type classes. Other extensions such as
multi-parameter type classes are highly desired but are still not officially
supported by Haskell. Subtle issues arise with extensions, which may lead to a
loss of feasible type inference or ambiguous programs. A proper logical basis
for type class systems seems to be missing. Such a basis would allow extensions
to be characterised and studied rigorously. We propose to employ Constraint
Handling Rules as a tool to study and develop type class systems in a uniform
way.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.02848v1,"Rethinking Sustainability Requirements: Drivers, Barriers and Impacts of
  Digitalisation from the Viewpoint of Experts","Requirements engineering (RE) is a key area to address sustainability
concerns in system development. Approaches have been proposed to elicit
sustainability requirements from interested stakeholders before system design.
However, existing strategies lack the proper high-level view to deal with the
societal and long-term impacts of the transformation entailed by the
introduction of a new technological solution. This paper proposes to go beyond
the concept of system requirements and stakeholders' goals, and raise the
degree of abstraction by focusing on the notions of drivers, barriers and
impacts that a system can have on the environment in which it is deployed.
Furthermore, we suggest to narrow the perspective to a single domain, as the
effect of a technology is context-dependent. To put this vision into practice,
we interview 30 cross-disciplinary experts in the representative domain of
rural areas, and we analyse the transcripts to identify common themes. As a
result, we provide drivers, barriers and positive or negative impacts
associated to the introduction of novel technical solutions in rural areas.
This RE-relevant information could hardly be identified if interested
stakeholders were interviewed before the development of a single specific
system. This paper contributes to the literature with a fresh perspective on
sustainability requirements, and with a domain-specific framework grounded on
experts' opinions. The conceptual framework resulting from our analysis can be
used as a reference baseline for requirements elicitation endeavours in rural
areas that need to account for sustainability concerns.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1906.06224v2,Deep neural network for fringe pattern filtering and normalisation,"We propose a new framework for processing Fringe Patterns (FP). Our novel
approach builds upon the hypothesis that the denoising and normalisation of FPs
can be learned by a deep neural network if enough pairs of corrupted and ideal
FPs are provided. The main contributions of this paper are the following: (1)
We propose the use of the U-net neural network architecture for FP
normalisation tasks; (2) we propose a modification for the distribution of
weights in the U-net, called here the V-net model, which is more convenient for
reconstruction tasks, and we conduct extensive experimental evidence in which
the V-net produces high-quality results for FP filtering and normalisation. (3)
We also propose two modifications of the V-net scheme, namely, a residual
version called ResV-net and a fast operating version of the V-net, to evaluate
the potential improvements when modify our proposal. We evaluate the
performance of our methods in various scenarios: FPs corrupted with different
degrees of noise, and corrupted with different noise distributions. We compare
our methodology versus other state-of-the-art methods. The experimental results
(on both synthetic and real data) demonstrate the capabilities and potential of
this new paradigm for processing interferograms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1011.5696v2,"Quantifying and qualifying trust: Spectral decomposition of trust
  networks","In a previous FAST paper, I presented a quantitative model of the process of
trust building, and showed that trust is accumulated like wealth: the rich get
richer. This explained the pervasive phenomenon of adverse selection of trust
certificates, as well as the fragility of trust networks in general. But a
simple explanation does not always suggest a simple solution. It turns out that
it is impossible to alter the fragile distribution of trust without sacrificing
some of its fundamental functions. A solution for the vulnerability of trust
must thus be sought elsewhere, without tampering with its distribution. This
observation was the starting point of the present paper. It explores a
different method for securing trust: not by redistributing it, but by mining
for its sources. The method used to break privacy is thus also used to secure
trust. A high level view of the mining methods that connect the two is provided
in terms of *similarity networks*, and *spectral decomposition* of similarity
preserving maps. This view may be of independent interest, as it uncovers a
common conceptual and structural foundation of mathematical classification
theory on one hand, and of the spectral methods of graph clustering and data
mining on the other hand.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0
http://arxiv.org/abs/1708.00578v1,Emerging Topics in Internet Technology: A Complex Networks Approach,"Communication networks, in general, and internet technology, in particular,
is a fast-evolving area of research. While it is important to keep track of
emerging trends in this domain, it is such a fast-growing area that it can be
very difficult to keep track of literature. The problem is compounded by the
fast-growing number of citation databases. While other databases are gradually
indexing a large set of reliable content, currently the Web of Science
represents one of the most highly valued databases. Research indexed in this
database is known to highlight key advancements in any domain. In this paper,
we present a Complex Network-based analytical approach to analyze recent data
from the Web of Science in communication networks. Taking bibliographic records
from the recent period of 2014 to 2017, we model and analyze complex
scientometric networks. Using bibliometric coupling applied over complex
citation data we present answers to co-citation patterns of documents,
co-occurrence patterns of terms, as well as the most influential articles,
among others, We also present key pivot points and intellectual turning points.
Complex network analysis of the data demonstrates a considerably high level of
interest in two key clusters labeled descriptively as ""social networks"" and
""computer networks"". In addition, key themes in highly cited literature were
clearly identified as ""communication networks,"" ""social networks,"" and ""complex
networks"".",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1611.10228v1,"Behavior-Based Machine-Learning: A Hybrid Approach for Predicting Human
  Decision Making","A large body of work in behavioral fields attempts to develop models that
describe the way people, as opposed to rational agents, make decisions. A
recent Choice Prediction Competition (2015) challenged researchers to suggest a
model that captures 14 classic choice biases and can predict human decisions
under risk and ambiguity. The competition focused on simple decision problems,
in which human subjects were asked to repeatedly choose between two gamble
options.
  In this paper we present our approach for predicting human decision behavior:
we suggest to use machine learning algorithms with features that are based on
well-established behavioral theories. The basic idea is that these
psychological features are essential for the representation of the data and are
important for the success of the learning process. We implement a vanilla model
in which we train SVM models using behavioral features that rely on the
psychological properties underlying the competition baseline model. We show
that this basic model captures the 14 choice biases and outperforms all the
other learning-based models in the competition. The preliminary results suggest
that such hybrid models can significantly improve the prediction of human
decision making, and are a promising direction for future research.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0807.1949v5,"Virtual Transmission Method, A New Distributed Algorithm to Solve Sparse
  Linear System","In this paper, we propose a new parallel algorithm which could work naturally
on the parallel computer with arbitrary number of processors. This algorithm is
named Virtual Transmission Method (VTM). Its physical backgroud is the lossless
transmission line and microwave network. The basic idea of VTM is to insert
lossless transmission lines into the sparse linear system to achieve
distributed computing.
  VTM is proved to be convergent to solve SPD linear system. Preconditioning
method and performance model are presented. Numerical experiments show that VTM
is efficient, accurate and stable.
  Accompanied with VTM, we bring in a new technique to partition the symmetric
linear system, which is named Generalized Node & Branch Tearing (GNBT). It is
based on Kirchhoff's Current Law from circuit theory. We proved that GNBT is
feasible to partition any SPD linear system.",0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1407.0981v2,"A Centralized Mechanism to Make Predictions Based on Data From Multiple
  WSNs","In this work, we present a method that exploits a scenario with
inter-Wireless Sensor Networks (WSNs) information exchange by making
predictions and adapting the workload of a WSN according to their outcomes. We
show the feasibility of an approach that intelligently utilizes information
produced by other WSNs that may or not belong to the same administrative
domain. To illustrate how the predictions using data from external WSNs can be
utilized, a specific use-case is considered, where the operation of a WSN
measuring relative humidity is optimized using the data obtained from a WSN
measuring temperature. Based on a dedicated performance score, the simulation
results show that this new approach can find the optimal operating point
associated to the trade-off between energy consumption and quality of
measurements. Moreover, we outline the additional challenges that need to be
overcome, and draw conclusions to guide the future work in this field.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1311.7536v1,Proceedings Sixth Transformation Tool Contest,"The aim of the Transformation Tool Contest (TTC) series is to compare the
expressiveness, the usability and the performance of graph and model
transformation tools along a number of selected case studies. Participants want
to learn about the pros and cons of each tool considering different
applications. A deeper understanding of the relative merits of different tool
features will help to further improve graph and model transformation tools and
to indicate open problems.
  TTC 2013 involved 18 offline case study solutions: 6 solutions to the
FlowGraphs case, 9 solutions to the Petri Nets to Statecharts case and 3
solutions to the Restructuring case. 13 of the 18 solutions have undergone a
non-blind peer review before the workshop and were presented and evaluated
during the workshop in Budapest. This volume contains the submissions that have
passed an additional (post-workshop, blind) reviewing round.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1210.6110v1,"Automatic WSDL-guided Test Case Generation for PropEr Testing of Web
  Services","With web services already being key ingredients of modern web systems,
automatic and easy-to-use but at the same time powerful and expressive testing
frameworks for web services are increasingly important. Our work aims at fully
automatic testing of web services: ideally the user only specifies properties
that the web service is expected to satisfy, in the form of input-output
relations, and the system handles all the rest. In this paper we present in
detail the component which lies at the heart of this system: how the WSDL
specification of a web service is used to automatically create test case
generators that can be fed to PropEr, a property-based testing tool, to create
structurally valid random test cases for its operations and check its
responses. Although the process is fully automatic, our tool optionally allows
the user to easily modify its output to either add semantic information to the
generators or write properties that test for more involved functionality of the
web services.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0304022v1,Self-Replicating Machines in Continuous Space with Virtual Physics,"JohnnyVon is an implementation of self-replicating machines in continuous
two-dimensional space. Two types of particles drift about in a virtual liquid.
The particles are automata with discrete internal states but continuous
external relationships. Their internal states are governed by finite state
machines but their external relationships are governed by a simulated physics
that includes Brownian motion, viscosity, and spring-like attractive and
repulsive forces. The particles can be assembled into patterns that can encode
arbitrary strings of bits. We demonstrate that, if an arbitrary ""seed"" pattern
is put in a ""soup"" of separate individual particles, the pattern will replicate
by assembling the individual particles into copies of itself. We also show
that, given sufficient time, a soup of separate individual particles will
eventually spontaneously form self-replicating patterns. We discuss the
implications of JohnnyVon for research in nanotechnology, theoretical biology,
and artificial life.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.09203v3,Modular network for high accuracy object detection,"We present a novel modular object detection convolutional neural network that
significantly improves the accuracy of object detection. The network consists
of two stages in a hierarchical structure. The first stage is a network that
detects general classes. The second stage consists of separate networks to
refine the classification and localization of each of the general classes
objects. Compared to a state of the art object detection networks the
classification error in the modular network is improved by approximately 3-5
times, from 12% to 2.5 %-4.5%. This network is easy to implement and has a 0.94
mAP. The network architecture can be a platform to improve the accuracy of
widespread state of the art object detection networks and other kinds of deep
learning networks. We show that a deep learning network initialized by transfer
learning becomes more accurate as the number of classes it later trained to
detect becomes smaller.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0805.2368v1,A Kernel Method for the Two-Sample Problem,"We propose a framework for analyzing and comparing distributions, allowing us
to design statistical tests to determine if two samples are drawn from
different distributions. Our test statistic is the largest difference in
expectations over functions in the unit ball of a reproducing kernel Hilbert
space (RKHS). We present two tests based on large deviation bounds for the test
statistic, while a third is based on the asymptotic distribution of this
statistic. The test statistic can be computed in quadratic time, although
efficient linear time approximations are available. Several classical metrics
on distributions are recovered when the function space used to compute the
difference in expectations is allowed to be more general (eg. a Banach space).
We apply our two-sample tests to a variety of problems, including attribute
matching for databases using the Hungarian marriage method, where they perform
strongly. Excellent performance is also obtained when comparing distributions
over graphs, for which these are the first such tests.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1307.0803v2,Data Fusion by Matrix Factorization,"For most problems in science and engineering we can obtain data sets that
describe the observed system from various perspectives and record the behavior
of its individual components. Heterogeneous data sets can be collectively mined
by data fusion. Fusion can focus on a specific target relation and exploit
directly associated data together with contextual data and data about system's
constraints. In the paper we describe a data fusion approach with penalized
matrix tri-factorization (DFMF) that simultaneously factorizes data matrices to
reveal hidden associations. The approach can directly consider any data that
can be expressed in a matrix, including those from feature-based
representations, ontologies, associations and networks. We demonstrate the
utility of DFMF for gene function prediction task with eleven different data
sources and for prediction of pharmacologic actions by fusing six data sources.
Our data fusion algorithm compares favorably to alternative data integration
approaches and achieves higher accuracy than can be obtained from any single
data source alone.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.01246v1,"Two Shifts for Crop Mapping: Leveraging Aggregate Crop Statistics to
  Improve Satellite-based Maps in New Regions","Crop type mapping at the field level is critical for a variety of
applications in agricultural monitoring, and satellite imagery is becoming an
increasingly abundant and useful raw input from which to create crop type maps.
Still, in many regions crop type mapping with satellite data remains
constrained by a scarcity of field-level crop labels for training supervised
classification models. When training data is not available in one region,
classifiers trained in similar regions can be transferred, but shifts in the
distribution of crop types as well as transformations of the features between
regions lead to reduced classification accuracy. We present a methodology that
uses aggregate-level crop statistics to correct the classifier by accounting
for these two types of shifts. To adjust for shifts in the crop type
composition we present a scheme for properly reweighting the posterior
probabilities of each class that are output by the classifier. To adjust for
shifts in features we propose a method to estimate and remove linear shifts in
the mean feature vector. We demonstrate that this methodology leads to
substantial improvements in overall classification accuracy when using Linear
Discriminant Analysis (LDA) to map crop types in Occitanie, France and in
Western Province, Kenya. When using LDA as our base classifier, we found that
in France our methodology led to percent reductions in misclassifications
ranging from 2.8% to 42.2% (mean = 21.9%) over eleven different training
departments, and in Kenya the percent reductions in misclassification were
6.6%, 28.4%, and 42.7% for three training regions. While our methodology was
statistically motivated by the LDA classifier, it can be applied to any type of
classifier. As an example, we demonstrate its successful application to improve
a Random Forest classifier.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1604.02180v1,"Service Chain and Virtual Network Embeddings: Approximations using
  Randomized Rounding","The SDN and NFV paradigms enable novel network services which can be realized
and embedded in a flexible and rapid manner. For example, SDN can be used to
flexibly steer traffic from a source to a destination through a sequence of
virtualized middleboxes, in order to realize so-called service chains. The
service chain embedding problem consists of three tasks: admission control,
finding suitable locations to allocate the virtualized middleboxes and
computing corresponding routing paths. This paper considers the offline batch
embedding of multiple service chains. Concretely, we consider the objectives of
maximizing the profit by embedding an optimal subset of requests or minimizing
the costs when all requests need to be embedded. Interestingly, while the
service chain embedding problem has recently received much attention, so far,
only non- polynomial time algorithms (based on integer programming) as well as
heuristics (which do not provide any formal guarantees) are known. This paper
presents the first polynomial time service chain approximation algorithms both
for the case with admission and without admission control. Our algorithm is
based on a novel extension of the classic linear programming and randomized
rounding technique, which may be of independent interest. In particular, we
show that our approach can also be extended to more complex service graphs,
containing cycles or sub-chains, hence also providing new insights into the
classic virtual network embedding problem.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1709.02209v1,"Discovering Neighbor Devices in Computer Network: Development of CDP and
  LLDP Simulation Modules for OMNeT++","The purpose of data-link layer discovery protocols is to provide the network
administrator with the current information (i.e., various Layer 2 and 3
parameters) about neighbor devices. These protocols are invaluable for network
monitoring, maintenance, and troubleshooting. However, they start to play an
important role in the operation of data-centers and other high-availability
networks. This paper outlines design, implementation and deployment of Cisco
Discovery Protocol and Link Layer Discovery Protocol simulation modules in
OMNeT++ simulator.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.06506v3,"Every super-polynomial proof in purely implicational minimal logic has a
  polynomially sized proof in classical implicational propositional logic","In this article we show how any formula A with a proof in minimal
implicational logic that is super-polynomially sized has a polynomially-sized
proof in classical implicational propositional logic . This fact provides an
argument in favor that any classical propositional tautology has short proofs,
i.e., NP=CoNP.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1501.00820v4,Software Safety Demonstration and Idemnification,"Computers may control safety-critical operations in machines having embedded
software. This memoir proposes a regimen to verify such algorithms at
prescribed levels of statistical confidence. The United States Department of
Defense standard for system safety engineering (MIL-STD-882E) defines
development procedures for safety-critical systems. However, a problem exists:
the Standard fails to distinguish quantitative product assurance technique from
categorical process assurance method for software development. Resulting is
conflict in the technical definition of the term risk. The primary goal here is
to show that a quantitative risk-based product assurance method exists and is
consistent with hardware practice. Discussion appears in two major parts:
theory, which shows the relationship between automata and software; and
application, which covers demonstration and indemnification. Demonstration is a
technique for generating random tests; indemnification converts pass/fail test
results to compound Poisson parameters (severity and intensity). Together,
demonstration and indemnification yield statistical confidence that
safety-critical code meets design intent. Statistical confidence is the
keystone of quantitative product assurance. A secondary goal is resolving the
conflict over the term risk. The first meaning is an accident model known in
mathematics as the compound Poisson stochastic process, and so is called
statistical risk. Various of its versions underlie the theories of safety and
reliability. The second is called developmental risk. It considers software
autonomy, which considers time until manual recovery of control. Once these
meanings are separated, MIL-STD-882 can properly support either formal
quantitative safety assurance or empirical process robustness, which differ in
impact.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.07547v1,Evaluation of Neural Network Classification Systems on Document Stream,"One major drawback of state of the art Neural Networks (NN)-based approaches
for document classification purposes is the large number of training samples
required to obtain an efficient classification. The minimum required number is
around one thousand annotated documents for each class. In many cases it is
very difficult, if not impossible, to gather this number of samples in real
industrial processes. In this paper, we analyse the efficiency of NN-based
document classification systems in a sub-optimal training case, based on the
situation of a company document stream. We evaluated three different
approaches, one based on image content and two on textual content. The
evaluation was divided into four parts: a reference case, to assess the
performance of the system in the lab; two cases that each simulate a specific
difficulty linked to document stream processing; and a realistic case that
combined all of these difficulties. The realistic case highlighted the fact
that there is a significant drop in the efficiency of NN-Based document
classification systems. Although they remain efficient for well represented
classes (with an over-fitting of the system for those classes), it is
impossible for them to handle appropriately less well represented classes.
NN-Based document classification systems need to be adapted to resolve these
two problems before they can be considered for use in a company document
stream.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1410.5416v1,"Software in e-Learning Architecture, Processes and Management","Our entire society is becoming more and more dependent on technology and
specifically on software. The integration of e-learning software systems into
our day by day life especially in e-learning applications generates
modifications upon the society and, at the same time, the society itself
changes the process of software development. This circle of continuous
determination engenders a highly dynamic environment. Lehman describes the
software development environment as being characterized by a high, necessary
and inevitable pressure for change. Changes are reflected in specific
uncertainties which impact the success and performance of the software project
development.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1003.0425v3,A logical basis for constructive systems,"The work is devoted to Computability Logic (CoL) -- the
philosophical/mathematical platform and long-term project for redeveloping
classical logic after replacing truth} by computability in its underlying
semantics (see http://www.cis.upenn.edu/~giorgi/cl.html). This article
elaborates some basic complexity theory for the CoL framework. Then it proves
soundness and completeness for the deductive system CL12 with respect to the
semantics of CoL, including the version of the latter based on polynomial time
computability instead of computability-in-principle. CL12 is a sequent calculus
system, where the meaning of a sequent intuitively can be characterized as ""the
succedent is algorithmically reducible to the antecedent"", and where formulas
are built from predicate letters, function letters, variables, constants,
identity, negation, parallel and choice connectives, and blind and choice
quantifiers. A case is made that CL12 is an adequate logical basis for
constructive applied theories, including complexity-oriented ones.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0803.1748v1,A Computational Framework for the Near Elimination of Spreadsheet Risk,"We present Risk Integrated's Enterprise Spreadsheet Platform (ESP), a
technical approach to the near-elimination of spreadsheet risk in the
enterprise computing environment, whilst maintaining the full flexibility of
spreadsheets for modelling complex financial structures and processes. In its
Basic Mode of use, the system comprises a secure and robust centralised
spreadsheet management framework. In Advanced Mode, the system can be viewed as
a robust computational framework whereby users can ""submit jobs"" to the
spreadsheet, and retrieve the results from the computations, but with no direct
access to the underlying spreadsheet. An example application, Monte Carlo
simulation, is presented to highlight the benefits of this approach with regard
to mitigating spreadsheet risk in complex, mission-critical, financial
calculations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1
http://arxiv.org/abs/2003.13670v4,"Anonymous Collocation Discovery: Harnessing Privacy to Tame the
  Coronavirus","Successful containment of the Coronavirus pandemic rests on the ability to
quickly and reliably identify those who have been in close proximity to a
contagious individual. Existing tools for doing so rely on the collection of
exact location information of individuals over lengthy time periods, and
combining this information with other personal information. This unprecedented
encroachment on individual privacy at national scales has created an outcry and
risks rejection of these tools.
  We propose an alternative: an extremely simple scheme for providing
fine-grained and timely alerts to users who have been in the close vicinity of
an infected individual. Crucially, this is done while preserving the anonymity
of all individuals, and without collecting or storing any personal information
or location history. Our approach is based on using short-range communication
mechanisms, like Bluetooth, that are available in all modern cell phones. It
can be deployed with very little infrastructure, and incurs a relatively low
false-positive rate compared to other collocation methods. We also describe a
number of extensions and tradeoffs.
  We believe that the privacy guarantees provided by the scheme will encourage
quick and broad voluntary adoption. When combined with sufficient testing
capacity and existing best practices from healthcare professionals, we hope
that this may significantly reduce the infection rate.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1602.03985v6,A complexity trichotomy for approximately counting list H-colourings,"We examine the computational complexity of approximately counting the list
H-colourings of a graph. We discover a natural graph-theoretic trichotomy based
on the structure of the graph H. If H is an irreflexive bipartite graph or a
reflexive complete graph then counting list H-colourings is trivially in
polynomial time. Otherwise, if H is an irreflexive bipartite permutation graph
or a reflexive proper interval graph then approximately counting list
H-colourings is equivalent to #BIS, the problem of approximately counting
independent sets in a bipartite graph. This is a well-studied problem which is
believed to be of intermediate complexity -- it is believed that it does not
have an FPRAS, but that it is not as difficult as approximating the most
difficult counting problems in #P. For every other graph H, approximately
counting list H-colourings is complete for #P with respect to
approximation-preserving reductions (so there is no FPRAS unless NP=RP). Two
pleasing features of the trichotomy are (i) it has a natural formulation in
terms of hereditary graph classes, and (ii) the proof is largely self-contained
and does not require any universal algebra (unlike similar dichotomies in the
weighted case). We are able to extend the hardness results to the
bounded-degree setting, showing that all hardness results apply to input graphs
with maximum degree at most 6.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1905.08764v2,Performance Analysis of Deep Learning Workloads on Leading-edge Systems,"This work examines the performance of leading-edge systems designed for
machine learning computing, including the NVIDIA DGX-2, Amazon Web Services
(AWS) P3, IBM Power System Accelerated Compute Server AC922, and a
consumer-grade Exxact TensorEX TS4 GPU server. Representative deep learning
workloads from the fields of computer vision and natural language processing
are the focus of the analysis. Performance analysis is performed along with a
number of important dimensions. Performance of the communication interconnects
and large and high-throughput deep learning models are considered. Different
potential use models for the systems as standalone and in the cloud also are
examined. The effect of various optimization of the deep learning models and
system configurations is included in the analysis.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.10542v1,Big Ramsey degrees of the generic partial order,"As a result of 33 intercontinental Zoom calls, we characterise big Ramsey
degrees of the generic partial order in a similar way as Devlin characterised
big Ramsey degrees of the generic linear order (the order of rationals).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.02690v2,On the Space Complexity of Set Agreement,"The $k$-set agreement problem is a generalization of the classical consensus
problem in which processes are permitted to output up to $k$ different input
values. In a system of $n$ processes, an $m$-obstruction-free solution to the
problem requires termination only in executions where the number of processes
taking steps is eventually bounded by $m$. This family of progress conditions
generalizes wait-freedom ($m=n$) and obstruction-freedom ($m=1$). In this
paper, we prove upper and lower bounds on the number of registers required to
solve $m$-obstruction-free $k$-set agreement, considering both one-shot and
repeated formulations. In particular, we show that repeated $k$ set agreement
can be solved using $n+2m-k$ registers and establish a nearly matching lower
bound of $n+m-k$.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1312.6635v1,"Topic and Sentiment Analysis on OSNs: a Case Study of Advertising
  Strategies on Twitter","Social media have substantially altered the way brands and businesses
advertise: Online Social Networks provide brands with more versatile and
dynamic channels for advertisement than traditional media (e.g., TV and radio).
Levels of engagement in such media are usually measured in terms of content
adoption (e.g., likes and retweets) and sentiment, around a given topic.
However, sentiment analysis and topic identification are both non-trivial
tasks.
  In this paper, using data collected from Twitter as a case study, we analyze
how engagement and sentiment in promoted content spread over a 10-day period.
We find that promoted tweets lead to higher positive sentiment than promoted
trends; although promoted trends pay off in response volume. We observe that
levels of engagement for the brand and promoted content are highest on the
first day of the campaign, and fall considerably thereafter. However, we show
that these insights depend on the use of robust machine learning and natural
language processing techniques to gather focused, relevant datasets, and to
accurately gauge sentiment, rather than relying on the simple keyword- or
frequency-based metrics sometimes used in social media research.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1607.07765v4,"Rightsstatements.org White Paper: Requirements for the Technical
  Infrastructure for Standardized International Rights Statements","This document is part of the deliverables created by the RightsStatements.org
consortium. It provides the technical requirements for implementation of the
Standardized International Rights Statements. These requirements are based on
the principles and specifications found in the normative Recommendations for
Standardized International Rights Statements. This document replaces and
supersedes the previously released Recommendations for the Technical
Infrastructure for Standardized Rights Statements, released by this working
group. The Requirements for the Technical Infrastructure for Standardized
International Rights Statements describes the expected behaviours for a service
that enables the delivery of human and machine-readable representations of the
rights statements. It documents the fundamental decisions that informed the
development of a data model grounded in Linked Data approaches. This document
also provides proposed implementation guidelines and a non-normative set of
examples for incorporating rights statements into provider metadata.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1807.06149v1,Probably approximately correct learning of Horn envelopes from queries,"We propose an algorithm for learning the Horn envelope of an arbitrary domain
using an expert, or an oracle, capable of answering certain types of queries
about this domain. Attribute exploration from formal concept analysis is a
procedure that solves this problem, but the number of queries it may ask is
exponential in the size of the resulting Horn formula in the worst case. We
recall a well-known polynomial-time algorithm for learning Horn formulas with
membership and equivalence queries and modify it to obtain a polynomial-time
probably approximately correct algorithm for learning the Horn envelope of an
arbitrary domain.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1111.5305v2,On a Linear Program for Minimum-Weight Triangulation,"Minimum-weight triangulation (MWT) is NP-hard. It has a polynomial-time
constant-factor approximation algorithm, and a variety of effective polynomial-
time heuristics that, for many instances, can find the exact MWT. Linear
programs (LPs) for MWT are well-studied, but previously no connection was known
between any LP and any approximation algorithm or heuristic for MWT. Here we
show the first such connections: for an LP formulation due to Dantzig et al.
(1985): (i) the integrality gap is bounded by a constant; (ii) given any
instance, if the aforementioned heuristics find the MWT, then so does the LP.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1212.6857v1,A Trichotomy for Regular Simple Path Queries on Graphs,"Regular path queries (RPQs) select nodes connected by some path in a graph.
The edge labels of such a path have to form a word that matches a given regular
expression. We investigate the evaluation of RPQs with an additional constraint
that prevents multiple traversals of the same nodes. Those regular simple path
queries (RSPQs) find several applications in practice, yet they quickly become
intractable, even for basic languages such as (aa)* or a*ba*.
  In this paper, we establish a comprehensive classification of regular
languages with respect to the complexity of the corresponding regular simple
path query problem. More precisely, we identify the fragment that is maximal in
the following sense: regular simple path queries can be evaluated in polynomial
time for every regular language L that belongs to this fragment and evaluation
is NP-complete for languages outside this fragment. We thus fully characterize
the frontier between tractability and intractability for RSPQs, and we refine
our results to show the following trichotomy: Evaluations of RSPQs is either
AC0, NL-complete or NP-complete in data complexity, depending on the regular
language L. The fragment identified also admits a simple characterization in
terms of regular expressions.
  Finally, we also discuss the complexity of the following decision problem:
decide, given a language L, whether finding a regular simple path for L is
tractable. We consider several alternative representations of L: DFAs, NFAs or
regular expressions, and prove that this problem is NL-complete for the first
representation and PSPACE-complete for the other two. As a conclusion we extend
our results from edge-labeled graphs to vertex-labeled graphs and vertex-edge
labeled graphs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.01608v2,Obstacle Avoidance Using a Monocular Camera,"A collision avoidance system based on simple digital cameras would help
enable the safe integration of small UAVs into crowded, low-altitude
environments. In this work, we present an obstacle avoidance system for small
UAVs that uses a monocular camera with a hybrid neural network and path planner
controller. The system is comprised of a vision network for estimating depth
from camera images, a high-level control network, a collision prediction
network, and a contingency policy. This system is evaluated on a simulated UAV
navigating an obstacle course in a constrained flight pattern. Results show the
proposed system achieves low collision rates while maintaining operationally
relevant flight speeds.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1403.7044v1,Proceedings Ninth Workshop on Model-Based Testing,"This volume contains the proceedings of the Ninth Workshop on Model-Based
Testing (MBT 2014), which was held in Grenoble, France on April 6, 2014 as a
satellite workshop of the European Joint Conferences on Theory and Practice of
Software (ETAPS 2014).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0308008v1,A Grid Based Architecture for High-Performance NLP,"We describe the design and early implementation of an extensible,
component-based software architecture for natural language engineering
applications which interfaces with high performance distributed computing
services. The architecture leverages existing linguistic resource description
and discovery mechanisms based on metadata descriptions, combining these in a
compatible fashion with other software definition abstractions. Within this
architecture, application design is highly flexible, allowing disparate
components to be combined to suit the overall application functionality, and
formally described independently of processing concerns. An application
specification language provides abstraction from the programming environment
and allows ease of interface with high performance computational grids via a
broker.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1108.0463v1,Innocent strategies as presheaves and interactive equivalences for CCS,"Seeking a general framework for reasoning about and comparing programming
languages, we derive a new view of Milner's CCS. We construct a category E of
plays, and a subcategory V of views. We argue that presheaves on V adequately
represent innocent strategies, in the sense of game semantics. We then equip
innocent strategies with a simple notion of interaction. This results in an
interpretation of CCS.
  Based on this, we propose a notion of interactive equivalence for innocent
strategies, which is close in spirit to Beffara's interpretation of testing
equivalences in concurrency theory. In this framework we prove that the
analogues of fair and must testing equivalences coincide, while they differ in
the standard setting.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1406.2812v1,"On the Development of Methodology for Planning and Cost-Modeling of a
  Wide Area Network","The most important stages in designing a computer network in a wider
geographical area include: definition of requirements, topological description,
identification and calculation of relevant parameters (i.e. traffic matrix),
determining the shortest path between nodes, quantification of the effect of
various levels of technical and technological development of urban areas
involved, the cost of technology, and the cost of services. These parameters
differ for WAN networks in different regions - their calculation depends
directly on the data ""in the field"": number of inhabitants, distance between
populated areas, network traffic density, as well as available bandwidth. The
main reason for identification and evaluation of these parameters is to develop
a model that could meet the constraints imposed by potential beneficiaries. In
this paper, we develop a methodology for planning and cost-modeling of a wide
area network and validate it in a case study, under the supposition that
behavioral interactions of individuals and groups play a significant role and
have to be taken into consideration by employing either simple or composite
indicators of socioeconomic status.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.09104v2,"Completely Reachable Automata, Primitive Groups and the State Complexity
  of the Set of Synchronizing Words","We give a new characterization of primitive permutation groups tied to the
notion of completely reachable automata. Also, we introduce sync-maximal
permutation groups tied to the state complexity of the set of synchronizing
words of certain associated automata and show that they are contained between
the $2$-homogeneous and the primitive groups. Lastly, we define $k$-reachable
groups in analogy with synchronizing groups and motivated by our
characterization of primitive permutation groups. But the results show that a
$k$-reachable permutation group of degree $n$ with $6 \le k \le n - 6$ is
either the alternating or the symmetric group.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.01413v1,Classes of intersection digraphs with good algorithmic properties,"An intersection digraph is a digraph where every vertex $v$ is represented by
an ordered pair $(S_v, T_v)$ of sets such that there is an edge from $v$ to $w$
if and only if $S_v$ and $T_w$ intersect. An intersection digraph is reflexive
if $S_v\cap T_v\neq \emptyset$ for every vertex $v$. Compared to well-known
undirected intersection graphs like interval graphs and permutation graphs, not
many algorithmic applications on intersection digraphs have been developed.
Motivated by the successful story on algorithmic applications of intersection
graphs using a graph width parameter called mim-width, we introduce its
directed analogue called `bi-mim-width' and prove that various classes of
reflexive intersection digraphs have bounded bi-mim-width. In particular, we
show that as a natural extension of $H$-graphs, reflexive $H$-digraphs have
linear bi-mim-width at most $12|E(H)|$, which extends a bound on the linear
mim-width of $H$-graphs [On the Tractability of Optimization Problems on
$H$-Graphs. Algorithmica 2020]. For applications, we introduce a novel
framework of directed versions of locally checkable problems, that streamlines
the definitions and the study of many problems in the literature and
facilitates their common algorithmic treatment. We obtain unified
polynomial-time algorithms for these problems on digraphs of bounded
bi-mim-width, when a branch decomposition is given. Locally checkable problems
include Kernel, Dominating Set, and Directed $H$-Homomorphism.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0006010v1,"Light Affine Logic (Proof Nets, Programming Notation, P-Time Correctness
  and Completeness)","This paper is a structured introduction to Light Affine Logic, and to its
intuitionistic fragment. Light Affine Logic has a polynomially costing cut
elimination (P-Time correctness), and encodes all P-Time Turing machines
(P-Time completeness). P-Time correctness is proved by introducing the Proof
nets for Intuitionistic Light Affine Logic. P-Time completeness is demonstrated
in full details thanks to a very compact program notation. On one side, the
proof of P-Time correctness describes how the complexity of cut elimination is
controlled, thanks to a suitable cut elimination strategy that exploits
structural properties of the Proof nets. This allows to have a good catch on
the meaning of the ``paragraph'' modality, which is a peculiarity of light
logics. On the other side, the proof of P-Time completeness, together with a
lot of programming examples, gives a flavor of the non trivial task of
programming with resource limitations, using Intuitionistic Light Affine Logic
derivations as programs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.13726v1,Exposing Paid Opinion Manipulation Trolls,"Recently, Web forums have been invaded by opinion manipulation trolls. Some
trolls try to influence the other users driven by their own convictions, while
in other cases they can be organized and paid, e.g., by a political party or a
PR agency that gives them specific instructions what to write. Finding paid
trolls automatically using machine learning is a hard task, as there is no
enough training data to train a classifier; yet some test data is possible to
obtain, as these trolls are sometimes caught and widely exposed. In this paper,
we solve the training data problem by assuming that a user who is called a
troll by several different people is likely to be such, and one who has never
been called a troll is unlikely to be such. We compare the profiles of (i) paid
trolls vs. (ii)""mentioned"" trolls vs. (iii) non-trolls, and we further show
that a classifier trained to distinguish (ii) from (iii) does quite well also
at telling apart (i) from (iii).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1312.6668v4,A pumping lemma for non-cooperative self-assembly,"We prove the computational weakness of a model of tile assembly that has so
far resisted many attempts of formal analysis or positive constructions.
Specifically, we prove that, in Winfree's abstract Tile Assembly Model, when
restricted to use only noncooperative bindings, any long enough path that can
grow in all terminal assemblies is pumpable, meaning that this path can be
extended into an infinite, ultimately periodic path.
  This result can be seen as a geometric generalization of the pumping lemma of
finite state automata, and closes the question of what can be computed
deterministically in this model. Moreover, this question has motivated the
development of a new method called visible glues. We believe that this method
can also be used to tackle other long-standing problems in computational
geometry, in relation for instance with self-avoiding paths.
  Tile assembly (including non-cooperative tile assembly) was originally
introduced by Winfree and Rothemund in STOC 2000 to understand how to program
shapes. The non-cooperative variant, also known as temperature 1 tile assembly,
is the model where tiles are allowed to bind as soon as they match on one side,
whereas in cooperative tile assembly, some tiles need to match on several sides
in order to bind. In this work, we prove that only very simple shapes can
indeed be programmed, whereas exactly one known result (SODA 2014) showed a
restriction on the assemblies general non-cooperative self-assembly could
achieve, without any implication on its computational expressiveness. With
non-square tiles (like polyominos, SODA 2015), other recent works have shown
that the model quickly becomes computationally powerful.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1209.5473v1,Some characteristics of matroids through rough sets,"At present, practical application and theoretical discussion of rough sets
are two hot problems in computer science. The core concepts of rough set theory
are upper and lower approximation operators based on equivalence relations.
Matroid, as a branch of mathematics, is a structure that generalizes linear
independence in vector spaces. Further, matroid theory borrows extensively from
the terminology of linear algebra and graph theory. We can combine rough set
theory with matroid theory through using rough sets to study some
characteristics of matroids. In this paper, we apply rough sets to matroids
through defining a family of sets which are constructed from the upper
approximation operator with respect to an equivalence relation. First, we prove
the family of sets satisfies the support set axioms of matroids, and then we
obtain a matroid. We say the matroids induced by the equivalence relation and a
type of matroid, namely support matroid, is induced. Second, through rough
sets, some characteristics of matroids such as independent sets, support sets,
bases, hyperplanes and closed sets are investigated.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.13628v1,"Distributed algorithms for covering, packing and maximum weighted
  matching","This paper gives poly-logarithmic-round, distributed D-approximation
algorithms for covering problems with submodular cost and monotone covering
constraints (Submodular-cost Covering). The approximation ratio D is the
maximum number of variables in any constraint. Special cases include Covering
Mixed Integer Linear Programs (CMIP), and Weighted Vertex Cover (with D=2). Via
duality, the paper also gives poly-logarithmic-round, distributed
D-approximation algorithms for Fractional Packing linear programs (where D is
the maximum number of constraints in which any variable occurs), and for Max
Weighted c-Matching in hypergraphs (where D is the maximum size of any of the
hyperedges; for graphs D=2). The paper also gives parallel (RNC)
2-approximation algorithms for CMIP with two variables per constraint and
Weighted Vertex Cover. The algorithms are randomized. All of the approximation
ratios exactly match those of comparable centralized algorithms.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.08802v2,"Proceedings of the 3rd International Workshop on Overlay Architectures
  for FPGAs (OLAF 2017)","The 3rd International Workshop on Overlay Architectures for FPGAs (OLAF 2017)
was held on 22 Feb, 2017 as a co-located workshop at the 25th ACM/SIGDA
International Symposium on Field-Programmable Gate Arrays (FPGA 2017). This
year, the program committee selected 3 papers and 3 extended abstracts to be
presented at the workshop, which are subsequently collected in this online
volume.",0,0,0,0,0,1,1,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1909.03459v1,Blind Geometric Distortion Correction on Images Through Deep Learning,"We propose the first general framework to automatically correct different
types of geometric distortion in a single input image. Our proposed method
employs convolutional neural networks (CNNs) trained by using a large synthetic
distortion dataset to predict the displacement field between distorted images
and corrected images. A model fitting method uses the CNN output to estimate
the distortion parameters, achieving a more accurate prediction. The final
corrected image is generated based on the predicted flow using an efficient,
high-quality resampling method. Experimental results demonstrate that our
algorithm outperforms traditional correction methods, and allows for
interesting applications such as distortion transfer, distortion exaggeration,
and co-occurring distortion correction.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.02365v1,Finding Optimal Sinks for Random Walkers in a Network,"In a model of network communication based on a random walk in an undirected
graph, what subset of nodes (subject to constraints on the set size), enables
the fastest spread of information? In this paper, we assume the dynamics of
spread is described by a network consensus process, but to find the most
effective seeds we consider the target set of a random walk--the process dual
to network consensus spread. Thus an optimal set $A$ minimizes the sum of the
expected first hitting times $F(A)$ of random walks that start at nodes outside
the set. We introduce a submodular, non-decreasing rank function $\rho$, that
permits some comparison between the solution obtained by the classical greedy
algorithm and one obtained by our methods. The supermodularity and
non-increasing properties of $F$ are used to show that the rank of our solution
is at least $(1-\frac{1}{e})$ times the rank of the optimal set. When our
approximation has a higher rank than the greedy solution, this can be improved
to $(1-\frac{1}{e})(1+\chi)$ where $\chi >0$ is a constant. A non-zero lower
bound for $\chi$ can be obtained when the curvature and increments of $\rho$
are known.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0011023v1,Optimal Bidding Algorithms Against Cheating in Multiple-Object Auctions,"This paper studies some basic problems in a multiple-object auction model
using methodologies from theoretical computer science. We are especially
concerned with situations where an adversary bidder knows the bidding
algorithms of all the other bidders. In the two-bidder case, we derive an
optimal randomized bidding algorithm, by which the disadvantaged bidder can
procure at least half of the auction objects despite the adversary's a priori
knowledge of his algorithm. In the general $k$-bidder case, if the number of
objects is a multiple of $k$, an optimal randomized bidding algorithm is found.
If the $k-1$ disadvantaged bidders employ that same algorithm, each of them can
obtain at least $1/k$ of the objects regardless of the bidding algorithm the
adversary uses. These two algorithms are based on closed-form solutions to
certain multivariate probability distributions. In situations where a
closed-form solution cannot be obtained, we study a restricted class of bidding
algorithms as an approximation to desired optimal algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0009001v3,Complexity analysis for algorithmically simple strings,"Given a reference computer, Kolmogorov complexity is a well defined function
on all binary strings. In the standard approach, however, only the asymptotic
properties of such functions are considered because they do not depend on the
reference computer. We argue that this approach can be more useful if it is
refined to include an important practical case of simple binary strings.
Kolmogorov complexity calculus may be developed for this case if we restrict
the class of available reference computers. The interesting problem is to
define a class of computers which is restricted in a {\it natural} way modeling
the real-life situation where only a limited class of computers is physically
available to us. We give an example of what such a natural restriction might
look like mathematically, and show that under such restrictions some error
terms, even logarithmic in complexity, can disappear from the standard
complexity calculus.
  Keywords: Kolmogorov complexity; Algorithmic information theory.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1707.05227v1,Auxiliary Objectives for Neural Error Detection Models,"We investigate the utility of different auxiliary objectives and training
strategies within a neural sequence labeling approach to error detection in
learner writing. Auxiliary costs provide the model with additional linguistic
information, allowing it to learn general-purpose compositional features that
can then be exploited for other objectives. Our experiments show that a joint
learning approach trained with parallel labels on in-domain data improves
performance over the previous best error detection system. While the resulting
model has the same number of parameters, the additional objectives allow it to
be optimised more efficiently and achieve better performance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1407.4409v1,SoundLoc: Acoustic Method for Indoor Localization without Infrastructure,"Identifying locations of occupants is beneficial to energy management in
buildings. A key observation in indoor environment is that distinct functional
areas are typically controlled by separate HVAC and lighting systems and room
level localization is sufficient to provide a powerful tool for energy usage
reduction by occupancy-based actuation of the building facilities. Based upon
this observation, this paper focuses on identifying the room where a person or
a mobile device is physically present. Existing room localization methods,
however, require special infrastructure to annotate rooms.
  SoundLoc is a room-level localization system that exploits the intrinsic
acoustic properties of individual rooms and obviates the needs for
infrastructures. As we show in the study, rooms' acoustic properties can be
characterized by Room Impulse Response (RIR). Nevertheless, obtaining precise
RIRs is a time-consuming and expensive process. The main contributions of our
work are the following. First, a cost-effective RIR measurement system is
implemented and the Noise Adaptive Extraction of Reverberation (NAER) algorithm
is developed to estimate room acoustic parameters in noisy conditions. Second,
a comprehensive physical and statistical analysis of features extracted from
RIRs is performed. Also, SoundLoc is evaluated using the dataset consisting of
ten (10) different rooms. The overall accuracy of 97.8% achieved demonstrates
the potential to be integrated into automatic mapping of building space.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1908.08787v1,Swarm Intelligence for Morphogenetic Engineering,"We argue that embryological morphogenesis provides a model of how massive
swarms of microscopic agents can be coordinated to assemble complex, multiscale
hierarchical structures. This is accomplished by understanding natural
morphogenetic processes in mathematical terms, abstracting from the biological
specifics, and implementing these mathematical principles in artificial
systems. We have developed a notation based on partial differential equations
for artificial morphogenesis and have designed a prototype morphogenetic
programming language, which permits precise description of morphogenetic
algorithms and their automatic translation to simulation software.
Morphogenetic programming is illustrated by two examples: (1) use of a modified
flocking algorithm to route dense fiber bundles between regions of an
artificial cortex while avoiding other bundles; (2) use of the
clock-and-wavefront model of spinal segmentation for the assembly of the
segmented spine of an insect-like robot body and for assembling segmented legs
on the robot's spine. Finally, we show how a variation of smoothed particle
hydrodynamics (SPH) swarm robotic control can be applied to the global-to-local
compilation problem, that is, the derivation of individual agent control from
global PDE specifications.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.07780v1,"Linear kernels for edge deletion problems to immersion-closed graph
  classes","Suppose $\mathcal{F}$ is a finite family of graphs. We consider the following
meta-problem, called $\mathcal{F}$-Immersion Deletion: given a graph $G$ and
integer $k$, decide whether the deletion of at most $k$ edges of $G$ can result
in a graph that does not contain any graph from $\mathcal{F}$ as an immersion.
This problem is a close relative of the $\mathcal{F}$-Minor Deletion problem
studied by Fomin et al. [FOCS 2012], where one deletes vertices in order to
remove all minor models of graphs from $\mathcal{F}$.
  We prove that whenever all graphs from $\mathcal{F}$ are connected and at
least one graph of $\mathcal{F}$ is planar and subcubic, then the
$\mathcal{F}$-Immersion Deletion problem admits: a constant-factor
approximation algorithm running in time $O(m^3 \cdot n^3 \cdot \log m)$; a
linear kernel that can be computed in time $O(m^4 \cdot n^3 \cdot \log m)$; and
a $O(2^{O(k)} + m^4 \cdot n^3 \cdot \log m)$-time fixed-parameter algorithm,
where $n,m$ count the vertices and edges of the input graph.
  These results mirror the findings of Fomin et al. [FOCS 2012], who obtained a
similar set of algorithmic results for $\mathcal{F}$-Minor Deletion, under the
assumption that at least one graph from $\mathcal{F}$ is planar. An important
difference is that we are able to obtain a linear kernel for
$\mathcal{F}$-Immersion Deletion, while the exponent of the kernel of Fomin et
al. for $\mathcal{F}$-Minor Deletion depends heavily on the family
$\mathcal{F}$. In fact, this dependence is unavoidable under plausible
complexity assumptions, as proven by Giannopoulou et al. [ICALP 2015]. This
reveals that the kernelization complexity of $\mathcal{F}$-Immersion Deletion
is quite different than that of $\mathcal{F}$-Minor Deletion.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1808.00121v1,"Social Robots for People with Developmental Disabilities: A User Study
  on Design Features of a Graphical User Interface","Social robots, also known as service or assistant robots, have been developed
to improve the quality of human life in recent years. The design of socially
capable and intelligent robots can vary, depending on the target user groups.
In this work, we assess the effect of social robots' roles, functions, and
communication approaches in the context of a social agent providing service or
entertainment to users with developmental disabilities. In this paper, we
describe an exploratory study of interface design for a social robot that
assists people suffering from developmental disabilities. We developed series
of prototypes and tested one in a user study that included three residents with
various function levels. This entire study had been recorded for the following
qualitative data analysis. Results show that each design factor played a
different role in delivering information and in increasing engagement. We also
note that some of the fundamental design principles that would work for
ordinary users did not apply to our target user group. We conclude that social
robots could benefit our target users, and acknowledge that these robots were
not suitable for certain scenarios based on the feedback from our users.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.02785v2,"What Makes a Top-Performing Precision Medicine Search Engine? Tracing
  Main System Features in a Systematic Way","From 2017 to 2019 the Text REtrieval Conference (TREC) held a challenge task
on precision medicine using documents from medical publications (PubMed) and
clinical trials. Despite lots of performance measurements carried out in these
evaluation campaigns, the scientific community is still pretty unsure about the
impact individual system features and their weights have on the overall system
performance. In order to overcome this explanatory gap, we first determined
optimal feature configurations using the Sequential Model-based Algorithm
Configuration (SMAC) program and applied its output to a BM25-based search
engine. We then ran an ablation study to systematically assess the individual
contributions of relevant system features: BM25 parameters, query type and
weighting schema, query expansion, stop word filtering, and keyword boosting.
For evaluation, we employed the gold standard data from the three TREC-PM
installments to evaluate the effectiveness of different features using the
commonly shared infNDCG metric.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1006.1881v1,Mix and Match,"Consider a matching problem on a graph where disjoint sets of vertices are
privately owned by self-interested agents. An edge between a pair of vertices
indicates compatibility and allows the vertices to match. We seek a mechanism
to maximize the number of matches despite self-interest, with agents that each
want to maximize the number of their own vertices that match. Each agent can
choose to hide some of its vertices, and then privately match the hidden
vertices with any of its own vertices that go unmatched by the mechanism. A
prominent application of this model is to kidney exchange, where agents
correspond to hospitals and vertices to donor-patient pairs. Here hospitals may
game an exchange by holding back pairs and harm social welfare. In this paper
we seek to design mechanisms that are strategyproof, in the sense that agents
cannot benefit from hiding vertices, and approximately maximize efficiency,
i.e., produce a matching that is close in cardinality to the maximum
cardinality matching. Our main result is the design and analysis of the
eponymous Mix-and-Match mechanism; we show that this randomized mechanism is
strategyproof and provides a 2-approximation. Lower bounds establish that the
mechanism is near optimal.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1302.3912v1,"An Online Environment for Democratic Deliberation: Motivations,
  Principles, and Design","We have created a platform for online deliberation called Deme (which rhymes
with 'team'). Deme is designed to allow groups of people to engage in
collaborative drafting, focused discussion, and decision making using the
Internet. The Deme project has evolved greatly from its beginning in 2003. This
chapter outlines the thinking behind Deme's initial design: our motivations for
creating it, the principles that guided its construction, and its most
important design features. The version of Deme described here was written in
PHP and was deployed in 2004 and used by several groups (including organizers
of the 2005 Online Deliberation Conference). Other papers describe later
developments in the Deme project (see Davies et al. 2005, 2008; Davies and
Mintz 2009).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1202.1639v1,"FastSIR Algorithm: A Fast Algorithm for simulation of epidemic spread in
  large networks by using SIR compartment model","The epidemic spreading on arbitrary complex networks is studied in SIR
(Susceptible Infected Recovered) compartment model. We propose our
implementation of a Naive SIR algorithm for epidemic simulation spreading on
networks that uses data structures efficiently to reduce running time. The
Naive SIR algorithm models full epidemic dynamics and can be easily upgraded to
parallel version. We also propose novel algorithm for epidemic simulation
spreading on networks called the FastSIR algorithm that has better average case
running time than the Naive SIR algorithm. The FastSIR algorithm uses novel
approach to reduce average case running time by constant factor by using
probability distributions of the number of infected nodes. Moreover, the
FastSIR algorithm does not follow epidemic dynamics in time, but still captures
all infection transfers. Furthermore, we also propose an efficient recursive
method for calculating probability distributions of the number of infected
nodes. Average case running time of both algorithms has also been derived and
experimental analysis was made on five different empirical complex networks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0912.1902v1,"Strong, Weak and Branching Bisimulation for Transition Systems and
  Markov Reward Chains: A Unifying Matrix Approach","We first study labeled transition systems with explicit successful
termination. We establish the notions of strong, weak, and branching
bisimulation in terms of boolean matrix theory, introducing thus a novel and
powerful algebraic apparatus. Next we consider Markov reward chains which are
standardly presented in real matrix theory. By interpreting the obtained matrix
conditions for bisimulations in this setting, we automatically obtain the
definitions of strong, weak, and branching bisimulation for Markov reward
chains. The obtained strong and weak bisimulations are shown to coincide with
some existing notions, while the obtained branching bisimulation is new, but
its usefulness is questionable.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1411.6275v1,"Detection of Non-Stationary Photometric Perturbations on Projection
  Screens","Interfaces based on projection screens have become increasingly more popular
in recent years, mainly due to the large screen size and resolution that they
provide, as well as their stereo-vision capabilities. This work shows a local
method for real-time detection of non-stationary photometric perturbations in
projected images by means of computer vision techniques. The method is based on
the computation of differences between the images in the projector's frame
buffer and the corresponding images on the projection screen observed by the
camera. It is robust under spatial variations in the intensity of light emitted
by the projector on the projection surface and also robust under stationary
photometric perturbations caused by external factors. Moreover, we describe the
experiments carried out to show the reliability of the method.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0005001v1,Robustness of Regional Matching Scheme over Global Matching Scheme,"The paper has established and verified the theory prevailing widely among
image and pattern recognition specialists that the bottom-up indirect regional
matching process is the more stable and the more robust than the global
matching process against concentrated types of noise represented by clutter,
outlier or occlusion in the imagery. We have demonstrated this by analyzing the
effect of concentrated noise on a typical decision making process of a
simplified two candidate voting model where our theorem establishes the lower
bounds to a critical breakdown point of election (or decision) result by the
bottom-up matching process are greater than the exact bound of the global
matching process implying that the former regional process is capable of
accommodating a higher level of noise than the latter global process before the
result of decision overturns. We present a convincing experimental verification
supporting not only the theory by a white-black flag recognition problem in the
presence of localized noise but also the validity of the conjecture by a facial
recognition problem that the theorem remains valid for other decision making
processes involving an important dimension-reducing transform such as principal
component analysis or a Gabor transform.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1409.0575v3,ImageNet Large Scale Visual Recognition Challenge,"The ImageNet Large Scale Visual Recognition Challenge is a benchmark in
object category classification and detection on hundreds of object categories
and millions of images. The challenge has been run annually from 2010 to
present, attracting participation from more than fifty institutions.
  This paper describes the creation of this benchmark dataset and the advances
in object recognition that have been possible as a result. We discuss the
challenges of collecting large-scale ground truth annotation, highlight key
breakthroughs in categorical object recognition, provide a detailed analysis of
the current state of the field of large-scale image classification and object
detection, and compare the state-of-the-art computer vision accuracy with human
accuracy. We conclude with lessons learned in the five years of the challenge,
and propose future directions and improvements.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1202.6641v2,Search versus Decision for Election Manipulation Problems,"Most theoretical definitions about the complexity of manipulating elections
focus on the decision problem of recognizing which instances can be
successfully manipulated, rather than the search problem of finding the
successful manipulative actions. Since the latter is a far more natural goal
for manipulators, that definitional focus may be misguided if these two
complexities can differ. Our main result is that they probably do differ: If
integer factoring is hard, then for election manipulation, election bribery,
and some types of election control, there are election systems for which
recognizing which instances can be successfully manipulated is in polynomial
time but producing the successful manipulations cannot be done in polynomial
time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0903.0173v1,Optimal Interdiction of Unreactive Markovian Evaders,"The interdiction problem arises in a variety of areas including military
logistics, infectious disease control, and counter-terrorism. In the typical
formulation of network interdiction, the task of the interdictor is to find a
set of edges in a weighted network such that the removal of those edges would
maximally increase the cost to an evader of traveling on a path through the
network.
  Our work is motivated by cases in which the evader has incomplete information
about the network or lacks planning time or computational power, e.g. when
authorities set up roadblocks to catch bank robbers, the criminals do not know
all the roadblock locations or the best path to use for their escape.
  We introduce a model of network interdiction in which the motion of one or
more evaders is described by Markov processes and the evaders are assumed not
to react to interdiction decisions. The interdiction objective is to find an
edge set of size B, that maximizes the probability of capturing the evaders.
  We prove that similar to the standard least-cost formulation for
deterministic motion this interdiction problem is also NP-hard. But unlike that
problem our interdiction problem is submodular and the optimal solution can be
approximated within 1-1/e using a greedy algorithm. Additionally, we exploit
submodularity through a priority evaluation strategy that eliminates the linear
complexity scaling in the number of network edges and speeds up the solution by
orders of magnitude. Taken together the results bring closer the goal of
finding realistic solutions to the interdiction problem on global-scale
networks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.11208v2,An Adaptive Solver for Systems of Linear Equations,"Computational implementations for solving systems of linear equations often
rely on a one-size-fits-all approach based on LU decomposition of dense
matrices stored in column-major format. Such solvers are typically implemented
with the aid of the xGESV set of functions available in the low-level LAPACK
software, with the aim of reducing development time by taking advantage of
well-tested routines. However, this straightforward approach does not take into
account various matrix properties which can be exploited to reduce the
computational effort and/or to increase numerical stability. Furthermore,
direct use of LAPACK functions can be error-prone for non-expert users and
results in source code that has little resemblance to originating mathematical
expressions. We describe an adaptive solver that we have implemented inside
recent versions of the high-level Armadillo C++ library for linear algebra. The
solver automatically detects several common properties of a given system
(banded, triangular, symmetric positive definite), followed by solving the
system via mapping to a set of suitable LAPACK functions best matched to each
property. The solver also detects poorly conditioned systems and automatically
seeks a solution via singular value decomposition as a fallback. We show that
the adaptive solver leads to notable speedups, while also freeing the user from
using direct calls to cumbersome LAPACK functions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.06765v2,"libdlr: Efficient imaginary time calculations using the discrete Lehmann
  representation","We introduce libdlr, a library implementing the recently introduced discrete
Lehmann representation (DLR) of imaginary time Green's functions. The DLR basis
consists of a collection of exponentials chosen by the interpolative
decomposition to ensure stable and efficient recovery of Green's functions from
imaginary time or Matsbuara frequency samples. The library provides subroutines
to build the DLR basis and grids, and to carry out various standard operations.
The simplicity of the DLR makes it straightforward to incorporate into existing
codes as a replacement for less efficient representations of imaginary time
Green's functions, and libdlr is intended to facilitate this process. libdlr is
written in Fortran, and contains a Python module pydlr. We also introduce a
stand-alone Julia implementation, Lehmann.jl.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.05069v1,A Non-Negative Matrix Factorization Game,"We present a novel game-theoretic formulation of Non-Negative Matrix
Factorization (NNMF), a popular data-analysis method with many scientific and
engineering applications. The game-theoretic formulation is shown to have
favorable scaling and parallelization properties, while retaining
reconstruction and convergence performance comparable to the traditional
Multiplicative Updates algorithm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1409.0085v1,"Designing Path Planning Algorithms for Mobile Anchor towards Range-Free
  Localization","Localization is one of the most important factor in wireless sensor networks
as many applications demand position information of sensors. Recently there is
an increasing interest on the use of mobile anchors for localizing sensors.
Most of the works available in the literature either looks into the aspect of
reducing path length of mobile anchor or tries to increase localization
accuracy. The challenge is to design a movement strategy for a mobile anchor
that reduces path length while meeting the requirements of a good range-free
localization technique. In this paper we propose two cost-effective movement
strategies i.e., path planning for a mobile anchor so that localization can be
done using the localization scheme \cite{Lee2009}. In one strategy we use a
hexagonal movement pattern for the mobile anchor to localize all sensors inside
a bounded rectangular region with lesser movement compared to the existing
works in literature. In other strategy we consider a connected network in an
unbounded region where the mobile anchor moves in the hexagonal pattern to
localize the sensors. In this approach, we guarantee localization of all
sensors within $r/2$ error-bound where $r$ is the communication range of the
mobile anchor and sensors. Our simulation results support theoretical results
along with localization accuracy.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1906.03648v1,LSTM Networks Can Perform Dynamic Counting,"In this paper, we systematically assess the ability of standard recurrent
networks to perform dynamic counting and to encode hierarchical
representations. All the neural models in our experiments are designed to be
small-sized networks both to prevent them from memorizing the training sets and
to visualize and interpret their behaviour at test time. Our results
demonstrate that the Long Short-Term Memory (LSTM) networks can learn to
recognize the well-balanced parenthesis language (Dyck-$1$) and the shuffles of
multiple Dyck-$1$ languages, each defined over different parenthesis-pairs, by
emulating simple real-time $k$-counter machines. To the best of our knowledge,
this work is the first study to introduce the shuffle languages to analyze the
computational power of neural networks. We also show that a single-layer LSTM
with only one hidden unit is practically sufficient for recognizing the
Dyck-$1$ language. However, none of our recurrent networks was able to yield a
good performance on the Dyck-$2$ language learning task, which requires a model
to have a stack-like mechanism for recognition.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.10259v3,Analyzing COVID-19 Tweets with Transformer-based Language Models,"This paper describes a method for using Transformer-based Language Models
(TLMs) to understand public opinion from social media posts. In this approach,
we train a set of GPT models on several COVID-19 tweet corpora that reflect
populations of users with distinctive views. We then use prompt-based queries
to probe these models to reveal insights into the biases and opinions of the
users. We demonstrate how this approach can be used to produce results which
resemble polling the public on diverse social, political and public health
issues. The results on the COVID-19 tweet data show that transformer language
models are promising tools that can help us understand public opinions on
social media at scale.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1409.0166v5,Verifying Procedural Programs via Constrained Rewriting Induction,"This paper aims to develop a verification method for procedural programs via
a transformation into Logically Constrained Term Rewriting Systems (LCTRSs). To
this end, we extend transformation methods based on integer TRSs to handle
arbitrary data types, global variables, function calls and arrays, as well as
encode safety checks. Then we adapt existing rewriting induction methods to
LCTRSs and propose a simple yet effective method to generalize equations. We
show that we can automatically verify memory safety and prove correctness of
realistic functions. Our approach proves equivalence between two
implementations, so in contrast to other works, we do not require an explicit
specification in a separate specification language.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0811.3448v2,Binar Sort: A Linear Generalized Sorting Algorithm,"Sorting is a common and ubiquitous activity for computers. It is not
surprising that there exist a plethora of sorting algorithms. For all the
sorting algorithms, it is an accepted performance limit that sorting algorithms
are linearithmic or O(N lg N). The linearithmic lower bound in performance
stems from the fact that the sorting algorithms use the ordering property of
the data. The sorting algorithm uses comparison by the ordering property to
arrange the data elements from an initial permutation into a sorted
permutation.
  Linear O(N) sorting algorithms exist, but use a priori knowledge of the data
to use a specific property of the data and thus have greater performance. In
contrast, the linearithmic sorting algorithms are generalized by using a
universal property of data-comparison, but have a linearithmic performance
lower bound. The trade-off in sorting algorithms is generality for performance
by the chosen property used to sort the data elements.
  A general-purpose, linear sorting algorithm in the context of the trade-off
of performance for generality at first consideration seems implausible. But,
there is an implicit assumption that only the ordering property is universal.
But, as will be discussed and examined, it is not the only universal property
for data elements. The binar sort is a general-purpose sorting algorithm that
uses this other universal property to sort linearly.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1304.4320v2,"An Algorithm for Computing Constrained Reflection Paths in Simple
  Polygon","Let $s$ be a source point and $t$ be a destination point inside an $n$-vertex
simple polygon $P$. Euclidean shortest paths and minimum-link paths between $s$
and $t$ inside $P$ have been well studied. Both these kinds of paths are simple
and piecewise-convex. However, computing optimal paths in the context of
diffuse or specular reflections does not seem to be an easy task. A path from a
light source $s$ to $t$ inside $P$ is called a diffuse reflection path if the
turning points of the path lie in the interiors of the boundary edges of $P$. A
diffuse reflection path is said to be optimal if it has the minimum number of
turning points amongst all diffuse reflection paths between $s$ and $t$. The
minimum diffuse reflection path may not be simple. The problem of computing the
minimum diffuse reflection path in low degree polynomial time has remained
open.
  In our quest for understanding the geometric structure of the minimum diffuse
reflection paths vis-a-vis shortest paths and minimum link paths, we define a
new kind of diffuse reflection path called a constrained diffuse reflection
path where (i) the path is simple, (ii) it intersects only the eaves of the
Euclidean shortest path between $s$ and $t$, and (iii) it intersects each eave
exactly once. For computing a minimum constrained diffuse reflection path from
$s$ to $t$, we present an $O(n(n+\beta))$ time algorithm, where $\beta =\Theta
(n^2)$ in the worst case. Here, $\beta$ depends on the shape of the polygon. We
also establish some properties relating minimum constrained diffuse reflection
paths and minimum diffuse reflection paths. Constrained diffuse reflection
paths introduced in this paper provide new geometric insights into the hitherto
unknown structures and shapes of optimal reflection paths.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1407.3519v1,"Showing invariance compositionally for a process algebra for network
  protocols","This paper presents the mechanization of a process algebra for Mobile Ad hoc
Networks and Wireless Mesh Networks, and the development of a compositional
framework for proving invariant properties. Mechanizing the core process
algebra in Isabelle/HOL is relatively standard, but its layered structure
necessitates special treatment. The control states of reactive processes, such
as nodes in a network, are modelled by terms of the process algebra. We propose
a technique based on these terms to streamline proofs of inductive invariance.
This is not sufficient, however, to state and prove invariants that relate
states across multiple processes (entire networks). To this end, we propose a
novel compositional technique for lifting global invariants stated at the level
of individual nodes to networks of nodes.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1411.0593v1,Two-Variable Ehrenfeucht-Fraisse Games over Omega-Terms,"Fragments of first-order logic over words can often be characterized in terms
of finite monoids, and identities of omega-terms are an effective mechanism for
specifying classes of monoids. Huschenbett and the first author have shown how
to use infinite Ehrenfeucht-Fraisse games on linear orders for showing that
some given fragment satisfies an identity of omega-terms (STACS 2014). After
revisiting this result, we show that for two-variable logic one can use simpler
linear orders.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.09654v1,"Feature selection in machine learning: R√©nyi min-entropy vs Shannon
  entropy","Feature selection, in the context of machine learning, is the process of
separating the highly predictive feature from those that might be irrelevant or
redundant. Information theory has been recognized as a useful concept for this
task, as the prediction power stems from the correlation, i.e., the mutual
information, between features and labels. Many algorithms for feature selection
in the literature have adopted the Shannon-entropy-based mutual information. In
this paper, we explore the possibility of using R\'enyi min-entropy instead. In
particular, we propose an algorithm based on a notion of conditional R\'enyi
min-entropy that has been recently adopted in the field of security and
privacy, and which is strictly related to the Bayes error. We prove that in
general the two approaches are incomparable, in the sense that we show that we
can construct datasets on which the R\'enyi-based algorithm performs better
than the corresponding Shannon-based one, and datasets on which the situation
is reversed. In practice, however, when considering datasets of real data, it
seems that the R\'enyi-based algorithm tends to outperform the other one. We
have effectuate several experiments on the BASEHOCK, SEMEION, and GISETTE
datasets, and in all of them we have indeed observed that the R\'enyi-based
algorithm gives better results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1410.2652v1,More Natural Models of Electoral Control by Partition,"""Control"" studies attempts to set the outcome of elections through the
addition, deletion, or partition of voters or candidates. The set of benchmark
control types was largely set in the seminal 1992 paper by Bartholdi, Tovey,
and Trick that introduced control, and there now is a large literature studying
how many of the benchmark types various election systems are vulnerable to,
i.e., have polynomial-time attack algorithms for.
  However, although the longstanding benchmark models of addition and deletion
model relatively well the real-world settings that inspire them, the
longstanding benchmark models of partition model settings that are arguably
quite distant from those they seek to capture.
  In this paper, we introduce--and for some important cases analyze the
complexity of--new partition models that seek to better capture many real-world
partition settings. In particular, in many partition settings one wants the two
parts of the partition to be of (almost) equal size, or is partitioning into
more than two parts, or has groups of actors who must be placed in the same
part of the partition. Our hope is that having these new partition types will
allow studies of control attacks to include such models that more realistically
capture many settings.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0004008v1,"How to Evaluate your Question Answering System Every Day and Still Get
  Real Work Done","In this paper, we report on Qaviar, an experimental automated evaluation
system for question answering applications. The goal of our research was to
find an automatically calculated measure that correlates well with human
judges' assessment of answer correctness in the context of question answering
tasks. Qaviar judges the response by computing recall against the stemmed
content words in the human-generated answer key. It counts the answer correct
if it exceeds agiven recall threshold. We determined that the answer
correctness predicted by Qaviar agreed with the human 93% to 95% of the time.
41 question-answering systems were ranked by both Qaviar and human assessors,
and these rankings correlated with a Kendall's Tau measure of 0.920, compared
to a correlation of 0.956 between human assessors on the same data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.13510v3,"Synthetic Observational Health Data with GANs: from slow adoption to a
  boom in medical research and ultimately digital twins?","After being collected for patient care, Observational Health Data (OHD) can
further benefit patient well-being by sustaining the development of health
informatics and medical research. Vast potential is unexploited because of the
fiercely private nature of patient-related data and regulations to protect it.
  Generative Adversarial Networks (GANs) have recently emerged as a
groundbreaking way to learn generative models that produce realistic synthetic
data. They have revolutionized practices in multiple domains such as
self-driving cars, fraud detection, digital twin simulations in industrial
sectors, and medical imaging.
  The digital twin concept could readily apply to modelling and quantifying
disease progression. In addition, GANs posses many capabilities relevant to
common problems in healthcare: lack of data, class imbalance, rare diseases,
and preserving privacy. Unlocking open access to privacy-preserving OHD could
be transformative for scientific research. In the midst of COVID-19, the
healthcare system is facing unprecedented challenges, many of which of are data
related for the reasons stated above.
  Considering these facts, publications concerning GAN applied to OHD seemed to
be severely lacking. To uncover the reasons for this slow adoption, we broadly
reviewed the published literature on the subject. Our findings show that the
properties of OHD were initially challenging for the existing GAN algorithms
(unlike medical imaging, for which state-of-the-art model were directly
transferable) and the evaluation synthetic data lacked clear metrics.
  We find more publications on the subject than expected, starting slowly in
2017, and since then at an increasing rate. The difficulties of OHD remain, and
we discuss issues relating to evaluation, consistency, benchmarking, data
modelling, and reproducibility.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1305.1459v1,"EURETILE 2010-2012 summary: first three years of activity of the
  European Reference Tiled Experiment","This is the summary of first three years of activity of the EURETILE FP7
project 247846. EURETILE investigates and implements brain-inspired and
fault-tolerant foundational innovations to the system architecture of massively
parallel tiled computer architectures and the corresponding programming
paradigm. The execution targets are a many-tile HW platform, and a many-tile
simulator. A set of SW process - HW tile mapping candidates is generated by the
holistic SW tool-chain using a combination of analytic and bio-inspired
methods. The Hardware dependent Software is then generated, providing OS
services with maximum efficiency/minimal overhead. The many-tile simulator
collects profiling data, closing the loop of the SW tool chain. Fine-grain
parallelism inside processes is exploited by optimized intra-tile compilation
techniques, but the project focus is above the level of the elementary tile.
The elementary HW tile is a multi-processor, which includes a fault tolerant
Distributed Network Processor (for inter-tile communication) and ASIP
accelerators. Furthermore, EURETILE investigates and implements the innovations
for equipping the elementary HW tile with high-bandwidth, low-latency
brain-like inter-tile communication emulating 3 levels of connection hierarchy,
namely neural columns, cortical areas and cortex, and develops a dedicated
cortical simulation benchmark: DPSNN-STDP (Distributed Polychronous Spiking
Neural Net with synaptic Spiking Time Dependent Plasticity). EURETILE leverages
on the multi-tile HW paradigm and SW tool-chain developed by the FET-ACA SHAPES
Integrated Project (2006-2009).",0,0,0,0,0,0,0,1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1205.2726v1,Non-Interactive Differential Privacy: a Survey,"OpenData movement around the globe is demanding more access to information
which lies locked in public or private servers. As recently reported by a
McKinsey publication, this data has significant economic value, yet its release
has potential to blatantly conflict with people privacy. Recent UK government
inquires have shown concern from various parties about publication of
anonymized databases, as there is concrete possibility of user identification
by means of linkage attacks. Differential privacy stands out as a model that
provides strong formal guarantees about the anonymity of the participants in a
sanitized database. Only recent results demonstrated its applicability on
real-life datasets, though. This paper covers such breakthrough discoveries, by
reviewing applications of differential privacy for non-interactive publication
of anonymized real-life datasets. Theory, utility and a data-aware comparison
are discussed on a variety of principles and concrete applications.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1605.02948v3,"Different approaches for identifying important concepts in probabilistic
  biomedical text summarization","Automatic text summarization tools help users in biomedical domain to acquire
their intended information from various textual resources more efficiently.
Some of the biomedical text summarization systems put the basis of their
sentence selection approach on the frequency of concepts extracted from the
input text. However, it seems that exploring other measures rather than the
frequency for identifying the valuable content of the input document, and
considering the correlations existing between concepts may be more useful for
this type of summarization. In this paper, we describe a Bayesian summarizer
for biomedical text documents. The Bayesian summarizer initially maps the input
text to the Unified Medical Language System (UMLS) concepts, then it selects
the important ones to be used as classification features. We introduce
different feature selection approaches to identify the most important concepts
of the text and to select the most informative content according to the
distribution of these concepts. We show that with the use of an appropriate
feature selection approach, the Bayesian biomedical summarizer can improve the
performance of summarization. We perform extensive evaluations on a corpus of
scientific papers in biomedical domain. The results show that the Bayesian
summarizer outperforms the biomedical summarizers that rely on the frequency of
concepts, the domain-independent and baseline methods based on the
Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics. Moreover,
the results suggest that using the meaningfulness measure and considering the
correlations of concepts in the feature selection step lead to a significant
increase in the performance of summarization.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0801.1630v3,Computational Solutions for Today's Navy,"New methods are being employed to meet the Navy's changing
software-development environment.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0801.2226v1,Programming an interpreter using molecular dynamics,"PGA (ProGram Algebra) is an algebra of programs which concerns programs in
their simplest form: sequences of instructions. Molecular dynamics is a simple
model of computation developed in the setting of PGA, which bears on the use of
dynamic data structures in programming. We consider the programming of an
interpreter for a program notation that is close to existing assembly languages
using PGA with the primitives of molecular dynamics as basic instructions. It
happens that, although primarily meant for explaining programming language
features relating to the use of dynamic data structures, the collection of
primitives of molecular dynamics in itself is suited to our programming wants.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.02173v3,Multi-Agent Variational Occlusion Inference Using People as Sensors,"Autonomous vehicles must reason about spatial occlusions in urban
environments to ensure safety without being overly cautious. Prior work
explored occlusion inference from observed social behaviors of road agents,
hence treating people as sensors. Inferring occupancy from agent behaviors is
an inherently multimodal problem; a driver may behave similarly for different
occupancy patterns ahead of them (e.g., a driver may move at constant speed in
traffic or on an open road). Past work, however, does not account for this
multimodality, thus neglecting to model this source of aleatoric uncertainty in
the relationship between driver behaviors and their environment. We propose an
occlusion inference method that characterizes observed behaviors of human
agents as sensor measurements, and fuses them with those from a standard sensor
suite. To capture the aleatoric uncertainty, we train a conditional variational
autoencoder with a discrete latent space to learn a multimodal mapping from
observed driver trajectories to an occupancy grid representation of the view
ahead of the driver. Our method handles multi-agent scenarios, combining
measurements from multiple observed drivers using evidential theory to solve
the sensor fusion problem. Our approach is validated on a cluttered, real-world
intersection, outperforming baselines and demonstrating real-time capable
performance. Our code is available at
https://github.com/sisl/MultiAgentVariationalOcclusionInference .",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1510.03895v2,A faster subquadratic algorithm for finding outlier correlations,"We study the problem of detecting outlier pairs of strongly correlated
variables among a collection of $n$ variables with otherwise weak pairwise
correlations. After normalization, this task amounts to the geometric task
where we are given as input a set of $n$ vectors with unit Euclidean norm and
dimension $d$, and for some constants $0<\tau<\rho<1$, we are asked to find all
the outlier pairs of vectors whose inner product is at least $\rho$ in absolute
value, subject to the promise that all but at most $q$ pairs of vectors have
inner product at most $\tau$ in absolute value.
  Improving on an algorithm of G. Valiant [FOCS 2012; J. ACM 2015], we present
a randomized algorithm that for Boolean inputs ($\{-1,1\}$-valued data
normalized to unit Euclidean length) runs in time \[ \tilde
O\bigl(n^{\max\,\{1-\gamma+M(\Delta\gamma,\gamma),\,M(1-\gamma,2\Delta\gamma)\}}+qdn^{2\gamma}\bigr)\,,
\] where $0<\gamma<1$ is a constant tradeoff parameter and $M(\mu,\nu)$ is the
exponent to multiply an $\lfloor n^\mu\rfloor\times\lfloor n^\nu\rfloor$ matrix
with an $\lfloor n^\nu\rfloor\times \lfloor n^\mu\rfloor$ matrix and
$\Delta=1/(1-\log_\tau\rho)$. As corollaries we obtain randomized algorithms
that run in time \[ \tilde
O\bigl(n^{\frac{2\omega}{3-\log_\tau\rho}}+qdn^{\frac{2(1-\log_\tau\rho)}{3-\log_\tau\rho}}\bigr)
\] and in time \[ \tilde
O\bigl(n^{\frac{4}{2+\alpha(1-\log_\tau\rho)}}+qdn^{\frac{2\alpha(1-\log_\tau\rho)}{2+\alpha(1-\log_\tau\rho)}}\bigr)\,,
\] where $2\leq\omega<2.38$ is the exponent for square matrix multiplication
and $0.3<\alpha\leq 1$ is the exponent for rectangular matrix multiplication.
The notation $\tilde O(\cdot)$ hides polylogarithmic factors in $n$ and $d$
whose degree may depend on $\rho$ and $\tau$. We present further corollaries
for the light bulb problem and for learning sparse Boolean functions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9910013v1,Map Graphs,"We consider a modified notion of planarity, in which two nations of a map are
considered adjacent when they share any point of their boundaries (not
necessarily an edge, as planarity requires). Such adjacencies define a map
graph. We give an NP characterization for such graphs, and a cubic time
recognition algorithm for a restricted version: given a graph, decide whether
it is realized by adjacencies in a map without holes, in which at most four
nations meet at any point.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1510.02163v1,"Performance Analysis of an Astrophysical Simulation Code on the Intel
  Xeon Phi Architecture","We have developed the astrophysical simulation code XFLAT to study neutrino
oscillations in supernovae. XFLAT is designed to utilize multiple levels of
parallelism through MPI, OpenMP, and SIMD instructions (vectorization). It can
run on both CPU and Xeon Phi co-processors based on the Intel Many Integrated
Core Architecture (MIC). We analyze the performance of XFLAT on configurations
with CPU only, Xeon Phi only and both CPU and Xeon Phi. We also investigate the
impact of I/O and the multi-node performance of XFLAT on the Xeon Phi-equipped
Stampede supercomputer at the Texas Advanced Computing Center (TACC).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1401.6396v4,Symbolic Abstractions of Networked Control Systems,"The last decade has witnessed significant attention on networked control
systems (NCS) due to their ubiquitous presence in industrial applications, and,
in the particular case of wireless NCS, because of their architectural
flexibility and low installation and maintenance costs. In wireless NCS the
communication between sensors, controllers, and actuators is supported by a
communication channel that is likely to introduce variable communication
delays, packet losses, limited bandwidth, and other practical non-idealities
leading to numerous technical challenges. Although stability properties of NCS
have been investigated extensively in the literature, results for NCS under
more complex and general objectives, and in particular results dealing with
verification or controller synthesis for logical specifications, are much more
limited. This work investigates how to address such complex objectives by
constructively deriving symbolic models of NCS, while encompassing the
mentioned network non-idealities. The obtained abstracted (symbolic) models can
then be employed to synthesize hybrid controllers enforcing rich logical
specifications over the concrete NCS models. Examples of such general
specifications include properties expressed as formulae in linear temporal
logic (LTL) or as automata on infinite strings. We thus provide a general
synthesis framework that can be flexibly adapted to a number of NCS setups. We
illustrate the effectiveness of the results over some case studies.",0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1507.00773v1,Truthful Online Scheduling with Commitments,"We study online mechanisms for preemptive scheduling with deadlines, with the
goal of maximizing the total value of completed jobs. This problem is
fundamental to deadline-aware cloud scheduling, but there are strong lower
bounds even for the algorithmic problem without incentive constraints. However,
these lower bounds can be circumvented under the natural assumption of deadline
slackness, i.e., that there is a guaranteed lower bound $s > 1$ on the ratio
between a job's size and the time window in which it can be executed.
  In this paper, we construct a truthful scheduling mechanism with a constant
competitive ratio, given slackness $s > 1$. Furthermore, we show that if $s$ is
large enough then we can construct a mechanism that also satisfies a commitment
property: it can be determined whether or not a job will finish, and the
requisite payment if so, well in advance of each job's deadline. This is
notable because, in practice, users with strict deadlines may find it
unacceptable to discover only very close to their deadline that their job has
been rejected.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2002.12460v1,Correlated Feature Selection with Extended Exclusive Group Lasso,"In many high dimensional classification or regression problems set in a
biological context, the complete identification of the set of informative
features is often as important as predictive accuracy, since this can provide
mechanistic insight and conceptual understanding. Lasso and related algorithms
have been widely used since their sparse solutions naturally identify a set of
informative features. However, Lasso performs erratically when features are
correlated. This limits the use of such algorithms in biological problems,
where features such as genes often work together in pathways, leading to sets
of highly correlated features. In this paper, we examine the performance of a
Lasso derivative, the exclusive group Lasso, in this setting. We propose fast
algorithms to solve the exclusive group Lasso, and introduce a solution to the
case when the underlying group structure is unknown. The solution combines
stability selection with random group allocation and introduction of artificial
features. Experiments with both synthetic and real-world data highlight the
advantages of this proposed methodology over Lasso in comprehensive selection
of informative features.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.07532v1,6G Communication Technology: A Vision on Intelligent Healthcare,"6G is a promising communication technology that will dominate the entire
health market from 2030 onward. It will dominate not only health sector but
also diverse sectors. It is expected that 6G will revolutionize many sectors
including healthcare. Healthcare will be fully AI-driven and dependent on 6G
communication technology, which will change our perception of lifestyle.
Currently, time and space are the key barriers to health care and 6G will be
able to overcome these barriers. Also, 6G will be proven as a game changing
technology for healthcare. Therefore, in this perspective, we envision
healthcare system for the era of 6G communication technology. Also, various new
methodologies have to be introduced to enhance our lifestyle, which is
addressed in this perspective, including Quality of Life (QoL), Intelligent
Wearable Devices (IWD), Intelligent Internet of Medical Things (IIoMT),
Hospital-to-Home (H2H) services, and new business model. In addition, we expose
the role of 6G communication technology in telesurgery, Epidemic and Pandemic.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.06989v1,Model Synthesis for Communication Traces of System-on-Chip Designs,"Concise and abstract models of system-level behaviors are invaluable in
design analysis, testing, and validation. In this paper, we consider the
problem of inferring models from communication traces of system-on-chip~(SoC)
designs. The traces capture communications among different blocks of a SoC
design in terms of messages exchanged. The extracted models characterize the
system-level communication protocols governing how blocks exchange messages,
and coordinate with each other to realize various system functions. In this
paper, the above problem is formulated as a constraint satisfaction problem,
which is then fed to a SMT solver. The solutions returned by the SMT solver are
used to extract the models that accept the input traces. In the experiments, we
demonstrate the proposed approach with traces collected from a
transaction-level simulation model of a multicore SoC design and traces of a
more detailed multicore SoC design developed in GEM5 environment.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.08513v1,Learnable Graph-regularization for Matrix Decomposition,"Low-rank approximation models of data matrices have become important machine
learning and data mining tools in many fields including computer vision, text
mining, bioinformatics and many others. They allow for embedding
high-dimensional data into low-dimensional spaces, which mitigates the effects
of noise and uncovers latent relations. In order to make the learned
representations inherit the structures in the original data,
graph-regularization terms are often added to the loss function. However, the
prior graph construction often fails to reflect the true network connectivity
and the intrinsic relationships. In addition, many graph-regularized methods
fail to take the dual spaces into account. Probabilistic models are often used
to model the distribution of the representations, but most of previous methods
often assume that the hidden variables are independent and identically
distributed for simplicity. To this end, we propose a learnable
graph-regularization model for matrix decomposition (LGMD), which builds a
bridge between graph-regularized methods and probabilistic matrix decomposition
models. LGMD learns two graphical structures (i.e., two precision matrices) in
real-time in an iterative manner via sparse precision matrix estimation and is
more robust to noise and missing entries. Extensive numerical results and
comparison with competing methods demonstrate its effectiveness.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1503.01812v1,"Ontology-Based Quality Evaluation of Value Generalization Hierarchies
  for Data Anonymization","In privacy-preserving data publishing, approaches using Value Generalization
Hierarchies (VGHs) form an important class of anonymization algorithms. VGHs
play a key role in the utility of published datasets as they dictate how the
anonymization of the data occurs. For categorical attributes, it is imperative
to preserve the semantics of the original data in order to achieve a higher
utility. Despite this, semantics have not being formally considered in the
specification of VGHs. Moreover, there are no methods that allow the users to
assess the quality of their VGH. In this paper, we propose a measurement
scheme, based on ontologies, to quantitatively evaluate the quality of VGHs, in
terms of semantic consistency and taxonomic organization, with the aim of
producing higher-quality anonymizations. We demonstrate, through a case study,
how our evaluation scheme can be used to compare the quality of multiple VGHs
and can help to identify faulty VGHs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1005.3010v9,A Proof for P =? NP Problem,"The $\textbf{P}$ vs. $\textbf{NP}$ problem is an important problem in
contemporary mathematics and theoretical computer science. Many proofs have
been proposed to this problem. This paper proposes a theoretic proof for
$\textbf{P}$ vs. $\textbf{NP}$ problem. The central idea of this proof is a
recursive definition for Turing machine (shortly TM) that accepts the encoding
strings of valid TMs. By the definition, an infinite sequence of TM is
constructed, and it is proven that the sequence includes all valid TMs. Based
on these TMs, the class $\textbf{D}$ that includes all decidable languages and
the union and reduction operators are defined. By constructing a language
$\textbf{Up}$ of the union of $\textbf{D}$, it is proved that
$\textbf{P}=\textbf{Up}$ and $\textbf{Up}=\textbf{NP}$, and the result
$\textbf{P}=\textbf{NP}$ is proven.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.11202v2,"MathNet: Haar-Like Wavelet Multiresolution-Analysis for Graph
  Representation and Learning","Graph Neural Networks (GNNs) have recently caught great attention and
achieved significant progress in graph-level applications. In this paper, we
propose a framework for graph neural networks with multiresolution Haar-like
wavelets, or MathNet, with interrelated convolution and pooling strategies. The
underlying method takes graphs in different structures as input and assembles
consistent graph representations for readout layers, which then accomplishes
label prediction. To achieve this, the multiresolution graph representations
are first constructed and fed into graph convolutional layers for processing.
The hierarchical graph pooling layers are then involved to downsample graph
resolution while simultaneously remove redundancy within graph signals. The
whole workflow could be formed with a multi-level graph analysis, which not
only helps embed the intrinsic topological information of each graph into the
GNN, but also supports fast computation of forward and adjoint graph
transforms. We show by extensive experiments that the proposed framework
obtains notable accuracy gains on graph classification and regression tasks
with performance stability. The proposed MathNet outperforms various existing
GNN models, especially on big data sets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1012.5248v1,Matrix Insertion-Deletion Systems,"In this article, we consider for the first time the operations of insertion
and deletion working in a matrix controlled manner. We show that, similarly as
in the case of context-free productions, the computational power is strictly
increased when using a matrix control: computational completeness can be
obtained by systems with insertion or deletion rules involving at most two
symbols in a contextual or in a context-free manner and using only binary
matrices.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0810.4576v2,"New Constructions for Query-Efficient Locally Decodable Codes of
  Subexponential Length","A $(k,\delta,\epsilon)$-locally decodable code $C: F_{q}^{n} \to F_{q}^{N}$
is an error-correcting code that encodes each message
$\vec{x}=(x_{1},x_{2},...,x_{n}) \in F_{q}^{n}$ to $C(\vec{x}) \in F_{q}^{N}$
and has the following property: For any $\vec{y} \in {\bf F}_{q}^{N}$ such that
$d(\vec{y},C(\vec{x})) \leq \delta N$ and each $1 \leq i \leq n$, the symbol
$x_{i}$ of $\vec{x}$ can be recovered with probability at least $1-\epsilon$ by
a randomized decoding algorithm looking only at $k$ coordinates of $\vec{y}$.
The efficiency of a $(k,\delta,\epsilon)$-locally decodable code $C: F_{q}^{n}
\to F_{q}^{N}$ is measured by the code length $N$ and the number $k$ of
queries. For any $k$-query locally decodable code $C: F_{q}^{n} \to F_{q}^{N}$,
the code length $N$ is conjectured to be exponential of $n$, however, this was
disproved. Yekhanin [In Proc. of STOC, 2007] showed that there exists a 3-query
locally decodable code $C: F_{2}^{n} \to F_{2}^{N}$ such that
$N=\exp(n^{(1/\log \log n)})$ assuming that the number of Mersenne primes is
infinite. For a 3-query locally decodable code $C: F_{q}^{n} \to F_{q}^{N}$,
Efremenko [ECCC Report No.69, 2008] reduced the code length further to
$N=\exp(n^{O((\log \log n/ \log n)^{1/2})})$, and also showed that for any
integer $r>1$, there exists a $k$-query locally decodable code $C: F_{q}^{n}
\to F_{q}^{N}$ such that $k \leq 2^{r}$ and $N=\exp(n^{O((\log \log n/ \log
n)^{1-1/r})})$. In this paper, we present a query-efficient locally decodable
code and show that for any integer $r>1$, there exists a $k$-query locally
decodable code $C: F_{q}^{n} \to F_{q}^{N}$ such that $k \leq 3 \cdot 2^{r-2}$
and $N=\exp(n^{O((\log \log n/ \log n)^{1-1/r})})$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0003065v1,"Image Compression with Iterated Function Systems, Finite Automata and
  Zerotrees: Grand Unification","Fractal image compression, Culik's image compression and zerotree prediction
coding of wavelet image decomposition coefficients succeed only because typical
images being compressed possess a significant degree of self-similarity.
Besides the common concept, these methods turn out to be even more tightly
related, to the point of algorithmical reducibility of one technique to
another. The goal of the present paper is to demonstrate these relations.
  The paper offers a plain-term interpretation of Culik's image compression, in
regular image processing terms, without resorting to finite state machines and
similar lofty language. The interpretation is shown to be algorithmically
related to an IFS fractal image compression method: an IFS can be exactly
transformed into Culik's image code. Using this transformation, we will prove
that in a self-similar (part of an) image any zero wavelet coefficient is the
root of a zerotree, or its branch.
  The paper discusses the zerotree coding of (wavelet/projection) coefficients
as a common predictor/corrector, applied vertically through different layers of
a multiresolutional decomposition, rather than within the same view. This
interpretation leads to an insight into the evolution of image compression
techniques: from a causal single-layer prediction, to non-causal same-view
predictions (wavelet decomposition among others) and to a causal cross-layer
prediction (zero-trees, Culik's method).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.02282v1,Dependent Types for Multi-Rate Flows in Synchronous Programming,"Synchronous programming languages emerged in the 1980s as tools for
implementing reactive systems, which interact with events from physical
environments and often must do so under strict timing constraints. In this
report, we encode inside ATS various real-time primitives in an experimental
synchronous language called Prelude, where ATS is a statically typed language
with an ML-like functional core that supports both dependent types (of
DML-style) and linear types. We show that the verification requirements imposed
on these primitives can be formally expressed in terms of dependent types in
ATS. Moreover, we modify the Prelude compiler to automatically generate ATS
code from Prelude source. This modified compiler allows us to solely rely on
typechecking in ATS to discharge proof obligations originating from the need to
typecheck Prelude code. Whereas ATS is typically used as a general purpose
programming language, we hereby demonstrate that it can also be conveniently
used to support some forms of advanced static checking in languages equipped
with less expressive types.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.00098v1,"Long term dynamics of the subgradient method for Lipschitz path
  differentiable functions","We consider the long-term dynamics of the vanishing stepsize subgradient
method in the case when the objective function is neither smooth nor convex. We
assume that this function is locally Lipschitz and path differentiable, i.e.,
admits a chain rule. Our study departs from other works in the sense that we
focus on the behavoir of the oscillations, and to do this we use closed
measures. We recover known convergence results, establish new ones, and show a
local principle of oscillation compensation for the velocities. Roughly
speaking, the time average of gradients around one limit point vanishes. This
allows us to further analyze the structure of oscillations, and establish their
perpendicularity to the general drift.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0501032v1,On Partially Additive Kleene Algebras,"We define the notion of a partially additive Kleene algebra, which is a
Kleene algebra where the + operation need only be partially defined. These
structures formalize a number of examples that cannot be handled directly by
Kleene algebras. We relate partially additive Kleene algebras to existing
algebraic structures, by exhibiting categorical connections with Kleene
algebras, partially additive categories, and closed semirings.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1709.01784v1,Cross-Domain Image Retrieval with Attention Modeling,"With the proliferation of e-commerce websites and the ubiquitousness of smart
phones, cross-domain image retrieval using images taken by smart phones as
queries to search products on e-commerce websites is emerging as a popular
application. One challenge of this task is to locate the attention of both the
query and database images. In particular, database images, e.g. of fashion
products, on e-commerce websites are typically displayed with other
accessories, and the images taken by users contain noisy background and large
variations in orientation and lighting. Consequently, their attention is
difficult to locate. In this paper, we exploit the rich tag information
available on the e-commerce websites to locate the attention of database
images. For query images, we use each candidate image in the database as the
context to locate the query attention. Novel deep convolutional neural network
architectures, namely TagYNet and CtxYNet, are proposed to learn the attention
weights and then extract effective representations of the images. Experimental
results on public datasets confirm that our approaches have significant
improvement over the existing methods in terms of the retrieval accuracy and
efficiency.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.08127v1,Addressing Cognitive Biases in Augmented Business Decision Systems,"How do algorithmic decision aids introduced in business decision processes
affect task performance? In a first experiment, we study effective
collaboration. Faced with a decision, subjects alone have a success rate of
72%; Aided by a recommender that has a 75% success rate, their success rate
reaches 76%. The human-system collaboration had thus a greater success rate
than each taken alone. However, we noted a complacency/authority bias that
degraded the quality of decisions by 5% when the recommender was wrong. This
suggests that any lingering algorithmic bias may be amplified by decision aids.
In a second experiment, we evaluated the effectiveness of 5 presentation
variants in reducing complacency bias. We found that optional presentation
increases subjects' resistance to wrong recommendations. We conclude by arguing
that our metrics, in real usage scenarios, where decision aids are embedded as
system-wide features in Business Process Management software, can lead to
enhanced benefits.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0110026v1,Information retrieval in Current Research Information Systems,"In this paper we describe the requirements for research information systems
and problems which arise in the development of such system. Here is shown which
problems could be solved by using of knowledge markup technologies. Ontology
for Research Information System offered. Architecture for collecting research
data and providing access to it is described.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1310.4588v1,Arbitrary Sequence RAMs,"It is known that in some cases a Random Access Machine (RAM) benefits from
having an additional input that is an arbitrary number, satisfying only the
criterion of being sufficiently large. This is known as the ARAM model. We
introduce a new type of RAM, which we refer to as the Arbitrary Sequence RAM
(ASRAM), that generalises the ARAM by allowing the generation of additional
arbitrary large numbers at will during execution time. We characterise the
power contribution of this ability under several RAM variants.
  In particular, we demonstrate that an arithmetic ASRAM is more powerful than
an arithmetic ARAM, that a sufficiently equipped ASRAM can recognise any
language in the arithmetic hierarchy in constant time (and more, if it is given
more time), and that, on the other hand, in some cases the ASRAM is no more
powerful than its underlying RAM.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1803.09602v1,Comparison of angular spread for 6 and 60 GHz based on 3GPP standard,"In an urban environment, a multipath propagation is one of the basic
phenomena affecting a quality of received signals. This causes dispersions in
time and angular domains. Basic parameters describing these dispersions are the
rms delay spread and rms angle spread, respectively. The delay spread is
related to a frequency of the transmitted signal and the nature of the
propagation environment. In this paper, we show a mutual relationship between
the time and angular dispersions in the received signal. The obtained
simulation results present a comparison of the described dispersions for two
different frequencies. In this case, the multi-elliptical propagation model and
standard model developed by 3GPP are the basis for the simulation analysis of
new communication system solutions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.09709v1,Approximate Weighted $CR$ Coded Matrix Multiplication,"One of the most common, but at the same time expensive operations in linear
algebra, is multiplying two matrices $A$ and $B$. With the rapid development of
machine learning and increases in data volume, performing fast matrix intensive
multiplications has become a major hurdle. Two different approaches to
overcoming this issue are, 1) to approximate the product; and 2) to perform the
multiplication distributively. A \textit{$CR$-multiplication} is an
approximation where columns and rows of $A$ and $B$ are respectively sampled
with replacement. In the distributed setting, multiple workers perform matrix
multiplication subtasks in parallel. Some of the workers may be stragglers,
meaning they do not complete their task in time. We present a novel
\textit{approximate weighted $CR$ coded matrix multiplication} scheme, that
achieves improved performance for distributed matrix multiplication.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9812006v1,"A High Quality Text-To-Speech System Composed of Multiple Neural
  Networks","While neural networks have been employed to handle several different
text-to-speech tasks, ours is the first system to use neural networks
throughout, for both linguistic and acoustic processing. We divide the
text-to-speech task into three subtasks, a linguistic module mapping from text
to a linguistic representation, an acoustic module mapping from the linguistic
representation to speech, and a video module mapping from the linguistic
representation to animated images. The linguistic module employs a
letter-to-sound neural network and a postlexical neural network. The acoustic
module employs a duration neural network and a phonetic neural network. The
visual neural network is employed in parallel to the acoustic module to drive a
talking head. The use of neural networks that can be retrained on the
characteristics of different voices and languages affords our system a degree
of adaptability and naturalness heretofore unavailable.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1604.07202v1,An Approach to Find Missing Values in Medical Datasets,"Mining medical datasets is a challenging problem before data mining
researchers as these datasets have several hidden challenges compared to
conventional datasets.Starting from the collection of samples through field
experiments and clinical trials to performing classification,there are numerous
challenges at every stage in the mining process. The preprocessing phase in the
mining process itself is a challenging issue when, we work on medical datasets.
One of the prime challenges in mining medical datasets is handling missing
values which is part of preprocessing phase. In this paper, we address the
issue of handling missing values in medical dataset consisting of categorical
attribute values. The main contribution of this research is to use the proposed
imputation measure to estimate and fix the missing values. We discuss a case
study to demonstrate the working of proposed measure.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0801.3581v1,"Shallow, Low, and Light Trees, and Tight Lower Bounds for Euclidean
  Spanners","We show that for every $n$-point metric space $M$ there exists a spanning
tree $T$ with unweighted diameter $O(\log n)$ and weight $\omega(T) = O(\log n)
\cdot \omega(MST(M))$. Moreover, there is a designated point $rt$ such that for
every point $v$, $dist_T(rt,v) \le (1+\epsilon) \cdot dist_M(rt,v)$, for an
arbitrarily small constant $\epsilon > 0$. We extend this result, and provide a
tradeoff between unweighted diameter and weight, and prove that this tradeoff
is \emph{tight up to constant factors} in the entire range of parameters. These
results enable us to settle a long-standing open question in Computational
Geometry. In STOC'95 Arya et al. devised a construction of Euclidean Spanners
with unweighted diameter $O(\log n)$ and weight $O(\log n) \cdot
\omega(MST(M))$. Ten years later in SODA'05 Agarwal et al. showed that this
result is tight up to a factor of $O(\log \log n)$. We close this gap and show
that the result of Arya et al. is tight up to constant factors.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0307038v1,Manifold Learning with Geodesic Minimal Spanning Trees,"In the manifold learning problem one seeks to discover a smooth low
dimensional surface, i.e., a manifold embedded in a higher dimensional linear
vector space, based on a set of measured sample points on the surface. In this
paper we consider the closely related problem of estimating the manifold's
intrinsic dimension and the intrinsic entropy of the sample points.
Specifically, we view the sample points as realizations of an unknown
multivariate density supported on an unknown smooth manifold. We present a
novel geometrical probability approach, called the
geodesic-minimal-spanning-tree (GMST), to obtaining asymptotically consistent
estimates of the manifold dimension and the R\'{e}nyi $\alpha$-entropy of the
sample density on the manifold. The GMST approach is striking in its simplicity
and does not require reconstructing the manifold or estimating the multivariate
density of the samples. The GMST method simply constructs a minimal spanning
tree (MST) sequence using a geodesic edge matrix and uses the overall lengths
of the MSTs to simultaneously estimate manifold dimension and entropy. We
illustrate the GMST approach for dimension and entropy estimation of a human
face dataset.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1304.8034v2,A Syntactic-Semantic Approach to Incremental Verification,"Software verification of evolving systems is challenging mainstream
methodologies and tools. Formal verification techniques often conflict with the
time constraints imposed by change management practices for evolving systems.
Since changes in these systems are often local to restricted parts, an
incremental verification approach could be beneficial.
  This paper introduces SiDECAR, a general framework for the definition of
verification procedures, which are made incremental by the framework itself.
Verification procedures are driven by the syntactic structure (defined by a
grammar) of the system and encoded as semantic attributes associated with the
grammar. Incrementality is achieved by coupling the evaluation of semantic
attributes with an incremental parsing technique.
  We show the application of SiDECAR to the definition of two verification
procedures: probabilistic verification of reliability requirements and
verification of safety properties.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1904.01361v2,An Algorithmic Theory of Integer Programming,"We study the general integer programming problem where the number of
variables $n$ is a variable part of the input. We consider two natural
parameters of the constraint matrix $A$: its numeric measure $a$ and its
sparsity measure $d$. We show that integer programming can be solved in time
$g(a,d)\textrm{poly}(n,L)$, where $g$ is some computable function of the
parameters $a$ and $d$, and $L$ is the binary encoding length of the input. In
particular, integer programming is fixed-parameter tractable parameterized by
$a$ and $d$, and is solvable in polynomial time for every fixed $a$ and $d$.
Our results also extend to nonlinear separable convex objective functions.
Moreover, for linear objectives, we derive a strongly-polynomial algorithm,
that is, with running time $g(a,d)\textrm{poly}(n)$, independent of the rest of
the input data.
  We obtain these results by developing an algorithmic framework based on the
idea of iterative augmentation: starting from an initial feasible solution, we
show how to quickly find augmenting steps which rapidly converge to an optimum.
A central notion in this framework is the Graver basis of the matrix $A$, which
constitutes a set of fundamental augmenting steps. The iterative augmentation
idea is then enhanced via the use of other techniques such as new and improved
bounds on the Graver basis, rapid solution of integer programs with bounded
variables, proximity theorems and a new proximity-scaling algorithm, the notion
of a reduced objective function, and others.
  As a consequence of our work, we advance the state of the art of solving
block-structured integer programs. In particular, we develop near-linear time
algorithms for $n$-fold, tree-fold, and $2$-stage stochastic integer programs.
We also discuss some of the many applications of these classes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1309.2525v3,Max-Flows on Sparse and Dense Networks,"In this paper, we present an improved algorithm for the maximum flow problem
on general networks with $n$ vertices and $m$ arcs. We show how to solve the
problem in $O(mn)$ time, when $m = O(n^{2-\epsilon})$, for some $0 <\epsilon
\leq 1$. This improves upon the results of both Orlin and King, et. al., who
solved the problem in $O(mn + m^{31/16} \log^2 n)$ and $O(mn\log_{m/n\log n}n)$
time, respectively. Our main result is reducing the number of nonsaturating
pushes to $O(mn)$ across all scaling phases. Our algorithm can be seen as
complementary to King, et. al., in the sense that we solve the max-flow problem
in $O(mn)$ time when $m = O(n^{2-\epsilon})$ (all sparse and non-dense
networks), whereas King, et. al. solve it in $O(mn)$ time when $m =
\Omega(n^{1+\epsilon})$ (all dense and non-sparse networks).
  Our improvement is reached by a novel combination of Ahuja and Orlin's excess
scaling method and Orlin's compact flow networks. To our knowledge, this is the
first $O(mn)$ time max-flow algorithm that runs on this range of networks.
Further, we extend the range of Orlin's $O(mn)$ time algorithm from
$O(n^{16/15-\epsilon})$ to $O(n^{2-\epsilon})$, which is an improvement of
approximately $O(n^{0.94})$. Our result also establishes that the problem can
be solved for all $n$ and $m$ using exclusively the push-relabel method. We
also give improved algorithms for parametric flows and efficiently constructing
Gomory-Hu trees, and suggest a new approach to the minimum-cost flow problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1001.2940v1,"Parallel computation of real solving bivariate polynomial systems by
  zero-matching method","We present a new algorithm for solving the real roots of a bivariate
polynomial system $\Sigma=\{f(x,y),g(x,y)\}$ with a finite number of solutions
by using a zero-matching method. The method is based on a lower bound for
bivariate polynomial system when the system is non-zero. Moreover, the
multiplicities of the roots of $\Sigma=0$ can be obtained by a given
neighborhood. From this approach, the parallelization of the method arises
naturally. By using a multidimensional matching method this principle can be
generalized to the multivariate equation systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0402010v1,Encapsulation for Practical Simplification Procedures,"ACL2 was used to prove properties of two simplification procedures. The
procedures differ in complexity but solve the same programming problem that
arises in the context of a resolution/paramodulation theorem proving system.
Term rewriting is at the core of the two procedures, but details of the
rewriting procedure itself are irrelevant. The ACL2 encapsulate construct was
used to assert the existence of the rewriting function and to state some of its
properties. Termination, irreducibility, and soundness properties were
established for each procedure. The availability of the encapsulation mechanism
in ACL2 is considered essential to rapid and efficient verification of this
kind of algorithm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0104018v1,"Several new domain-type and boundary-type numerical discretization
  schemes with radial basis function","This paper is concerned with a few novel RBF-based numerical schemes
discretizing partial differential equations. For boundary-type methods, we
derive the indirect and direct symmetric boundary knot methods (BKM). The
resulting interpolation matrix of both is always symmetric irrespective of
boundary geometry and conditions. In particular, the direct BKM applies the
practical physical variables rather than expansion coefficients and becomes
very competitive to the boundary element method. On the other hand, based on
the multiple reciprocity principle, we invent the RBF-based boundary particle
method (BPM) for general inhomogeneous problems without a need using inner
nodes. The direct and symmetric BPM schemes are also developed.
  For domain-type RBF discretization schemes, by using the Green integral we
develop a new Hermite RBF scheme called as the modified Kansa method (MKM),
which differs from the symmetric Hermite RBF scheme in that the MKM discretizes
both governing equation and boundary conditions on the same boundary nodes. The
local spline version of the MKM is named as the finite knot method (FKM). Both
MKM and FKM significantly reduce calculation errors at nodes adjacent to
boundary. In addition, the nonsingular high-order fundamental or general
solution is strongly recommended as the RBF in the domain-type methods and dual
reciprocity method approximation of particular solution relating to the BKM.
  It is stressed that all the above discretization methods of boundary-type and
domain-type are symmetric, meshless, and integration-free. The spline-based
schemes will produce desirable symmetric sparse banded interpolation matrix. In
appendix, we present a Hermite scheme to eliminate edge effect on the RBF
geometric modeling and imaging.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.02806v1,"A Survey of Convolutional Neural Networks: Analysis, Applications, and
  Prospects","Convolutional Neural Network (CNN) is one of the most significant networks in
the deep learning field. Since CNN made impressive achievements in many areas,
including but not limited to computer vision and natural language processing,
it attracted much attention both of industry and academia in the past few
years. The existing reviews mainly focus on the applications of CNN in
different scenarios without considering CNN from a general perspective, and
some novel ideas proposed recently are not covered. In this review, we aim to
provide novel ideas and prospects in this fast-growing field as much as
possible. Besides, not only two-dimensional convolution but also
one-dimensional and multi-dimensional ones are involved. First, this review
starts with a brief introduction to the history of CNN. Second, we provide an
overview of CNN. Third, classic and advanced CNN models are introduced,
especially those key points making them reach state-of-the-art results. Fourth,
through experimental analysis, we draw some conclusions and provide several
rules of thumb for function selection. Fifth, the applications of
one-dimensional, two-dimensional, and multi-dimensional convolution are
covered. Finally, some open issues and promising directions for CNN are
discussed to serve as guidelines for future work.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1409.3854v1,"Linear, Deterministic, and Order-Invariant Initialization Methods for
  the K-Means Clustering Algorithm","Over the past five decades, k-means has become the clustering algorithm of
choice in many application domains primarily due to its simplicity, time/space
efficiency, and invariance to the ordering of the data points. Unfortunately,
the algorithm's sensitivity to the initial selection of the cluster centers
remains to be its most serious drawback. Numerous initialization methods have
been proposed to address this drawback. Many of these methods, however, have
time complexity superlinear in the number of data points, which makes them
impractical for large data sets. On the other hand, linear methods are often
random and/or sensitive to the ordering of the data points. These methods are
generally unreliable in that the quality of their results is unpredictable.
Therefore, it is common practice to perform multiple runs of such methods and
take the output of the run that produces the best results. Such a practice,
however, greatly increases the computational requirements of the otherwise
highly efficient k-means algorithm. In this chapter, we investigate the
empirical performance of six linear, deterministic (non-random), and
order-invariant k-means initialization methods on a large and diverse
collection of data sets from the UCI Machine Learning Repository. The results
demonstrate that two relatively unknown hierarchical initialization methods due
to Su and Dy outperform the remaining four methods with respect to two
objective effectiveness criteria. In addition, a recent method due to Erisoglu
et al. performs surprisingly poorly.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0302004v1,Unique Pattern Matching in Strings,"Regular expression patterns are a key feature of document processing
languages like Perl and XDuce. It is in this context that the first and longest
match policies have been proposed to disambiguate the pattern matching process.
We formally define a matching semantics with these policies and show that the
generally accepted method of simulating longest match by first match and
recursion is incorrect. We continue by solving the associated type inference
problem, which consists in calculating for every subexpression the set of words
the subexpression can still match when these policies are in effect, and show
how this algorithm can be used to efficiently implement the matching process.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0601051v2,A Constructive Semantic Characterization of Aggregates in ASP,"This technical note describes a monotone and continuous fixpoint operator to
compute the answer sets of programs with aggregates. The fixpoint operator
relies on the notion of aggregate solution. Under certain conditions, this
operator behaves identically to the three-valued immediate consequence operator
$\Phi^{aggr}_P$ for aggregate programs, independently proposed Pelov et al.
This operator allows us to closely tie the computational complexity of the
answer set checking and answer sets existence problems to the cost of checking
a solution of the aggregates in the program. Finally, we relate the semantics
described by the operator to other proposals for logic programming with
aggregates.
  To appear in Theory and Practice of Logic Programming (TPLP).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1610.09984v1,Submodular Optimization over Sliding Windows,"Maximizing submodular functions under cardinality constraints lies at the
core of numerous data mining and machine learning applications, including data
diversification, data summarization, and coverage problems. In this work, we
study this question in the context of data streams, where elements arrive one
at a time, and we want to design low-memory and fast update-time algorithms
that maintain a good solution. Specifically, we focus on the sliding window
model, where we are asked to maintain a solution that considers only the last
$W$ items.
  In this context, we provide the first non-trivial algorithm that maintains a
provable approximation of the optimum using space sublinear in the size of the
window. In particular we give a $\frac{1}{3} - \epsilon$ approximation
algorithm that uses space polylogarithmic in the spread of the values of the
elements, $\Phi$, and linear in the solution size $k$ for any constant
$\epsilon > 0$ . At the same time, processing each element only requires a
polylogarithmic number of evaluations of the function itself. When a better
approximation is desired, we show a different algorithm that, at the cost of
using more memory, provides a $\frac{1}{2} - \epsilon$ approximation and allows
a tunable trade-off between average update time and space. This algorithm
matches the best known approximation guarantees for submodular optimization in
insertion-only streams, a less general formulation of the problem.
  We demonstrate the efficacy of the algorithms on a number of real world
datasets, showing that their practical performance far exceeds the theoretical
bounds. The algorithms preserve high quality solutions in streams with millions
of items, while storing a negligible fraction of them.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0205064v6,Three complete deterministic polynomial algorithms for 3SAT,"Three algorithms are presented that determine the existence of satisfying
assignments for 3SAT Boolean satisfiability expressions. One algorithm is
presented for determining an instance of a satisfying assignment, where such
exists. The algorithms are each deterministic and of polynomial complexity. The
algorithms determining existence are complete as each produces a certificate of
non-satisfiability, for instances where no satisfying assignment exists, and of
satisfiability for such assignment does exist.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1012.5024v1,"Shortest Paths with Pairwise-Distinct Edge Labels: Finding Biochemical
  Pathways in Metabolic Networks","A problem studied in Systems Biology is how to find shortest paths in
metabolic networks. Unfortunately, simple (i.e., graph theoretic) shortest
paths do not properly reflect biochemical facts. An approach to overcome this
issue is to use edge labels and search for paths with distinct labels.
  In this paper, we show that such biologically feasible shortest paths are
hard to compute. Moreover, we present solutions to find such paths in networks
in reasonable time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.04360v1,A Semantic Framework for PEGs,"Parsing Expression Grammars (PEGs) are a recognition-based formalism which
allows to describe the syntactical and the lexical elements of a language. The
main difference between Context-Free Grammars (CFGs) and PEGs relies on the
interpretation of the choice operator: while the CFGs' unordered choice e | e'
is interpreted as the union of the languages recognized by e and e, the PEGs'
prioritized choice e/e' discards e' if e succeeds. Such subtle, but important
difference, changes the language recognized and yields more efficient parsing
algorithms. This paper proposes a rewriting logic semantics for PEGs. We start
with a rewrite theory giving meaning to the usual constructs in PEGs. Later, we
show that cuts, a mechanism for controlling backtracks in PEGs, finds also a
natural representation in our framework. We generalize such mechanism, allowing
for both local and global cuts with a precise, unified and formal semantics.
Hence, our work strives at better understanding and controlling backtracks in
parsers for PEGs. The semantics we propose is executable and, besides being a
parser with modest efficiency, it can be used as a playground to test different
optimization ideas. More importantly, it is a mathematical tool that can be
used for different analyses.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9906027v1,Human-Computer Conversation,"The article surveys a little of the history of the technology, sets out the
main current theoretical approaches in brief, and discusses the on-going
opposition between theoretical and empirical approaches. It illustrates the
situation with some discussion of CONVERSE, a system that won the Loebner prize
in 1997 and which displays features of both approaches.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1411.1945v1,Systematic Review on Project Actuality,"Nowadays much is written about how to manage projects, but too little on what
really happens in project actuality. Project Actuality came out in the
Rethinking Project Management (RPM) agenda in 2006 and it aims at understanding
what really happens at project context. To be able to understand project
actuality phenomenon, we first need to get a better comprehension on its
definition and discover how to observe it and analyse it. This paper presents
the results of the systematic review conducted to collect evidence on Project
Actuality. The research focused on four search engines, in publications from
1994 to 2013. Among others, the study concludes that project actuality has been
analysed by several methods and techniques, mostly on large organization and
public sectors, in Northern Europe. The most common definitions, techniques,
and tips were identified as well as the intent of transforming the results in
knowledge.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2111.01631v2,"SO{U}RCERER: Developer-Driven Security Testing Framework for Android
  Apps","Frequently advised secure development recommendations often fall short in
practice for app developers. Tool-driven (e.g., using static analysis tools)
approaches lack context and domain-specific requirements of an app being
tested. App developers struggle to find an actionable and prioritized list of
vulnerabilities from a laundry list of security warnings reported by static
analysis tools. Process-driven (e.g., applying threat modeling methods)
approaches require substantial resources (e.g., security testing team, budget)
and security expertise, which small to medium-scale app dev teams could barely
afford. To help app developers securing their apps, we propose SO{U}RCERER, a
guiding framework for Android app developers for security testing. SO{U}RCERER
guides developers to identify domain-specific assets of an app, detect and
prioritize vulnerabilities, and mitigate those vulnerabilities based on secure
development guidelines. We evaluated SO{U}RCERER with a case study on analyzing
and testing 36 Android mobile money apps. We found that by following activities
guided by SO{U}RCERER, an app developer could get a concise and actionable list
of vulnerabilities (24-61% fewer security warnings produced by SO{U}RCERER than
a standalone static analyzer), directly affecting a mobile money app's critical
assets, and devise a mitigation plan. Our findings from this preliminary study
indicate a viable approach to Android app security testing without being
overwhelmingly complex for app developers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0
http://arxiv.org/abs/1904.08333v1,"Classification of Existing Virtualization Methods Used in
  Telecommunication Networks","This article studies the existing methods of virtualization of different
resources. The positive and negative aspects of each of the methods are
analyzed, the perspectivity of the approach is noted. It is also made an
attempt to classify virtualization methods according to the application domain,
which allows us to discover the method weaknesses which are needed to be
optimized.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.00118v3,An algebra of synchronous atomic steps,"This research started with an algebra for reasoning about rely/guarantee
concurrency for a shared memory model. The approach taken led to a more
abstract algebra of atomic steps, in which atomic steps synchronise (rather
than interleave) when composed in parallel. The algebra of rely/guarantee
concurrency then becomes an interpretation of the more abstract algebra. Many
of the core properties needed for rely/guarantee reasoning can be shown to hold
in the abstract algebra where their proofs are simpler and hence allow a higher
degree of automation. Moreover, the realisation that the synchronisation
mechanisms of standard process algebras, such as CSP and CCS/SCCS, can be
interpreted in our abstract algebra gives evidence of its unifying power. The
algebra has been encoded in Isabelle/HOL to provide a basis for tool support.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1512.08986v1,"Is Your Data Gone? Comparing Perceived Effectiveness of Thumb Drive
  Deletion Methods to Actual Effectiveness","Previous studies have shown that many users do not use effective data
deletion techniques upon sale or surrender of storage devices. A logical
assumption is that many users are still confused concerning proper sanitization
techniques of devices upon surrender. This paper strives to measure this
assumption through a buyback study with a survey component. We recorded
participants' thoughts and beliefs concerning deletion, as well as general
demographic information, in relation to actual deletion effectiveness on USB
thumb drives. Thumb drives were chosen for this study due to their relative low
cost, ease of use, and ubiquity. In addition, we also bought used thumb drives
from eBay and Amazon Marketplace to use as a comparison to the wider world.
  We found that there is no statistically significant difference between
buyback and market drives in terms of deletion methods nor presence of
sensitive data, and thus our study may be predictive of the perceptions of the
market sellers. In our combined data sets, we found over 60% of the drives
tested still had recoverable sensitive data, and in the buyback group, we found
no correlation between users' perceived versus actual effectiveness of deletion
methods. Our results suggest the security community may need to take a
different approach to increase the usability, availability, and/or necessity of
strong deletion methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0807.2120v2,Derandomizing the Lovasz Local Lemma more effectively,"The famous Lovasz Local Lemma [EL75] is a powerful tool to non-constructively
prove the existence of combinatorial objects meeting a prescribed collection of
criteria. Kratochvil et al. applied this technique to prove that a k-CNF in
which each variable appears at most 2^k/(ek) times is always satisfiable
[KST93]. In a breakthrough paper, Beck found that if we lower the occurrences
to O(2^(k/48)/k), then a deterministic polynomial-time algorithm can find a
satisfying assignment to such an instance [Bec91]. Alon randomized the
algorithm and required O(2^(k/8)/k) occurrences [Alo91]. In [Mos06], we
exhibited a refinement of his method which copes with O(2^(k/6)/k) of them. The
hitherto best known randomized algorithm is due to Srinivasan and is capable of
solving O(2^(k/4)/k) occurrence instances [Sri08]. Answering two questions
asked by Srinivasan, we shall now present an approach that tolerates
O(2^(k/2)/k) occurrences per variable and which can most easily be
derandomized. The new algorithm bases on an alternative type of witness tree
structure and drops a number of limiting aspects common to all previous
methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1905.05494v2,"A practical algorithm for volume estimation based on billiard
  trajectories and simulated annealing","We tackle the problem of efficiently approximating the volume of convex
polytopes, when these are given in three different representations:
H-polytopes, which have been studied extensively, V-polytopes, and zonotopes
(Z-polytopes). We design a novel practical Multiphase Monte Carlo algorithm
that leverages random walks based on billiard trajectories, as well as new
empirical convergence tests and a simulated annealing schedule of adaptive
convex bodies. We present a detailed experimental evaluation of our algorithm
using a rich dataset containing Birkhoff polytopes and polytopes from
structural biology. Our open-source implementation tackles problems that have
been intractable so far, offering the first software to scale up in thousands
of dimensions for H-polytopes and in the hundreds for V- and Z-polytopes on
moderate hardware. Last, we illustrate our software in evaluating Z-polytope
approximations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0412033v1,"The modelling of the build constructions in a CAD of the renovation of
  the enterprises by means of units in the drawings","The parametric model of build constructions and features of design operations
are described for making drawings, which are the common component of the
different parts of the projects of renovation of enterprises. The key moment of
the deep design automation is the using of so-called units in the drawings,
which are joining a visible graphic part and invisible parameters. The model
has passed check during designing of several hundreds of drawings",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.05723v1,Prediction of Political Leanings of Chinese Speaking Twitter Users,"This work presents a supervised method for generating a classifier model of
the stances held by Chinese-speaking politicians and other Twitter users. Many
previous works of political tweets prediction exist on English tweets, but to
the best of our knowledge, this is the first work that builds prediction model
on Chinese political tweets. It firstly collects data by scraping tweets of
famous political figure and their related users. It secondly defines the
political spectrum in two groups: the group that shows approvals to the Chinese
Communist Party and the group that does not. Since there are not space between
words in Chinese to identify the independent words, it then completes
segmentation and vectorization by Jieba, a Chinese segmentation tool. Finally,
it trains the data collected from political tweets and produce a classification
model with high accuracy for understanding users' political stances from their
tweets on Twitter.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1005.4732v2,Tensor sparsification via a bound on the spectral norm of random tensors,"Given an order-$d$ tensor $\tensor A \in \R^{n \times n \times...\times n}$,
we present a simple, element-wise sparsification algorithm that zeroes out all
sufficiently small elements of $\tensor A$, keeps all sufficiently large
elements of $\tensor A$, and retains some of the remaining elements with
probabilities proportional to the square of their magnitudes. We analyze the
approximation accuracy of the proposed algorithm using a powerful inequality
that we derive. This inequality bounds the spectral norm of a random tensor and
is of independent interest. As a result, we obtain novel bounds for the tensor
sparsification problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1904.00306v1,Decomposition and Modeling in the Non-Manifold domain,"The problem of decomposing non-manifold object has already been studied in
solid modeling. However, the few proposed solutions are limited to the problem
of decomposing solids described through their boundaries. In this thesis we
study the problem of decomposing an arbitrary non-manifold simplicial complex
into more regular components. A formal notion of decomposition is developed
using combinatorial topology. The proposed decomposition is unique, for a given
complex, and is computable for complexes of any dimension. A decomposition
algorithm is proposed that is linear w.r.t. the size of the input. In three or
higher dimensions a decomposition into manifold parts is not always possible.
Thus, in higher dimensions, we decompose a non-manifold into a decidable super
class of manifolds, that we call, Initial-Quasi-Manifolds. We also defined a
two-layered data structure, the Extended Winged data structure. This data
structure is a dimension independent data structure conceived to model
non-manifolds through their decomposition into initial-quasi-manifold parts.
Our two layered data structure describes the structure of the decomposition and
each component separately. In the second layer we encode the connectivity
structure of the decomposition. We analyze the space requirements of the
Extended Winged data structure and give algorithms to build and navigate it.
Finally, we discuss time requirements for the computation of topological
relations and show that, for surfaces and tetrahedralizations, embedded in real
3D space, all topological relations can be extracted in optimal time. This
approach offers a compact, dimension independent, representation for
non-manifolds that can be useful whenever the modeled object has few
non-manifold singularities.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1306.1953v1,A Formal Approach To Firewalls Testing Techniques,"Traditional technologies of firewall testing are overlooked. A new formalized
approach is presented. Recommendations on optimization of test procedures are
given.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
http://arxiv.org/abs/1308.2630v2,"Novel Virtual Moving Sound-based Spatial Auditory Brain-Computer
  Interface Paradigm","This paper reports on a study in which a novel virtual moving sound-based
spatial auditory brain-computer interface (BCI) paradigm is developed. Classic
auditory BCIs rely on spatially static stimuli, which are often boring and
difficult to perceive when subjects have non-uniform spatial hearing perception
characteristics. The concept of moving sound proposed and tested in the paper
allows for the creation of a P300 oddball paradigm of necessary target and
non-target auditory stimuli, which are more interesting and easier to
distinguish. We present a report of our study of seven healthy subjects, which
proves the concept of moving sound stimuli usability for a novel BCI. We
compare online BCI classification results in static and moving sound paradigms
yielding similar accuracy results. The subject preference reports suggest that
the proposed moving sound protocol is more comfortable and easier to
discriminate with the online BCI.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0405084v1,A Framework for Interoperability,"Practical implementations of high-level languages must provide access to
libraries and system services that have APIs specified in a low-level language
(usually C). An important characteristic of such mechanisms is the
foreign-interface policy that defines how to bridge the semantic gap between
the high-level language and C. For example, IDL-based tools generate code to
marshal data into and out of the high-level representation according to user
annotations. The design space of foreign-interface policies is large and there
are pros and cons to each approach. Rather than commit to a particular policy,
we choose to focus on the problem of supporting a gamut of interoperability
policies. In this paper, we describe a framework for language interoperability
that is expressive enough to support very efficient implementations of a wide
range of different foreign-interface policies. We describe two tools that
implement substantially different policies on top of our framework and present
benchmarks that demonstrate their efficiency.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.14460v1,AWLCO: All-Window Length Co-Occurrence,"Analyzing patterns in a sequence of events has applications in text analysis,
computer programming, and genomics research. In this paper, we consider the
all-window-length analysis model which analyzes a sequence of events with
respect to windows of all lengths. We study the exact co-occurrence counting
problem for the all-window-length analysis model. Our first algorithm is an
offline algorithm that counts all-window-length co-occurrences by performing
multiple passes over a sequence and computing single-window-length
co-occurrences. This algorithm has the time complexity $O(n)$ for each window
length and thus a total complexity of $O(n^2)$ and the space complexity
$O(|I|)$ for a sequence of size n and an itemset of size $|I|$. We propose
AWLCO, an online algorithm that computes all-window-length co-occurrences in a
single pass with the expected time complexity of $O(n)$ and space complexity of
$O( \sqrt{ n|I| })$. Following this, we generalize our use case to patterns in
which we propose an algorithm that computes all-window-length co-occurrence
with expected time complexity $O(n|I|)$ and space complexity $O( \sqrt{n|I|} +
e_{max}|I|)$, where $e_{max}$ is the length of the largest pattern.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1210.1097v1,Turing machines based on unsharp quantum logic,"In this paper, we consider Turing machines based on unsharp quantum logic.
For a lattice-ordered quantum multiple-valued (MV) algebra E, we introduce
E-valued non-deterministic Turing machines (ENTMs) and E-valued deterministic
Turing machines (EDTMs). We discuss different E-valued recursively enumerable
languages from width-first and depth-first recognition. We find that
width-first recognition is equal to or less than depth-first recognition in
general. The equivalence requires an underlying E value lattice to degenerate
into an MV algebra. We also study variants of ENTMs. ENTMs with a classical
initial state and ENTMs with a classical final state have the same power as
ENTMs with quantum initial and final states. In particular, the latter can be
simulated by ENTMs with classical transitions under a certain condition. Using
these findings, we prove that ENTMs are not equivalent to EDTMs and that ENTMs
are more powerful than EDTMs. This is a notable difference from the classical
Turing machines.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.03703v1,"Distributed Bayesian Matrix Decomposition for Big Data Mining and
  Clustering","Matrix decomposition is one of the fundamental tools to discover knowledge
from big data generated by modern applications. However, it is still
inefficient or infeasible to process very big data using such a method in a
single machine. Moreover, big data are often distributedly collected and stored
on different machines. Thus, such data generally bear strong heterogeneous
noise. It is essential and useful to develop distributed matrix decomposition
for big data analytics. Such a method should scale up well, model the
heterogeneous noise, and address the communication issue in a distributed
system. To this end, we propose a distributed Bayesian matrix decomposition
model (DBMD) for big data mining and clustering. Specifically, we adopt three
strategies to implement the distributed computing including 1) the accelerated
gradient descent, 2) the alternating direction method of multipliers (ADMM),
and 3) the statistical inference. We investigate the theoretical convergence
behaviors of these algorithms. To address the heterogeneity of the noise, we
propose an optimal plug-in weighted average that reduces the variance of the
estimation. Synthetic experiments validate our theoretical results, and
real-world experiments show that our algorithms scale up well to big data and
achieves superior or competing performance compared to other distributed
methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.07726v1,Leveraging Semantic Parsing for Relation Linking over Knowledge Bases,"Knowledgebase question answering systems are heavily dependent on relation
extraction and linking modules. However, the task of extracting and linking
relations from text to knowledgebases faces two primary challenges; the
ambiguity of natural language and lack of training data. To overcome these
challenges, we present SLING, a relation linking framework which leverages
semantic parsing using Abstract Meaning Representation (AMR) and distant
supervision. SLING integrates multiple relation linking approaches that capture
complementary signals such as linguistic cues, rich semantic representation,
and information from the knowledgebase. The experiments on relation linking
using three KBQA datasets; QALD-7, QALD-9, and LC-QuAD 1.0 demonstrate that the
proposed approach achieves state-of-the-art performance on all benchmarks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9908004v1,Extending the Stable Model Semantics with More Expressive Rules,"The rules associated with propositional logic programs and the stable model
semantics are not expressive enough to let one write concise programs. This
problem is alleviated by introducing some new types of propositional rules.
Together with a decision procedure that has been used as a base for an
efficient implementation, the new rules supplant the standard ones in practical
applications of the stable model semantics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.14812v2,SegmentMeIfYouCan: A Benchmark for Anomaly Segmentation,"State-of-the-art semantic or instance segmentation deep neural networks
(DNNs) are usually trained on a closed set of semantic classes. As such, they
are ill-equipped to handle previously-unseen objects. However, detecting and
localizing such objects is crucial for safety-critical applications such as
perception for automated driving, especially if they appear on the road ahead.
While some methods have tackled the tasks of anomalous or out-of-distribution
object segmentation, progress remains slow, in large part due to the lack of
solid benchmarks; existing datasets either consist of synthetic data, or suffer
from label inconsistencies. In this paper, we bridge this gap by introducing
the ""SegmentMeIfYouCan"" benchmark. Our benchmark addresses two tasks: Anomalous
object segmentation, which considers any previously-unseen object category; and
road obstacle segmentation, which focuses on any object on the road, may it be
known or unknown. We provide two corresponding datasets together with a test
suite performing an in-depth method analysis, considering both established
pixel-wise performance metrics and recent component-wise ones, which are
insensitive to object sizes. We empirically evaluate multiple state-of-the-art
baseline methods, including several models specifically designed for anomaly /
obstacle segmentation, on our datasets and on public ones, using our test
suite. The anomaly and obstacle segmentation results show that our datasets
contribute to the diversity and difficulty of both data landscapes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0011033v1,Web Mining Research: A Survey,"With the huge amount of information available online, the World Wide Web is a
fertile area for data mining research. The Web mining research is at the cross
road of research from several research communities, such as database,
information retrieval, and within AI, especially the sub-areas of machine
learning and natural language processing. However, there is a lot of confusions
when comparing research efforts from different point of views. In this paper,
we survey the research in the area of Web mining, point out some confusions
regarded the usage of the term Web mining and suggest three Web mining
categories. Then we situate some of the research with respect to these three
categories. We also explore the connection between the Web mining categories
and the related agent paradigm. For the survey, we focus on representation
issues, on the process, on the learning algorithm, and on the application of
the recent works as the criteria. We conclude the paper with some research
issues.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.07530v1,"m-arcsinh: An Efficient and Reliable Function for SVM and MLP in
  scikit-learn","This paper describes the 'm-arcsinh', a modified ('m-') version of the
inverse hyperbolic sine function ('arcsinh'). Kernel and activation functions
enable Machine Learning (ML)-based algorithms, such as Support Vector Machine
(SVM) and Multi-Layer Perceptron (MLP), to learn from data in a supervised
manner. m-arcsinh, implemented in the open source Python library
'scikit-learn', is hereby presented as an efficient and reliable kernel and
activation function for SVM and MLP respectively. Improvements in reliability
and speed to convergence in classification tasks on fifteen (N = 15) datasets
available from scikit-learn and the University California Irvine (UCI) Machine
Learning repository are discussed. Experimental results demonstrate the overall
competitive classification performance of both SVM and MLP, achieved via the
proposed function. This function is compared to gold standard kernel and
activation functions, demonstrating its overall competitive reliability
regardless of the complexity of the classification tasks involved.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0709.3826v1,Research Paper on Transaction-Oriented Simulation In Ad Hoc Grids,"This paper analyses the requirements of performing parallel
transaction-oriented simulations with a special focus on the space-parallel
approach and discrete event simulation synchronisation algorithms that are
suitable for transaction-oriented simulation and the target environment of Ad
Hoc Grids. To demonstrate the findings a Java-based parallel
transaction-oriented simulator for the simulation language GPSS/H is
implemented on the basis of the most promising Shock Resistant Time Warp
synchronisation algorithm and using the Grid framework ProActive. The
validation of this parallel simulator shows that the Shock Resistant Time Warp
algorithm can successfully reduce the number of rolled back Transaction moves
but it also reveals circumstances in which the Shock Resistant Time Warp
algorithm can be outperformed by the normal Time Warp algorithm. The conclusion
of this paper suggests possible improvements to the Shock Resistant Time Warp
algorithm to avoid such problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.04007v1,"EXPOSED: An occupant exposure model for confined spaces to retrofit
  crowd models during a pandemic","Crowd models can be used for the simulation of people movement in the built
environment. Crowd model outputs have been used for evaluating safety and
comfort of pedestrians, inform crowd management and perform forensic
investigations. Microscopic crowd models allow the representation of each
person and the obtainment of information concerning their location over time
and interactions with the physical space/other people. Pandemics such as
COVID-19 have posed several questions on safe building usage, given the risk of
disease transmission among building occupants. Here we show how crowd modelling
can be used to assess occupant exposure in confined spaces. The policies
adopted concerning building usage and social distancing during a pandemic can
vary greatly, and they are mostly based on the macroscopic analysis of the
spread of disease rather than a safety assessment performed at a building
level. The proposed model allows the investigation of occupant exposure in
buildings based on the analysis of microscopic people movement. Risk assessment
is performed by retrofitting crowd models with a universal model for exposure
assessment which can account for different types of disease transmissions. This
work allows policy makers to perform informed decisions concerning building
usage during a pandemic.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0807.2972v1,DescribeX: A Framework for Exploring and Querying XML Web Collections,"This thesis introduces DescribeX, a powerful framework that is capable of
describing arbitrarily complex XML summaries of web collections, providing
support for more efficient evaluation of XPath workloads. DescribeX permits the
declarative description of document structure using all axes and language
constructs in XPath, and generalizes many of the XML indexing and summarization
approaches in the literature. DescribeX supports the construction of
heterogeneous summaries where different document elements sharing a common
structure can be declaratively defined and refined by means of path regular
expressions on axes, or axis path regular expression (AxPREs). DescribeX can
significantly help in the understanding of both the structure of complex,
heterogeneous XML collections and the behaviour of XPath queries evaluated on
them.
  Experimental results demonstrate the scalability of DescribeX summary
refinements and stabilizations (the key enablers for tailoring summaries) with
multi-gigabyte web collections. A comparative study suggests that using a
DescribeX summary created from a given workload can produce query evaluation
times orders of magnitude better than using existing summaries. DescribeX's
light-weight approach of combining summaries with a file-at-a-time XPath
processor can be a very competitive alternative, in terms of performance, to
conventional fully-fledged XML query engines that provide DB-like functionality
such as security, transaction processing, and native storage.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.12582v3,Differentials and distances in probabilistic coherence spaces,"In probabilistic coherence spaces, a denotational model of probabilistic
functional languages, morphisms are analytic and therefore smooth. We explore
two related applications of the corresponding derivatives. First we show how
derivatives allow to compute the expectation of execution time in the weak head
reduction of probabilistic PCF (pPCF). Next we apply a general notion of
""local"" differential of morphisms to the proof of a Lipschitz property of these
morphisms allowing in turn to relate the observational distance on pPCF terms
to a distance the model is naturally equipped with. This suggests that
extending probabilistic programming languages with derivatives, in the spirit
of the differential lambda-calculus, could be quite meaningful.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1309.1264v1,Reversible Logic Elements with Memory and Their Universality,"Reversible computing is a paradigm of computation that reflects physical
reversibility, one of the fundamental microscopic laws of Nature. In this
survey, we discuss topics on reversible logic elements with memory (RLEM),
which can be used to build reversible computing systems, and their
universality. An RLEM is called universal, if any reversible sequential machine
(RSM) can be realized as a circuit composed only of it. Since a finite-state
control and a tape cell of a reversible Turing machine (RTM) are formalized as
RSMs, any RTM can be constructed from a universal RLEM. Here, we investigate
2-state RLEMs, and show that infinitely many kinds of non-degenerate RLEMs are
all universal besides only four exceptions. Non-universality of these
exceptional RLEMs is also argued.",0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1608.05327v2,"A Short Counterexample Property for Safety and Liveness Verification of
  Fault-tolerant Distributed Algorithms","Distributed algorithms have many mission-critical applications ranging from
embedded systems and replicated databases to cloud computing. Due to
asynchronous communication, process faults, or network failures, these
algorithms are difficult to design and verify. Many algorithms achieve fault
tolerance by using threshold guards that, for instance, ensure that a process
waits until it has received an acknowledgment from a majority of its peers.
Consequently, domain-specific languages for fault-tolerant distributed systems
offer language support for threshold guards.
  We introduce an automated method for model checking of safety and liveness of
threshold-guarded distributed algorithms in systems where the number of
processes and the fraction of faulty processes are parameters. Our method is
based on a short counterexample property: if a distributed algorithm violates a
temporal specification (in a fragment of LTL), then there is a counterexample
whose length is bounded and independent of the parameters. We prove this
property by (i) characterizing executions depending on the structure of the
temporal formula, and (ii) using commutativity of transitions to accelerate and
shorten executions. We extended the ByMC toolset (Byzantine Model Checker) with
our technique, and verified liveness and safety of 10 prominent fault-tolerant
distributed algorithms, most of which were out of reach for existing
techniques.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.14434v3,"Nonnegative Tucker Decomposition with Beta-divergence for Music
  Structure Analysis of audio signals","Nonnegative Tucker Decomposition (NTD), a tensor decomposition model, has
received increased interest in the recent years because of its ability to
blindly extract meaningful patterns in tensor data. Nevertheless, existing
algorithms to compute NTD are mostly designed for the Euclidean loss. On the
other hand, NTD has recently proven to be a powerful tool in Music Information
Retrieval. This work proposes a Multiplicative Updates algorithm to compute NTD
with the beta-divergence loss, often considered a better loss for audio
processing. We notably show how to implement efficiently the multiplicative
rules using tensor algebra, a naive approach being intractable. Finally, we
show on a Music Structure Analysis task that unsupervised NTD fitted with
beta-divergence loss outperforms earlier results obtained with the Euclidean
loss.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.6334v1,"TATI -- A Logo-like interface for microworlds and simulations for
  physics teaching in Second Life","Student difficulties in learning Physics have been thoroughly discussed in
the scientific literature. Already in 1980, Papert complained that schools
teach Newtonian motion by manipulating equations rather than by manipulating
the Newtonian objects themselves, what would be possible in a 'physics
microworld'. On the other hand, Second Life and its scripting language have a
remarkable learning curve that discourages most teachers at using it as an
environment for educational computer simulations and microworlds. The objective
of this work is to describe TATI, a textual interface which, through TATILogo,
an accessible Logo language extension, allows the generation of various physics
microworlds in Second Life, containing different types of objects that follow
different physical laws, providing a learning path into Newtonian Physics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1811.05116v2,Programs as the Language of Science,"Currently it is widely accepted that the language of science is mathematics.
This book explores an alternative idea where the future of science is based on
the language of algorithms and programs. How such a language can actually be
implemented in the sciences is outlined in some detail. We start by
constructing a simple formal system where statements are represented as
programs and inference is based on computability as opposed to the classical
notion of truth value assignments.
  The focus is on theories where the intrinsic properties and dynamic state of
real world objects can be defined in terms of information and subject to laws
based on simple deterministic rules and finite state arithmetic. Such models,
it is argued, not only offer alternative simulation tools, as opposed to those
based on discrete approximations of conventional continuum theories, but in
themselves can be regarded as a language that describes the physical laws at a
fundamental level. This book does not examine any specific application in
detail but rather attempts to lay down a foundation for the validation of such
theories by employing the inference scheme based on computability logic.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.07002v1,Jittering Samples using a kd-Tree Stratification,"Monte Carlo sampling techniques are used to estimate high-dimensional
integrals that model the physics of light transport in virtual scenes for
computer graphics applications. These methods rely on the law of large numbers
to estimate expectations via simulation, typically resulting in slow
convergence. Their errors usually manifest as undesirable grain in the pictures
generated by image synthesis algorithms. It is well known that these errors
diminish when the samples are chosen appropriately. A well known technique for
reducing error operates by subdividing the integration domain, estimating
integrals in each \emph{stratum} and aggregating these values into a stratified
sampling estimate. Na\""{i}ve methods for stratification, based on a lattice
(grid) are known to improve the convergence rate of Monte Carlo, but require
samples that grow exponentially with the dimensionality of the domain.
  We propose a simple stratification scheme for $d$ dimensional hypercubes
using the kd-tree data structure. Our scheme enables the generation of an
arbitrary number of equal volume partitions of the rectangular domain, and $n$
samples can be generated in $O(n)$ time. Since we do not always need to
explicitly build a kd-tree, we provide a simple procedure that allows the
sample set to be drawn fully in parallel without any precomputation or storage,
speeding up sampling to $O(\log n)$ time per sample when executed on $n$ cores.
If the tree is implicitly precomputed ($O(n)$ storage) the parallelised run
time reduces to $O(1)$ on $n$ cores. In addition to these benefits, we provide
an upper bound on the worst case star-discrepancy for $n$ samples matching that
of lattice-based sampling strategies, which occur as a special case of our
proposed method. We use a number of quantitative and qualitative tests to
compare our method against state of the art samplers for image synthesis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.07800v2,"A Separation Logic to Verify Termination of Busy-Waiting for Abrupt
  Program Exit","Programs for multiprocessor machines commonly perform busy-waiting for
synchronisation. In this paper, we make a first step towards proving
termination of such programs. We approximate (i) arbitrary waitable events by
abrupt program termination and (ii) busy-waiting for events by busy-waiting to
be abruptly terminated.
  We propose a separation logic for modularly verifying termination (under fair
scheduling) of programs where some threads eventually abruptly terminate the
program, and other threads busy-wait for this to happen.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0304015v1,"A Performance Study of Monitoring and Information Services for
  Distributed Systems","Monitoring and information services form a key component of a distributed
system, or Grid. A quantitative study of such services can aid in understanding
the performance limitations, advise in the deployment of the systems, and help
evaluate future development work. To this end, we study the performance of
three monitoring and information services for distributed systems: the Globus
Toolkit's Monitoring and Discovery Service (MDS), the European Data Grid
Relational Grid Monitoring Architecture (R-GMA), and Hawkeye, part of the
Condor project. We perform experiments to test their scalability with respect
to number of users, number of resources, and amount of data collected. Our
study shows that each approach has different behaviors, often due to their
different design goals. In the four sets of experiments we conducted to
evaluate the performance of the service components under different
circumstances, we found a strong advantage to caching or prefetching the data,
as well as the need to have primary components at well connected sites due to
high load seen by all systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.11589v1,"SBNet: Segmentation-based Network for Natural Language-based Vehicle
  Search","Natural language-based vehicle retrieval is a task to find a target vehicle
within a given image based on a natural language description as a query. This
technology can be applied to various areas including police searching for a
suspect vehicle. However, it is challenging due to the ambiguity of language
descriptions and the difficulty of processing multi-modal data. To tackle this
problem, we propose a deep neural network called SBNet that performs natural
language-based segmentation for vehicle retrieval. We also propose two
task-specific modules to improve performance: a substitution module that helps
features from different domains to be embedded in the same space and a future
prediction module that learns temporal information. SBnet has been trained
using the CityFlow-NL dataset that contains 2,498 tracks of vehicles with three
unique natural language descriptions each and tested 530 unique vehicle tracks
and their corresponding query sets. SBNet achieved a significant improvement
over the baseline in the natural language-based vehicle tracking track in the
AI City Challenge 2021.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.3427v1,The Geometry of Synchronization (Long Version),"We graft synchronization onto Girard's Geometry of Interaction in its most
concrete form, namely token machines. This is realized by introducing
proof-nets for SMLL, an extension of multiplicative linear logic with a
specific construct modeling synchronization points, and of a multi-token
abstract machine model for it. Interestingly, the correctness criterion ensures
the absence of deadlocks along reduction and in the underlying machine, this
way linking logical and operational properties.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0201019v1,"Structure from Motion: Theoretical Foundations of a Novel Approach Using
  Custom Built Invariants","We rephrase the problem of 3D reconstruction from images in terms of
intersections of projections of orbits of custom built Lie groups actions. We
then use an algorithmic method based on moving frames ""a la Fels-Olver"" to
obtain a fundamental set of invariants of these groups actions. The invariants
are used to define a set of equations to be solved by the points of the 3D
object, providing a new technique for recovering 3D structure from motion.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0311050v1,"Data mining and Privacy in Public Sector using Intelligent Agents
  (discussion paper)","The public sector comprises government agencies, ministries, education
institutions, health providers and other types of government, commercial and
not-for-profit organisations. Unlike commercial enterprises, this environment
is highly heterogeneous in all aspects. This forms a complex network which is
not always optimised. A lack of optimisation and communication hinders
information sharing between the network nodes limiting the flow of information.
Another limiting aspect is privacy of personal information and security of
operations of some nodes or segments of the network. Attempts to reorganise the
network or improve communications to make more information available for
sharing and analysis may be hindered or completely halted by public concerns
over privacy, political agendas, social and technological barriers. This paper
discusses a technical solution for information sharing while addressing the
privacy concerns with no need for reorganisation of the existing public sector
infrastructure . The solution is based on imposing an additional layer of
Intelligent Software Agents and Knowledge Bases for data mining and analysis.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1006.5096v1,"Automatic Probabilistic Program Verification through Random Variable
  Abstraction","The weakest pre-expectation calculus has been proved to be a mature theory to
analyze quantitative properties of probabilistic and nondeterministic programs.
We present an automatic method for proving quantitative linear properties on
any denumerable state space using iterative backwards fixed point calculation
in the general framework of abstract interpretation. In order to accomplish
this task we present the technique of random variable abstraction (RVA) and we
also postulate a sufficient condition to achieve exact fixed point computation
in the abstract domain. The feasibility of our approach is shown with two
examples, one obtaining the expected running time of a probabilistic program,
and the other the expected gain of a gambling strategy.
  Our method works on general guarded probabilistic and nondeterministic
transition systems instead of plain pGCL programs, allowing us to easily model
a wide range of systems including distributed ones and unstructured programs.
We present the operational and weakest precondition semantics for this programs
and prove its equivalence.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.01197v3,Sample Prior Guided Robust Model Learning to Suppress Noisy Labels,"Imperfect labels are ubiquitous in real-world datasets and seriously harm the
model performance. Several recent effective methods for handling noisy labels
have two key steps: 1) dividing samples into cleanly labeled and wrongly
labeled sets by training loss, 2) using semi-supervised methods to generate
pseudo-labels for samples in the wrongly labeled set. However, current methods
always hurt the informative hard samples due to the similar loss distribution
between the hard samples and the noisy ones. In this paper, we proposed PGDF
(Prior Guided Denoising Framework), a novel framework to learn a deep model to
suppress noise by generating the samples' prior knowledge, which is integrated
into both dividing samples step and semi-supervised step. Our framework can
save more informative hard clean samples into the cleanly labeled set. Besides,
our framework also promotes the quality of pseudo-labels during the
semi-supervised step by suppressing the noise in the current pseudo-labels
generating scheme. To further enhance the hard samples, we reweight the samples
in the cleanly labeled set during training. We evaluated our method using
synthetic datasets based on CIFAR-10 and CIFAR-100, as well as on the
real-world datasets WebVision and Clothing1M. The results demonstrate
substantial improvements over state-of-the-art methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1404.7765v2,"A semantic network-based evolutionary algorithm for computational
  creativity","We introduce a novel evolutionary algorithm (EA) with a semantic
network-based representation. For enabling this, we establish new formulations
of EA variation operators, crossover and mutation, that we adapt to work on
semantic networks. The algorithm employs commonsense reasoning to ensure all
operations preserve the meaningfulness of the networks, using ConceptNet and
WordNet knowledge bases. The algorithm can be interpreted as a novel memetic
algorithm (MA), given that (1) individuals represent pieces of information that
undergo evolution, as in the original sense of memetics as it was introduced by
Dawkins; and (2) this is different from existing MA, where the word ""memetic""
has been used as a synonym for local refinement after global optimization. For
evaluating the approach, we introduce an analogical similarity-based fitness
measure that is computed through structure mapping. This setup enables the
open-ended generation of networks analogous to a given base network.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.13566v2,"An Experience of Introducing Primary School Children to Programming
  using Ozobots (Practical Report)","Algorithmic thinking is a central concept in the context of computational
thinking, and it is commonly taught by computer programming. A recent trend is
to introduce basic programming concepts already very early on at primary school
level. There are, however, several challenges in teaching programming at this
level: Schools and teachers are often neither equipped nor trained
appropriately, and the best way to move from initial ""unplugged"" activities to
creating programs on a computer are still a matter of open debate. In this
paper, we describe our experience of a small INTERREG-project aiming at
supporting local primary schools in introducing children to programming
concepts using Ozobot robots. These robots have two distinct advantages: First,
they can be programmed with and without computers, thus helping the transition
from unplugged programming to programming with a computer. Second, they are
small and easy to transport, even when used together with tablet computers.
Although we learned in our outreach events that the use of Ozobots is not
without challenges, our overall experience is positive and can hopefully
support others in setting up first encounters with programming at primary
schools.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1704.01785v1,Geometry of Policy Improvement,"We investigate the geometry of optimal memoryless time independent decision
making in relation to the amount of information that the acting agent has about
the state of the system. We show that the expected long term reward, discounted
or per time step, is maximized by policies that randomize among at most $k$
actions whenever at most $k$ world states are consistent with the agent's
observation. Moreover, we show that the expected reward per time step can be
studied in terms of the expected discounted reward. Our main tool is a
geometric version of the policy improvement lemma, which identifies a
polyhedral cone of policy changes in which the state value function increases
for all states.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9908018v1,Construction of regular languages and recognizability of polynomials,"A generalization of numeration system in which the set N of the natural
numbers is recognizable by finite automata can be obtained by describing a
lexicographically ordered infinite regular language. Here we show that if P
belonging to Q[x] is a polynomial such that P(N) is a subset of N then we can
construct a numeration system in which the set of representations of P(N) is
regular. The main issue in this construction is to setup a regular language
with a density function equals to P(n+1)-P(n) for n large enough.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1010.2354v1,Predicting Coding Effort in Projects Containing XML Code,"This paper studies the problem of predicting the coding effort for a
subsequent year of development by analysing metrics extracted from project
repositories, with an emphasis on projects containing XML code. The study
considers thirteen open source projects and applies machine learning algorithms
to generate models to predict one-year coding effort, measured in terms of
lines of code added, modified and deleted. Both organisational and code metrics
associated to revisions are taken into account. The results show that coding
effort is highly determined by the expertise of developers while source code
metrics have little effect on improving the accuracy of estimations of coding
effort. The study also shows that models trained on one project are unreliable
at estimating effort in other projects.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0103015v1,Fitness Uniform Selection to Preserve Genetic Diversity,"In evolutionary algorithms, the fitness of a population increases with time
by mutating and recombining individuals and by a biased selection of more fit
individuals. The right selection pressure is critical in ensuring sufficient
optimization progress on the one hand and in preserving genetic diversity to be
able to escape from local optima on the other. We propose a new selection
scheme, which is uniform in the fitness values. It generates selection pressure
towards sparsely populated fitness regions, not necessarily towards higher
fitness, as is the case for all other selection schemes. We show that the new
selection scheme can be much more effective than standard selection schemes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1008.5357v1,Preference Elicitation in Prioritized Skyline Queries,"Preference queries incorporate the notion of binary preference relation into
relational database querying. Instead of returning all the answers, such
queries return only the best answers, according to a given preference relation.
Preference queries are a fast growing area of database research. Skyline
queries constitute one of the most thoroughly studied classes of preference
queries. A well known limitation of skyline queries is that skyline preference
relations assign the same importance to all attributes. In this work, we study
p-skyline queries that generalize skyline queries by allowing varying attribute
importance in preference relations. We perform an in-depth study of the
properties of p-skyline preference relations. In particular,we study the
problems of containment and minimal extension. We apply the obtained results to
the central problem of the paper: eliciting relative importance of attributes.
Relative importance is implicit in the constructed p-skyline preference
relation. The elicitation is based on user-selected sets of superior (positive)
and inferior (negative) examples. We show that the computational complexity of
elicitation depends on whether inferior examples are involved. If they are not,
elicitation can be achieved in polynomial time. Otherwise, it is NP-complete.
Our experiments show that the proposed elicitation algorithm has high accuracy
and good scalability",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0306066v1,The COMPASS Event Store in 2002,"COMPASS, the fixed-target experiment at CERN studying the structure of the
nucleon and spectroscopy, collected over 260 TB during summer 2002 run. All
these data, together with reconstructed events information, were put from the
beginning in a database infrastructure based on Objectivity/DB and on the
hierarchical storage manager CASTOR. The experience in the usage of the
database is reviewed and the evolution of the system outlined.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0109034v1,"Relevant Knowledge First - Reinforcement Learning and Forgetting in
  Knowledge Based Configuration","In order to solve complex configuration tasks in technical domains, various
knowledge based methods have been developed. However their applicability is
often unsuccessful due to their low efficiency. One of the reasons for this is
that (parts of the) problems have to be solved again and again, instead of
being ""learnt"" from preceding processes. However, learning processes bring with
them the problem of conservatism, for in technical domains innovation is a
deciding factor in competition. On the other hand a certain amount of
conservatism is often desired since uncontrolled innovation as a rule is also
detrimental. This paper proposes the heuristic RKF (Relevant Knowledge First)
for making decisions in configuration processes based on the so-called
relevance of objects in a knowledge base. The underlying relevance-function has
two components, one based on reinforcement learning and the other based on
forgetting (fading). Relevance of an object increases with its successful use
and decreases with age when it is not used. RKF has been developed to speed up
the configuration process and to improve the quality of the solutions relative
to the reward value that is given by users.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.08529v1,"A polynomial-time randomized reduction from tournament isomorphism to
  tournament asymmetry","The paper develops a new technique to extract a characteristic subset from a
random source that repeatedly samples from a set of elements. Here a
characteristic subset is a set that when containing an element contains all
elements that have the same probability. With this technique at hand the paper
looks at the special case of the tournament isomorphism problem that stands in
the way towards a polynomial-time algorithm for the graph isomorphism problem.
Noting that there is a reduction from the automorphism (asymmetry) problem to
the isomorphism problem, a reduction in the other direction is nevertheless not
known and remains a thorny open problem. Applying the new technique, we develop
a randomized polynomial-time Turing-reduction from the tournament isomorphism
problem to the tournament automorphism problem. This is the first such
reduction for any kind of combinatorial object not known to have a
polynomial-time solvable isomorphism problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1105.0121v1,Methods of Hierarchical Clustering,"We survey agglomerative hierarchical clustering algorithms and discuss
efficient implementations that are available in R and other software
environments. We look at hierarchical self-organizing maps, and mixture models.
We review grid-based clustering, focusing on hierarchical density-based
approaches. Finally we describe a recently developed very efficient (linear
time) hierarchical clustering algorithm, which can also be viewed as a
hierarchical grid-based algorithm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.03190v1,"Generative Melody Composition with Human-in-the-Loop Bayesian
  Optimization","Deep generative models allow even novice composers to generate various
melodies by sampling latent vectors. However, finding the desired melody is
challenging since the latent space is unintuitive and high-dimensional. In this
work, we present an interactive system that supports generative melody
composition with human-in-the-loop Bayesian optimization (BO). This system
takes a mixed-initiative approach; the system generates candidate melodies to
evaluate, and the user evaluates them and provides preferential feedback (i.e.,
picking the best melody among the candidates) to the system. This process is
iteratively performed based on BO techniques until the user finds the desired
melody. We conducted a pilot study using our prototype system, suggesting the
potential of this approach.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1303.2981v3,On the Complexity of the Orbit Problem,"We consider higher-dimensional versions of Kannan and Lipton's Orbit
Problem---determining whether a target vector space V may be reached from a
starting point x under repeated applications of a linear transformation A.
Answering two questions posed by Kannan and Lipton in the 1980s, we show that
when V has dimension one, this problem is solvable in polynomial time, and when
V has dimension two or three, the problem is in NP^{RP}.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.02842v1,Concurrency Theorems for Non-linear Rewriting Theories,"Sesqui-pushout (SqPO) rewriting along non-linear rules and for monic matches
is well-known to permit the modeling of fusing and cloning of vertices and
edges, yet to date, no construction of a suitable concurrency theorem was
available. The lack of such a theorem, in turn, rendered compositional
reasoning for such rewriting systems largely infeasible. We develop in this
paper a suitable concurrency theorem for non-linear SqPO-rewriting in
categories that are quasi-topoi (subsuming the example of adhesive categories)
and with matches required to be regular monomorphisms of the given category.
Our construction reveals an interesting ""backpropagation effect"" in computing
rule compositions. We derive in addition a concurrency theorem for non-linear
double pushout (DPO) rewriting in rm-adhesive categories. Our results open
non-linear SqPO and DPO semantics to the rich static analysis techniques
available from concurrency, rule algebra and tracelet theory.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.04771v2,DopeLearning: A Computational Approach to Rap Lyrics Generation,"Writing rap lyrics requires both creativity to construct a meaningful,
interesting story and lyrical skills to produce complex rhyme patterns, which
form the cornerstone of good flow. We present a rap lyrics generation method
that captures both of these aspects. First, we develop a prediction model to
identify the next line of existing lyrics from a set of candidate next lines.
This model is based on two machine-learning techniques: the RankSVM algorithm
and a deep neural network model with a novel structure. Results show that the
prediction model can identify the true next line among 299 randomly selected
lines with an accuracy of 17%, i.e., over 50 times more likely than by random.
Second, we employ the prediction model to combine lines from existing songs,
producing lyrics with rhyme and a meaning. An evaluation of the produced lyrics
shows that in terms of quantitative rhyme density, the method outperforms the
best human rappers by 21%. The rap lyrics generator has been deployed as an
online tool called DeepBeat, and the performance of the tool has been assessed
by analyzing its usage logs. This analysis shows that machine-learned rankings
correlate with user preferences.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.13404v1,"Compounding Injustice: History and Prediction in Carceral
  Decision-Making","Risk assessment algorithms in criminal justice put people's lives at the
discretion of a simple statistical tool. This thesis explores how algorithmic
decision-making in criminal policy can exhibit feedback effects, where
disadvantage accumulates among those deemed 'high risk' by the state. Evidence
from Philadelphia suggests that risk - and, by extension, criminality - is not
fundamental or in any way exogenous to political decision-making. A close look
at the geographical and demographic properties of risk calls into question the
current practice of prediction in criminal policy. Using court docket summaries
from Philadelphia, we find evidence of a criminogenic effect of incarceration,
even controlling for existing determinants of 'criminal risk'. With evidence
that criminal treatment can influence future criminal convictions, we explore
the theoretical implications of compounding effects in repeated carceral
decisions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,1,1,0,0,0
http://arxiv.org/abs/1808.06921v1,Stable divisorial gonality is in NP,"Divisorial gonality and stable divisorial gonality are graph parameters,
which have an origin in algebraic geometry. Divisorial gonality of a connected
graph $G$ can be defined with help of a chip firing game on $G$. The stable
divisorial gonality of $G$ is the minimum divisorial gonality over all
subdivisions of edges of $G$.
  In this paper we prove that deciding whether a given connected graph has
stable divisorial gonality at most a given integer $k$ belongs to the class NP.
Combined with the result that (stable) divisorial gonality is NP-hard by
Gijswijt, we obtain that stable divisorial gonality is NP-complete. The proof
consist of a partial certificate that can be verified by solving an Integer
Linear Programming instance. As a corollary, we have that the number of
subdivisions needed for minimum stable divisorial gonality of a graph with $n$
vertices is bounded by $2^{p(n)}$ for a polynomial $p$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1905.03899v1,Integrating Artificial Intelligence into Weapon Systems,"The integration of Artificial Intelligence (AI) into weapon systems is one of
the most consequential tactical and strategic decisions in the history of
warfare. Current AI development is a remarkable combination of accelerating
capability, hidden decision mechanisms, and decreasing costs. Implementation of
these systems is in its infancy and exists on a spectrum from resilient and
flexible to simplistic and brittle. Resilient systems should be able to
effectively handle the complexities of a high-dimensional battlespace.
Simplistic AI implementations could be manipulated by an adversarial AI that
identifies and exploits their weaknesses.
  In this paper, we present a framework for understanding the development of
dynamic AI/ML systems that interactively and continuously adapt to their user's
needs. We explore the implications of increasingly capable AI in the kill chain
and how this will lead inevitably to a fully automated, always on system,
barring regulation by treaty. We examine the potential of total integration of
cyber and physical security and how this likelihood must inform the development
of AI-enabled systems with respect to the ""fog of war"", human morals, and
ethics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
http://arxiv.org/abs/0906.3895v1,A Parallelism-Based Approach to Network Anonymization,"Considering topologies of anonymous networks we used to organizing anonymous
communication into hard to trace paths hiding its origin or destination. In
anonymity the company is crucial, however the serial transportation imposes a
costly tradeoff between a level of privacy and a speed of communication.
  This paper introduces a framework of a novel architecture for anonymous
networks that hides initiators of communications by parallelization of
anonymous links. The new approach, which is based on the grounds of the
anonymous P2P network called P2Priv, does not require content forwarding via a
chain of proxy nodes to assure high degree of anonymity. Contrary to P2Priv,
the new architecture can be suited to anonymization of various network
communications, including anonymous access to distributed as well as
client-server services. In particular, it can be considered as an anonymization
platform for these network applications where both privacy and low delays are
required.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.05951v1,Deducing properties of ODEs from their discretization,"We show that some hard to detect properties of quadratic ODEs (eg certain
preserved integrals and measures) can be deduced more or less algorithmically
from their Kahan discretization, using Darboux Polynomials (DPs). Somewhat
similar results hold for ODEs of higher degree/order using certain other
birational discretization methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1211.5656v1,On Groupoids and Hypergraphs,"We present a novel construction of finite groupoids whose Cayley graphs have
large girth even w.r.t. a discounted distance measure that contracts
arbitrarily long sequences of edges from the same colour class (sub-groupoid),
and only counts transitions between colour classes (cosets). These groupoids
are employed towards a generic construction method for finite hypergraphs that
realise specified overlap patterns and avoid small cyclic configurations. The
constructions are based on reduced products with groupoids generated by the
elementary local extension steps, and can be made to preserve the symmetries of
the given overlap pattern. In particular, we obtain highly symmetric, finite
hypergraph coverings without short cycles. The groupoids and their application
in reduced products are sufficiently generic to be applicable to other
constructions that are specified in terms of local glueing operations and
require global finite closure.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.02333v1,Random VLAD based Deep Hashing for Efficient Image Retrieval,"Image hash algorithms generate compact binary representations that can be
quickly matched by Hamming distance, thus become an efficient solution for
large-scale image retrieval. This paper proposes RV-SSDH, a deep image hash
algorithm that incorporates the classical VLAD (vector of locally aggregated
descriptors) architecture into neural networks. Specifically, a novel neural
network component is formed by coupling a random VLAD layer with a latent hash
layer through a transform layer. This component can be combined with
convolutional layers to realize a hash algorithm. We implement RV-SSDH as a
point-wise algorithm that can be efficiently trained by minimizing
classification error and quantization loss. Comprehensive experiments show this
new architecture significantly outperforms baselines such as NetVLAD and SSDH,
and offers a cost-effective trade-off in the state-of-the-art. In addition, the
proposed random VLAD layer leads to satisfactory accuracy with low complexity,
thus shows promising potentials as an alternative to NetVLAD.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1703.07682v2,A Weakest Pre-Expectation Semantics for Mixed-Sign Expectations,"We present a weakest-precondition-style calculus for reasoning about the
expected values (pre-expectations) of \emph{mixed-sign unbounded} random
variables after execution of a probabilistic program. The semantics of a
while-loop is well-defined as the limit of iteratively applying a functional to
a zero-element just as in the traditional weakest pre-expectation calculus,
even though a standard least fixed point argument is not applicable in this
context. A striking feature of our semantics is that it is always well-defined,
even if the expected values do not exist. We show that the calculus is sound,
allows for compositional reasoning, and present an invariant-based approach for
reasoning about pre-expectations of loops.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1112.3062v1,"Using Provenance to support Good Laboratory Practice in Grid
  Environments","Conducting experiments and documenting results is daily business of
scientists. Good and traceable documentation enables other scientists to
confirm procedures and results for increased credibility. Documentation and
scientific conduct are regulated and termed as ""good laboratory practice.""
Laboratory notebooks are used to record each step in conducting an experiment
and processing data. Originally, these notebooks were paper based. Due to
computerised research systems, acquired data became more elaborate, thus
increasing the need for electronic notebooks with data storage, computational
features and reliable electronic documentation. As a new approach to this, a
scientific data management system (DataFinder) is enhanced with features for
traceable documentation. Provenance recording is used to meet requirements of
traceability, and this information can later be queried for further analysis.
DataFinder has further important features for scientific documentation: It
employs a heterogeneous and distributed data storage concept. This enables
access to different types of data storage systems (e. g. Grid data
infrastructure, file servers). In this chapter we describe a number of building
blocks that are available or close to finished development. These components
are intended for assembling an electronic laboratory notebook for use in Grid
environments, while retaining maximal flexibility on usage scenarios as well as
maximal compatibility overlap towards each other. Through the usage of such a
system, provenance can successfully be used to trace the scientific workflow of
preparation, execution, evaluation, interpretation and archiving of research
data. The reliability of research results increases and the research process
remains transparent to remote research partners.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/cs/0003025v1,Logic Programming for Describing and Solving Planning Problems,"A logic programming paradigm which expresses solutions to problems as stable
models has recently been promoted as a declarative approach to solving various
combinatorial and search problems, including planning problems. In this
paradigm, all program rules are considered as constraints and solutions are
stable models of the rule set. This is a rather radical departure from the
standard paradigm of logic programming. In this paper we revisit abductive
logic programming and argue that it allows a programming style which is as
declarative as programming based on stable models. However, within abductive
logic programming, one has two kinds of rules. On the one hand predicate
definitions (which may depend on the abducibles) which are nothing else than
standard logic programs (with their non-monotonic semantics when containing
with negation); on the other hand rules which constrain the models for the
abducibles. In this sense abductive logic programming is a smooth extension of
the standard paradigm of logic programming, not a radical departure.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0508003v5,Model Checking Probabilistic Pushdown Automata,"We consider the model checking problem for probabilistic pushdown automata
(pPDA) and properties expressible in various probabilistic logics. We start
with properties that can be formulated as instances of a generalized random
walk problem. We prove that both qualitative and quantitative model checking
for this class of properties and pPDA is decidable. Then we show that model
checking for the qualitative fragment of the logic PCTL and pPDA is also
decidable. Moreover, we develop an error-tolerant model checking algorithm for
PCTL and the subclass of stateless pPDA. Finally, we consider the class of
omega-regular properties and show that both qualitative and quantitative model
checking for pPDA is decidable.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.10057v3,"Retracing some paths in categorical semantics: From
  process-propositions-as-types to categorified reals and computers","The logical parallelism of propositional connectives and type constructors
extends beyond the static realm of predicates, to the dynamic realm of
processes. Understanding the logical parallelism of process propositions and
dynamic types was one of the central problems of the semantics of computation,
albeit not always clear or explicit. It sprung into clarity through the early
work of Samson Abramsky, where the central ideas of denotational semantics and
process calculus were brought together and analyzed by categorical tools, e.g.
in the structure of interaction categories. While some logical structures borne
of dynamics of computation immediately started to emerge, others had to wait,
be it because the underlying logical principles (mainly those arising from
coinduction) were not yet sufficiently well-understood, or simply because the
research community was more interested in other semantical tasks. Looking back,
it seems that the process logic uncovered by those early semantical efforts
might still be starting to emerge and that the vast field of results that have
been obtained in the meantime might be a valley on a tip of an iceberg.
  In the present paper, I try to provide a logical overview of the gamut of
interaction categories and to distinguish those that model computation from
those that capture processes in general. The main coinductive constructions
turn out to be of this latter kind, as illustrated towards the end of the paper
by a compact category of all real numbers as processes, computable and
uncomputable, with polarized bisimulations as morphisms. The addition of the
reals arises as the biproduct, real vector spaces are the enriched
bicompletions, and linear algebra arises from the enriched kan extensions. At
the final step, I sketch a structure that characterizes the computable fragment
of categorical semantics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.05828v2,A Co-contextual Type Checker for Featherweight Java (incl. Proofs),"This paper addresses compositional and incremental type checking for
object-oriented programming languages. Recent work achieved incremental type
checking for structurally typed functional languages through co-contextual
typing rules, a constraint-based formulation that removes any context
dependency for expression typings. However, that work does not cover key
features of object-oriented languages: Subtype polymorphism, nominal typing,
and implementation inheritance. Type checkers encode these features in the form
of class tables, an additional form of typing context inhibiting
incrementalization. In the present work, we demonstrate that an appropriate
co-contextual notion to class tables exists, paving the way to efficient
incremental type checkers for object-oriented languages. This yields a novel
formulation of Igarashi et al.'s Featherweight Java (FJ) type system, where we
replace class tables by the dual concept of class table requirements and class
table operations by dual operations on class table requirements. We prove the
equivalence of FJ's type system and our co-contextual formulation. Based on our
formulation, we implemented an incremental FJ type checker and compared its
performance against javac on a number of realistic example programs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1608.00375v1,Hiding Individuals and Communities in a Social Network,"The Internet and social media have fueled enormous interest in social network
analysis. New tools continue to be developed and used to analyse our personal
connections, with particular emphasis on detecting communities or identifying
key individuals in a social network. This raises privacy concerns that are
likely to exacerbate in the future. With this in mind, we ask the question: Can
individuals or groups actively manage their connections to evade social network
analysis tools?
  By addressing this question, the general public may better protect their
privacy, oppressed activist groups may better conceal their existence, and
security agencies may better understand how terrorists escape detection. We
first study how an individual can evade ""network centrality"" analysis without
compromising his or her influence within the network. We prove that an optimal
solution to this problem is hard to compute. Despite this hardness, we
demonstrate that even a simple heuristic, whereby attention is restricted to
the individual's immediate neighbourhood, can be surprisingly effective in
practice. For instance, it could disguise Mohamed Atta's leading position
within the WTC terrorist network, and that is by rewiring a strikingly-small
number of connections. Next, we study how a community can increase the
likelihood of being overlooked by community-detection algorithms. We propose a
measure of concealment, expressing how well a community is hidden, and use it
to demonstrate the effectiveness of a simple heuristic, whereby members of the
community either ""unfriend"" certain other members, or ""befriend"" some
non-members, in a coordinated effort to camouflage their community.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1102.5433v4,"On the van der Waerden numbers w(2;3,t)","We present results and conjectures on the van der Waerden numbers w(2;3,t)
and on the new palindromic van der Waerden numbers pdw(2;3,t). We have computed
the new number w(2;3,19) = 349, and we provide lower bounds for 20 <= t <= 39,
where for t <= 30 we conjecture these lower bounds to be exact. The lower
bounds for 24 <= t <= 30 refute the conjecture that w(2;3,t) <= t^2, and we
present an improved conjecture. We also investigate regularities in the good
partitions (certificates) to better understand the lower bounds.
  Motivated by such reglarities, we introduce *palindromic van der Waerden
numbers* pdw(k; t_0,...,t_{k-1}), defined as ordinary van der Waerden numbers
w(k; t_0,...,t_{k-1}), however only allowing palindromic solutions (good
partitions), defined as reading the same from both ends. Different from the
situation for ordinary van der Waerden numbers, these ""numbers"" need actually
to be pairs of numbers. We compute pdw(2;3,t) for 3 <= t <= 27, and we provide
lower bounds, which we conjecture to be exact, for t <= 35.
  All computations are based on SAT solving, and we discuss the various
relations between SAT solving and Ramsey theory. Especially we introduce a
novel (open-source) SAT solver, the tawSolver, which performs best on the SAT
instances studied here, and which is actually the original DLL-solver, but with
an efficient implementation and a modern heuristic typical for look-ahead
solvers (applying the theory developed in the SAT handbook article of the
second author).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1509.05631v2,Measuring Verifiability in Online Information,"The verifiability of online information is important, but difficult to assess
systematically. We examine verifiability in the case of Wikipedia, one of the
world's largest and most consulted online information sources. We extend prior
work about quality of Wikipedia articles, knowledge production, and sources to
consider the quality of Wikipedia references. We propose a multidimensional
measure of verifiability that takes into account technical accuracy and
practical accessibility of sources. We calculate article verifiability scores
for a sample of 5,000 articles and 295,800 citations, and compare differently
weighted models to illustrate effects of emphasizing particular elements of
verifiability over others. We find that, while the quality of references in the
overall sample is reasonably high, verifiability varies significantly by
article, particularly when emphasizing the use of standard digital identifiers
and taking into account the practical availability of referenced sources. We
discuss the implications of these findings for measuring verifiability in
online information more generally.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1105.1383v1,Topological Considerations for Tuning and Fingering Stringed Instruments,"We present a formal language for assigning pitches to strings for fingered
multi-string instruments, particularly the six-string guitar. Given the
instrument's tuning (the strings' open pitches) and the compass of the fingers
of the hand stopping the strings, the formalism yields a framework for
simultaneously optimizing three things: the mapping of pitches to strings, the
choice of instrument tuning, and the key of the composition. Final optimization
relies on heuristics idiomatic to the tuning, the particular musical style, and
the performer's proficiency.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0511019v1,A Counterexample to Cover's 2P Conjecture on Gaussian Feedback Capacity,"We provide a counterexample to Cover's conjecture that the feedback capacity
$C_\textrm{FB}$ of an additive Gaussian noise channel under power constraint
$P$ be no greater than the nonfeedback capacity $C$ of the same channel under
power constraint $2P$, i.e., $C_\textrm{FB}(P) \le C(2P)$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1710.08191v1,Human-in-the-loop Artificial Intelligence,"Little by little, newspapers are revealing the bright future that Artificial
Intelligence (AI) is building. Intelligent machines will help everywhere.
However, this bright future has a dark side: a dramatic job market contraction
before its unpredictable transformation. Hence, in a near future, large numbers
of job seekers will need financial support while catching up with these novel
unpredictable jobs. This possible job market crisis has an antidote inside. In
fact, the rise of AI is sustained by the biggest knowledge theft of the recent
years. Learning AI machines are extracting knowledge from unaware skilled or
unskilled workers by analyzing their interactions. By passionately doing their
jobs, these workers are digging their own graves.
  In this paper, we propose Human-in-the-loop Artificial Intelligence (HIT-AI)
as a fairer paradigm for Artificial Intelligence systems. HIT-AI will reward
aware and unaware knowledge producers with a different scheme: decisions of AI
systems generating revenues will repay the legitimate owners of the knowledge
used for taking those decisions. As modern Robin Hoods, HIT-AI researchers
should fight for a fairer Artificial Intelligence that gives back what it
steals.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0204030v4,Fast Hands-free Writing by Gaze Direction,"We describe a method for text entry based on inverse arithmetic coding that
relies on gaze direction and which is faster and more accurate than using an
on-screen keyboard.
  These benefits are derived from two innovations: the writing task is matched
to the capabilities of the eye, and a language model is used to make
predictable words and phrases easier to write.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2007.15843v1,"Artificial Intelligence in Music and Performance: A Subjective
  Art-Research Inquiry","This article presents a five-year collaboration situated at the intersection
of Art practice and Scientific research in Human-Computer Interaction (HCI). At
the core of our collaborative work is a hybrid, Art and Science methodology
that combines computational learning technology -- Machine Learning (ML) and
Artificial Intelligence (AI) -- with interactive music performance and
choreography. This article first exposes our thoughts on combining art,
science, movement and sound research. We then describe two of our artistic
works \textit{Corpus Nil} and \textit{Humane Methods} -- created five years
apart from each other -- that crystallize our collaborative research process.
We present the scientific and artistic motivations, framed through our research
interests and cultural environment of the time. We conclude by reflecting on
the methodology we developed during the collaboration and on the conceptual
shift of computational learning technologies, from ML to AI, and its impact on
Music performance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.03476v1,"Twin-Load: Building a Scalable Memory System over the Non-Scalable
  Interface","Commodity memory interfaces have difficulty in scaling memory capacity to
meet the needs of modern multicore and big data systems. DRAM device density
and maximum device count are constrained by technology, package, and signal in-
tegrity issues that limit total memory capacity. Synchronous DRAM protocols
require data to be returned within a fixed latency, and thus memory extension
methods over commodity DDRx interfaces fail to support scalable topologies.
Current extension approaches either use slow PCIe interfaces, or require
expensive changes to the memory interface, which limits commercial
adoptability. Here we propose twin-load, a lightweight asynchronous memory
access mechanism over the synchronous DDRx interface. Twin-load uses two
special loads to accomplish one access request to extended memory, the first
serves as a prefetch command to the DRAM system, and the second asynchronously
gets the required data. Twin-load requires no hardware changes on the processor
side and only slight soft- ware modifications. We emulate this system on a
prototype to demonstrate the feasibility of our approach. Twin-load has
comparable performance to NUMA extended memory and outperforms a page-swapping
PCIe-based system by several orders of magnitude. Twin-load thus enables
instant capacity increases on commodity platforms, but more importantly, our
architecture opens opportunities for the design of novel, efficient, scalable,
cost-effective memory subsystems.",0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1410.0600v2,Cell Stores,"Cell stores provide a relational-like, tabular level of abstraction to
business users while leveraging recent database technologies, such as key-value
stores and document stores. This allows to scale up and out the efficient
storage and retrieval of highly dimensional data. Cells are the primary
citizens and exist in different forms, which can be explained with an analogy
to the state of matter: as a gas for efficient storage, as a solid for
efficient retrieval, and as a liquid for efficient interaction with the
business users. Cell stores were abstracted from, and are compatible with the
XBRL standard for importing and exporting data. The first cell store repository
contains roughly 200GB of SEC filings data, and proves that retrieving data
cubes can be performed in real time (the threshold acceptable by a human user
being at most a few seconds).",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.00183v1,"Stochastic Performance Modeling for Practical Byzantine Fault Tolerance
  Consensus in Blockchain","The practical Byzantine fault tolerant (PBFT) consensus mechanism is one of
the most basic consensus algorithms (or protocols) in blockchain technologies,
thus its performance evaluation is an interesting and challenging topic due to
a higher complexity of its consensus work in the peer-to-peer network. This
paper describes a simple stochastic performance model of the PBFT consensus
mechanism, which is refined as not only a queueing system with complicated
service times but also a level-independent quasi-birth-and-death (QBD) process.
From the level-independent QBD process, we apply the matrix-geometric solution
to obtain a necessary and sufficient condition under which the PBFT consensus
system is stable, and to be able to numerically compute the stationary
probability vector of the QBD process. Thus we provide four useful performance
measures of the PBFT consensus mechanism, and can numerically calculate the
four performance measures. Finally, we use some numerical examples to verify
the validity of our theoretical results, and show how the four performance
measures are influenced by some key parameters of the PBFT consensus. By means
of the theory of multi-dimensional Markov processes, we are optimistic that the
methodology and results given in this paper are applicable in a wide range
research of PBFT consensus mechanism and even other types of consensus
mechanisms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1409.4253v1,Three Metrics to Explore the Openness of GitHub projects,"Open source software projects evolve thanks to a group of volunteers that
help in their development. Thus, the success of these projects depends on their
ability to attract (and keep) developers. We believe the openness of a project,
i.e., how easy is for a new user to actively contribute to it, can help to make
a project more attractive. To explore the openness of a software project, we
propose three metrics focused on: (1) the distribution of the project
community, (2) the rate of acceptance of external contributions and (3) the
time it takes to become an official collaborator of the project. We have
adapted and applied these metrics to a subset of GitHub projects, thus giving
some practical findings on their openness.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.01629v1,"LeoPARD --- A Generic Platform for the Implementation of Higher-Order
  Reasoners","LeoPARD supports the implementation of knowledge representation and reasoning
tools for higher-order logic(s). It combines a sophisticated data structure
layer (polymorphically typed {\lambda}-calculus with nameless spine notation,
explicit substitutions, and perfect term sharing) with an ambitious multi-agent
blackboard architecture (supporting prover parallelism at the term, clause, and
search level). Further features of LeoPARD include a parser for all TPTP
dialects, a command line interpreter, and generic means for the integration of
external reasoners.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.09244v1,Evaluations of The Hierarchical Subspace Iteration Method,"This document contains additional experiments concerned with the evaluation
of the Hierarchical Subspace Iteration Method, which is introduced
in~\cite{Nasikun2021}}",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0205045v1,Balancing Minimum Spanning and Shortest Path Trees,"This paper give a simple linear-time algorithm that, given a weighted
digraph, finds a spanning tree that simultaneously approximates a shortest-path
tree and a minimum spanning tree. The algorithm provides a continuous
trade-off: given the two trees and epsilon > 0, the algorithm returns a
spanning tree in which the distance between any vertex and the root of the
shortest-path tree is at most 1+epsilon times the shortest-path distance, and
yet the total weight of the tree is at most 1+2/epsilon times the weight of a
minimum spanning tree. This is the best tradeoff possible. The paper also
describes a fast parallel implementation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1503.08104v1,Evaluating Asymmetric Multicore Systems-on-Chip using Iso-Metrics,"The end of Dennard scaling has pushed power consumption into a first order
concern for current systems, on par with performance. As a result,
near-threshold voltage computing (NTVC) has been proposed as a potential means
to tackle the limited cooling capacity of CMOS technology. Hardware operating
in NTV consumes significantly less power, at the cost of lower frequency, and
thus reduced performance, as well as increased error rates. In this paper, we
investigate if a low-power systems-on-chip, consisting of ARM's asymmetric
big.LITTLE technology, can be an alternative to conventional high performance
multicore processors in terms of power/energy in an unreliable scenario. For
our study, we use the Conjugate Gradient solver, an algorithm representative of
the computations performed by a large range of scientific and engineering
codes.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.00287v1,"Complex Event Forecasting with Prediction Suffix Trees: Extended
  Technical Report","Complex Event Recognition (CER) systems have become popular in the past two
decades due to their ability to ""instantly"" detect patterns on real-time
streams of events. However, there is a lack of methods for forecasting when a
pattern might occur before such an occurrence is actually detected by a CER
engine. We present a formal framework that attempts to address the issue of
Complex Event Forecasting (CEF). Our framework combines two formalisms: a)
symbolic automata which are used to encode complex event patterns; and b)
prediction suffix trees which can provide a succinct probabilistic description
of an automaton's behavior. We compare our proposed approach against
state-of-the-art methods and show its advantage in terms of accuracy and
efficiency. In particular, prediction suffix trees, being variable-order Markov
models, have the ability to capture long-term dependencies in a stream by
remembering only those past sequences that are informative enough. Our
experimental results demonstrate the benefits, in terms of accuracy, of being
able to capture such long-term dependencies. This is achieved by increasing the
order of our model beyond what is possible with full-order Markov models that
need to perform an exhaustive enumeration of all possible past sequences of a
given order. We also discuss extensively how CEF solutions should be best
evaluated on the quality of their forecasts.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1402.4718v2,"Turing Kernelization for Finding Long Paths and Cycles in Restricted
  Graph Classes","The NP-complete $k$-Path problem asks whether a given undirected graph has a
(simple) path of length at least $k$. We prove that $k$-Path has
polynomial-size Turing kernels when restricted to planar graphs, graphs of
bounded degree, claw-free graphs, or to $K_{3,t}$-minor-free graphs for some
constant $t$. This means that there is an algorithm that, given a $k$-Path
instance $(G,k)$ belonging to one of these graph classes, computes its answer
in polynomial time when given access to an oracle that solves $k$-Path
instances of size polynomial in $k$ in a single step. The difficulty of
$k$-Path can therefore be confined to subinstances whose size is independent of
the total input size, but is bounded by a polynomial in the parameter $k$
alone. These results contrast existing superpolynomial lower bounds for the
sizes of traditional kernels for the $k$-Path problem on these graph classes:
there is no polynomial-time algorithm that reduces any instance $(G,k)$ to a
single, equivalent instance $(G',k')$ of size polynomial in $k$ unless $NP
\subseteq coNP/poly$. The same positive and negative results apply to the
$k$-Cycle problem, which asks for the existence of a cycle of length at least
$k$. Our kernelization schemes are based on a new methodology called
Decompose-Query-Reduce.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.00995v2,Fuzzy Approach Topic Discovery in Health and Medical Corpora,"The majority of medical documents and electronic health records (EHRs) are in
text format that poses a challenge for data processing and finding relevant
documents. Looking for ways to automatically retrieve the enormous amount of
health and medical knowledge has always been an intriguing topic. Powerful
methods have been developed in recent years to make the text processing
automatic. One of the popular approaches to retrieve information based on
discovering the themes in health & medical corpora is topic modeling, however,
this approach still needs new perspectives. In this research we describe fuzzy
latent semantic analysis (FLSA), a novel approach in topic modeling using fuzzy
perspective. FLSA can handle health & medical corpora redundancy issue and
provides a new method to estimate the number of topics. The quantitative
evaluations show that FLSA produces superior performance and features to latent
Dirichlet allocation (LDA), the most popular topic model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0703002v9,"Integral Biomathics: A Post-Newtonian View into the Logos of Bios (On
  the New Meaning, Relations and Principles of Life in Science)","This work is an attempt for a state-of-the-art survey of natural and life
sciences with the goal to define the scope and address the central questions of
an original research program. It is focused on the phenomena of emergence,
adaptive dynamics and evolution of self-assembling, self-organizing,
self-maintaining and self-replicating biosynthetic systems viewed from a
newly-arranged perspective and understanding of computation and communication
in the living nature.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1612.06633v2,"The Best of Both Worlds: Linear Functional Programming without
  Compromise","We present a linear functional calculus with both the safety guarantees
expressible with linear types and the rich language of combinators and
composition provided by functional programming. Unlike previous combinations of
linear typing and functional programming, we compromise neither the linear side
(for example, our linear values are first-class citizens of the language) nor
the functional side (for example, we do not require duplicate definitions of
compositions for linear and unrestricted functions). To do so, we must
generalize abstraction and application to encompass both linear and
unrestricted functions. We capture the typing of the generalized constructs
with a novel use of qualified types. Our system maintains the metatheoretic
properties of the theory of qualified types, including principal types and
decidable type inference. Finally, we give a formal basis for our claims of
expressiveness, by showing that evaluation respects linearity, and that our
language is a conservative extension of existing functional calculi.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1510.04935v2,Holographic Embeddings of Knowledge Graphs,"Learning embeddings of entities and relations is an efficient and versatile
method to perform machine learning on relational data such as knowledge graphs.
In this work, we propose holographic embeddings (HolE) to learn compositional
vector space representations of entire knowledge graphs. The proposed method is
related to holographic models of associative memory in that it employs circular
correlation to create compositional representations. By using correlation as
the compositional operator HolE can capture rich interactions but
simultaneously remains efficient to compute, easy to train, and scalable to
very large datasets. In extensive experiments we show that holographic
embeddings are able to outperform state-of-the-art methods for link prediction
in knowledge graphs and relational learning benchmark datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1302.7278v2,"Using cascading Bloom filters to improve the memory usage for de Brujin
  graphs","De Brujin graphs are widely used in bioinformatics for processing
next-generation sequencing data. Due to a very large size of NGS datasets, it
is essential to represent de Bruijn graphs compactly, and several approaches to
this problem have been proposed recently. In this work, we show how to reduce
the memory required by the algorithm of [3] that represents de Brujin graphs
using Bloom filters. Our method requires 30% to 40% less memory with respect to
the method of [3], with insignificant impact to construction time. At the same
time, our experiments showed a better query time compared to [3]. This is, to
our knowledge, the best practical representation for de Bruijn graphs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0601073v1,A Theory of Routing for Large-Scale Wireless Ad-Hoc Networks,"In this work we develop a new theory to analyse the process of routing in
large-scale ad-hoc wireless networks. We use a path integral formulation to
examine the properties of the paths generated by different routing strategies
in these kinds of networks. Using this theoretical framework, we calculate the
statistical distribution of the distances between any source to any destination
in the network, hence we are able to deduce a length parameter that is unique
for each routing strategy. This parameter, defined as the {\it effective
radius}, effectively encodes the routing information required by a node.
Analysing the aforementioned statistical distribution for different routing
strategies, we obtain a threefold result for practical Large-Scale Wireless
Ad-Hoc Networks: 1) We obtain the distribution of the lengths of all the paths
in a network for any given routing strategy, 2) We are able to identify ""good""
routing strategies depending on the evolution of its effective radius as the
number of nodes, $N$, increases to infinity, 3) For any routing strategy with
finite effective radius, we demonstrate that, in a large-scale network, is
equivalent to a random routing strategy and that its transport capacity scales
as $\Theta(\sqrt{N})$ bit-meters per second, thus retrieving the scaling law
that Gupta and Kumar (2000) obtained as the limit for single-route large-scale
wireless networks.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.12898v1,"Generating Answer Candidates for Quizzes and Answer-Aware Question
  Generators","In education, open-ended quiz questions have become an important tool for
assessing the knowledge of students. Yet, manually preparing such questions is
a tedious task, and thus automatic question generation has been proposed as a
possible alternative. So far, the vast majority of research has focused on
generating the question text, relying on question answering datasets with
readily picked answers, and the problem of how to come up with answer
candidates in the first place has been largely ignored. Here, we aim to bridge
this gap. In particular, we propose a model that can generate a specified
number of answer candidates for a given passage of text, which can then be used
by instructors to write questions manually or can be passed as an input to
automatic answer-aware question generators. Our experiments show that our
proposed answer candidate generation model outperforms several baselines.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1807.02098v2,"MAT-CNN-SOPC: Motionless Analysis of Traffic Using Convolutional Neural
  Networks on System-On-a-Programmable-Chip","Intelligent Transportation Systems (ITS) have become an important pillar in
modern ""smart city"" framework which demands intelligent involvement of
machines. Traffic load recognition can be categorized as an important and
challenging issue for such systems. Recently, Convolutional Neural Network
(CNN) models have drawn considerable amount of interest in many areas such as
weather classification, human rights violation detection through images, due to
its accurate prediction capabilities. This work tackles real-life traffic load
recognition problem on System-On-a-Programmable-Chip (SOPC) platform and coin
it as MAT-CNN- SOPC, which uses an intelligent re-training mechanism of the CNN
with known environments. The proposed methodology is capable of enhancing the
efficacy of the approach by 2.44x in comparison to the state-of-art and proven
through experimental analysis. We have also introduced a mathematical equation,
which is capable of quantifying the suitability of using different CNN models
over the other for a particular application based implementation.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.10361v1,Neural Face Models for Example-Based Visual Speech Synthesis,"Creating realistic animations of human faces with computer graphic models is
still a challenging task. It is often solved either with tedious manual work or
motion capture based techniques that require specialised and costly hardware.
Example based animation approaches circumvent these problems by re-using
captured data of real people. This data is split into short motion samples that
can be looped or concatenated in order to create novel motion sequences. The
obvious advantages of this approach are the simplicity of use and the high
realism, since the data exhibits only real deformations. Rather than tuning
weights of a complex face rig, the animation task is performed on a higher
level by arranging typical motion samples in a way such that the desired facial
performance is achieved. Two difficulties with example based approaches,
however, are high memory requirements as well as the creation of artefact-free
and realistic transitions between motion samples. We solve these problems by
combining the realism and simplicity of example-based animations with the
advantages of neural face models. Our neural face model is capable of
synthesising high quality 3D face geometry and texture according to a compact
latent parameter vector. This latent representation reduces memory requirements
by a factor of 100 and helps creating seamless transitions between concatenated
motion samples. In this paper, we present a marker-less approach for facial
motion capture based on multi-view video. Based on the captured data, we learn
a neural representation of facial expressions, which is used to seamlessly
concatenate facial performances during the animation procedure. We demonstrate
the effectiveness of our approach by synthesising mouthings for Swiss-German
sign language based on viseme query sequences.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0005029v1,"Ranking suspected answers to natural language questions using predictive
  annotation","In this paper, we describe a system to rank suspected answers to natural
language questions. We process both corpus and query using a new technique,
predictive annotation, which augments phrases in texts with labels anticipating
their being targets of certain kinds of questions. Given a natural language
question, an IR system returns a set of matching passages, which are then
analyzed and ranked according to various criteria described in this paper. We
provide an evaluation of the techniques based on results from the TREC Q&A
evaluation in which our system participated.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1708.01131v1,"A computer simulation of the Volga River hydrological regime: a problem
  of water-retaining dam optimal location","We investigate of a special dam optimal location at the Volga river in area
of the Akhtuba left sleeve beginning (7 \, km to the south of the Volga
Hydroelectric Power Station dam). We claim that a new water-retaining dam can
resolve the key problem of the Volga-Akhtuba floodplain related to insufficient
water amount during the spring flooding due to the overregulation of the Lower
Volga. By using a numerical integration of Saint-Vacant equations we study the
water dynamics across the northern part of the Volga-Akhtuba floodplain with
taking into account its actual topography. As the result we found an amount of
water $V_A$ passing to the Akhtuba during spring period for a given water flow
through the Volga Hydroelectric Power Station (so-called hydrograph which
characterises the water flow per unit of time). By varying the location of the
water-retaining dam $ x_d, y_d $ we obtained various values of $V_A (x_d, y_d)
$ as well as various flow spatial structure on the territory during the flood
period. Gradient descent method provide us the dam coordinated with the maximum
value of ${V_A}$. Such approach to the dam location choice let us to find the
best solution, that the value $V_A$ increases by a factor of 2. Our analysis
demonstrate a good potential of the numerical simulations in the field of
hydraulic works.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1911.11924v2,"In Perfect Shape: Certifiably Optimal 3D Shape Reconstruction from 2D
  Landmarks","We study the problem of 3D shape reconstruction from 2D landmarks extracted
in a single image. We adopt the 3D deformable shape model and formulate the
reconstruction as a joint optimization of the camera pose and the linear shape
parameters. Our first contribution is to apply Lasserre's hierarchy of convex
Sums-of-Squares (SOS) relaxations to solve the shape reconstruction problem and
show that the SOS relaxation of minimum order 2 empirically solves the original
non-convex problem exactly. Our second contribution is to exploit the structure
of the polynomial in the objective function and find a reduced set of basis
monomials for the SOS relaxation that significantly decreases the size of the
resulting semidefinite program (SDP) without compromising its accuracy. These
two contributions, to the best of our knowledge, lead to the first certifiably
optimal solver for 3D shape reconstruction, that we name Shape*. Our third
contribution is to add an outlier rejection layer to Shape* using a truncated
least squares (TLS) robust cost function and leveraging graduated non-convexity
to solve TLS without initialization. The result is a robust reconstruction
algorithm, named Shape#, that tolerates a large amount of outlier measurements.
We evaluate the performance of Shape* and Shape# in both simulated and real
experiments, showing that Shape* outperforms local optimization and previous
convex relaxation techniques, while Shape# achieves state-of-the-art
performance and is robust against 70% outliers in the FG3DCar dataset.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0705.1939v1,Towards Informative Statistical Flow Inversion,"A problem which has recently attracted research attention is that of
estimating the distribution of flow sizes in internet traffic. On high traffic
links it is sometimes impossible to record every packet. Researchers have
approached the problem of estimating flow lengths from sampled packet data in
two separate ways. Firstly, different sampling methodologies can be tried to
more accurately measure the desired system parameters. One such method is the
sample-and-hold method where, if a packet is sampled, all subsequent packets in
that flow are sampled. Secondly, statistical methods can be used to ``invert''
the sampled data and produce an estimate of flow lengths from a sample.
  In this paper we propose, implement and test two variants on the
sample-and-hold method. In addition we show how the sample-and-hold method can
be inverted to get an estimation of the genuine distribution of flow sizes.
Experiments are carried out on real network traces to compare standard packet
sampling with three variants of sample-and-hold. The methods are compared for
their ability to reconstruct the genuine distribution of flow sizes in the
traffic.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1101.1932v1,Efficient tilings of de Bruijn and Kautz graphs,"Kautz and de Bruijn graphs have a high degree of connectivity which makes
them ideal candidates for massively parallel computer network topologies. In
order to realize a practical computer architecture based on these graphs, it is
useful to have a means of constructing a large-scale system from smaller,
simpler modules. In this paper we consider the mathematical problem of
uniformly tiling a de Bruijn or Kautz graph. This can be viewed as a
generalization of the graph bisection problem. We focus on the problem of graph
tilings by a set of identical subgraphs. Tiles should contain a maximal number
of internal edges so as to minimize the number of edges connecting distinct
tiles. We find necessary and sufficient conditions for the construction of
tilings. We derive a simple lower bound on the number of edges which must leave
each tile, and construct a class of tilings whose number of edges leaving each
tile agrees asymptotically in form with the lower bound to within a constant
factor. These tilings make possible the construction of large-scale computing
systems based on de Bruijn and Kautz graph topologies.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1309.0373v1,ENFrame: A Platform for Processing Probabilistic Data,"This paper introduces ENFrame, a unified data processing platform for
querying and mining probabilistic data. Using ENFrame, users can write programs
in a fragment of Python with constructs such as bounded-range loops, list
comprehension, aggregate operations on lists, and calls to external database
engines. The program is then interpreted probabilistically by ENFrame.
  The realisation of ENFrame required novel contributions along several
directions. We propose an event language that is expressive enough to
succinctly encode arbitrary correlations, trace the computation of user
programs, and allow for computation of discrete probability distributions of
program variables. We exemplify ENFrame on three clustering algorithms:
k-means, k-medoids, and Markov Clustering. We introduce sequential and
distributed algorithms for computing the probability of interconnected events
exactly or approximately with error guarantees. Experiments with k-medoids
clustering of sensor readings from energy networks show orders-of-magnitude
improvements of exact clustering using ENFrame over na\""ive clustering in each
possible world, of approximate over exact, and of distributed over sequential
algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0410062v1,"Automatic Keyword Extraction from Spoken Text. A Comparison of two
  Lexical Resources: the EDR and WordNet","Lexical resources such as WordNet and the EDR electronic dictionary have been
used in several NLP tasks. Probably, partly due to the fact that the EDR is not
freely available, WordNet has been used far more often than the EDR. We have
used both resources on the same task in order to make a comparison possible.
The task is automatic assignment of keywords to multi-party dialogue episodes
(i.e. thematically coherent stretches of spoken text). We show that the use of
lexical resources in such a task results in slightly higher performances than
the use of a purely statistically based method.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1508.01420v1,Privacy-Preserving Multi-Document Summarization,"State-of-the-art extractive multi-document summarization systems are usually
designed without any concern about privacy issues, meaning that all documents
are open to third parties. In this paper we propose a privacy-preserving
approach to multi-document summarization. Our approach enables other parties to
obtain summaries without learning anything else about the original documents'
content. We use a hashing scheme known as Secure Binary Embeddings to convert
documents representation containing key phrases and bag-of-words into bit
strings, allowing the computation of approximate distances, instead of exact
ones. Our experiments indicate that our system yields similar results to its
non-private counterpart on standard multi-document evaluation datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2011.05782v1,"Reinforcement Learning Experiments and Benchmark for Solving Robotic
  Reaching Tasks","Reinforcement learning has shown great promise in robotics thanks to its
ability to develop efficient robotic control procedures through self-training.
In particular, reinforcement learning has been successfully applied to solving
the reaching task with robotic arms. In this paper, we define a robust,
reproducible and systematic experimental procedure to compare the performance
of various model-free algorithms at solving this task. The policies are trained
in simulation and are then transferred to a physical robotic manipulator. It is
shown that augmenting the reward signal with the Hindsight Experience Replay
exploration technique increases the average return of off-policy agents between
7 and 9 folds when the target position is initialised randomly at the beginning
of each episode.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0805.2286v1,"Secure Network Coding Against the Contamination and Eavesdropping
  Adversaries","In this paper, we propose an algorithm that targets contamination and
eavesdropping adversaries. We consider the case when the number of independent
packets available to the eavesdropper is less than the multicast capacity of
the network. By means of our algorithm every node can verify the integrity of
the received packets easily and an eavesdropper is unable to get any meaningful
information about the source. We call it practical security if an eavesdropper
is unable to get any meaningful information about the source.We show that, by
giving up a small amount of overall capacity, our algorithm achieves achieves
the practically secure condition at a probability of one. Furthermore, the
communication overhead of our algorithm are negligible compared with previous
works, since the transmission of the hash values and the code coefficients are
both avoided.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.07506v3,On Reductions of Hintikka Sets for Higher-Order Logic,"Steen's (2018) Hintikka set properties for Church's type theory based on
primitive equality are reduced to the Hintikka set properties of Brown (2007).
Using this reduction, a model existence theorem for Steen's properties is
derived.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.09946v1,Identifying biases in legal data: An algorithmic fairness perspective,"The need to address representation biases and sentencing disparities in legal
case data has long been recognized. Here, we study the problem of identifying
and measuring biases in large-scale legal case data from an algorithmic
fairness perspective. Our approach utilizes two regression models: A baseline
that represents the decisions of a ""typical"" judge as given by the data and a
""fair"" judge that applies one of three fairness concepts. Comparing the
decisions of the ""typical"" judge and the ""fair"" judge allows for quantifying
biases across demographic groups, as we demonstrate in four case studies on
criminal data from Cook County (Illinois).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0
http://arxiv.org/abs/1011.6012v1,Proceedings 17th International Workshop on Expressiveness in Concurrency,"This volume contains the proceedings of the 17th International Workshop on
Expressiveness in Concurrency (EXPRESS'10), which took place on 30th August
2010 in Paris, co-located with CONCUR'10. The EXPRESS workshop series aim at
bringing together researchers who are interested in the expressiveness and
comparison of formal models that broadly relate to concurrency. In particular,
this also includes emergent fields such as logic and interaction,
game-theoretic models, and service-oriented computing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1002.4661v1,Complementary approaches to understanding the plant circadian clock,"Circadian clocks are oscillatory genetic networks that help organisms adapt
to the 24-hour day/night cycle. The clock of the green alga Ostreococcus tauri
is the simplest plant clock discovered so far. Its many advantages as an
experimental system facilitate the testing of computational predictions.
  We present a model of the Ostreococcus clock in the stochastic process
algebra Bio-PEPA and exploit its mapping to different analysis techniques, such
as ordinary differential equations, stochastic simulation algorithms and
model-checking. The small number of molecules reported for this system tests
the limits of the continuous approximation underlying differential equations.
We investigate the difference between continuous-deterministic and
discrete-stochastic approaches. Stochastic simulation and model-checking allow
us to formulate new hypotheses on the system behaviour, such as the presence of
self-sustained oscillations in single cells under constant light conditions.
  We investigate how to model the timing of dawn and dusk in the context of
model-checking, which we use to compute how the probability distributions of
key biochemical species change over time. These show that the relative
variation in expression level is smallest at the time of peak expression,
making peak time an optimal experimental phase marker. Building on these
analyses, we use approaches from evolutionary systems biology to investigate
how changes in the rate of mRNA degradation impacts the phase of a key protein
likely to affect fitness. We explore how robust this circadian clock is towards
such potential mutational changes in its underlying biochemistry. Our work
shows that multiple approaches lead to a more complete understanding of the
clock.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0101029v1,Tap Tips: Lightweight Discovery of Touchscreen Targets,"We describe tap tips, a technique for providing touch-screen target location
hints. Tap tips are lightweight in that they are non-modal, appear only when
needed, require a minimal number of user gestures, and do not add to the
standard touchscreen gesture vocabulary. We discuss our implementation of tap
tips in an electronic guidebook system and some usability test results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0908.0703v1,"Evaluating Collaborative Search Interfaces with Information Seeking
  Theory","Despite the many implicit references to the social aspects of search within
Information Seeking and Retrieval research, there has been relatively little
work that has specifically investigated the additional requirements for
collaborative search software. In this paper we re-assess a recent evaluation
framework, designed for individual information seeking experiences, to see a)
how it could still be applied to collaborative search software; b) how it could
produce additional requirements for collaborative search; and c) how it could
be extended in future work to be even more appropriate for collaborative search
evaluation. The position held after the assessment is that it can be used to
evaluate collaborative search software, while providing new insights into their
requirements. Finally, future work will validate the frameworks applicability
to collaborative search and investigate roles within collaborative groups as a
means to extend the framework.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.03260v1,"Automatic, Dynamic, and Nearly Optimal Learning Rate Specification by
  Local Quadratic Approximation","In deep learning tasks, the learning rate determines the update step size in
each iteration, which plays a critical role in gradient-based optimization.
However, the determination of the appropriate learning rate in practice
typically replies on subjective judgement. In this work, we propose a novel
optimization method based on local quadratic approximation (LQA). In each
update step, given the gradient direction, we locally approximate the loss
function by a standard quadratic function of the learning rate. Then, we
propose an approximation step to obtain a nearly optimal learning rate in a
computationally efficient way. The proposed LQA method has three important
features. First, the learning rate is automatically determined in each update
step. Second, it is dynamically adjusted according to the current loss function
value and the parameter estimates. Third, with the gradient direction fixed,
the proposed method leads to nearly the greatest reduction in terms of the loss
function. Extensive experiments have been conducted to prove the strengths of
the proposed LQA method.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.07955v1,X575: writing rengas with web services,"Our software system simulates the classical collaborative Japanese poetry
form, renga, made of linked haikus. We used NLP methods wrapped up as web
services. Our experiments were only a partial success, since results fail to
satisfy classical constraints. To gather ideas for future work, we examine
related research in semiotics, linguistics, and computing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1410.6754v2,Practical Massively Parallel Sorting,"Previous parallel sorting algorithms do not scale to the largest available
machines, since they either have prohibitive communication volume or
prohibitive critical path length. We describe algorithms that are a viable
compromise and overcome this gap both in theory and practice. The algorithms
are multi-level generalizations of the known algorithms sample sort and
multiway mergesort. In particular our sample sort variant turns out to be very
scalable. Some tools we develop may be of independent interest -- a simple,
practical, and flexible sorting algorithm for small inputs working in
logarithmic time, a near linear time optimal algorithm for solving a
constrained bin packing problem, and an algorithm for data delivery, that
guarantees a small number of message startups on each processor.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.07082v2,"Unsupervised Data Extraction from Computer-generated Documents with
  Single Line Formatting","Processing large amounts of data is an essential problem of the big data era.
Most of the data exchange is done via direct communication (using APIs) and
well-structured file formats (JSON, XML, EDI, etc.), but a significant portion
of the data is transferred using arbitrary formatted computer-generated
documents (such as invoices, purchase orders, financial reports, etc.), which
require sophisticated processing and human intervention for data interpretation
and extraction. The currently available solutions, ranging from manual data
entry to low-level scripting and data extraction tools, are costly and require
human intervention. This paper describes the principle methodology for
unsupervised, fully automatic data extraction from a wide range of
computer-generated documents, assuming that their formatting reflects the
original structure of the data sources. The presented methodology falls into
the category of unsupervised machine learning and consists of the three main
parts: (1) - detecting repeating patterns of text formatting by employing the
relative feature space clustering and adaptive weighted feature score maps, (2)
- detecting hierarchical formatting structures via collapsing and noise
filtering procedure applied to the repeating formatting patterns and (3) -
automatic configuration of the interactive data extraction tool (SiMX
TextConverter) for fully automated processing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1804.02158v4,Microblog Topic Identification using Linked Open Data,"The extensive use of social media for sharing and obtaining information has
resulted in the development of topic detection models to facilitate the
comprehension of the overwhelming amount of short and distributed posts.
Probabilistic topic models, such as Latent Dirichlet Allocation, and matrix
factorization based approaches such as Latent Semantic Analysis and
Non-negative Matrix Factorization represent topics as sets of terms that are
useful for many automated processes. However, the determination of what a topic
is about is left as a further task. Alternatively, techniques that produce
summaries are human comprehensible, but less suitable for automated processing.
This work proposes an approach that utilizes Linked Open Data (LOD) resources
to extract semantically represented topics from collections of microposts. The
proposed approach utilizes entity linking to identify the elements of topics
from microposts. The elements are related through co-occurrence graphs, which
are processed to yield topics. The topics are represented using an ontology
that is introduced for this purpose. A prototype of the approach is used to
identify topics from 11 datasets consisting of more than one million posts
collected from Twitter during various events, such as the 2016 US election
debates and the death of Carrie Fisher. The characteristics of the approach
with more than 5 thousand generated topics are described in detail. The
potentials of semantic topics in revealing information, that is not otherwise
easily observable, is demonstrated with semantic queries of various
complexities. A human evaluation of topics from 36 randomly selected intervals
resulted in a precision of 81.0% and F1 score of 93.3%. Furthermore, they are
compared with topics generated from the same datasets from an approach that
produces human readable topics from microblog post collections.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1601.04017v2,Concurrent Hash Tables: Fast and General?(!),"Concurrent hash tables are one of the most important concurrent data
structures with numerous applications. Since hash table accesses can dominate
the execution time of the overall application, we need implementations that
achieve good speedup. Unfortunately, currently available concurrent hashing
libraries turn out to be far away from this requirement in particular when
contention on some elements occurs.
  Our starting point for better performing data structures is a fast and simple
lock-free concurrent hash table based on linear probing that is limited to
word-sized key-value types and does not support dynamic size adaptation. We
explain how to lift these limitations in a provably scalable way and
demonstrate that dynamic growing has a performance overhead comparable to the
same generalization in sequential hash tables.
  We perform extensive experiments comparing the performance of our
implementations with six of the most widely used concurrent hash tables. Ours
are considerably faster than the best algorithms with similar restrictions and
an order of magnitude faster than the best more general tables. In some extreme
cases, the difference even approaches four orders of magnitude.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1104.4997v3,"Concentration and Moment Inequalities for Polynomials of Independent
  Random Variables","In this work we design a general method for proving moment inequalities for
polynomials of independent random variables. Our method works for a wide range
of random variables including Gaussian, Boolean, exponential, Poisson and many
others. We apply our method to derive general concentration inequalities for
polynomials of independent random variables. We show that our method implies
concentration inequalities for some previously open problems, e.g. permanent of
a random symmetric matrices. We show that our concentration inequality is
stronger than the well-known concentration inequality due to Kim and Vu. The
main advantage of our method in comparison with the existing ones is a wide
range of random variables we can handle and bounds for previously intractable
regimes of high degree polynomials and small expectations. On the negative side
we show that even for boolean random variables each term in our concentration
inequality is tight.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1105.5881v2,"On the random access performance of Cell Broadband Engine with graph
  analysis application","The Cell Broad Engine (BE) Processor has unique memory access architecture
besides its powerful computing engines. Many computing-intensive applications
have been ported to Cell/BE successfully. But memory-intensive applications are
rarely investigated except for several micro benchmarks. Since Cell/BE has
powerful software visible DMA engine, this paper studies on whether Cell/BE is
suit for applica- tions with large amount of random memory accesses. Two
benchmarks, GUPS and SSCA#2, are used. The latter is a rather complex one that
in representative of real world graph analysis applications. We find both
benchmarks have good performance on Cell/BE based IBM QS20/22. Com- pared with
2 conventional multi-processor systems with the same core/thread number, GUPS
is about 40-80% fast and SSCA#2 about 17-30% fast. The dynamic load balanc- ing
and software pipeline for optimizing SSCA#2 are intro- duced. Based on the
experiment, the potential of Cell/BE for random access is analyzed in detail as
well as its limita- tions of memory controller, atomic engine and TLB manage-
ment.Our research shows although more programming effort are needed, Cell/BE
has the potencial for irregular memory access applications.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1601.00481v1,"Data Portraits and Intermediary Topics: Encouraging Exploration of
  Politically Diverse Profiles","In micro-blogging platforms, people connect and interact with others.
However, due to cognitive biases, they tend to interact with like-minded people
and read agreeable information only. Many efforts to make people connect with
those who think differently have not worked well. In this paper, we
hypothesize, first, that previous approaches have not worked because they have
been direct -- they have tried to explicitly connect people with those having
opposing views on sensitive issues. Second, that neither recommendation or
presentation of information by themselves are enough to encourage behavioral
change. We propose a platform that mixes a recommender algorithm and a
visualization-based user interface to explore recommendations. It recommends
politically diverse profiles in terms of distance of latent topics, and
displays those recommendations in a visual representation of each user's
personal content. We performed an ""in the wild"" evaluation of this platform,
and found that people explored more recommendations when using a biased
algorithm instead of ours. In line with our hypothesis, we also found that the
mixture of our recommender algorithm and our user interface, allowed
politically interested users to exhibit an unbiased exploration of the
recommended profiles. Finally, our results contribute insights in two aspects:
first, which individual differences are important when designing platforms
aimed at behavioral change; and second, which algorithms and user interfaces
should be mixed to help users avoid cognitive mechanisms that lead to biased
behavior.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.03617v1,"MuSeM: Detecting Incongruent News Headlines using Mutual Attentive
  Semantic Matching","Measuring the congruence between two texts has several useful applications,
such as detecting the prevalent deceptive and misleading news headlines on the
web. Many works have proposed machine learning based solutions such as text
similarity between the headline and body text to detect the incongruence. Text
similarity based methods fail to perform well due to different inherent
challenges such as relative length mismatch between the news headline and its
body content and non-overlapping vocabulary. On the other hand, more recent
works that use headline guided attention to learn a headline derived contextual
representation of the news body also result in convoluting overall
representation due to the news body's lengthiness. This paper proposes a method
that uses inter-mutual attention-based semantic matching between the original
and synthetically generated headlines, which utilizes the difference between
all pairs of word embeddings of words involved. The paper also investigates two
more variations of our method, which use concatenation and dot-products of word
embeddings of the words of original and synthetic headlines. We observe that
the proposed method outperforms prior arts significantly for two publicly
available datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0203024v1,The structure of broad topics on the Web,"The Web graph is a giant social network whose properties have been measured
and modeled extensively in recent years. Most such studies concentrate on the
graph structure alone, and do not consider textual properties of the nodes.
Consequently, Web communities have been characterized purely in terms of graph
structure and not on page content. We propose that a topic taxonomy such as
Yahoo! or the Open Directory provides a useful framework for understanding the
structure of content-based clusters and communities. In particular, using a
topic taxonomy and an automatic classifier, we can measure the background
distribution of broad topics on the Web, and analyze the capability of recent
random walk algorithms to draw samples which follow such distributions. In
addition, we can measure the probability that a page about one broad topic will
link to another broad topic. Extending this experiment, we can measure how
quickly topic context is lost while walking randomly on the Web graph.
Estimates of this topic mixing distance may explain why a global PageRank is
still meaningful in the context of broad queries. In general, our measurements
may prove valuable in the design of community-specific crawlers and link-based
ranking systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1208.0257v2,Hamming Approximation of NP Witnesses,"Given a satisfiable 3-SAT formula, how hard is it to find an assignment to
the variables that has Hamming distance at most n/2 to a satisfying assignment?
More generally, consider any polynomial-time verifier for any NP-complete
language. A d(n)-Hamming-approximation algorithm for the verifier is one that,
given any member x of the language, outputs in polynomial time a string a with
Hamming distance at most d(n) to some witness w, where (x,w) is accepted by the
verifier. Previous results have shown that, if P != NP, then every NP-complete
language has a verifier for which there is no
(n/2-n^(2/3+d))-Hamming-approximation algorithm, for various constants d > 0.
  Our main result is that, if P != NP, then every paddable NP-complete language
has a verifier that admits no (n/2+O(sqrt(n log n)))-Hamming-approximation
algorithm. That is, one cannot get even half the bits right. We also consider
natural verifiers for various well-known NP-complete problems. They do have
n/2-Hamming-approximation algorithms, but, if P != NP, have no
(n/2-n^epsilon)-Hamming-approximation algorithms for any constant epsilon > 0.
  We show similar results for randomized algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1409.6021v2,"On $k$-connectivity and minimum vertex degree in random $s$-intersection
  graphs","Random $s$-intersection graphs have recently received much interest in a wide
range of application areas. Broadly speaking, a random $s$-intersection graph
is constructed by first assigning each vertex a set of items in some random
manner, and then putting an undirected edge between all pairs of vertices that
share at least $s$ items (the graph is called a random intersection graph when
$s=1$). A special case of particular interest is a uniform random
$s$-intersection graph, where each vertex independently selects the same number
of items uniformly at random from a common item pool. Another important case is
a binomial random $s$-intersection graph, where each item from a pool is
independently assigned to each vertex with the same probability. Both models
have found numerous applications thus far including cryptanalysis, and the
modeling of recommender systems, secure sensor networks, online social
networks, trust networks and small-world networks (uniform random
$s$-intersection graphs), as well as clustering analysis, classification, and
the design of integrated circuits (binomial random $s$-intersection graphs).
  In this paper, for binomial/uniform random $s$-intersection graphs, we
present results related to $k$-connectivity and minimum vertex degree.
Specifically, we derive the asymptotically exact probabilities and zero-one
laws for the following three properties: (i) $k$-vertex-connectivity, (ii)
$k$-edge-connectivity and (iii) the property of minimum vertex degree being at
least $k$.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0011037v2,"A syntactical analysis of non-size-increasing polynomial time
  computation","A syntactical proof is given that all functions definable in a certain affine
linear typed lambda-calculus with iteration in all types are polynomial time
computable. The proof provides explicit polynomial bounds that can easily be
calculated.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1605.01886v1,"From Sazonov's Non-Dcpo Natural Domains to Closed Directed-Lub Partial
  Orders","Normann proved that the domains of the game model of PCF (the domains of
sequential functionals) need not be dcpos. Sazonov has defined natural domains
for a theory of such incomplete domains. This paper further develops that
theory. It defines lub-rules that infer natural lubs from existing natural
lubs, and lub-rule classes that describe axiom systems like that of natural
domains. There is a canonical proper subcategory of the natural domains, the
closed directed lub partial orders (cdlubpo), that corresponds to the complete
lub-rule class of all valid lub-rules. Cdlubpos can be completed to restricted
dcpos, which are dcpos that retain the data of the incomplete cdlubpo as a
subset.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1806.10478v2,Neural Machine Translation for Query Construction and Composition,"Research on question answering with knowledge base has recently seen an
increasing use of deep architectures. In this extended abstract, we study the
application of the neural machine translation paradigm for question parsing. We
employ a sequence-to-sequence model to learn graph patterns in the SPARQL graph
query language and their compositions. Instead of inducing the programs through
question-answer pairs, we expect a semi-supervised approach, where alignments
between questions and queries are built through templates. We argue that the
coverage of language utterances can be expanded using late notable works in
natural language generation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1011.3241v1,"New Methods of Analysis of Narrative and Semantics in Support of
  Interactivity","Our work has focused on support for film or television scriptwriting. Since
this involves potentially varied story-lines, we note the implicit or latent
support for interactivity. Furthermore the film, television, games, publishing
and other sectors are converging, so that cross-over and re-use of one form of
product in another of these sectors is ever more common. Technically our work
has been largely based on mathematical algorithms for data clustering and
display. Operationally, we also discuss how our algorithms can support
collective, distributed problem-solving.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1805.07703v2,Lightweight Unsupervised Deep Loop Closure,"Robust efficient loop closure detection is essential for large-scale
real-time SLAM. In this paper, we propose a novel unsupervised deep neural
network architecture of a feature embedding for visual loop closure that is
both reliable and compact. Our model is built upon the autoencoder
architecture, tailored specifically to the problem at hand. To train our
network, we inflict random noise on our input data as the denoising autoencoder
does, but, instead of applying random dropout, we warp images with randomized
projective transformations to emulate natural viewpoint changes due to robot
motion. Moreover, we utilize the geometric information and illumination
invariance provided by histogram of oriented gradients (HOG), forcing the
encoder to reconstruct a HOG descriptor instead of the original image. As a
result, our trained model extracts features robust to extreme variations in
appearance directly from raw images, without the need for labeled training data
or environment-specific training. We perform extensive experiments on various
challenging datasets, showing that the proposed deep loop-closure model
consistently outperforms the state-of-the-art methods in terms of effectiveness
and efficiency. Our model is fast and reliable enough to close loops in real
time with no dimensionality reduction, and capable of replacing generic
off-the-shelf networks in state-of-the-art ConvNet-based loop closure systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.02936v1,Distilling Programs to Prove Termination,"The problem of determining whether or not any program terminates was shown to
be undecidable by Turing, but recent advances in the area have allowed this
information to be determined for a large class of programs. The classic method
for deciding whether a program terminates dates back to Turing himself and
involves finding a ranking function that maps a program state to a well-order,
and then proving that the result of this function decreases for every possible
program transition. More recent approaches to proving termination have involved
moving away from the search for a single ranking function and toward a search
for a set of ranking functions; this set is a choice of ranking functions and a
disjunctive termination argument is used. In this paper, we describe a new
technique for determining whether programs terminate. Our technique is applied
to the output of the distillation program transformation that converts programs
into a simplified form called distilled form. Programs in distilled form are
converted into a corresponding labelled transition system and termination can
be demonstrated by showing that all possible infinite traces through this
labelled transition system would result in an infinite descent of well-founded
data values. We demonstrate our technique on a number of examples, and compare
it to previous work.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1908.07152v1,Unfolding Polyhedra,"Starting with the unsolved ""D\""urer's problem"" of edge-unfolding a convex
polyhedron to a net, we specialize and generalize (a) the types of cuts
permitted, and (b) the polyhedra shapes, to highlight both advances established
and which problems remain open.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.02737v1,Secure Memory Management on Modern Hardware,"Almost all modern hardware, from phone SoCs to high-end servers with
accelerators, contain memory translation and protection hardware like IOMMUs,
firewalls, and lookup tables which make it impossible to reason about, and
enforce protection and isolation based solely on the processor's MMUs. This has
led to numerous bugs and security vulnerabilities in today's system software.
  In this paper we regain the ability to reason about and enforce access
control using the proven concept of a reference monitor mediating accesses to
memory resources. We present a fine-grained, realistic memory protection model
that makes this traditional concept applicable today, and bring system software
in line with the complexity of modern, heterogeneous hardware.
  Our design is applicable to any operating system, regardless of architecture.
We show that it not only enforces the integrity properties of a system, but
does so with no inherent performance overhead and it is even amenable to
automation through code generation from trusted hardware specifications.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.04077v2,"Temporally-smooth Antialiasing and Lens Distortion with Rasterization
  Map","Current GPU rasterization procedure is limited to narrow views in rectilinear
perspective. While industries demand curvilinear perspective in wide-angle
views, like Virtual Reality and Virtual Film Production industry. This paper
delivers new rasterization method using industry-standard STMaps. Additionally
new antialiasing rasterization method is proposed, which outperforms MSAA in
both quality and performance. It is an improvement upon previous solutions
found in paper Perspective picture from Visual Sphere by yours truly.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1003.2851v3,The complexity of UNO,"This paper investigates the popular card game UNO from the viewpoint of
algorithmic combinatorial game theory. We define simple and concise
mathematical models for the game, including both cooperative and uncooperative
versions, and analyze their computational complexity. In particular, we prove
that even a single-player version of UNO is NP-complete, although some
restricted cases are in P. Surprisingly, we show that the uncooperative
two-player version is also in P.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0804.4740v1,An Affine-invariant Time-dependent Triangulation of Spatio-temporal Data,"In the geometric data model for spatio-temporal data, introduced by Chomicki
and Revesz, spatio-temporal data are modelled as a finite collection of
triangles that are transformed by time-dependent affinities of the plane. To
facilitate querying and animation of spatio-temporal data, we present a normal
form for data in the geometric data model. We propose an algorithm for
constructing this normal form via a spatio-temporal triangulation of geometric
data objects. This triangulation algorithm generates new geometric data objects
that partition the given objects both in space and in time. A particular
property of the proposed partition is that it is invariant under time-dependent
affine transformations, and hence independent of the particular choice of
coordinate system used to describe he spatio-temporal data in. We can show that
our algorithm works correctly and has a polynomial time complexity (of
reasonably low degree in the number of input triangles and the maximal degree
of the polynomial functions that describe the transformation functions). We
also discuss several possible applications of this spatio-temporal
triangulation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1809.07600v1,"MIDI-VAE: Modeling Dynamics and Instrumentation of Music with
  Applications to Style Transfer","We introduce MIDI-VAE, a neural network model based on Variational
Autoencoders that is capable of handling polyphonic music with multiple
instrument tracks, as well as modeling the dynamics of music by incorporating
note durations and velocities. We show that MIDI-VAE can perform style transfer
on symbolic music by automatically changing pitches, dynamics and instruments
of a music piece from, e.g., a Classical to a Jazz style. We evaluate the
efficacy of the style transfer by training separate style validation
classifiers. Our model can also interpolate between short pieces of music,
produce medleys and create mixtures of entire songs. The interpolations
smoothly change pitches, dynamics and instrumentation to create a harmonic
bridge between two music pieces. To the best of our knowledge, this work
represents the first successful attempt at applying neural style transfer to
complete musical compositions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.2667v2,Computing Exact Distances in the Congested Clique,"This paper gives simple distributed algorithms for the fundamental problem of
computing graph distances in the Congested Clique model. One of the main
components of our algorithms is fast matrix multiplication, for which we show
an $O(n^{1/3})$-round algorithm when the multiplication needs to be performed
over a semi-ring, and an $O(n^{0.157})$-round algorithm when the computation
can be performed over a field. We propose to denote by $\kappa$ the exponent of
matrix multiplication in this model, which gives $\kappa < 0.157$.
  We show how to compute all-pairs-shortest-paths (APSP) in $O(n^{1/3}\log{n})$
rounds in weighted graphs of $n$ nodes, implying also the computation of the
graph diameter $D$. In unweighted graphs, APSP can be computed in
$O(\min\{n^{1/3}\log{D},n^{\kappa} D\})$ rounds, and the diameter can be
computed in $O(n^{\kappa}\log{D})$ rounds. Furthermore, we show how to compute
the girth of a graph in $O(n^{1/3})$ rounds, and provide triangle detection and
4-cycle detection algorithms that complete in $O(n^{\kappa})$ rounds.
  All our algorithms are deterministic. Our triangle detection and 4-cycle
detection algorithms improve upon the previously best known algorithms in this
model, and refute a conjecture that $\tilde \Omega (n^{1/3})$ rounds are
required for detecting triangles by any deterministic oblivious algorithm. Our
distance computation algorithms are exact, and improve upon the previously best
known $\tilde O(n^{1/2})$ algorithm of Nanongkai [STOC 2014] for computing a
$(2+o(1))$-approximation of APSP.
  Finally, we give lower bounds that match the above for natural families of
algorithms. For the Congested Clique Broadcast model, we derive unconditioned
lower bounds for matrix multiplication and APSP. The matrix multiplication
algorithms and lower bounds are adapted from parallel computations, which is a
connection of independent interest.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1807.00212v2,"Automation of the Export Data from Open Journal Systems to the Russian
  Science Citation Index","It is shown that the calculation of scientometric indicators of the scientist
and also the scientific journal continues to be an actual problem nowadays. It
is revealed that the leading scientometric databases have the capabilities of
automated metadata collection from the scientific journal website by the use of
specialized electronic document management systems, in particular Open Journal
Systems. It is established that Open Journal Systems successfully exports
metadata about an article from scientific journals to scientometric databases
Scopus, Web of Science and Google Scholar. However, there is no standard method
of export from Open Journal Systems to such scientometric databases as the
Russian Science Citation Index and Index Copernicus, which determined the need
for research. The aim of the study is to develop the plug-in to the Open
Journal Systems for the export of data from this system to scientometric
database Russian Science Citation Index. As a result of the study, an
infological model for exporting metadata from Open Journal Systems to the
Russian Science Citation Index was proposed. The SirenExpo plug-in was
developed to export data from Open Journal Systems to the Russian Science
Citation Index by the use of the Articulus release preparation system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1503.01058v1,An algorithm for multiplication of split-octonions,"In this paper we introduce efficient algorithm for the multiplication of
split-octonions. The direct multiplication of two split-octonions requires 64
real multiplications and 56 real additions. More effective solutions still do
not exist. We show how to compute a product of the split-octonions with 28 real
multiplications and 92 real additions. During synthesis of the discussed
algorithm we use the fact that product of two split-octonions may be
represented as vector-matrix product. The matrix that participates in the
product calculating has unique structural properties that allow performing its
advantageous decomposition. Namely this decomposition leads to significant
reducing of the multiplicative complexity of split-octonions multiplication.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0001014v3,Nondeterministic Quantum Query and Quantum Communication Complexities,"We study nondeterministic quantum algorithms for Boolean functions f. Such
algorithms have positive acceptance probability on input x iff f(x)=1. In the
setting of query complexity, we show that the nondeterministic quantum
complexity of a Boolean function is equal to its ``nondeterministic
polynomial'' degree. We also prove a quantum-vs-classical gap of 1 vs n for
nondeterministic query complexity for a total function. In the setting of
communication complexity, we show that the nondeterministic quantum complexity
of a two-party function is equal to the logarithm of the rank of a
nondeterministic version of the communication matrix. This implies that the
quantum communication complexities of the equality and disjointness functions
are n+1 if we do not allow any error probability. We also exhibit a total
function in which the nondeterministic quantum communication complexity is
exponentially smaller than its classical counterpart.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1410.6153v5,"Be-CoDiS: A mathematical model to predict the risk of human diseases
  spread between countries. Validation and application to the 2014-15 Ebola
  Virus Disease epidemic","Ebola virus disease is a lethal human and primate disease that currently
requires a particular attention from the international health authorities due
to important outbreaks in some Western African countries and isolated cases in
the United Kingdom, the USA and Spain. Regarding the emergency of this
situation, there is a need of development of decision tools, such as
mathematical models, to assist the authorities to focus their efforts in
important factors to eradicate Ebola. In this work, we propose a novel
deterministic spatial-temporal model, called Be-CoDiS (Between-Countries
Disease Spread), to study the evolution of human diseases within and between
countries. The main interesting characteristics of Be-CoDiS are the
consideration of the movement of people between countries, the control measure
effects and the use of time dependent coefficients adapted to each country.
First, we focus on the mathematical formulation of each component of the model
and explain how its parameters and inputs are obtained. Then, in order to
validate our approach, we consider two numerical experiments regarding the
2014-15 Ebola epidemic. The first one studies the ability of the model in
predicting the EVD evolution between countries starting from the index cases in
Guinea in December 2013. The second one consists of forecasting the evolution
of the epidemic by using some recent data. The results obtained with Be-CoDiS
are compared to real data and other models outputs found in the literature.
Finally, a brief parameter sensitivity analysis is done. A free Matlab version
of Be-CoDiS is available at: http://www.mat.ucm.es/momat/software.htm",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0107027v2,Fixed-parameter complexity of semantics for logic programs,"A decision problem is called parameterized if its input is a pair of strings.
One of these strings is referred to as a parameter. The problem: given a
propositional logic program P and a non-negative integer k, decide whether P
has a stable model of size no more than k, is an example of a parameterized
decision problem with k serving as a parameter. Parameterized problems that are
NP-complete often become solvable in polynomial time if the parameter is fixed.
The problem to decide whether a program P has a stable model of size no more
than k, where k is fixed and not a part of input, can be solved in time
O(mn^k), where m is the size of P and n is the number of atoms in P. Thus, this
problem is in the class P. However, algorithms with the running time given by a
polynomial of order k are not satisfactory even for relatively small values of
k.
  The key question then is whether significantly better algorithms (with the
degree of the polynomial not dependent on k) exist. To tackle it, we use the
framework of fixed-parameter complexity. We establish the fixed-parameter
complexity for several parameterized decision problems involving models,
supported models and stable models of logic programs. We also establish the
fixed-parameter complexity for variants of these problems resulting from
restricting attention to Horn programs and to purely negative programs. Most of
the problems considered in the paper have high fixed-parameter complexity.
Thus, it is unlikely that fixing bounds on models (supported models, stable
models) will lead to fast algorithms to decide the existence of such models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1812.05297v1,"Comparing several calculi for first-order infinite-valued ≈Åukasiewicz
  logic","From the viewpoint of provability, we compare some Gentzen-type hypersequent
calculi for first-order infinite-valued {\L}ukasiewicz logic and for
first-order rational Pavelka logic with each other and with H\'ajek's
Hilbert-type calculi for these logics. The key aspect of our comparison is a
density elimination proof for one of the hypersequent calculi considered.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0707.1644v1,Fast and Simple Relational Processing of Uncertain Data,"This paper introduces U-relations, a succinct and purely relational
representation system for uncertain databases. U-relations support
attribute-level uncertainty using vertical partitioning. If we consider
positive relational algebra extended by an operation for computing possible
answers, a query on the logical level can be translated into, and evaluated as,
a single relational algebra query on the U-relation representation. The
translation scheme essentially preserves the size of the query in terms of
number of operations and, in particular, number of joins. Standard techniques
employed in off-the-shelf relational database management systems are effective
for optimizing and processing queries on U-relations. In our experiments we
show that query evaluation on U-relations scales to large amounts of data with
high degrees of uncertainty.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1411.4073v1,Decremental All-Pairs ALL Shortest Paths and Betweenness Centrality,"We consider the all pairs all shortest paths (APASP) problem, which maintains
the shortest path dag rooted at every vertex in a directed graph G=(V,E) with
positive edge weights. For this problem we present a decremental algorithm
(that supports the deletion of a vertex, or weight increases on edges incident
to a vertex). Our algorithm runs in amortized O(\vstar^2 \cdot \log n) time per
update, where n=|V|, and \vstar bounds the number of edges that lie on shortest
paths through any given vertex. Our APASP algorithm can be used for the
decremental computation of betweenness centrality (BC), a graph parameter that
is widely used in the analysis of large complex networks. No nontrivial
decremental algorithm for either problem was known prior to our work. Our
method is a generalization of the decremental algorithm of Demetrescu and
Italiano [DI04] for unique shortest paths, and for graphs with \vstar =O(n), we
match the bound in [DI04]. Thus for graphs with a constant number of shortest
paths between any pair of vertices, our algorithm maintains APASP and BC scores
in amortized time O(n^2 \log n) under decremental updates, regardless of the
number of edges in the graph.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.04921v1,"How near-duplicate detection improves editors' and authors' publishing
  experience","We describe a system that helps identify manuscripts submitted to multiple
journals at the same time. Also, we discuss potential applications of the
near-duplicate detection technology when run with manuscript text content,
including identification of simultaneous submissions, prevention of duplicate
published articles, and improving article transfer service.",0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2110.01677v1,Inductive learning for product assortment graph completion,"Global retailers have assortments that contain hundreds of thousands of
products that can be linked by several types of relationships like style
compatibility, ""bought together"", ""watched together"", etc. Graphs are a natural
representation for assortments, where products are nodes and relations are
edges. Relations like style compatibility are often produced by a manual
process and therefore do not cover uniformly the whole graph. We propose to use
inductive learning to enhance a graph encoding style compatibility of a fashion
assortment, leveraging rich node information comprising textual descriptions
and visual data. Then, we show how the proposed graph enhancement improves
substantially the performance on transductive tasks with a minor impact on
graph sparsity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.01514v1,"Monitoring Data Distribution and Exploitation in a Global-Scale
  Microservice Artefact Observatory","Reusable microservice artefacts are often deployed as black or grey boxes,
with little concern for their properties and quality, beyond a syntactical
interface description. This leads application developers to chaotic and
opportunistic assumptions about how a composite application will behave in the
real world. Systematically analyzing and tracking these publicly available
artefacts will grant much needed predictability to microservice-based
deployments. By establishing a distributed observatory and knowledge base, it
is possible to track microservice repositories and analyze the artefacts
reliably, and provide insights on their properties and quality to developers
and researchers alike. This position paper argues for a federated research
infrastructure with consensus voting among participants to establish and
preserve ground truth about the insights.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/cs/0701184v2,"Structure and Problem Hardness: Goal Asymmetry and DPLL Proofs in<br>
  SAT-Based Planning","In Verification and in (optimal) AI Planning, a successful method is to
formulate the application as boolean satisfiability (SAT), and solve it with
state-of-the-art DPLL-based procedures. There is a lack of understanding of why
this works so well. Focussing on the Planning context, we identify a form of
problem structure concerned with the symmetrical or asymmetrical nature of the
cost of achieving the individual planning goals. We quantify this sort of
structure with a simple numeric parameter called AsymRatio, ranging between 0
and 1. We run experiments in 10 benchmark domains from the International
Planning Competitions since 2000; we show that AsymRatio is a good indicator of
SAT solver performance in 8 of these domains. We then examine carefully crafted
synthetic planning domains that allow control of the amount of structure, and
that are clean enough for a rigorous analysis of the combinatorial search
space. The domains are parameterized by size, and by the amount of structure.
The CNFs we examine are unsatisfiable, encoding one planning step less than the
length of the optimal plan. We prove upper and lower bounds on the size of the
best possible DPLL refutations, under different settings of the amount of
structure, as a function of size. We also identify the best possible sets of
branching variables (backdoors). With minimum AsymRatio, we prove exponential
lower bounds, and identify minimal backdoors of size linear in the number of
variables. With maximum AsymRatio, we identify logarithmic DPLL refutations
(and backdoors), showing a doubly exponential gap between the two structural
extreme cases. The reasons for this behavior -- the proof arguments --
illuminate the prototypical patterns of structure causing the empirical
behavior observed in the competition benchmarks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1007.1324v3,Separating the basic logics of the basic recurrences,"This paper shows that, even at the most basic level, the parallel, countable
branching and uncountable branching recurrences of Computability Logic (see
http://www.cis.upenn.edu/~giorgi/cl.html) validate different principles.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0111056v2,"Some Facets of Complexity Theory and Cryptography: A Five-Lectures
  Tutorial","In this tutorial, selected topics of cryptology and of computational
complexity theory are presented. We give a brief overview of the history and
the foundations of classical cryptography, and then move on to modern
public-key cryptography. Particular attention is paid to cryptographic
protocols and the problem of constructing the key components of such protocols
such as one-way functions. A function is one-way if it is easy to compute, but
hard to invert. We discuss the notion of one-way functions both in a
cryptographic and in a complexity-theoretic setting. We also consider
interactive proof systems and present some interesting zero-knowledge
protocols. In a zero-knowledge protocol one party can convince the other party
of knowing some secret information without disclosing any bit of this
information. Motivated by these protocols, we survey some complexity-theoretic
results on interactive proof systems and related complexity classes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0404003v1,Enhancing the expressive power of the U-Datalog language,"U-Datalog has been developed with the aim of providing a set-oriented logical
update language, guaranteeing update parallelism in the context of a
Datalog-like language. In U-Datalog, updates are expressed by introducing
constraints (+p(X), to denote insertion, and [minus sign]p(X), to denote
deletion) inside Datalog rules. A U-Datalog program can be interpreted as a CLP
program. In this framework, a set of updates (constraints) is satisfiable if it
does not represent an inconsistent theory, that is, it does not require the
insertion and the deletion of the same fact. This approach resembles a very
simple form of negation. However, on the other hand, U-Datalog does not provide
any mechanism to explicitly deal with negative information, resulting in a
language with limited expressive power. In this paper, we provide a semantics,
based on stratification, handling the use of negated atoms in U-Datalog
programs, and we show which problems arise in defining a compositional
semantics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.05658v1,Ten Research Challenge Areas in Data Science,"Although data science builds on knowledge from computer science, mathematics,
statistics, and other disciplines, data science is a unique field with many
mysteries to unlock: challenging scientific questions and pressing questions of
societal importance. This article starts with meta-questions about data science
as a discipline and then elaborates on ten ideas for the basis of a research
agenda for data science.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1904.02266v3,Continuous Direct Sparse Visual Odometry from RGB-D Images,"This paper reports on a novel formulation and evaluation of visual odometry
from RGB-D images. Assuming a static scene, the developed theoretical framework
generalizes the widely used direct energy formulation (photometric error
minimization) technique for obtaining a rigid body transformation that aligns
two overlapping RGB-D images to a continuous formulation. The continuity is
achieved through functional treatment of the problem and representing the
process models over RGB-D images in a reproducing kernel Hilbert space;
consequently, the registration is not limited to the specific image resolution
and the framework is fully analytical with a closed-form derivation of the
gradient. We solve the problem by maximizing the inner product between two
functions defined over RGB-D images, while the continuous action of the rigid
body motion Lie group is captured through the integration of the flow in the
corresponding Lie algebra. Energy-based approaches have been extremely
successful and the developed framework in this paper shares many of their
desired properties such as the parallel structure on both CPUs and GPUs,
sparsity, semi-dense tracking, avoiding explicit data association which is
computationally expensive, and possible extensions to the simultaneous
localization and mapping frameworks. The evaluations on experimental data and
comparison with the equivalent energy-based formulation of the problem confirm
the effectiveness of the proposed technique, especially, when the lack of
structure and texture in the environment is evident.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1301.5076v1,Mathematics Is Imprecise,"We commonly think of mathematics as bringing precision to application
domains, but its relationship with computer science is more complex. This
experience report on the use of Racket and Haskell to teach a required first
university CS course to students with very good mathematical skills focusses on
the ways that programming forces one to get the details right, with consequent
benefits in the mathematical domain. Conversely, imprecision in mathematical
abstractions and notation can work to the benefit of beginning programmers, if
handled carefully.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1308.5330v1,Combinatorial Abstractions of Dynamical Systems,"Formal verification has been successfully developed in computer science for
verifying combinatorial classes of models and specifications. In like manner,
formal verification methods have been developed for dynamical systems. However,
the verification of system properties, such as safety, is based on reachability
calculations, which are the sources of insurmountable complexity. This talk
addresses indirect verification methods, which are based on abstracting the
dynamical systems by models of reduced complexity and preserving central
properties of the original systems.",0,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.02789v2,Computing envy-freeable allocations with limited subsidies,"Fair division has emerged as a very hot topic in multiagent systems, and
envy-freeness is among the most compelling fairness concepts. An allocation of
indivisible items to agents is envy-free if no agent prefers the bundle of any
other agent to his own in terms of value. As envy-freeness is rarely a feasible
goal, there is a recent focus on relaxations of its definition. An approach in
this direction is to complement allocations with payments (or subsidies) to the
agents. A feasible goal then is to achieve envy-freeness in terms of the total
value an agent gets from the allocation and the subsidies.
  We consider the natural optimization problem of computing allocations that
are {\em envy-freeable} using the minimum amount of subsidies. As the problem
is NP-hard, we focus on the design of approximation algorithms. On the positive
side, we present an algorithm that, for a constant number of agents,
approximates the minimum amount of subsidies within any required accuracy, at
the expense of a graceful increase in the running time. On the negative side,
we show that, for a super-constant number of agents, the problem of minimizing
subsidies for envy-freeness is not only hard to compute exactly (as a folklore
argument shows) but also, more importantly, hard to approximate.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9301101v1,Verifying the Unification Algorithm in LCF,"Manna and Waldinger's theory of substitutions and unification has been
verified using the Cambridge LCF theorem prover. A proof of the monotonicity of
substitution is presented in detail, as an example of interaction with LCF.
Translating the theory into LCF's domain-theoretic logic is largely
straightforward. Well-founded induction on a complex ordering is translated
into nested structural inductions. Correctness of unification is expressed
using predicates for such properties as idempotence and most-generality. The
verification is presented as a series of lemmas. The LCF proofs are compared
with the original ones, and with other approaches. It appears difficult to find
a logic that is both simple and flexible, especially for proving termination.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1507.02119v2,Locating a Tree in a Reticulation-Visible Network in Cubic Time,"In this work, we answer an open problem in the study of phylogenetic
networks. Phylogenetic trees are rooted binary trees in which all edges are
directed away from the root, whereas phylogenetic networks are rooted acyclic
digraphs. For the purpose of evolutionary model validation, biologists often
want to know whether or not a phylogenetic tree is contained in a phylogenetic
network. The tree containment problem is NP-complete even for very restricted
classes of networks such as tree-sibling phylogenetic networks. We prove that
this problem is solvable in cubic time for stable phylogenetic networks. A
linear time algorithm is also presented for the cluster containment problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.11734v1,"Predicting the Critical Number of Layers for Hierarchical Support Vector
  Regression","Hierarchical support vector regression (HSVR) models a function from data as
a linear combination of SVR models at a range of scales, starting at a coarse
scale and moving to finer scales as the hierarchy continues. In the original
formulation of HSVR, there were no rules for choosing the depth of the model.
In this paper, we observe in a number of models a phase transition in the
training error -- the error remains relatively constant as layers are added,
until a critical scale is passed, at which point the training error drops close
to zero and remains nearly constant for added layers. We introduce a method to
predict this critical scale a priori with the prediction based on the support
of either a Fourier transform of the data or the Dynamic Mode Decomposition
(DMD) spectrum. This allows us to determine the required number of layers prior
to training any models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1610.04789v3,"Bsmooth: Learning from user feedback to disambiguate query terms in
  interactive data retrieval","There is great interest in supporting imprecise queries (e.g., keyword search
or natural language queries) over databases today. To support such queries, the
database system is typically required to disambiguate parts of the
user-specified query against the database, using whatever resources are
intrinsically available to it (the database schema, data values distributions,
natural language models etc). Often, systems will also have a user-interaction
log available, which can serve as an extrinsic resource to supplement their
model based on their own intrinsic resources. This leads to a problem of how
best to combine the system's prior ranking with insight derived from the
user-interaction log. Statistical inference techniques such as maximum
likelihood or Bayesian updates from a subjective prior turn out not to apply in
a straightforward way due to possible noise from user search behavior and to
encoding biases endemic to the system's models. In this paper, we address such
learning problem in interactive data retrieval, with specific focus on type
classification for user-specified query terms. We develop a novel Bayesian
smoothing algorithm, Bsmooth, which is simple, fast, flexible and accurate. We
analytically establish some desirable properties and show, through experiments
against an independent benchmark, that the addition of such a learning layer
performs much better than standard methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1605.04343v1,"A Relatively Small Turing Machine Whose Behavior Is Independent of Set
  Theory","Since the definition of the Busy Beaver function by Rado in 1962, an
interesting open question has been the smallest value of n for which BB(n) is
independent of ZFC set theory. Is this n approximately 10, or closer to
1,000,000, or is it even larger? In this paper, we show that it is at most
7,910 by presenting an explicit description of a 7,910-state Turing machine Z
with 1 tape and a 2-symbol alphabet that cannot be proved to run forever in ZFC
(even though it presumably does), assuming ZFC is consistent. The machine is
based on the work of Harvey Friedman on independent statements involving
order-invariant graphs. In doing so, we give the first known upper bound on the
highest provable Busy Beaver number in ZFC. To create Z, we develop and use a
higher-level language, Laconic, which is much more convenient than direct state
manipulation. We also use Laconic to design two Turing machines, G and R, that
halt if and only if there are counterexamples to Goldbach's Conjecture and the
Riemann Hypothesis, respectively.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9911006v1,Question Answering System Using Syntactic Information,"Question answering task is now being done in TREC8 using English documents.
We examined question answering task in Japanese sentences. Our method selects
the answer by matching the question sentence with knowledge-based data written
in natural language. We use syntactic information to obtain highly accurate
answers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.15236v2,"SD-Access: Practical Experiences in Designing and Deploying Software
  Defined Enterprise Networks","Enterprise Networks, over the years, have become more and more complex trying
to keep up with new requirements that challenge traditional solutions. Just to
mention one out of many possible examples, technologies such as Virtual LANs
(VLANs) struggle to address the scalability and operational requirements
introduced by Internet of Things (IoT) use cases. To keep up with these
challenges we have identified four main requirements that are common across
modern enterprise networks: (i) scalable mobility, (ii) endpoint segmentation,
(iii) simplified administration, and (iv) resource optimization. To address
these challenges we designed SDA (Software Defined Access), a solution for
modern enterprise networks that leverages Software-Defined Networking (SDN) and
other state of the art techniques. In this paper we present the design,
implementation and evaluation of SDA. Specifically, SDA: (i) leverages a
combination of an overlay approach with an event-driven protocol (LISP) to
dynamically adapt to traffic and mobility patterns while preserving resources,
and (ii) enforces dynamic endpoint groups for scalable segmentation with low
operational burden. We present our experience with deploying SDA in two
real-life scenarios: an enterprise campus, and a large warehouse with mobile
robots. Our evaluation shows that SDA, when compared with traditional
enterprise networks, can (i) reduce overall data plane forwarding state up to
70% thanks to a reactive protocol using a centralized routing server, and (ii)
reduce by an order of magnitude the handover delays in scenarios of massive
mobility with respect to other approaches. Finally, we discuss lessons learned
while deploying and operating SDA, and possible optimizations regarding the use
of an event-driven protocol and group-based segmentation.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1510.08276v1,Approximate Association via Dissociation,"A vertex set $X$ of a graph $G$ is an association set if each component of $G
- X$ is a clique, or a dissociation set if each component of $G - X$ is a
single vertex or a single edge. Interestingly, $G - X$ is then precisely a
graph containing no induced $P_3$'s or containing no $P_3$'s, respectively. We
observe some special structures and show that if none of them exists, then the
minimum association set problem can be reduced to the minimum (weighted)
dissociation set problem. This yields the first nontrivial approximation
algorithm for association set, and its approximation ratio is 2.5, matching the
best result of the closely related cluster editing problem. The reduction is
based on a combinatorial study of modular decomposition of graphs free of these
special structures. Further, a novel algorithmic use of modular decomposition
enables us to implement this approach in $O(m n + n^2)$ time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.04630v6,Forecasting using incomplete models,"We consider the task of forecasting an infinite sequence of future
observations based on some number of past observations, where the probability
measure generating the observations is ""suspected"" to satisfy one or more of a
set of incomplete models, i.e. convex sets in the space of probability
measures. This setting is in some sense intermediate between the realizable
setting where the probability measure comes from some known set of probability
measures (which can be addressed using e.g. Bayesian inference) and the
unrealizable setting where the probability measure is completely arbitrary. We
demonstrate a method of forecasting which guarantees that, whenever the true
probability measure satisfies an incomplete model in a given countable set, the
forecast converges to the same incomplete model in the (appropriately
normalized) Kantorovich-Rubinstein metric. This is analogous to merging of
opinions for Bayesian inference, except that convergence in the
Kantorovich-Rubinstein metric is weaker than convergence in total variation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.01629v1,A Formal Semantics for Data Analytics Pipelines,"In this report, we present a new programming model based on Pipelines and
Operators, which are the building blocks of programs written in PiCo, a DSL for
Data Analytics Pipelines. In the model we propose, we use the term Pipeline to
denote a workflow that processes data collections -- rather than a
computational process -- as is common in the data processing community. The
novelty with respect to other frameworks is that all PiCo operators are
polymorphic with respect to data types. This makes it possible to 1) re-use the
same algorithms and pipelines on different data models (e.g., streams, lists,
sets, etc); 2) reuse the same operators in different contexts, and 3) update
operators without affecting the calling context, i.e., the previous and
following stages in the pipeline. Notice that in other mainstream frameworks,
such as Spark, the update of a pipeline by changing a transformation with
another is not necessarily trivial, since it may require the development of an
input and output proxy to adapt the new transformation for the calling context.
In the same line, we provide a formal framework (i.e., typing and semantics)
that characterizes programs from the perspective of how they transform the data
structures they process -- rather than the computational processes they
represent. This approach allows to reason about programs at an abstract level,
without taking into account any aspect from the underlying execution model or
implementation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1909.05349v2,Cache Where you Want! Reconciling Predictability and Coherent Caching,"Real-time and cyber-physical systems need to interact with and respond to
their physical environment in a predictable time. While multicore platforms
provide incredible computational power and throughput, they also introduce new
sources of unpredictability. Large fluctuations in latency to access data
shared between multiple cores is an important contributor to the overall
execution-time variability. In addition to the temporal unpredictability
introduced by caching, parallel applications with data shared across multiple
cores also pay additional latency overheads due to data coherence. Analyzing
the impact of data coherence on the worst-case execution-time of real-time
applications is challenging because only scarce implementation details are
revealed by manufacturers. This paper presents application level control for
caching data at different levels of the cache hierarchy. The rationale is that
by caching data only in shared cache it is possible to bypass private caches.
The access latency to data present in caches becomes independent of its
coherence state. We discuss the existing architectural support as well as the
required hardware and OS modifications to support the proposed cacheability
control. We evaluate the system on an architectural simulator. We show that the
worst case execution time for a single memory write request is reduced by 52%.
Benchmark evaluations show that proposed technique has a minimal impact on
average performance.",0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.02244v1,Content-Aware GAN Compression,"Generative adversarial networks (GANs), e.g., StyleGAN2, play a vital role in
various image generation and synthesis tasks, yet their notoriously high
computational cost hinders their efficient deployment on edge devices. Directly
applying generic compression approaches yields poor results on GANs, which
motivates a number of recent GAN compression works. While prior works mainly
accelerate conditional GANs, e.g., pix2pix and CycleGAN, compressing
state-of-the-art unconditional GANs has rarely been explored and is more
challenging. In this paper, we propose novel approaches for unconditional GAN
compression. We first introduce effective channel pruning and knowledge
distillation schemes specialized for unconditional GANs. We then propose a
novel content-aware method to guide the processes of both pruning and
distillation. With content-awareness, we can effectively prune channels that
are unimportant to the contents of interest, e.g., human faces, and focus our
distillation on these regions, which significantly enhances the distillation
quality. On StyleGAN2 and SN-GAN, we achieve a substantial improvement over the
state-of-the-art compression method. Notably, we reduce the FLOPs of StyleGAN2
by 11x with visually negligible image quality loss compared to the full-size
model. More interestingly, when applied to various image manipulation tasks,
our compressed model forms a smoother and better disentangled latent manifold,
making it more effective for image editing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.02310v1,Parallel Hyperedge Replacement Grammars,"In 2018, it was shown that all finitely generated virtually Abelian groups
have multiple context-free word problems, and it is still an open problem as to
where to precisely place the word problems of hyperbolic groups in the formal
language hierarchy. Motivated by this, we introduce a new language class, the
parallel hyperedge replacement string languages, containing all multiple
context-free and ET0L languages. We show that parallel hyperedge replacement
grammars can be ""synchronised"", which allows us to establish many useful formal
language closure results relating to both the hypergraph and string languages
generated by various families of parallel hyperedge replacement grammars,
laying the foundations for future work in this area.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0603026v1,The Snowblower Problem,"We introduce the snowblower problem (SBP), a new optimization problem that is
closely related to milling problems and to some material-handling problems. The
objective in the SBP is to compute a short tour for the snowblower to follow to
remove all the snow from a domain (driveway, sidewalk, etc.). When a snowblower
passes over each region along the tour, it displaces snow into a nearby region.
The constraint is that if the snow is piled too high, then the snowblower
cannot clear the pile.
  We give an algorithmic study of the SBP. We show that in general, the problem
is NP-complete, and we present polynomial-time approximation algorithms for
removing snow under various assumptions about the operation of the snowblower.
Most commercially-available snowblowers allow the user to control the direction
in which the snow is thrown. We differentiate between the cases in which the
snow can be thrown in any direction, in any direction except backwards, and
only to the right. For all cases, we give constant-factor approximation
algorithms; the constants increase as the throw direction becomes more
restricted.
  Our results are also applicable to robotic vacuuming (or lawnmowing) with
bounded capacity dust bin and to some versions of material-handling problems,
in which the goal is to rearrange cartons on the floor of a warehouse.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1806.09383v2,Resolution with Counting: Dag-Like Lower Bounds and Different Moduli,"Resolution over linear equations is a natural extension of the popular
resolution refutation system, augmented with the ability to carry out basic
counting. Denoted Res(lin_R), this refutation system operates with disjunctions
of linear equations with boolean variables over a ring R, to refute
unsatisfiable sets of such disjunctions. Beginning in the work of [RT08],
through the work of [IS14] which focused on tree-like lower bounds, this
refutation system was shown to be fairly strong. Subsequent work (cf.[Kra17,
IS14, KO18, GK18]) made it evident that establishing lower bounds against
general Res(lin_R) refutations is a challenging and interesting task since the
system captures a 'minimal' extension of resolution with counting gates for
which no super-polynomial lower bounds are known to date.
  We provide the first super-polynomial size lower bounds on general (dag-like)
resolution over linear equations refutations in the large characteristic
regime. In particular we prove that the subset-sum principle 1+x1+...+2^n xn=0
requires refutations of exponential size over Q. Our proof technique is
nontrivial and novel: roughly speaking, we show that under certain conditions
every refutation of a subset-sum instance f=0 must pass through a fat clause
containing an equation f=alpha for each alpha in the image of f under boolean
assignments. We develop a somewhat different approach to prove exponential
lower bounds against tree-like refutations of any subset-sum instance that
depends on n variables, hence also separating tree-like from dag-like
refutations over the rationals. (Abstract continued in the full paper.)",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1604.03655v12,"A Discrete and Bounded Envy-Free Cake Cutting Protocol for Any Number of
  Agents","We consider the well-studied cake cutting problem in which the goal is to
find an envy-free allocation based on queries from $n$ agents. The problem has
received attention in computer science, mathematics, and economics. It has been
a major open problem whether there exists a discrete and bounded envy-free
protocol. We resolve the problem by proposing a discrete and bounded envy-free
protocol for any number of agents. The maximum number of queries required by
the protocol is $n^{n^{n^{n^{n^n}}}}$. We additionally show that even if we do
not run our protocol to completion, it can find in at most $n^3{(n^2)}^n$
queries a partial allocation of the cake that achieves proportionality (each
agent gets at least $1/n$ of the value of the whole cake) and envy-freeness.
Finally we show that an envy-free partial allocation can be computed in at most
$n^3{(n^2)}^n$ queries such that each agent gets a connected piece that gives
the agent at least $1/(3n)$ of the value of the whole cake.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1605.04515v8,Machine Translation Evaluation Resources and Methods: A Survey,"We introduce the Machine Translation (MT) evaluation survey that contains
both manual and automatic evaluation methods. The traditional human evaluation
criteria mainly include the intelligibility, fidelity, fluency, adequacy,
comprehension, and informativeness. The advanced human assessments include
task-oriented measures, post-editing, segment ranking, and extended criteriea,
etc. We classify the automatic evaluation methods into two categories,
including lexical similarity scenario and linguistic features application. The
lexical similarity methods contain edit distance, precision, recall, F-measure,
and word order. The linguistic features can be divided into syntactic features
and semantic features respectively. The syntactic features include part of
speech tag, phrase types and sentence structures, and the semantic features
include named entity, synonyms, textual entailment, paraphrase, semantic roles,
and language models. The deep learning models for evaluation are very newly
proposed. Subsequently, we also introduce the evaluation methods for MT
evaluation including different correlation scores, and the recent quality
estimation (QE) tasks for MT.
  This paper differs from the existing works
\cite{GALEprogram2009,EuroMatrixProject2007} from several aspects, by
introducing some recent development of MT evaluation measures, the different
classifications from manual to automatic evaluation measures, the introduction
of recent QE tasks of MT, and the concise construction of the content.
  We hope this work will be helpful for MT researchers to easily pick up some
metrics that are best suitable for their specific MT model development, and
help MT evaluation researchers to get a general clue of how MT evaluation
research developed. Furthermore, hopefully, this work can also shine some light
on other evaluation tasks, except for translation, of NLP fields.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.03186v3,"Noise Estimation Using Density Estimation for Self-Supervised Multimodal
  Learning","One of the key factors of enabling machine learning models to comprehend and
solve real-world tasks is to leverage multimodal data. Unfortunately,
annotation of multimodal data is challenging and expensive. Recently,
self-supervised multimodal methods that combine vision and language were
proposed to learn multimodal representations without annotation. However, these
methods often choose to ignore the presence of high levels of noise and thus
yield sub-optimal results. In this work, we show that the problem of noise
estimation for multimodal data can be reduced to a multimodal density
estimation task. Using multimodal density estimation, we propose a noise
estimation building block for multimodal representation learning that is based
strictly on the inherent correlation between different modalities. We
demonstrate how our noise estimation can be broadly integrated and achieves
comparable results to state-of-the-art performance on five different benchmark
datasets for two challenging multimodal tasks: Video Question Answering and
Text-To-Video Retrieval. Furthermore, we provide a theoretical probabilistic
error bound substantiating our empirical results and analyze failure cases.
Code: https://github.com/elad-amrani/ssml.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1810.07346v3,"Spherical Triangle Algorithm: A Fast Oracle for Convex Hull Membership
  Queries","The it Convex Hull Membership(CHM) problem is: Given a point $p$ and a subset
$S$ of $n$ points in $\mathbb{R}^m$, is $p \in conv(S)$? CHM is not only a
fundamental problem in Linear Programming, Computational Geometry, Machine
Learning and Statistics, it also serves as a query problem in many applications
e.g. Topic Modeling, LP Feasibility, Data Reduction. The {\it Triangle
Algorithm} (TA) \cite{kalantari2015characterization} either computes an
approximate solution in the convex hull, or a separating hyperplane. The {\it
Spherical}-CHM is a CHM, where $p=0$ and each point in $S$ has unit norm.
First, we prove the equivalence of exact and approximate versions of CHM and
Spherical-CHM. On the one hand, this makes it possible to state a simple
version of the original TA. On the other hand, we prove that under the
satisfiability of a simple condition in each iteration, the complexity improves
to $O(1/\varepsilon)$. The analysis also suggests a strategy for when the
property does not hold at an iterate. This suggests the \textit{Spherical-TA}
which first converts a given CHM into a Spherical-CHM before applying the
algorithm. Next we introduce a series of applications of Spherical-TA. In
particular, Spherical-TA serves as a fast version of vanilla TA to boost its
efficiency. As an example, this results in a fast version of \emph{AVTA}
\cite{awasthi2018robust}, called \emph{AVTA$^+$} for solving exact or
approximate irredundancy problem. Computationally, we have considered CHM, LP
and Strict LP Feasibility and the Irredundancy problem. Based on substantial
amount of computing, Spherical-TA achieves better efficiency than state of the
art algorithms. Leveraging on the efficiency of Spherical-TA, we propose
AVTA$^+$ as a pre-processing step for data reduction which arises in such
applications as in computing the Minimum Volume Enclosing Ellipsoid
\cite{moshtagh2005minimum}.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1910.05379v1,"B-Splines for Sparse Grids: Algorithms and Application to
  Higher-Dimensional Optimization","In simulation technology, computationally expensive objective functions are
often replaced by cheap surrogates, which can be obtained by interpolation.
Full grid interpolation methods suffer from the so-called curse of
dimensionality, rendering them infeasible if the parameter domain of the
function is higher-dimensional (four or more parameters). Sparse grids
constitute a discretization method that drastically eases the curse, while the
approximation quality deteriorates only insignificantly. However, conventional
basis functions such as piecewise linear functions are not smooth (continuously
differentiable). Hence, these basis functions are unsuitable for applications
in which gradients are required. One example for such an application is
gradient-based optimization, in which the availability of gradients greatly
improves the speed of convergence and the accuracy of the results.
  This thesis demonstrates that hierarchical B-splines on sparse grids are
well-suited for obtaining smooth interpolants for higher dimensionalities. The
thesis is organized in two main parts: In the first part, we derive new
B-spline bases on sparse grids and study their implications on theory and
algorithms. In the second part, we consider three real-world applications in
optimization: topology optimization, biomechanical continuum-mechanics, and
dynamic portfolio choice models in finance. The results reveal that the
optimization problems of these applications can be solved accurately and
efficiently with hierarchical B-splines on sparse grids.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1603.06078v2,Deep Shading: Convolutional Neural Networks for Screen-Space Shading,"In computer vision, convolutional neural networks (CNNs) have recently
achieved new levels of performance for several inverse problems where RGB pixel
appearance is mapped to attributes such as positions, normals or reflectance.
In computer graphics, screen-space shading has recently increased the visual
quality in interactive image synthesis, where per-pixel attributes such as
positions, normals or reflectance of a virtual 3D scene are converted into RGB
pixel appearance, enabling effects like ambient occlusion, indirect light,
scattering, depth-of-field, motion blur, or anti-aliasing. In this paper we
consider the diagonal problem: synthesizing appearance from given per-pixel
attributes using a CNN. The resulting Deep Shading simulates various
screen-space effects at competitive quality and speed while not being
programmed by human experts but learned from example images.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0212029v1,A Theory of Cross-Validation Error,"This paper presents a theory of error in cross-validation testing of
algorithms for predicting real-valued attributes. The theory justifies the
claim that predicting real-valued attributes requires balancing the conflicting
demands of simplicity and accuracy. Furthermore, the theory indicates precisely
how these conflicting demands must be balanced, in order to minimize
cross-validation error. A general theory is presented, then it is developed in
detail for linear regression and instance-based learning.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1703.03346v6,Reasoning About Bounds in Weighted Transition Systems,"We propose a way of reasoning about minimal and maximal values of the weights
of transitions in a weighted transition system (WTS). This perspective induces
a notion of bisimulation that is coarser than the classic bisimulation: it
relates states that exhibit transitions to bisimulation classes with the
weights within the same boundaries. We propose a customized modal logic that
expresses these numeric boundaries for transition weights by means of
particular modalities. We prove that our logic is invariant under the proposed
notion of bisimulation. We show that the logic enjoys the finite model property
and we identify a complete axiomatization for the logic. Last but not least, we
use a tableau method to show that the satisfiability problem for the logic is
decidable.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0205004v1,"Weaves: A Novel Direct Code Execution Interface for Parallel High
  Performance Scientific Codes","Scientific codes are increasingly being used in compositional settings,
especially problem solving environments (PSEs). Typical compositional modeling
frameworks require significant buy-in, in the form of commitment to a
particular style of programming (e.g., distributed object components). While
this solution is feasible for newer generations of component-based scientific
codes, large legacy code bases present a veritable software engineering
nightmare. We introduce Weaves a novel framework that enables modeling,
composition, direct code execution, performance characterization, adaptation,
and control of unmodified high performance scientific codes. Weaves is an
efficient generalized framework for parallel compositional modeling that is a
proper superset of the threads and processes models of programming. In this
paper, our focus is on the transparent code execution interface enabled by
Weaves. We identify design constraints, their impact on implementation
alternatives, configuration scenarios, and present results from a prototype
implementation on Intel x86 architectures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0701192v5,The pitfalls of verifying floating-point computations,"Current critical systems commonly use a lot of floating-point computations,
and thus the testing or static analysis of programs containing floating-point
operators has become a priority. However, correctly defining the semantics of
common implementations of floating-point is tricky, because semantics may
change with many factors beyond source-code level, such as choices made by
compilers. We here give concrete examples of problems that can appear and
solutions to implement in analysis software.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1605.04986v1,A Constant-Factor Bi-Criteria Approximation Guarantee for $k$-means++,"This paper studies the $k$-means++ algorithm for clustering as well as the
class of $D^\ell$ sampling algorithms to which $k$-means++ belongs. It is shown
that for any constant factor $\beta > 1$, selecting $\beta k$ cluster centers
by $D^\ell$ sampling yields a constant-factor approximation to the optimal
clustering with $k$ centers, in expectation and without conditions on the
dataset. This result extends the previously known $O(\log k)$ guarantee for the
case $\beta = 1$ to the constant-factor bi-criteria regime. It also improves
upon an existing constant-factor bi-criteria result that holds only with
constant probability.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.09271v1,A scalable DG solver for the electroneutral Nernst-Planck equations,"The robust, scalable simulation of flowing electrochemical systems is
increasingly important due to the synergy between intermittent renewable energy
and electrochemical technologies such as energy storage and chemical
manufacturing. The high P\'eclet regime of many such applications prevents the
use of off-the-shelf discretization methods. In this work, we present a
high-order Discontinuous Galerkin scheme for the electroneutral Nernst-Planck
equations. The chosen charge conservation formulation allows for the specific
treatment of the different physics: upwinding for advection and migration, and
interior penalty for diffusion of ionic species as well the electric potential.
Similarly, the formulation enables different treatments in the preconditioner:
AMG for the potential blocks and ILU-based methods for the advection-dominated
concentration blocks. We evaluate the convergence rate of the discretization
scheme through numerical tests. Strong scaling results for two preconditioning
approaches are shown for a large 3D flow-plate reactor example.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.00278v2,A finite element model updating method based on global optimization,"Finite element model updating of a structure made of linear elastic materials
is based on the solution of a minimization problem. The goal is to find some
unknown parameters of the finite element model (elastic moduli, mass densities,
constraints and boundary conditions) that minimize an objective function which
evaluates the discrepancy between experimental and numerical dynamic
properties. The objective function depends nonlinearly on the parameters and
may have multiple local minimum points. This paper presents a numerical method
able to find a global minimum point and assess its reliability. The numerical
method has been tested on two simulated examples - a masonry tower and a domed
temple - and validated via a generic genetic algorithm and a global sensitivity
analysis tool. A real case study monitored under operational conditions has
also been addressed, and the structure's experimental modal properties have
been used in the model updating procedure to estimate the mechanical properties
of its constituent materials.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0805.1968v1,Heavy-Tailed Limits for Medium Size Jobs and Comparison Scheduling,"We study the conditional sojourn time distributions of processor sharing
(PS), foreground background processor sharing (FBPS) and shortest remaining
processing time first (SRPT) scheduling disciplines on an event where the job
size of a customer arriving in stationarity is smaller than exactly k>=0 out of
the preceding m>=k arrivals. Then, conditioning on the preceding event, the
sojourn time distribution of this newly arriving customer behaves
asymptotically the same as if the customer were served in isolation with a
server of rate (1-\rho)/(k+1) for PS/FBPS, and (1-\rho) for SRPT, respectively,
where \rho is the traffic intensity. Hence, the introduced notion of
conditional limits allows us to distinguish the asymptotic performance of the
studied schedulers by showing that SRPT exhibits considerably better asymptotic
behavior for relatively smaller jobs than PS/FBPS.
  Inspired by the preceding results, we propose an approximation to the SRPT
discipline based on a novel adaptive job grouping mechanism that uses relative
size comparison of a newly arriving job to the preceding m arrivals.
Specifically, if the newly arriving job is smaller than k and larger than m-k
of the previous m jobs, it is routed into class k. Then, the classes of smaller
jobs are served with higher priorities using the static priority scheduling.
The good performance of this mechanism, even for a small number of classes m+1,
is demonstrated using the asymptotic queueing analysis under the heavy-tailed
job requirements. We also discuss refinements of the comparison grouping
mechanism that improve the accuracy of job classification at the expense of a
small additional complexity.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.04191v2,Belief places and spaces: Mapping cognitive environments,"Beliefs are not facts, but they are factive - they feel like facts. This
property is what can make misinformation dangerous. Being able to deliberately
navigate through a landscape of often conflicting factive statements is
difficult when there is no way to show the relationships between them without
incorporating the information in linear, narrative forms. In this paper, we
present a mechanism to produce maps of belief places, where populations agree
on salient features of fictional environments, and belief spaces, where
subgroups have related but distinct perspectives. Using a model developed using
agent-based simulation, we show that by observing the repeated behaviors of
human participants in the same social context, it is possible to build maps
that show the shared narrative environment overlaid with traces that show
unique, individual or subgroup perspectives. Our contribution is a
proof-of-concept system, based on the affordances of fantasy tabletop
role-playing games, which support multiple groups interacting with the same
dungeon in a controlled, online environment. The techniques used in this
process are mathematically straightforward, and should be generalizable to
auto-generating larger-scale maps of belief spaces from other corpora, such as
discussions on social media.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1303.7331v1,The stack calculus,"We introduce a functional calculus with simple syntax and operational
semantics in which the calculi introduced so far in the Curry-Howard
correspondence for Classical Logic can be faithfully encoded. Our calculus
enjoys confluence without any restriction. Its type system enforces strong
normalization of expressions and it is a sound and complete system for full
implicational Classical Logic. We give a very simple denotational semantics
which allows easy calculations of the interpretation of expressions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1611.09590v3,Implicant based parallel all solution solver for Boolean satisfiability,"This paper develops a parallel computational solver for computing all
satifying assignments of a Boolean system of equations defined by Boolean
functions of several variables. While there are we known solvers for
satisfiability of Boolean formulas in CNF form, these are designed primarily
for deciding satisfiability of the formula and do not address the problem of
finding all satisfying solutions. Moreover development of parallel solvers for
satisfiability problems is still an unfinished problem of Computer Science. The
solver proposed in this paper is aimed at representing all solutions of Boolean
formulas even without the CNF form with a parallel algorithm. Algorithm
proposed is applied to Boolean functions in algebraic normal form (ANF). The
algorithm is based on the idea to represent the satisfying assignments in terms
of a complete set of implicants of the Boolean functions appearing as factors
of a Boolean formula. The algorithm is effective mainly in the case when the
factors of the formula are sparse (i.e. have a small fraction of the total
number of variables). This allows small computation of a complete set of
implicants of individual factors one at a time and reduce the formula at each
step. An algorithm is also proposed for finding a complete set of orthogonal
implicants of functions in ANF. An advantages of this algorithm is that all
solutions can be represented compactly in terms of implicants. Finally due to
small and distributed computation at every step as well as computation in terms
of independent threads, the solver proposed in this paper is expected to be
useful for developing heuristics for a well scalable parallel solver for large
size problems of Boolean satisfiability over large number of processors.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1109.5559v1,High performance cosmological simulations on a grid of supercomputers,"We present results from our cosmological N-body simulation which consisted of
2048x2048x2048 particles and ran distributed across three supercomputers
throughout Europe. The run, which was performed as the concluding phase of the
Gravitational Billion Body Problem DEISA project, integrated a 30 Mpc box of
dark matter using an optimized Tree/Particle Mesh N-body integrator. We ran the
simulation up to the present day (z=0), and obtained an efficiency of about
0.93 over 2048 cores compared to a single supercomputer run. In addition, we
share our experiences on using multiple supercomputers for high performance
computing and provide several recommendations for future projects.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.00496v1,"On the equivalence between Kolmogorov-Smirnov and ROC curve metrics for
  binary classification","Binary decisions are very common in artificial intelligence. Applying a
threshold on the continuous score gives the human decider the power to control
the operating point to separate the two classes. The classifier,s
discriminating power is measured along the continuous range of the score by the
Area Under the ROC curve (AUC_ROC) in most application fields. Only finances
uses the poor single point metric maximum Kolmogorov-Smirnov (KS) distance.
This paper proposes the Area Under the KS curve (AUC_KS) for performance
assessment and proves AUC_ROC = 0.5 + AUC_KS, as a simpler way to calculate the
AUC_ROC. That is even more important for ROC averaging in ensembles of
classifiers or n fold cross-validation. The proof is geometrically inspired on
rotating all KS curve to make it lie on the top of the ROC chance diagonal. On
the practical side, the independent variable on the abscissa on the KS curve
simplifies the calculation of the AUC_ROC. On the theoretical side, this
research gives insights on probabilistic interpretations of classifiers
assessment and integrates the existing body of knowledge of the information
theoretical ROC approach with the proposed statistical approach based on the
thoroughly known KS distribution.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1101.1718v1,"Precise Schedulability Analysis for unfeasible to notify separately for
  comprehensive - EDF Scheduling of interrupted Hard Real-Time Tasks on the
  similar Multiprocessors","In Real-time system, utilization based schedulability test is a common
approach to determine whether or not tasks can be admitted without violating
deadline requirements. The exact problem has previously been proven intractable
even upon single processors; sufficient conditions are presented here for
determining whether a given periodic task system will meet all deadlines if
scheduled non-preemptively upon a multiprocessor platform using the
earliest-deadline first scheduling algorithm. Many real-time scheduling
algorithms have been developed recently to reduce affinity in the portable
devices that use processors. Extensive power aware scheduling techniques have
been published for energy reduction, but most of them have been focused solely
on reducing the processor affinity. The non-preemptive scheduling of periodic
task systems upon processing platforms comprised of several same processors is
considered.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.12924v1,Benchmarking Deep Trackers on Aerial Videos,"In recent years, deep learning-based visual object trackers have achieved
state-of-the-art performance on several visual object tracking benchmarks.
However, most tracking benchmarks are focused on ground level videos, whereas
aerial tracking presents a new set of challenges. In this paper, we compare ten
trackers based on deep learning techniques on four aerial datasets. We choose
top performing trackers utilizing different approaches, specifically tracking
by detection, discriminative correlation filters, Siamese networks and
reinforcement learning. In our experiments, we use a subset of OTB2015 dataset
with aerial style videos; the UAV123 dataset without synthetic sequences; the
UAV20L dataset, which contains 20 long sequences; and DTB70 dataset as our
benchmark datasets. We compare the advantages and disadvantages of different
trackers in different tracking situations encountered in aerial data. Our
findings indicate that the trackers perform significantly worse in aerial
datasets compared to standard ground level videos. We attribute this effect to
smaller target size, camera motion, significant camera rotation with respect to
the target, out of view movement, and clutter in the form of occlusions or
similar looking distractors near tracked object.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.00069v1,"Unbiased Deterministic Total Ordering of Parallel Simulations with
  Simultaneous Events","In the area of discrete event simulation (DES), event simultaneity occurs
when any two events are scheduled to happen at the same point in simulated
time. Simulation determinism is the expectation that the same semantically
configured simulation will be guaranteed to repeatedly reproduce identical
results. Since events in DES are the sole mechanism for state change, ensuring
consistent real-time event processing order is crucial to maintaining
determinism. This is synonymous with finding a consistent total ordering of
events.
  In this work, we extend the concept of virtual time to utilize an
arbitrary-length series of tie-breaking values to preserve determinism in
parallel, optimistically executed simulations without imposing additional bias
influencing the ordering of otherwise incomparable events. Furthermore, by
changing the core pseudo-random number generator seed at initialization,
different orderings of events incomparable by standard virtual time can be
observed, allowing for fair probing of other potential simulation outcomes. We
implement and evaluate this extended definition of virtual time in the
Rensselaer Optimistic Simulation System (ROSS) with three simulation models and
discuss the importance of deterministic event ordering given the existence of
event ties.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1905.09950v4,Zero-shot task adaptation by homoiconic meta-mapping,"How can deep learning systems flexibly reuse their knowledge? Toward this
goal, we propose a new class of challenges, and a class of architectures that
can solve them. The challenges are meta-mappings, which involve systematically
transforming task behaviors to adapt to new tasks zero-shot. The key to
achieving these challenges is representing the task being performed in such a
way that this task representation is itself transformable. We therefore draw
inspiration from functional programming and recent work in meta-learning to
propose a class of Homoiconic Meta-Mapping (HoMM) approaches that represent
data points and tasks in a shared latent space, and learn to infer
transformations of that space. HoMM approaches can be applied to any type of
machine learning task. We demonstrate the utility of this perspective by
exhibiting zero-shot remapping of behavior to adapt to new tasks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.14109v1,Real-Time Camera Pose Estimation for Sports Fields,"Given an image sequence featuring a portion of a sports field filmed by a
moving and uncalibrated camera, such as the one of the smartphones, our goal is
to compute automatically in real time the focal length and extrinsic camera
parameters for each image in the sequence without using a priori knowledges of
the position and orientation of the camera. To this end, we propose a novel
framework that combines accurate localization and robust identification of
specific keypoints in the image by using a fully convolutional deep
architecture. Our algorithm exploits both the field lines and the players'
image locations, assuming their ground plane positions to be given, to achieve
accuracy and robustness that is beyond the current state of the art. We will
demonstrate its effectiveness on challenging soccer, basketball, and volleyball
benchmark datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0405064v1,"Designing Competent Mutation Operators via Probabilistic Model Building
  of Neighborhoods","This paper presents a competent selectomutative genetic algorithm (GA), that
adapts linkage and solves hard problems quickly, reliably, and accurately. A
probabilistic model building process is used to automatically identify key
building blocks (BBs) of the search problem. The mutation operator uses the
probabilistic model of linkage groups to find the best among competing building
blocks. The competent selectomutative GA successfully solves additively
separable problems of bounded difficulty, requiring only subquadratic number of
function evaluations. The results show that for additively separable problems
the probabilistic model building BB-wise mutation scales as O(2^km^{1.5}), and
requires O(k^{0.5}logm) less function evaluations than its selectorecombinative
counterpart, confirming theoretical results reported elsewhere (Sastry &
Goldberg, 2004).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0801.3113v1,iBOA: The Incremental Bayesian Optimization Algorithm,"This paper proposes the incremental Bayesian optimization algorithm (iBOA),
which modifies standard BOA by removing the population of solutions and using
incremental updates of the Bayesian network. iBOA is shown to be able to learn
and exploit unrestricted Bayesian networks using incremental techniques for
updating both the structure as well as the parameters of the probabilistic
model. This represents an important step toward the design of competent
incremental estimation of distribution algorithms that can solve difficult
nearly decomposable problems scalably and reliably.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.10141v1,"Language Inference with Multi-head Automata through Reinforcement
  Learning","The purpose of this paper is to use reinforcement learning to model learning
agents which can recognize formal languages. Agents are modeled as simple
multi-head automaton, a new model of finite automaton that uses multiple heads,
and six different languages are formulated as reinforcement learning problems.
Two different algorithms are used for optimization. First algorithm is
Q-learning which trains gated recurrent units to learn optimal policies. The
second one is genetic algorithm which searches for the optimal solution by
using evolution inspired operations. The results show that genetic algorithm
performs better than Q-learning algorithm in general but Q-learning algorithm
finds solutions faster for regular languages.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1511.06559v3,Approximating Directed Steiner Problems via Tree Embedding,"In the k-edge connected directed Steiner tree (k-DST) problem, we are given a
directed graph G on n vertices with edge-costs, a root vertex r, a set of h
terminals T and an integer k. The goal is to find a min-cost subgraph H of G
that connects r to each terminal t by k edge-disjoint r,t-paths. This problem
includes as special cases the well-known directed Steiner tree (DST) problem
(the case k = 1) and the group Steiner tree (GST) problem. Despite having been
studied and mentioned many times in literature, e.g., by Feldman et al.
[SODA'09, JCSS'12], by Cheriyan et al. [SODA'12, TALG'14] and by Laekhanukit
[SODA'14], there was no known non-trivial approximation algorithm for k-DST for
k >= 2 even in the special case that an input graph is directed acyclic and has
a constant number of layers. If an input graph is not acyclic, the complexity
status of k-DST is not known even for a very strict special case that k= 2 and
|T| = 2.
  In this paper, we make a progress toward developing a non-trivial
approximation algorithm for k-DST. We present an O(D k^{D-1} log
n)-approximation algorithm for k-DST on directed acyclic graphs (DAGs) with D
layers, which can be extended to a special case of k-DST on ""general graphs""
when an instance has a D-shallow optimal solution, i.e., there exist k
edge-disjoint r,t-paths, each of length at most D, for every terminal t. For
the case k= 1 (DST), our algorithm yields an approximation ratio of O(D log h),
thus implying an O(log^3 h)-approximation algorithm for DST that runs in
quasi-polynomial-time (due to the height-reduction of Zelikovsky
[Algorithmica'97]). Consequently, as our algorithm works for general graphs, we
obtain an O(D k^{D-1} log n)-approximation algorithm for a D-shallow instance
of the k-edge-connected directed Steiner subgraph problem, where we wish to
connect every pair of terminals by k-edge-disjoint paths.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1902.08028v1,Short-distance commuters in the smart city,"This study models and examines commuter's preferences for short-distance
transportation modes, namely: walking, taking a bus or riding a metro. It is
used a unique dataset from a large-scale field experiment in Singapore that
provides rich information about tens of thousands of commuters' behavior. In
contrast to the standard approach, this work does not relay on survey data.
Conversely, the chosen transportation modes are identified by processing raw
data (latitude, longitude, timestamp). The approach of this work exploits the
information generated by the smart transportation system in the city that make
suitable the task of obtaining granular and nearly real-time data. Novel
algorithms are proposed with the intention to generate proxies for walkability
and public transport attributes. The empirical results of the case study
suggest that commuters do no differentiate between public transport choices
(bus and metro), therefore possible nested structures for the public transport
modes are rejected.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1312.1529v4,Instruction sequences expressing multiplication algorithms,"For each function on bit strings, its restriction to bit strings of any given
length can be computed by a finite instruction sequence that contains only
instructions to set and get the content of Boolean registers, forward jump
instructions, and a termination instruction. We describe instruction sequences
of this kind that compute the function on bit strings that models
multiplication on natural numbers less than $2^N$ with respect to their binary
representation by bit strings of length $N$, for a fixed but arbitrary $N > 0$,
according to the long multiplication algorithm and the Karatsuba multiplication
algorithm. We find among other things that the instruction sequence expressing
the former algorithm is longer than the one expressing the latter algorithm
only if the length of the bit strings involved is greater than $2^8$. We also
go into the use of an instruction sequence with backward jump instructions for
expressing the long multiplication algorithm. This leads to an instruction
sequence that it is shorter than the other two if the length of the bit strings
involved is greater than $2$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1406.5687v1,"Parallel Algorithms for Counting Triangles in Networks with Large
  Degrees","Finding the number of triangles in a network is an important problem in the
analysis of complex networks. The number of triangles also has important
applications in data mining. Existing distributed memory parallel algorithms
for counting triangles are either Map-Reduce based or message passing interface
(MPI) based and work with overlapping partitions of the given network. These
algorithms are designed for very sparse networks and do not work well when the
degrees of the nodes are relatively larger. For networks with larger degrees,
Map-Reduce based algorithm generates prohibitively large intermediate data, and
in MPI based algorithms with overlapping partitions, each partition can grow as
large as the original network, wiping out the benefit of partitioning the
network.
  In this paper, we present two efficient MPI-based parallel algorithms for
counting triangles in massive networks with large degrees. The first algorithm
is a space-efficient algorithm for networks that do not fit in the main memory
of a single compute node. This algorithm divides the network into
non-overlapping partitions. The second algorithm is for the case where the main
memory of each node is large enough to contain the entire network. We observe
that for such a case, computation load can be balanced dynamically and present
a dynamic load balancing scheme which improves the performance significantly.
Both of our algorithms scale well to large networks and to a large number of
processors.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.12663v2,"CerebroVis: Designing an Abstract yet Spatially Contextualized Cerebral
  Arteries Network Visualization","Blood circulation in the human brain is supplied through a network of
cerebral arteries. If a clinician suspects a patient has a stroke or other
cerebrovascular condition they order imaging tests. Neuroradiologists visually
search the resulting scans for abnormalities. Their visual search tasks
correspond to the abstract network analysis tasks of browsing and path
following. To assist neuroradiologists in identifying cerebral artery
abnormalities we designed CerebroVis, a novel abstract---yet spatially
contextualized---cerebral artery network visualization. In this design study,
we contribute a novel framing and definition of the cerebral artery system in
terms of network theory and characterize neuroradiologist domain goals as
abstract visualization and network analysis tasks. Through an iterative,
user-centered design process we developed an abstract network layout technique
which incorporates cerebral artery spatial context. The abstract visualization
enables increased domain task performance over 3D geometry representations,
while including spatial context helps preserve the user's mental map of the
underlying geometry. We provide open source implementations of our network
layout technique and prototype cerebral artery visualization tool. We
demonstrate the robustness of our technique by successfully laying out 61 open
source brain scans. We evaluate the effectiveness of our layout through a mixed
methods study with three neuroradiologists. In a formative controlled
experiment our study participants used CerebroVis and a conventional 3D
visualization to examine real cerebral artery imaging data and to identify a
simulated intracranial artery stenosis. Participants were more accurate at
identifying stenoses using CerebroVis (absolute risk difference 13%). A free
copy of this paper, the evaluation stimuli and data, and source code are
available at https://osf.io/e5sxt/.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1610.10079v1,"Bounded Model Checking of State-Space Digital Systems: The Impact of
  Finite Word-Length Effects on the Implementation of Fixed-Point Digital
  Controllers Based on State-Space Modeling","The extensive use of digital controllers demands a growing effort to prevent
design errors that appear due to finite-word length (FWL) effects. However,
there is still a gap, regarding verification tools and methodologies to check
implementation aspects of control systems. Thus, the present paper describes an
approach, which employs bounded model checking (BMC) techniques, to verify
fixed-point digital controllers represented by state-space equations. The
experimental results demonstrate the sensitivity of such systems to FWL effects
and the effectiveness of the proposed approach to detect them. To the best of
my knowledge, this is the first contribution tackling formal verification
through BMC of fixed-point state-space digital controllers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0405099v1,Web search engine based on DNS,"Now no web search engine can cover more than 60 percent of all the pages on
Internet. The update interval of most pages database is almost one month. This
condition hasn't changed for many years. Converge and recency problems have
become the bottleneck problem of current web search engine. To solve these
problems, a new system, search engine based on DNS is proposed in this paper.
This system adopts the hierarchical distributed architecture like DNS, which is
different from any current commercial search engine. In theory, this system can
cover all the web pages on Internet. Its update interval could even be one day.
The original idea, detailed content and implementation of this system all are
introduced in this paper.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1509.07164v1,The Stan Math Library: Reverse-Mode Automatic Differentiation in C++,"As computational challenges in optimization and statistical inference grow
ever harder, algorithms that utilize derivatives are becoming increasingly more
important. The implementation of the derivatives that make these algorithms so
powerful, however, is a substantial user burden and the practicality of these
algorithms depends critically on tools like automatic differentiation that
remove the implementation burden entirely. The Stan Math Library is a C++,
reverse-mode automatic differentiation library designed to be usable, extensive
and extensible, efficient, scalable, stable, portable, and redistributable in
order to facilitate the construction and utilization of such algorithms.
  Usability is achieved through a simple direct interface and a cleanly
abstracted functional interface. The extensive built-in library includes
functions for matrix operations, linear algebra, differential equation solving,
and most common probability functions. Extensibility derives from a
straightforward object-oriented framework for expressions, allowing users to
easily create custom functions. Efficiency is achieved through a combination of
custom memory management, subexpression caching, traits-based metaprogramming,
and expression templates. Partial derivatives for compound functions are
evaluated lazily for improved scalability. Stability is achieved by taking care
with arithmetic precision in algebraic expressions and providing stable,
compound functions where possible. For portability, the library is
standards-compliant C++ (03) and has been tested for all major compilers for
Windows, Mac OS X, and Linux.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1508.04885v1,Efficient Computation of Exact IRV Margins,"The margin of victory is easy to compute for many election schemes but
difficult for Instant Runoff Voting (IRV). This is important because arguments
about the correctness of an election outcome usually rely on the size of the
electoral margin. For example, risk-limiting audits require a knowledge of the
margin of victory in order to determine how much auditing is necessary. This
paper presents a practical branch-and-bound algorithm for exact IRV margin
computation that substantially improves on the current best-known approach.
Although exponential in the worst case, our algorithm runs efficiently in
practice on all the real examples we could find. We can efficiently discover
exact margins on election instances that cannot be solved by the current
state-of-the-art.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0011011v1,Formal Properties of XML Grammars and Languages,"XML documents are described by a document type definition (DTD). An
XML-grammar is a formal grammar that captures the syntactic features of a DTD.
We investigate properties of this family of grammars. We show that every
XML-language basically has a unique XML-grammar. We give two characterizations
of languages generated by XML-grammars, one is set-theoretic, the other is by a
kind of saturation property. We investigate decidability problems and prove
that some properties that are undecidable for general context-free languages
become decidable for XML-languages. We also characterize those XML-grammars
that generate regular XML-languages.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1905.05840v2,A Learning based Branch and Bound for Maximum Common Subgraph Problems,"Branch-and-bound (BnB) algorithms are widely used to solve combinatorial
problems, and the performance crucially depends on its branching heuristic.In
this work, we consider a typical problem of maximum common subgraph (MCS), and
propose a branching heuristic inspired from reinforcement learning with a goal
of reaching a tree leaf as early as possible to greatly reduce the search tree
size.Extensive experiments show that our method is beneficial and outperforms
current best BnB algorithm for the MCS.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1802.01621v1,Un-unzippable Convex Caps,"An unzipping of a polyhedron P is a cut-path through its vertices that
unfolds P to a non-overlapping shape in the plane. It is an open problem to
decide if every convex P has an unzipping. Here we show that there are nearly
flat convex caps that have no unzipping. A convex cap is a ""top"" portion of a
convex polyhedron; it has a boundary, i.e., it is not closed by a base.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.03554v1,Free-Choice Nets With Home Clusters Are Lucent,"A marked Petri net is lucent if there are no two different reachable markings
enabling the same set of transitions, i.e., states are fully characterized by
the transitions they enable. Characterizing the class of systems that are
lucent is a foundational and also challenging question. However, little
research has been done on the topic. In this paper, it is shown that all
free-choice nets having a home cluster are lucent. These nets have a so-called
home marking such that it is always possible to reach this marking again. Such
a home marking can serve as a regeneration point or as an end-point. The result
is highly relevant because in many applications, we want the system to be
lucent and many well-behaved process models fall into the class identified in
this paper. Unlike previous work, we do not require the marked Petri net to be
live and strongly connected. Most of the analysis techniques for free-choice
nets are tailored towards well-formed nets. The approach presented in this
paper provides a novel perspective enabling new analysis techniques for
free-choice nets that do not need to be well-formed. Therefore, we can also
model systems and processes that are terminating and/or have an initialization
phase.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1912.11146v2,"A Replication Strategy for Mobile Opportunistic Networks based on
  Utility Clustering","Dynamic replication is a wide-spread multi-copy routing approach for
efficiently coping with the intermittent connectivity in mobile opportunistic
networks. According to it, a node forwards a message replica to an encountered
node based on a utility value that captures the latter's fitness for delivering
the message to the destination. The popularity of the approach stems from its
flexibility to effectively operate in networks with diverse characteristics
without requiring special customization. Nonetheless, its drawback is the
tendency to produce a high number of replicas that consume limited resources
such as energy and storage. To tackle the problem we make the observation that
network nodes can be grouped, based on their utility values, into clusters that
portray different delivery capabilities. We exploit this finding to transform
the basic forwarding strategy, which is to move a packet using nodes of
increasing utility, and actually forward it through clusters of increasing
delivery capability. The new strategy works in synergy with the basic dynamic
replication algorithms and is fully configurable, in the sense that it can be
used with virtually any utility function. We also extend our approach to work
with two utility functions at the same time, a feature that is especially
efficient in mobile networks that exhibit social characteristics. By conducting
experiments in a wide set of real-life networks, we empirically show that our
method is robust in reducing the overall number of replicas in networks with
diverse connectivity characteristics without at the same time hindering
delivery efficiency.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1008.1371v2,A GPU-based hyperbolic SVD algorithm,"A one-sided Jacobi hyperbolic singular value decomposition (HSVD) algorithm,
using a massively parallel graphics processing unit (GPU), is developed. The
algorithm also serves as the final stage of solving a symmetric indefinite
eigenvalue problem. Numerical testing demonstrates the gains in speed and
accuracy over sequential and MPI-parallelized variants of similar Jacobi-type
HSVD algorithms. Finally, possibilities of hybrid CPU--GPU parallelism are
discussed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.06987v1,"Development of recommendation systems for software engineering: the
  CROSSMINER experience","To perform their daily tasks, developers intensively make use of existing
resources by consulting open-source software (OSS) repositories. Such platforms
contain rich data sources, e.g., code snippets, documentation, and user
discussions, that can be useful for supporting development activities. Over the
last decades, several techniques and tools have been promoted to provide
developers with innovative features, aiming to bring in improvements in terms
of development effort, cost savings, and productivity. In the context of the EU
H2020 CROSSMINER project, a set of recommendation systems has been conceived to
assist software programmers in different phases of the development process. The
systems provide developers with various artifacts, such as third-party
libraries, documentation about how to use the APIs being adopted, or relevant
API function calls. To develop such recommendations, various technical choices
have been made to overcome issues related to several aspects including the lack
of baselines, limited data availability, decisions about the performance
measures, and evaluation approaches. This paper is an experience report to
present the knowledge pertinent to the set of recommendation systems developed
through the CROSSMINER project. We explain in detail the challenges we had to
deal with, together with the related lessons learned when developing and
evaluating these systems. Our aim is to provide the research community with
concrete takeaway messages that are expected to be useful for those who want to
develop or customize their own recommendation systems. The reported experiences
can facilitate interesting discussions and research work, which in the end
contribute to the advancement of recommendation systems applied to solve
different issues in Software Engineering.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1910.01565v1,"On partisan bias in redistricting: computational complexity meets the
  science of gerrymandering","The topic of this paper is ""gerrymandering"", namely the curse of deliberate
creations of district maps with highly asymmetric electoral outcomes to
disenfranchise voters, and it has a long legal history. Measuring and
eliminating gerrymandering has enormous implications to sustain the backbone of
democratic principles of a society. Although there is no dearth of legal briefs
involving gerrymandering over many years, it is only more recently that
mathematicians and applied computational researchers have started to
investigate this topic. However, it has received relatively little attention so
far from the computational complexity researchers dealing with theoretical
analysis of computational complexity issues, such as computational hardness,
approximability issues, etc. There could be many reasons for this, such as
descriptions of these problem non-CS non-math (often legal or political)
journals that theoretical CS (TCS) people usually do not follow, or the lack of
coverage of these topics in TCS publication venues. One of our modest goals in
writing this article is to improve upon this situation by stimulating further
interactions between the gerrymandering and TCS researchers. To this effect,
our main contributions are twofold: (1) we provide formalization of several
models, related concepts, and corresponding problem statements using TCS
frameworks from the descriptions of these problems as available in existing
non-TCS (perhaps legal) venues, and (2) we also provide computational
complexity analysis of some versions of these problems, leaving other versions
for future research.
  The goal of writing this article is not to have the final word on
gerrymandering, but to introduce a series of concepts, models and problems to
the TCS community and to show that science of gerrymandering involves an
intriguing set of partitioning problems involving geometric and combinatorial
optimization.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1402.5565v1,"Semi-Supervised Nonlinear Distance Metric Learning via Forests of
  Max-Margin Cluster Hierarchies","Metric learning is a key problem for many data mining and machine learning
applications, and has long been dominated by Mahalanobis methods. Recent
advances in nonlinear metric learning have demonstrated the potential power of
non-Mahalanobis distance functions, particularly tree-based functions. We
propose a novel nonlinear metric learning method that uses an iterative,
hierarchical variant of semi-supervised max-margin clustering to construct a
forest of cluster hierarchies, where each individual hierarchy can be
interpreted as a weak metric over the data. By introducing randomness during
hierarchy training and combining the output of many of the resulting
semi-random weak hierarchy metrics, we can obtain a powerful and robust
nonlinear metric model. This method has two primary contributions: first, it is
semi-supervised, incorporating information from both constrained and
unconstrained points. Second, we take a relaxed approach to constraint
satisfaction, allowing the method to satisfy different subsets of the
constraints at different levels of the hierarchy rather than attempting to
simultaneously satisfy all of them. This leads to a more robust learning
algorithm. We compare our method to a number of state-of-the-art benchmarks on
$k$-nearest neighbor classification, large-scale image retrieval and
semi-supervised clustering problems, and find that our algorithm yields results
comparable or superior to the state-of-the-art, and is significantly more
robust to noise.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1612.06879v1,Robust mixture of experts modeling using the skew $t$ distribution,"Mixture of Experts (MoE) is a popular framework in the fields of statistics
and machine learning for modeling heterogeneity in data for regression,
classification and clustering. MoE for continuous data are usually based on the
normal distribution. However, it is known that for data with asymmetric
behavior, heavy tails and atypical observations, the use of the normal
distribution is unsuitable. We introduce a new robust non-normal mixture of
experts modeling using the skew $t$ distribution. The proposed skew $t$ mixture
of experts, named STMoE, handles these issues of the normal mixtures experts
regarding possibly skewed, heavy-tailed and noisy data. We develop a dedicated
expectation conditional maximization (ECM) algorithm to estimate the model
parameters by monotonically maximizing the observed data log-likelihood. We
describe how the presented model can be used in prediction and in model-based
clustering of regression data. Numerical experiments carried out on simulated
data show the effectiveness and the robustness of the proposed model in fitting
non-linear regression functions as well as in model-based clustering. Then, the
proposed model is applied to the real-world data of tone perception for musical
data analysis, and the one of temperature anomalies for the analysis of climate
change data. The obtained results confirm the usefulness of the model for
practical data analysis applications.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0902.2073v2,Polynomial Size Analysis of First-Order Shapely Functions,"We present a size-aware type system for first-order shapely function
definitions. Here, a function definition is called shapely when the size of the
result is determined exactly by a polynomial in the sizes of the arguments.
Examples of shapely function definitions may be implementations of matrix
multiplication and the Cartesian product of two lists. The type system is
proved to be sound w.r.t. the operational semantics of the language. The type
checking problem is shown to be undecidable in general. We define a natural
syntactic restriction such that the type checking becomes decidable, even
though size polynomials are not necessarily linear or monotonic. Furthermore,
we have shown that the type-inference problem is at least semi-decidable (under
this restriction). We have implemented a procedure that combines run-time
testing and type-checking to automatically obtain size dependencies. It
terminates on total typable function definitions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.10884v2,"Outliers Detection Is Not So Hard: Approximation Algorithms for Robust
  Clustering Problems Using Local Search Techniques","In this paper, we consider two types of robust models of the
$k$-median/$k$-means problems: the outlier-version ($k$-MedO/$k$-MeaO) and the
penalty-version ($k$-MedP/$k$-MeaP), in which we can mark some points as
outliers and discard them. In $k$-MedO/$k$-MeaO, the number of outliers is
bounded by a given integer. In $k$-MedP/$k$-MeaP, we do not bound the number of
outliers, but each outlier will incur a penalty cost. We develop a new
technique to analyze the approximation ratio of local search algorithms for
these two problems by introducing an adapted cluster that can capture useful
information about outliers in the local and the global optimal solution. For
$k$-MeaP, we improve the best known approximation ratio based on local search
from $25+\varepsilon$ to $9+\varepsilon$. For $k$-MedP, we obtain the best
known approximation ratio. For $k$-MedO/$k$-MeaO, there exists only two
bi-criteria approximation algorithms based on local search. One violates the
outlier constraint (the constraint on the number of outliers), while the other
violates the cardinality constraint (the constraint on the number of clusters).
We consider the former algorithm and improve its approximation ratios from
$17+\varepsilon$ to $3+\varepsilon$ for $k$-MedO, and from $274+\varepsilon$ to
$9+\varepsilon$ for $k$-MeaO.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.05293v1,A Comparison of Big Data Frameworks on a Layered Dataflow Model,"In the world of Big Data analytics, there is a series of tools aiming at
simplifying programming applications to be executed on clusters. Although each
tool claims to provide better programming, data and execution models, for which
only informal (and often confusing) semantics is generally provided, all share
a common underlying model, namely, the Dataflow model. The Dataflow model we
propose shows how various tools share the same expressiveness at different
levels of abstraction. The contribution of this work is twofold: first, we show
that the proposed model is (at least) as general as existing batch and
streaming frameworks (e.g., Spark, Flink, Storm), thus making it easier to
understand high-level data-processing applications written in such frameworks.
Second, we provide a layered model that can represent tools and applications
following the Dataflow paradigm and we show how the analyzed tools fit in each
level.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1512.04052v1,"Big Data Scaling through Metric Mapping: Exploiting the Remarkable
  Simplicity of Very High Dimensional Spaces using Correspondence Analysis","We present new findings in regard to data analysis in very high dimensional
spaces. We use dimensionalities up to around one million. A particular benefit
of Correspondence Analysis is its suitability for carrying out an orthonormal
mapping, or scaling, of power law distributed data. Power law distributed data
are found in many domains. Correspondence factor analysis provides a latent
semantic or principal axes mapping. Our experiments use data from digital
chemistry and finance, and other statistically generated data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.01190v1,"The 4th International Workshop on Smart Simulation and Modelling for
  Complex Systems","Computer-based modelling and simulation have become useful tools to
facilitate humans to understand systems in different domains, such as physics,
astrophysics, chemistry, biology, economics, engineering and social science. A
complex system is featured with a large number of interacting components
(agents, processes, etc.), whose aggregate activities are nonlinear and
self-organized. Complex systems are hard to be simulated or modelled by using
traditional computational approaches due to complex relationships among system
components, distributed features of resources, and dynamics of environments.
Meanwhile, smart systems such as multi-agent systems have demonstrated
advantages and great potentials in modelling and simulating complex systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1612.08958v1,Efficient quantum walk on the grid with multiple marked elements,"We give a quantum algorithm for finding a marked element on the grid when
there are multiple marked elements. Our algorithm uses quadratically fewer
steps than a random walk on the grid, ignoring logarithmic factors. This is the
first known quantum walk that finds a marked element in a number of steps less
than the square-root of the extended hitting time. We also give a new tighter
upper bound on the extended hitting time of a marked subset, expressed in terms
of the hitting times of its members.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1011.1842v2,Orbits of linear maps and regular languages,"We settle the equivalence between the problem of hitting a polyhedral set by
the orbit of a linear map and the intersection of a regular language and a
language of permutations of binary words (the permutation filter realizability
problem). The decidability of the both problems is presently unknown and the
first one is a straightforward generalization of the famous Skolem problem and
the nonnegativity problem in the theory of linear recurrent sequences. To show
a `borderline' status of the permutation filter realizability problem with
respect to computability we present some decidable and undecidable problems
closely related to it.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.06142v1,"Reducing Data Complexity using Autoencoders with Class-informed Loss
  Functions","Available data in machine learning applications is becoming increasingly
complex, due to higher dimensionality and difficult classes. There exists a
wide variety of approaches to measuring complexity of labeled data, according
to class overlap, separability or boundary shapes, as well as group morphology.
Many techniques can transform the data in order to find better features, but
few focus on specifically reducing data complexity. Most data transformation
methods mainly treat the dimensionality aspect, leaving aside the available
information within class labels which can be useful when classes are somehow
complex.
  This paper proposes an autoencoder-based approach to complexity reduction,
using class labels in order to inform the loss function about the adequacy of
the generated variables. This leads to three different new feature learners,
Scorer, Skaler and Slicer. They are based on Fisher's discriminant ratio, the
Kullback-Leibler divergence and least-squares support vector machines,
respectively. They can be applied as a preprocessing stage for a binary
classification problem. A thorough experimentation across a collection of 27
datasets and a range of complexity and classification metrics shows that
class-informed autoencoders perform better than 4 other popular unsupervised
feature extraction techniques, especially when the final objective is using the
data for a classification task.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.03273v2,Tight Cutoffs for Guarded Protocols with Fairness,"Guarded protocols were introduced in a seminal paper by Emerson and Kahlon
(2000), and describe systems of processes whose transitions are enabled or
disabled depending on the existence of other processes in certain local states.
We study parameterized model checking and synthesis of guarded protocols, both
aiming at formal correctness arguments for systems with any number of
processes. Cutoff results reduce reasoning about systems with an arbitrary
number of processes to systems of a determined, fixed size. Our work stems from
the observation that existing cutoff results for guarded protocols i) are
restricted to closed systems, and ii) are of limited use for liveness
properties because reductions do not preserve fairness. We close these gaps and
obtain new cutoff results for open systems with liveness properties under
fairness assumptions. Furthermore, we obtain cutoffs for the detection of
global and local deadlocks, which are of paramount importance in synthesis.
Finally, we prove tightness or asymptotic tightness for the new cutoffs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1112.5370v1,"Enhancing Support for Knowledge Works: A relatively unexplored vista of
  computing research","Let us envision a new class of IT systems, the ""Support Systems for Knowledge
Works"" or SSKW. An SSKW can be defined as a system built for providing
comprehensive support to human knowledge-workers while performing instances of
complex knowledge-works of a particular type within a particular domain of
professional activities To get an idea what an SSKW-enabled work environment
can be like, let us look into a hypothetical scenario that depicts the
interaction between a physician and a patient-care SSKW during the activity of
diagnosing a patient.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0109020v1,"Modelling Semantic Association and Conceptual Inheritance for Semantic
  Analysis","Allowing users to interact through language borders is an interesting
challenge for information technology. For the purpose of a computer assisted
language learning system, we have chosen icons for representing meaning on the
input interface, since icons do not depend on a particular language. However, a
key limitation of this type of communication is the expression of articulated
ideas instead of isolated concepts. We propose a method to interpret sequences
of icons as complex messages by reconstructing the relations between concepts,
so as to build conceptual graphs able to represent meaning and to be used for
natural language sentence generation. This method is based on an electronic
dictionary containing semantic information.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0804.0797v1,Sarbanes-Oxley: What About all the Spreadsheets?,"The Sarbanes-Oxley Act of 2002 has finally forced corporations to examine the
validity of their spreadsheets. They are beginning to understand the
spreadsheet error literature, including what it tells them about the need for
comprehensive spreadsheet testing. However, controlling for fraud will require
a completely new set of capabilities, and a great deal of new research will be
needed to develop fraud control capabilities. This paper discusses the
riskiness of spreadsheets, which can now be quantified to a considerable
degree. It then discusses how to use control frameworks to reduce the dangers
created by spreadsheets. It focuses especially on testing, which appears to be
the most crucial element in spreadsheet controls.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,1
http://arxiv.org/abs/2012.14059v1,"Convolutional Neural Networks in Multi-Class Classification of Medical
  Data","We report applications of Convolutional Neural Networks (CNN) to
multi-classification classification of a large medical data set. We discuss in
detail how changes in the CNN model and the data pre-processing impact the
classification results. In the end, we introduce an ensemble model that
consists of both deep learning (CNN) and shallow learning models (Gradient
Boosting). The method achieves Accuracy of 64.93, the highest three-class
classification accuracy we achieved in this study. Our results also show that
CNN and the ensemble consistently obtain a higher Recall than Precision. The
highest Recall is 68.87, whereas the highest Precision is 65.04.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1406.2062v1,"Categorical Semantics for Functional Reactive Programming with Temporal
  Recursion and Corecursion","Functional reactive programming (FRP) makes it possible to express temporal
aspects of computations in a declarative way. Recently we developed two kinds
of categorical models of FRP: abstract process categories (APCs) and concrete
process categories (CPCs). Furthermore we showed that APCs generalize CPCs. In
this paper, we extend APCs with additional structure. This structure models
recursion and corecursion operators that are related to time. We show that the
resulting categorical models generalize those CPCs that impose an additional
constraint on time scales. This constraint boils down to ruling out
$\omega$-supertasks, which are closely related to Zeno's paradox of Achilles
and the tortoise.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.02407v2,Debiasing classifiers: is reality at variance with expectation?,"We present an empirical study of debiasing methods for classifiers, showing
that debiasers often fail in practice to generalize out-of-sample, and can in
fact make fairness worse rather than better. A rigorous evaluation of the
debiasing treatment effect requires extensive cross-validation beyond what is
usually done. We demonstrate that this phenomenon can be explained as a
consequence of bias-variance trade-off, with an increase in variance
necessitated by imposing a fairness constraint. Follow-up experiments validate
the theoretical prediction that the estimation variance depends strongly on the
base rates of the protected class. Considering fairness--performance trade-offs
justifies the counterintuitive notion that partial debiasing can actually yield
better results in practice on out-of-sample data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0207075v1,"Nonmonotonic Probabilistic Logics between Model-Theoretic Probabilistic
  Logic and Probabilistic Logic under Coherence","Recently, it has been shown that probabilistic entailment under coherence is
weaker than model-theoretic probabilistic entailment. Moreover, probabilistic
entailment under coherence is a generalization of default entailment in System
P. In this paper, we continue this line of research by presenting probabilistic
generalizations of more sophisticated notions of classical default entailment
that lie between model-theoretic probabilistic entailment and probabilistic
entailment under coherence. That is, the new formalisms properly generalize
their counterparts in classical default reasoning, they are weaker than
model-theoretic probabilistic entailment, and they are stronger than
probabilistic entailment under coherence. The new formalisms are useful
especially for handling probabilistic inconsistencies related to conditioning
on zero events. They can also be applied for probabilistic belief revision.
More generally, in the same spirit as a similar previous paper, this paper
sheds light on exciting new formalisms for probabilistic reasoning beyond the
well-known standard ones.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.08038v1,Towards a Benchmark Set for Program Repair Based on Partial Fixes,"Software bugs significantly contribute to software cost and increase the risk
of system malfunctioning. In recent years, many automated program-repair
approaches have been proposed to automatically fix undesired program behavior.
Despite of their great success, specific problems such as fixing bugs with
partial fixes still remain unresolved. A partial fix to a known software issue
is a programmer's failed attempt to fix the issue the first time. Even though
it fails, this fix attempt still conveys important information such as the
suspicious software region and the bug type. In this work we do not propose an
approach for program repair with partial fixes, but instead answer a
preliminary question: Do partial fixes occur often enough, in general, to be
relevant for the research area of automated program repair? We crawled 1500
open-source C repositories on GitHub for partial fixes. The result is a
benchmark set of 2204 benchmark tasks for automated program repair based on
partial fixes. The benchmark set is available open source and open to further
contributions and improvement.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.02094v4,Superposition for Lambda-Free Higher-Order Logic,"We introduce refutationally complete superposition calculi for intentional
and extensional clausal $\lambda$-free higher-order logic, two formalisms that
allow partial application and applied variables. The calculi are parameterized
by a term order that need not be fully monotonic, making it possible to employ
the $\lambda$-free higher-order lexicographic path and Knuth-Bendix orders. We
implemented the calculi in the Zipperposition prover and evaluated them on
Isabelle/HOL and TPTP benchmarks. They appear promising as a stepping stone
towards complete, highly efficient automatic theorem provers for full
higher-order logic.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0204020v1,"Seven Dimensions of Portability for Language Documentation and
  Description","The process of documenting and describing the world's languages is undergoing
radical transformation with the rapid uptake of new digital technologies for
capture, storage, annotation and dissemination. However, uncritical adoption of
new tools and technologies is leading to resources that are difficult to reuse
and which are less portable than the conventional printed resources they
replace. We begin by reviewing current uses of software tools and digital
technologies for language documentation and description. This sheds light on
how digital language documentation and description are created and managed,
leading to an analysis of seven portability problems under the following
headings: content, format, discovery, access, citation, preservation and
rights. After characterizing each problem we provide a series of value
statements, and this provides the framework for a broad range of best practice
recommendations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1610.04421v1,"ZeroSDN: A Message Bus for Flexible and Light-weight Network Control
  Distribution in SDN","Recent years have seen an evolution of SDN control plane architectures,
starting from simple monolithic controllers, over modular monolithic
controllers, to distributed controllers. We observe, however, that today's
distributed controllers still exhibit inflexibility with respect to the
distribution of control logic. Therefore, we propose a novel architecture of a
distributed SDN controller in this paper, providing maximum flexibility with
respect to distribution.
  Our architecture splits control logic into light-weight control modules,
called controllets, based on a micro-kernel approach, reducing common
controllet functionality to a bare minimum and factoring out all higher-level
functionality. Light-weight controllets also allow for pushing control logic
onto switches to minimize latency and communication overhead. Controllets are
interconnected through a message bus supporting the publish/subscribe
communication paradigm with specific extensions for content-based OpenFlow
message filtering. Publish/subscribe allows for complete decoupling of
controllets to further facilitate control plane distribution.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.08392v1,Decomposing Probabilistic Lambda-calculi,"A notion of probabilistic lambda-calculus usually comes with a prescribed
reduction strategy, typically call-by-name or call-by-value, as the calculus is
non-confluent and these strategies yield different results. This is a break
with one of the main advantages of lambda-calculus: confluence, which means
results are independent from the choice of strategy. We present a probabilistic
lambda-calculus where the probabilistic operator is decomposed into two
syntactic constructs: a generator, which represents a probabilistic event; and
a consumer, which acts on the term depending on a given event. The resulting
calculus, the Probabilistic Event Lambda-Calculus, is confluent, and interprets
the call-by-name and call-by-value strategies through different interpretations
of the probabilistic operator into our generator and consumer constructs. We
present two notions of reduction, one via fine-grained local rewrite steps, and
one by generation and consumption of probabilistic events. Simple types for the
calculus are essentially standard, and they convey strong normalization. We
demonstrate how we can encode call-by-name and call-by-value probabilistic
evaluation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.12481v1,GymFG: A Framework with a Gym Interface for FlightGear,"Over the past decades, progress in deployable autonomous flight systems has
slowly stagnated. This is reflected in today's production air-crafts, where
pilots only enable simple physics-based systems such as autopilot for takeoff,
landing, navigation, and terrain/traffic avoidance. Evidently, autonomy has not
gained the trust of the community where higher problem complexity and cognitive
workload are required. To address trust, we must revisit the process for
developing autonomous capabilities: modeling and simulation. Given the
prohibitive costs for live tests, we need to prototype and evaluate autonomous
aerial agents in a high fidelity flight simulator with autonomous learning
capabilities applicable to flight systems: such a open-source development
platform is not available. As a result, we have developed GymFG: GymFG couples
and extends a high fidelity, open-source flight simulator and a robust agent
learning framework to facilitate learning of more complex tasks. Furthermore,
we have demonstrated the use of GymFG to train an autonomous aerial agent using
Imitation Learning. With GymFG, we can now deploy innovative ideas to address
complex problems and build the trust necessary to move prototypes to the
real-world.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0601013v1,Forward slicing of functional logic programs by partial evaluation,"Program slicing has been mainly studied in the context of imperative
languages, where it has been applied to a wide variety of software engineering
tasks, like program understanding, maintenance, debugging, testing, code reuse,
etc. This work introduces the first forward slicing technique for declarative
multi-paradigm programs which integrate features from functional and logic
programming. Basically, given a program and a slicing criterion (a function
call in our setting), the computed forward slice contains those parts of the
original program which are reachable from the slicing criterion. Our approach
to program slicing is based on an extension of (online) partial evaluation.
Therefore, it provides a simple way to develop program slicing tools from
existing partial evaluators and helps to clarify the relation between both
methodologies. A slicing tool for the multi-paradigm language Curry, which
demonstrates the usefulness of our approach, has been implemented in Curry
itself.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0708.4149v2,On the complexity of nonnegative matrix factorization,"Nonnegative matrix factorization (NMF) has become a prominent technique for
the analysis of image databases, text databases and other information retrieval
and clustering applications. In this report, we define an exact version of NMF.
Then we establish several results about exact NMF: (1) that it is equivalent to
a problem in polyhedral combinatorics; (2) that it is NP-hard; and (3) that a
polynomial-time local search heuristic exists.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1807.08460v1,Models of using cloud technologies at the IT professionals training,"The article is devoted to the rationale of the use of cloud technologies in
teaching mathematical informatics students of technical universities. Purpose
of the article - the analysis of domestic and foreign experience in the use of
cloud-oriented ICT in the training of future professionals in the field of
information technology. Based on a review of experiences and comparisons tools
of distance learning technologies and cloud technologies identified the
advantages of using cloud technologies for different categories of the learning
process participants and models of cloud services, which should be used in
training the regulatory academic disciplines cycles of mathematical, scientific
and vocational & practical training of the future IT professionals.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1609.07827v1,"Semantic Information Measure with Two Types of Probability for
  Falsification and Confirmation","Logical Probability (LP) is strictly distinguished from Statistical
Probability (SP). To measure semantic information or confirm hypotheses, we
need to use sampling distribution (conditional SP function) to test or confirm
fuzzy truth function (conditional LP function). The Semantic Information
Measure (SIM) proposed is compatible with Shannon's information theory and
Fisher's likelihood method. It can ensure that the less the LP of a predicate
is and the larger the true value of the proposition is, the more information
there is. So the SIM can be used as Popper's information criterion for
falsification or test. The SIM also allows us to optimize the true-value of
counterexamples or degrees of disbelief in a hypothesis to get the optimized
degree of belief, i. e. Degree of Confirmation (DOC). To explain confirmation,
this paper 1) provides the calculation method of the DOC of universal
hypotheses; 2) discusses how to resolve Raven Paradox with new DOC and its
increment; 3) derives the DOC of rapid HIV tests: DOC of
test-positive=1-(1-specificity)/sensitivity, which is similar to Likelihood
Ratio (=sensitivity/(1-specificity)) but has the upper limit 1; 4) discusses
negative DOC for excessive affirmations, wrong hypotheses, or lies; and 5)
discusses the DOC of general hypotheses with GPS as example.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0410060v1,"Semantic filtering by inference on domain knowledge in spoken dialogue
  systems","General natural dialogue processing requires large amounts of domain
knowledge as well as linguistic knowledge in order to ensure acceptable
coverage and understanding. There are several ways of integrating lexical
resources (e.g. dictionaries, thesauri) and knowledge bases or ontologies at
different levels of dialogue processing. We concentrate in this paper on how to
exploit domain knowledge for filtering interpretation hypotheses generated by a
robust semantic parser. We use domain knowledge to semantically constrain the
hypothesis space. Moreover, adding an inference mechanism allows us to complete
the interpretation when information is not explicitly available. Further, we
discuss briefly how this can be generalized towards a predictive natural
interactive system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.08768v3,Adapting JPEG XS gains and priorities to tasks and contents,"Most current research in the domain of image compression focuses solely on
achieving state of the art compression ratio, but that is not always usable in
today's workflow due to the constraints on computing resources.
  Constant market requirements for a low-complexity image codec have led to the
recent development and standardization of a lightweight image codec named JPEG
XS.
  In this work we show that JPEG XS compression can be adapted to a specific
given task and content, such as preserving visual quality on desktop content or
maintaining high accuracy in neural network segmentation tasks, by optimizing
its gain and priority parameters using the covariance matrix adaptation
evolution strategy.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.00165v2,Coordinated Multi-Robot Trajectory Tracking over Sampled Communication,"In this paper, we propose an inverse-kinematics controller for a class of
multi-robot systems in the scenario of sampled communication. The goal is to
make a group of robots perform trajectory tracking {in a coordinated way} when
the sampling time of communications is non-negligible, disrupting the
theoretical convergence guarantees of standard control designs. Given a
feasible desired trajectory in the configuration space, the proposed controller
receives measurements from the system at sampled time instants and computes
velocity references for the robots, which are tracked by a low-level
controller. We propose a jointly designed feedback plus feedforward controller
with provable stability and error convergence guarantees, and further show that
the obtained controller is amenable of decentralized implementation. We test
the proposed control strategy via numerical simulations in the scenario of
cooperative aerial manipulation of a cable-suspended load using a realistic
simulator (Fly-Crane). Finally, we compare our proposed decentralized
controller with centralized approaches that adapt the feedback gain online
through smart heuristics, and show that it achieves comparable performance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1711.01972v2,Constant-Factor Approximation for Ordered k-Median,"We study the Ordered k-Median problem, in which the solution is evaluated by
first sorting the client connection costs and then multiplying them with a
predefined non-increasing weight vector (higher connection costs are taken with
larger weights). Since the 1990s, this problem has been studied extensively in
the discrete optimization and operations research communities and has emerged
as a framework unifying many fundamental clustering and location problems such
as k-Median and k-Center. This generality, however, renders the problem
intriguing from the algorithmic perspective and obtaining non-trivial
approximation algorithms was an open problem even for simple topologies such as
trees. Recently, Aouad and Segev were able to obtain an O(log n) approximation
algorithm for Ordered k-Median using a sophisticated local-search approach and
the concept of surrogate models thereby extending the result by Tamir (2001)
for the case of a rectangular weight vector, also known as k-Facility
p-Centrum.
  In this paper, we provide an LP-rounding constant-factor approximation
algorithm for the Ordered k-Median problem.
  We first provide a new analysis of the rounding process by Charikar and Li
(2012) for k-Median, when applied to a fractional solution obtained from
solving an LP relaxation over a non-metric, truncated cost vector, resulting in
an elegant 15-approximation for the rectangular case. Then, we show that a
simple weight bucketing can be applied to the general case resulting in O(log
n) rectangles and hence in a constant-factor approximation in quasi-polynomial
time. Finally, we show that also the clever distance bucketing by Aouad and
Segev can be combined with the objective-oblivious version of our LP-rounding
for the rectangular case, and that it results in a true, polynomial time,
constant-factor approximation algorithm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1801.09664v1,"Design and Analysis of 5G Scenarios with 'simmer': An R Package for Fast
  DES Prototyping","Simulation frameworks are important tools for the analysis and design of
communication networks and protocols, but they can result extremely costly
and/or complex (for the case of very specialized tools), or too naive and
lacking proper features and support (for the case of ad-hoc tools). In this
paper, we present an analysis of three 5G scenarios using 'simmer', a recent R
package for discrete-event simulation that sits between the above two
paradigms. As our results show, it provides a simple yet very powerful syntax,
supporting the efficient simulation of relatively complex scenarios at a low
implementation cost.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0311002v1,Computing Convex Hulls with a Linear Solver,"A programming tactic involving polyhedra is reported that has been widely
applied in the polyhedral analysis of (constraint) logic programs. The method
enables the computations of convex hulls that are required for polyhedral
analysis to be coded with linear constraint solving machinery that is available
in many Prolog systems.
  To appear in Theory and Practice of Logic Programming (TPLP)",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1407.5166v1,The Expressive Power of Epistemic $Œº$-Calculus,"While the $\mu$-calculus notoriously subsumes Alternating-time Temporal Logic
(ATL), we show that the epistemic $\mu$-calculus does not subsume ATL with
imperfect information (ATL$_i$) for the synchronous perfect-recall semantics.
To prove this we first establish that jumping parity tree automata (JTA), a
recently introduced extension of alternating parity tree automata, are
expressively equivalent to the epistemic $\mu$-calculus, and this for any
knowledge semantics. Using this result we also show that, for bounded-memory
semantics, the epistemic $\mu$-calculus is not more expressive than the
standard $\mu$-calculus, and that its satisfiability problem is
EXPTIME-complete.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1911.10871v1,Breaking the Barrier of 2 for the Storage Allocation Problem,"Packing problems are an important class of optimization problems. The
probably most well-known problem if this type is knapsack and many
generalizations of it have been studied in the literature like Two-dimensional
Geometric Knapsack (2DKP) and Unsplittable Flow on a Path (UFP). For the latter
two problems, recently the first polynomial time approximation algorithms with
better approximation ratios than 2 were presented [G\'alvez et al., FOCS
2017][Grandoni et al., STOC 2018].
  In this paper we break the barrier of 2 for the Storage Allocation Problem
(SAP) which is a natural intermediate problem between 2DKP and UFP. We are
given a path with capacitated edges and a set of tasks where each task has a
start vertex, an end vertex, a size, and a profit. We seek to select the most
profitable set of tasks that we can draw as non-overlapping rectangles
underneath the capacity profile of the edges where the height of each rectangle
equals the size of the corresponding task.
  This problem is motivated by settings of allocation resources like memory,
bandwidths, etc. where each request needs a contiguous portion of the resource.
The best known polynomial time approximation algorithm for SAP has an
approximation ratio of 2+epsilon$ [M\""omke and Wiese, ICALP 2015] and no better
quasi-polynomial time algorithm is known. We present a polynomial time (63/32)
< 1.969-approximation algorithm for the case of uniform edge capacities and a
quasi-polynomial time (1.997)-approximation algorithm for non-uniform
quasi-polynomially bounded edge capacities. Finally, we show that under slight
resource augmentation we can obtain approximation ratios of 3/2 + epsilon in
polynomial time and 1 + epsilon in quasi-polynomial time, both for arbitrary
edge capacities.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1811.03275v1,Quantum Semantic Correlations in Hate and Non-Hate Speeches,"This paper aims to apply the notions of quantum geometry and correlation to
the typification of semantic relations between couples of keywords in different
documents. In particular we analysed texts classified as hate / non hate
speeches, containing the keywords ""women"", ""white"", and ""black"". The paper
compares this approach to cosine similarity, a classical methodology, to cast
light on the notion of ""similar meaning"".",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1806.11527v4,"Representative families for matroid intersections, with applications to
  location, packing, and covering problems","We show algorithms for computing representative families for matroid
intersections and use them in fixed-parameter algorithms for set packing, set
covering, and facility location problems with multiple matroid constraints. We
complement our tractability results by hardness results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.6755v1,"An Improved Approximation Algorithm for the Traveling Salesman Problem
  with Relaxed Triangle Inequality","Given a complete edge-weighted graph G, we present a polynomial time
algorithm to compute a degree-four-bounded spanning Eulerian subgraph of 2G
that has at most 1.5 times the weight of an optimal TSP solution of G. Based on
this algorithm and a novel use of orientations in graphs, we obtain a (3 beta/4
+ 3 beta^2/4)-approximation algorithm for TSP with beta-relaxed triangle
inequality (beta-TSP), where beta >= 1. A graph G is an instance of beta-TSP,
if it is a complete graph with non-negative edge weights that are restricted as
follows. For each triple of vertices u,v,w in V(G), c({u,v}) <= beta (c({u,w})
+ c({w,v})).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1406.6453v1,A Quantitative Neural Coding Model of Sensory Memory,"The coding mechanism of sensory memory on the neuron scale is one of the most
important questions in neuroscience. We have put forward a quantitative neural
network model, which is self organized, self similar, and self adaptive, just
like an ecosystem following Darwin theory. According to this model, neural
coding is a mult to one mapping from objects to neurons. And the whole cerebrum
is a real-time statistical Turing Machine, with powerful representing and
learning ability. This model can reconcile some important disputations, such
as: temporal coding versus rate based coding, grandmother cell versus
population coding, and decay theory versus interference theory. And it has also
provided explanations for some key questions such as memory consolidation,
episodic memory, consciousness, and sentiment. Philosophical significance is
indicated at last.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.08937v4,Institutional Grammar 2.0 Codebook,"The Grammar of Institutions, or Institutional Grammar, is an established
approach to encode policy information in terms of institutional statements
based on a set of pre-defined syntactic components. This codebook provides
coding guidelines for a revised version of the Institutional Grammar, the
Institutional Grammar 2.0 (IG 2.0). IG 2.0 is a specification that aims at
facilitating the encoding of policy to meet varying analytical objectives. To
this end, it revises the grammar with respect to comprehensiveness,
flexibility, and specificity by offering multiple levels of expressiveness (IG
Core, IG Extended, IG Logico). In addition to the encoding of regulative
statements, it further introduces the encoding of constitutive institutional
statements, as well as statements that exhibit both constitutive and regulative
characteristics. Introducing those aspects, the codebook initially covers
fundamental concepts of IG 2.0, before providing an overview of pre-coding
steps relevant for document preparation. Detailed coding guidelines are
provided for both regulative and constitutive statements across all levels of
expressiveness, along with the encoding guidelines for statements of mixed form
-- hybrid and polymorphic institutional statements. The document further
provides an overview of taxonomies used in the encoding process and referred to
throughout the codebook. The codebook concludes with a summary and discussion
of relevant considerations to facilitate the coding process. An initial
Reader's Guide helps the reader tailor the content to her interest.
  Note that this codebook specifically focuses on operational aspects of IG 2.0
in the context of policy coding. Links to additional resources such as the
underlying scientific literature (that offers a comprehensive treatment of the
underlying theoretical concepts) are referred to in the DOI and the concluding
section of the codebook.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1308.1464v1,"ManyClaw: Slicing and dicing Riemann solvers for next generation highly
  parallel architectures","Next generation computer architectures will include order of magnitude more
intra-node parallelism; however, many application programmers have a difficult
time keeping their codes current with the state-of-the-art machines. In this
context, we analyze Hyperbolic PDE solvers, which are used in the solution of
many important applications in science and engineering. We present ManyClaw, a
project intended to explore the exploitation of intra-node parallelism in
hyperbolic PDE solvers via the Clawpack software package for solving hyperbolic
PDEs. Our goal is to separate the low level parallelism and the physical
equations thus providing users the capability to leverage intra-node
parallelism without explicitly writing code to take advantage of newer
architectures.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0702032v1,Finding large and small dense subgraphs,"We consider two optimization problems related to finding dense subgraphs. The
densest at-least-k-subgraph problem (DalkS) is to find an induced subgraph of
highest average degree among all subgraphs with at least k vertices, and the
densest at-most-k-subgraph problem (DamkS) is defined similarly. These problems
are related to the well-known densest k-subgraph problem (DkS), which is to
find the densest subgraph on exactly k vertices. We show that DalkS can be
approximated efficiently, while DamkS is nearly as hard to approximate as the
densest k-subgraph problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9904002v2,A geometric framework for modelling similarity search,"The aim of this paper is to propose a geometric framework for modelling
similarity search in large and multidimensional data spaces of general nature,
which seems to be flexible enough to address such issues as analysis of
complexity, indexability, and the `curse of dimensionality.' Such a framework
is provided by the concept of the so-called similarity workload, which is a
probability metric space $\Omega$ (query domain) with a distinguished finite
subspace $X$ (dataset), together with an assembly of concepts, techniques, and
results from metric geometry. They include such notions as metric transform,
$\e$-entropy, and the phenomenon of concentration of measure on
high-dimensional structures. In particular, we discuss the relevance of the
latter to understanding the curse of dimensionality. As some of those concepts
and techniques are being currently reinvented by the database community, it
seems desirable to try and bridge the gap between database research and the
relevant work already done in geometry and analysis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9904019v2,Bounds for Small-Error and Zero-Error Quantum Algorithms,"We present a number of results related to quantum algorithms with small error
probability and quantum algorithms that are zero-error. First, we give a tight
analysis of the trade-offs between the number of queries of quantum search
algorithms, their error probability, the size of the search space, and the
number of solutions in this space. Using this, we deduce new lower and upper
bounds for quantum versions of amplification problems. Next, we establish
nearly optimal quantum-classical separations for the query complexity of
monotone functions in the zero-error model (where our quantum zero-error model
is defined so as to be robust when the quantum gates are noisy). Also, we
present a communication complexity problem related to a total function for
which there is a quantum-classical communication complexity gap in the
zero-error model. Finally, we prove separations for monotone graph properties
in the zero-error and other error models which imply that the evasiveness
conjecture for such properties does not hold for quantum computers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1706.05750v3,Parallel-In-Time Simulation of Eddy Current Problems Using Parareal,"In this contribution the usage of the Parareal method is proposed for the
time-parallel solution of the eddy current problem. The method is adapted to
the particular challenges of the problem that are related to the differential
algebraic character due to non-conducting regions. It is shown how the
necessary modification can be automatically incorporated by using a suitable
time stepping method. The paper closes with a first demonstration of a
simulation of a realistic four-pole induction machine model using Parareal.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1012.0467v1,MT4j - A Cross-platform Multi-touch Development Framework,"This article describes requirements and challenges of crossplatform
multi-touch software engineering, and presents the open source framework
Multi-Touch for Java (MT4j) as a solution. MT4j is designed for rapid
development of graphically rich applications on a variety of contemporary
hardware, from common PCs and notebooks to large-scale ambient displays, as
well as different operating systems. The framework has a special focus on
making multi-touch software development easier and more efficient. Architecture
and abstractions used by MT4j are described, and implementations of several
common use cases are presented.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1506.05632v1,An Open Science Platform for the Next Generation of Data,"Imagine an online work environment where researchers have direct and
immediate access to myriad data sources and tools and data management
resources, useful throughout the research lifecycle. This is our vision for the
next generation of the Dataverse Network: an Open Science Platform (OSP). For
the first time, researchers would be able to seamlessly access and create
primary and derived data from a variety of sources: prior research results,
public data sets, harvested online data, physical instruments, private data
collections, and even data from other standalone repositories. Researchers
could recruit research participants and conduct research directly on the OSP,
if desired, using readily available tools. Researchers could create private or
shared workspaces to house data, access tools, and computation and could
publish data directly on the platform or publish elsewhere with persistent,
data citations on the OSP. This manuscript describes the details of an Open
Science Platform and its construction. Having an Open Science Platform will
especially impact the rate of new scientific discoveries and make scientific
findings more credible and accountable.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9810012v4,Ultrametric Distance in Syntax,"Phrase structure trees have a hierarchical structure. In many subjects, most
notably in Taxonomy such tree structures have been studied using ultrametrics.
Here syntactical hierarchical phrase trees are subject to a similar analysis,
which is much siompler as the branching structure is more readily discernible
and switched. The occurence of hierarchical structure elsewhere in linguistics
is mentioned. The phrase tree can be represented by a matrix and the elements
of the matrix can be represented by triangles. The height at which branching
occurs is not prescribed in previous syntatic models, but it is by using the
ultrametric matrix. The ambiguity of which branching height to choose is
resolved by postulating that branching occurs at the lowest height available.
An ultrametric produces a measure of the complexity of sentences: presumably
the complexity of sentence increases as a language is aquired so that this can
be tested. A All ultrametric triangles are equilateral or isocles, here it is
shown that X structur implies that there are no equilateral triangles.
Restricting attention to simple syntax a minium ultrametric distance between
lexical categories is calculatex. This ultrametric distance is shown to be
different than the matrix obtasined from feaures. It is shown that the
definition of c-commabnd can be replaced by an equivalent ultrametric
definition. The new definition invokes a minimum distance between nodes and
this is more aesthetically satisfing than previouv varieties of definitions.
  From the new definition of c-command follows a new definition of government.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0810.3136v3,"On the Complexity of Core, Kernel, and Bargaining Set","Coalitional games are mathematical models suited to analyze scenarios where
players can collaborate by forming coalitions in order to obtain higher worths
than by acting in isolation. A fundamental problem for coalitional games is to
single out the most desirable outcomes in terms of appropriate notions of worth
distributions, which are usually called solution concepts. Motivated by the
fact that decisions taken by realistic players cannot involve unbounded
resources, recent computer science literature reconsidered the definition of
such concepts by advocating the relevance of assessing the amount of resources
needed for their computation in terms of their computational complexity. By
following this avenue of research, the paper provides a complete picture of the
complexity issues arising with three prominent solution concepts for
coalitional games with transferable utility, namely, the core, the kernel, and
the bargaining set, whenever the game worth-function is represented in some
reasonable compact form (otherwise, if the worths of all coalitions are
explicitly listed, the input sizes are so large that complexity problems
are---artificially---trivial). The starting investigation point is the setting
of graph games, about which various open questions were stated in the
literature. The paper gives an answer to these questions, and in addition
provides new insights on the setting, by characterizing the computational
complexity of the three concepts in some relevant generalizations and
specializations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0003046v1,Linear Tabulated Resolution Based on Prolog Control Strategy,"Infinite loops and redundant computations are long recognized open problems
in Prolog. Two ways have been explored to resolve these problems: loop checking
and tabling. Loop checking can cut infinite loops, but it cannot be both sound
and complete even for function-free logic programs. Tabling seems to be an
effective way to resolve infinite loops and redundant computations. However,
existing tabulated resolutions, such as OLDT-resolution, SLG- resolution, and
Tabulated SLS-resolution, are non-linear because they rely on the
solution-lookup mode in formulating tabling. The principal disadvantage of
non-linear resolutions is that they cannot be implemented using a simple
stack-based memory structure like that in Prolog. Moreover, some strictly
sequential operators such as cuts may not be handled as easily as in Prolog.
  In this paper, we propose a hybrid method to resolve infinite loops and
redundant computations. We combine the ideas of loop checking and tabling to
establish a linear tabulated resolution called TP-resolution. TP-resolution has
two distinctive features: (1) It makes linear tabulated derivations in the same
way as Prolog except that infinite loops are broken and redundant computations
are reduced. It handles cuts as effectively as Prolog. (2) It is sound and
complete for positive logic programs with the bounded-term-size property. The
underlying algorithm can be implemented by an extension to any existing Prolog
abstract machines such as WAM or ATOAM.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.00754v1,P = FS: Parallel is Just Fast Serial,"We prove that parallel processing with homogeneous processors is logically
equivalent to fast serial processing. The reverse proposition can also be used
to identify obscure opportunities for applying parallelism. To our knowledge,
this theorem has not been previously reported in the queueing theory
literature. A plausible explanation is offered for why this might be. The basic
homogeneous theorem is also extended to optimizing the latency of heterogenous
parallel arrays.",0,0,0,0,0,0,0,0,1,0,0,1,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.02650v1,"Model-data-driven constitutive responses: application to a multiscale
  computational framework","Computational multiscale methods for analyzing and deriving constitutive
responses have been used as a tool in engineering problems because of their
ability to combine information at different length scales. However, their
application in a nonlinear framework can be limited by high computational
costs, numerical difficulties, and/or inaccuracies. In this paper, a hybrid
methodology is presented which combines classical constitutive laws
(model-based), a data-driven correction component, and computational multiscale
approaches. A model-based material representation is locally improved with data
from lower scales obtained by means of a nonlinear numerical homogenization
procedure leading to a model-data-driven approach. Therefore, macroscale
simulations explicitly incorporate the true microscale response, maintaining
the same level of accuracy that would be obtained with online micro-macro
simulations but with a computational cost comparable to classical model-driven
approaches. In the proposed approach, both model and data play a fundamental
role allowing for the synergistic integration between a physics-based response
and a machine learning black-box. Numerical applications are implemented in two
dimensions for different tests investigating both material and structural
responses in large deformation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0708.1529v1,Resolution over Linear Equations and Multilinear Proofs,"We develop and study the complexity of propositional proof systems of varying
strength extending resolution by allowing it to operate with disjunctions of
linear equations instead of clauses. We demonstrate polynomial-size refutations
for hard tautologies like the pigeonhole principle, Tseitin graph tautologies
and the clique-coloring tautologies in these proof systems. Using the
(monotone) interpolation by a communication game technique we establish an
exponential-size lower bound on refutations in a certain, considerably strong,
fragment of resolution over linear equations, as well as a general polynomial
upper bound on (non-monotone) interpolants in this fragment.
  We then apply these results to extend and improve previous results on
multilinear proofs (over fields of characteristic 0), as studied in
[RazTzameret06]. Specifically, we show the following:
  1. Proofs operating with depth-3 multilinear formulas polynomially simulate a
certain, considerably strong, fragment of resolution over linear equations.
  2. Proofs operating with depth-3 multilinear formulas admit polynomial-size
refutations of the pigeonhole principle and Tseitin graph tautologies. The
former improve over a previous result that established small multilinear proofs
only for the \emph{functional} pigeonhole principle. The latter are different
than previous proofs, and apply to multilinear proofs of Tseitin mod p graph
tautologies over any field of characteristic 0.
  We conclude by connecting resolution over linear equations with extensions of
the cutting planes proof system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.05220v1,On the undecidability of the Panopticon detection problem,"The Panopticon (which means ""watcher of everything"") is a well-known
structure of continuous surveillance and discipline proposed by Bentham in
1785. This device was, later, used by Foucault and other philosophers as a
paradigm and metaphor for the study of constitutional power and knowledge as
well as a model of individuals' deprivation of freedom. Nowadays, technological
achievements have given rise to new, non-physical (unlike prisons), means of
constant surveillance that transcend physical boundaries. This, combined with
the confession of some governmental institutions that they actually collaborate
with these Internet giants to collect or deduce information about people,
creates a worrisome situation of several co-existing Panopticons that can act
separately or in close collaboration. Thus, they can only be detected and
identified through the expense of (perhaps considerable) effort. In this paper
we provide a theoretical framework for studying the detectability status of
Panopticons that fall under two theoretical, but not unrealistic, definitions.
We show, using Oracle Turing Machines, that detecting modern day, ICT-based,
Panopticons is an undecidable problem. Furthermore, we show that for each
sufficiently expressive formal system, we can effectively construct a Turing
Machine for which it is impossible to prove, within the formal system, either
that it is a Panopticon or it is not a Panopticon.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1901.00825v1,XOS: An Application-Defined Operating System for Data Center Servers,"Rapid growth of datacenter (DC) scale, urgency of cost control, increasing
workload diversity, and huge software investment protection place unprecedented
demands on the operating system (OS) efficiency, scalability, performance
isolation, and backward-compatibility. The traditional OSes are not built to
work with deep-hierarchy software stacks, large numbers of cores, tail latency
guarantee, and increasingly rich variety of applications seen in modern DCs,
and thus they struggle to meet the demands of such workloads.
  This paper presents XOS, an application-defined OS for modern DC servers. Our
design moves resource management out of the OS kernel, supports customizable
kernel subsystems in user space, and enables elastic partitioning of hardware
resources. Specifically, XOS leverages modern hardware support for
virtualization to move resource management functionality out of the
conventional kernel and into user space, which lets applications achieve near
bare-metal performance. We implement XOS on top of Linux to provide backward
compatibility. XOS speeds up a set of DC workloads by up to 1.6X over our
baseline Linux on a 24-core server, and outperforms the state-of-the-art Dune
by up to 3.3X in terms of virtual memory management. In addition, XOS
demonstrates good scalability and strong performance isolation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1305.0943v1,Weighted Electoral Control,"Although manipulation and bribery have been extensively studied under
weighted voting, there has been almost no work done on election control under
weighted voting. This is unfortunate, since weighted voting appears in many
important natural settings. In this paper, we study the complexity of
controlling the outcome of weighted elections through adding and deleting
voters. We obtain polynomial-time algorithms, NP-completeness results, and for
many NP-complete cases, approximation algorithms. In particular, for scoring
rules we completely characterize the complexity of weighted voter control. Our
work shows that for quite a few important cases, either polynomial-time exact
algorithms or polynomial-time approximation algorithms exist.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.03872v1,"Mixed-Integer Approaches to Constrained Optimum Communication Spanning
  Tree Problem","Several novel mixed-integer linear and bilinear formulations are proposed for
the optimum communication spanning tree problem. They implement the
distance-based approach: graph distances are directly modeled by continuous,
integral, or binary variables, and interconnection between distance variables
is established using the recursive Bellman-type conditions or using matrix
equations from algebraic graph theory. These non-linear relations are used
either directly giving rise to the bilinear formulations, or, through the big-M
reformulation, resulting in the linear programs. A branch-and-bound framework
of Gurobi 9.0 optimization software is employed to compare performance of the
novel formulations on the example of an optimum requirement spanning tree
problem with additional vertex degree constraints. Several real-world
requirements matrices from transportation industry are used to generate a
number of examples of different size, and computational experiments show the
superiority of the two novel linear distance-based formulations over the the
traditional multicommodity flow model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1212.1100v1,"Making Early Predictions of the Accuracy of Machine Learning
  Applications","The accuracy of machine learning systems is a widely studied research topic.
Established techniques such as cross-validation predict the accuracy on unseen
data of the classifier produced by applying a given learning method to a given
training data set. However, they do not predict whether incurring the cost of
obtaining more data and undergoing further training will lead to higher
accuracy. In this paper we investigate techniques for making such early
predictions. We note that when a machine learning algorithm is presented with a
training set the classifier produced, and hence its error, will depend on the
characteristics of the algorithm, on training set's size, and also on its
specific composition. In particular we hypothesise that if a number of
classifiers are produced, and their observed error is decomposed into bias and
variance terms, then although these components may behave differently, their
behaviour may be predictable.
  We test our hypothesis by building models that, given a measurement taken
from the classifier created from a limited number of samples, predict the
values that would be measured from the classifier produced when the full data
set is presented. We create separate models for bias, variance and total error.
Our models are built from the results of applying ten different machine
learning algorithms to a range of data sets, and tested with ""unseen""
algorithms and datasets. We analyse the results for various numbers of initial
training samples, and total dataset sizes. Results show that our predictions
are very highly correlated with the values observed after undertaking the extra
training. Finally we consider the more complex case where an ensemble of
heterogeneous classifiers is trained, and show how we can accurately estimate
an upper bound on the accuracy achievable after further training.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1003.6036v3,Computational Complexity of Iterated Maps on the Interval,"The correct computation of orbits of discrete dynamical systems on the
interval is considered. Therefore, an arbitrary-precision floating-point
approach based on automatic error analysis is chosen and a general algorithm is
presented. The correctness of the algorithm is shown and the computational
complexity is analyzed. There are two main results. First, the computational
complexity measure considered here is related to the Lyapunov exponent of the
dynamical system under consideration. Second, the presented algorithm is
optimal with regard to that complexity measure.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.01257v3,cf4ocl: a C framework for OpenCL,"OpenCL is an open standard for parallel programming of heterogeneous compute
devices, such as GPUs, CPUs, DSPs or FPGAs. However, the verbosity of its C
host API can hinder application development. In this paper we present cf4ocl, a
software library for rapid development of OpenCL programs in pure C. It aims to
reduce the verbosity of the OpenCL API, offering straightforward memory
management, integrated profiling of events (e.g., kernel execution and data
transfers), simple but extensible device selection mechanism and user-friendly
error management. We compare two versions of a conceptual application example,
one based on cf4ocl, the other developed directly with the OpenCL host API.
Results show that the former is simpler to implement and offers more features,
at the cost of an effectively negligible computational overhead. Additionally,
the tools provided with cf4ocl allowed for a quick analysis on how to optimize
the application.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0602011v2,"The intuitionistic fragment of computability logic at the propositional
  level","This paper presents a soundness and completeness proof for propositional
intuitionistic calculus with respect to the semantics of computability logic.
The latter interprets formulas as interactive computational problems,
formalized as games between a machine and its environment. Intuitionistic
implication is understood as algorithmic reduction in the weakest possible --
and hence most natural -- sense, disjunction and conjunction as
deterministic-choice combinations of problems (disjunction = machine's choice,
conjunction = environment's choice), and ""absurd"" as a computational problem of
universal strength. See http://www.cis.upenn.edu/~giorgi/cl.html for a
comprehensive online source on computability logic.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0109005v1,"Architectural Framework for Large-Scale Multicast in Mobile Ad Hoc
  Networks","Emerging ad hoc networks are infrastructure-less networks consisting of
wireless devices with various power constraints, capabilities and mobility
characteristics. An essential capability in future ad hoc networks is the
ability to provide scalable multicast services. This paper presents a novel
adaptive architecture to support multicast services in large-scale wide-area ad
hoc networks. Existing works on multicast in ad hoc networks address only small
size networks. Our main design goals are scalability, robustness and
efficiency. We propose a self-configuring hierarchy extending zone-based
routing with the notion of contacts based on the small world graphs phenomenon
and new metrics of stability and mobility. We introduce a new geographic-based
multicast address allocation scheme coupled with adaptive anycast based on
group popularity. Our scheme is the first of its kind and promises efficient
and robust operation in the common case. Also, based on the new concept of
rendezvous regions, we provide a bootstrap mechanism for the multicast service;
a challenge generally ignored in previous work.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0809.1949v5,Protocol Channels,"Covert channel techniques are used by attackers to transfer data in a way
prohibited by the security policy. There are two main categories of covert
channels: timing channels and storage channels. This paper introduces a new
storage channel technique called a protocol channel. A protocol channel
switches one of at least two protocols to send a bit combination to a
destination. The main goal of a protocol channel is that packets containing
covert information look equal to all other packets within a network, what makes
a protocol channel hard to detect.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1010.0621v2,Local Optimality of User Choices and Collaborative Competitive Filtering,"While a user's preference is directly reflected in the interactive choice
process between her and the recommender, this wealth of information was not
fully exploited for learning recommender models. In particular, existing
collaborative filtering (CF) approaches take into account only the binary
events of user actions but totally disregard the contexts in which users'
decisions are made. In this paper, we propose Collaborative Competitive
Filtering (CCF), a framework for learning user preferences by modeling the
choice process in recommender systems. CCF employs a multiplicative latent
factor model to characterize the dyadic utility function. But unlike CF, CCF
models the user behavior of choices by encoding a local competition effect. In
this way, CCF allows us to leverage dyadic data that was previously lumped
together with missing data in existing CF models. We present two formulations
and an efficient large scale optimization algorithm. Experiments on three
real-world recommendation data sets demonstrate that CCF significantly
outperforms standard CF approaches in both offline and online evaluations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1112.4791v1,"Unfolding Orthogonal Polyhedra with Quadratic Refinement: The
  Delta-Unfolding Algorithm","We show that every orthogonal polyhedron homeomorphic to a sphere can be
unfolded without overlap while using only polynomially many (orthogonal) cuts.
By contrast, the best previous such result used exponentially many cuts. More
precisely, given an orthogonal polyhedron with n vertices, the algorithm cuts
the polyhedron only where it is met by the grid of coordinate planes passing
through the vertices, together with Theta(n^2) additional coordinate planes
between every two such grid planes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1905.05270v1,When Do People Trust Their Social Groups?,"Trust facilitates cooperation and supports positive outcomes in social
groups, including member satisfaction, information sharing, and task
performance. Extensive prior research has examined individuals' general
propensity to trust, as well as the factors that contribute to their trust in
specific groups. Here, we build on past work to present a comprehensive
framework for predicting trust in groups. By surveying 6,383 Facebook Groups
users about their trust attitudes and examining aggregated behavioral and
demographic data for these individuals, we show that (1) an individual's
propensity to trust is associated with how they trust their groups, (2)
smaller, closed, older, more exclusive, or more homogeneous groups are trusted
more, and (3) a group's overall friendship-network structure and an
individual's position within that structure can also predict trust. Last, we
demonstrate how group trust predicts outcomes at both individual and group
level such as the formation of new friendship ties.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1003.1251v2,Minimum Spanning Tree on Spatio-Temporal Networks,"Given a spatio-temporal network (ST network) where edge properties vary with
time, a time-sub-interval minimum spanning tree (TSMST) is a collection of
minimum spanning trees of the ST network, where each tree is associated with a
time interval. During this time interval, the total cost of tree is least among
all the spanning trees. The TSMST problem aims to identify a collection of
distinct minimum spanning trees and their respective time-sub-intervals under
the constraint that the edge weight functions are piecewise linear. This is an
important problem in ST network application domains such as wireless sensor
networks (e.g., energy efficient routing). Computing TSMST is challenging
because the ranking of candidate spanning trees is non-stationary over a given
time interval. Existing methods such as dynamic graph algorithms and kinetic
data structures assume separable edge weight functions. In contrast, we propose
novel algorithms to find TSMST for large ST networks by accounting for both
separable and non-separable piecewise linear edge weight functions. The
algorithms are based on the ordering of edges in edge-order-intervals and
intersection points of edge weight functions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1506.01115v1,"Hyperspectral Image Classification and Clutter Detection via Multiple
  Structural Embeddings and Dimension Reductions","We present a new and effective approach for Hyperspectral Image (HSI)
classification and clutter detection, overcoming a few long-standing challenges
presented by HSI data characteristics. Residing in a high-dimensional spectral
attribute space, HSI data samples are known to be strongly correlated in their
spectral signatures, exhibit nonlinear structure due to several physical laws,
and contain uncertainty and noise from multiple sources. In the presented
approach, we generate an adaptive, structurally enriched representation
environment, and employ the locally linear embedding (LLE) in it. There are two
structure layers external to LLE. One is feature space embedding: the HSI data
attributes are embedded into a discriminatory feature space where
spatio-spectral coherence and distinctive structures are distilled and
exploited to mitigate various difficulties encountered in the native
hyperspectral attribute space. The other structure layer encloses the ranges of
algorithmic parameters for LLE and feature embedding, and supports a
multiplexing and integrating scheme for contending with multi-source
uncertainty. Experiments on two commonly used HSI datasets with a small number
of learning samples have rendered remarkably high-accuracy classification
results, as well as distinctive maps of detected clutter regions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.06913v1,Classical (Co)Recursion: Programming,"Our aim here is to illustrate how the benefits of structural corecursion can
be found in a broader swath of the programming landscape than previously
thought. Beginning from a tutorial on structural corecursion in the total, pure
functional language Agda, we show how these same ideas are mapped to familiar
concepts in a variety of different languages. We show how corecursion can be
done in strict functional languages like Scheme, and even escapes the
functional paradigm entirely, showing up in the natural expression of common
object-oriented features found in languages like Python and Java. Opening up
structural corecursion to a much wider selection of languages and paradigms --
and therefore, also to a much larger audience of programmers -- lets us also
ask how corecursion interacts with computational effects. Of note, we
demonstrate that combining structural corecursion with effects can increase its
expressive power. We show a classical version of corecursion -- using
first-class control made possible by Scheme's classical call/cc -- that enables
us to write some new stream-processing algorithms that aren't possible in
effect-free languages.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9905005v1,General Principles of Learning-Based Multi-Agent Systems,"We consider the problem of how to design large decentralized multi-agent
systems (MAS's) in an automated fashion, with little or no hand-tuning. Our
approach has each agent run a reinforcement learning algorithm. This converts
the problem into one of how to automatically set/update the reward functions
for each of the agents so that the global goal is achieved. In particular we do
not want the agents to ``work at cross-purposes'' as far as the global goal is
concerned. We use the term artificial COllective INtelligence (COIN) to refer
to systems that embody solutions to this problem. In this paper we present a
summary of a mathematical framework for COINs. We then investigate the
real-world applicability of the core concepts of that framework via two
computer experiments: we show that our COINs perform near optimally in a
difficult variant of Arthur's bar problem (and in particular avoid the tragedy
of the commons for that problem), and we also illustrate optimal performance
for our COINs in the leader-follower problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.08884v1,On the best choice of Lasso program given data parameters,"Generalized compressed sensing (GCS) is a paradigm in which a structured
high-dimensional signal may be recovered from random, under-determined, and
corrupted linear measurements. Generalized Lasso (GL) programs are effective
for solving GCS problems due to their proven ability to leverage underlying
signal structure. Three popular GL programs are equivalent in a sense and
sometimes used interchangeably. Tuned by a governing parameter, each admit an
optimal parameter choice. For sparse or low-rank signal structures, this choice
yields minimax order-optimal error. While GCS is well-studied, existing theory
for GL programs typically concerns this optimally tuned setting. However, the
optimal parameter value for a GL program depends on properties of the data, and
is typically unknown in practical settings. Performance in empirical problems
thus hinges on a program's parameter sensitivity: it is desirable that small
variation about the optimal parameter choice begets small variation about the
optimal risk. We examine the risk for these three programs and demonstrate that
their parameter sensitivity can differ for the same data. We prove a
gauge-constrained GL program admits asymptotic cusp-like behaviour of its risk
in the limiting low-noise regime. We prove that a residual-constrained Lasso
program has asymptotically suboptimal risk for very sparse vectors. These
results contrast observations about an unconstrained Lasso program, which is
relatively less sensitive to its parameter choice. We support the asymptotic
theory with numerical simulations, demonstrating that parameter sensitivity of
GL programs is readily observed for even modest dimensional parameters.
Importantly, these simulations demonstrate regimes in which a GL program
exhibits sensitivity to its parameter choice, though the other two do not. We
hope this work aids practitioners in selecting a GL program for their problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0910.3383v4,Least and Greatest Fixed Points in Linear Logic,"The first-order theory of MALL (multiplicative, additive linear logic) over
only equalities is an interesting but weak logic since it cannot capture
unbounded (infinite) behavior. Instead of accounting for unbounded behavior via
the addition of the exponentials (! and ?), we add least and greatest fixed
point operators. The resulting logic, which we call muMALL, satisfies two
fundamental proof theoretic properties: we establish weak normalization for it,
and we design a focused proof system that we prove complete. That second result
provides a strong normal form for cut-free proof structures that can be used,
for example, to help automate proof search. We show how these foundations can
be applied to intuitionistic logic.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1603.02966v1,Solutions of Word Equations over Partially Commutative Structures,"We give NSPACE(n log n) algorithms solving the following decision problems.
Satisfiability: Is the given equation over a free partially commutative monoid
with involution (resp. a free partially commutative group) solvable?
Finiteness: Are there only finitely many solutions of such an equation? PSPACE
algorithms with worse complexities for the first problem are known, but so far,
a PSPACE algorithm for the second problem was out of reach. Our results are
much stronger: Given such an equation, its solutions form an EDT0L language
effectively representable in NSPACE(n log n). In particular, we give an
effective description of the set of all solutions for equations with
constraints in free partially commutative monoids and groups.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1509.02557v1,Taming the hydra: the word problem and extreme integer compression,"For a finitely presented group, the word problem asks for an algorithm which
declares whether or not words on the generators represent the identity. The
Dehn function is a complexity measure of a direct attack on the word problem by
applying the defining relations. Dison & Riley showed that a ""hydra phenomenon""
gives rise to novel groups with extremely fast growing (Ackermannian) Dehn
functions. Here we show that nevertheless, there are efficient (polynomial
time) solutions to the word problems of these groups. Our main innovation is a
means of computing efficiently with enormous integers which are represented in
compressed forms by strings of Ackermann functions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1903.05619v1,A polynomial version of Cereceda's conjecture,"Let $k$ and $d$ be such that $k \ge d+2$. Consider two $k$-colourings of a
$d$-degenerate graph $G$. Can we transform one into the other by recolouring
one vertex at each step while maintaining a proper coloring at any step?
Cereceda et al. answered that question in the affirmative, and exhibited a
recolouring sequence of exponential length. However, Cereceda conjectured that
there should exist one of quadratic length.
  The $k$-reconfiguration graph of $G$ is the graph whose vertices are the
proper $k$-colourings of $G$, with an edge between two colourings if they
differ on exactly one vertex. Cereceda's conjecture can be reformulated as
follows: the diameter of the $(d+2)$-reconfiguration graph of any
$d$-degenerate graph on $n$ vertices is $O(n^2)$. So far, the existence of a
polynomial diameter is open even for $d=2$.
  In this paper, we prove that the diameter of the $k$-reconfiguration graph of
a $d$-degenerate graph is $O(n^{d+1})$ for $k \ge d+2$. Moreover, we prove that
if $k \ge \frac 32 (d+1)$ then the diameter of the $k$-reconfiguration graph is
quadratic, improving the previous bound of $k \ge 2d+1$. We also show that the
$5$-reconfiguration graph of planar bipartite graphs has quadratic diameter,
confirming Cereceda's conjecture for this class of graphs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.08028v2,"Predicting Clinical Outcomes in COVID-19 using Radiomics and Deep
  Learning on Chest Radiographs: A Multi-Institutional Study","We predict mechanical ventilation requirement and mortality using
computational modeling of chest radiographs (CXRs) for coronavirus disease 2019
(COVID-19) patients. This two-center, retrospective study analyzed 530
deidentified CXRs from 515 COVID-19 patients treated at Stony Brook University
Hospital and Newark Beth Israel Medical Center between March and August 2020.
DL and machine learning classifiers to predict mechanical ventilation
requirement and mortality were trained and evaluated using patient CXRs. A
novel radiomic embedding framework was also explored for outcome prediction.
All results are compared against radiologist grading of CXRs (zone-wise expert
severity scores). Radiomic and DL classification models had mAUCs of
0.78+/-0.02 and 0.81+/-0.04, compared with expert scores mAUCs of 0.75+/-0.02
and 0.79+/-0.05 for mechanical ventilation requirement and mortality
prediction, respectively. Combined classifiers using both radiomics and expert
severity scores resulted in mAUCs of 0.79+/-0.04 and 0.83+/-0.04 for each
prediction task, demonstrating improvement over either artificial intelligence
or radiologist interpretation alone. Our results also suggest instances where
inclusion of radiomic features in DL improves model predictions, something that
might be explored in other pathologies. The models proposed in this study and
the prognostic information they provide might aid physician decision making and
resource allocation during the COVID-19 pandemic.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1910.04697v1,"Immersive virtual worlds: Multi-sensory virtual environments for health
  and safety training","Virtual environments (VEs) offer potential benefits to health and safety
training: exposure to dangerous (virtual) environments; the opportunity for
experiential learning; and a high level of control over the training, in that
aspects can be repeated or reviewed based on the trainee's performance.
However, VEs are typically presented as audiovisual (AV) systems, whereas
engagement of other senses could increase the immersion in the virtual
experience. Moreover, other senses play a key role in certain health and safety
contexts, for example the feel of heat and smell in a fire or smell in a fuel
leak. A multisensory (MS) VE was developed, which provided simulated heat and
smell in accordance with events in a virtual world. As users approached a
virtual fire, they felt heat from three 2 kW heaters and smelled smoke from a
scent diffuser. Behaviours in the MS VE demonstrated higher validity than those
in a comparable AV VE, which ratings and verbatim responses indicated was down
to a greater belief that participants were in a real fire. However, a study of
the effectiveness of the MS VE as a training tool demonstrated that it did not
offer benefits over AV as measured by a written knowledge test and subjective
ratings of engagement, attitude towards health and safety and desire to repeat.
However, the study found further evidence for the use of AV VEs in health and
safety training, particularly as the subjective ratings were generally better
than for PowerPoint based training. Despite the lack of evidence for MS
simulation on traditional measures of training, the different attitudes and
experiences of users suggest that it may have value as a system for changing
trainees' attitudes towards their personal safety and awareness. This view was
supported by feedback from industrial partners.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1
http://arxiv.org/abs/cs/0307021v1,"Tools and Techniques for Managing Clusters for SciDAC Lattice QCD at
  Fermilab","Fermilab operates several clusters for lattice gauge computing. Minimal
manpower is available to manage these clusters. We have written a number of
tools and developed techniques to cope with this task. We describe our tools
which use the IPMI facilities of our systems for hardware management tasks such
as remote power control, remote system resets, and health monitoring. We
discuss our techniques involving network booting for installation and upgrades
of the operating system on these computers, and for reloading BIOS and other
firmware. Finally, we discuss our tools for parallel command processing and
their use in monitoring and administrating the PBS batch queue system used on
our clusters.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0508066v1,"Can Small Museums Develop Compelling, Educational and Accessible Web
  Resources? The Case of Accademia Carrara","Due to the lack of budget, competence, personnel and time, small museums are
often unable to develop compelling, educational and accessible web resources
for their permanent collections or temporary exhibitions. In an attempt to
prove that investing in these types of resources can be very fruitful even for
small institutions, we will illustrate the case of Accademia Carrara, a museum
in Bergamo, northern Italy, which, for a current temporary exhibition on
Cezanne and Renoir's masterpieces from the Paul Guillaume collection, developed
a series of multimedia applications, including an accessible website, rich in
content and educational material [www.cezannerenoir.it].",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0
http://arxiv.org/abs/1110.4094v2,A Logic for True Concurrency,"We propose a logic for true concurrency whose formulae predicate about events
in computations and their causal dependencies. The induced logical equivalence
is hereditary history preserving bisimilarity, and fragments of the logic can
be identified which correspond to other true concurrent behavioural
equivalences in the literature: step, pomset and history preserving
bisimilarity. Standard Hennessy-Milner logic, and thus (interleaving)
bisimilarity, is also recovered as a fragment. We also propose an extension of
the logic with fixpoint operators, thus allowing to describe causal and
concurrency properties of infinite computations. We believe that this work
contributes to a rational presentation of the true concurrent spectrum and to a
deeper understanding of the relations between the involved behavioural
equivalences.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0401006v1,"Cluster computing performances using virtual processors and mathematical
  software","In this paper I describe some results on the use of virtual processors
technology for parallelize some SPMD computational programs in a cluster
environment. The tested technology is the INTEL Hyper Threading on real
processors, and the programs are MATLAB 6.5 Release 13 scripts for floating
points computation. By the use of this technology, I tested that a cluster can
run with benefit a number of concurrent processes double the amount of physical
processors. The conclusions of the work concern on the utility and limits of
the used approach. The main result is that using virtual processors is a good
technique for improving parallel programs not only for memory-based
computations, but in the case of massive disk-storage operations too.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.01803v1,"DeepRT: A Soft Real Time Scheduler for Computer Vision Applications on
  the Edge","The ubiquity of smartphone cameras and IoT cameras, together with the recent
boom of deep learning and deep neural networks, proliferate various computer
vision driven mobile and IoT applications deployed on the edge. This paper
focuses on applications which make soft real time requests to perform inference
on their data - they desire prompt responses within designated deadlines, but
occasional deadline misses are acceptable. Supporting soft real time
applications on a multi-tenant edge server is not easy, since the requests
sharing the limited GPU computing resources of an edge server interfere with
each other. In order to tackle this problem, we comprehensively evaluate how
latency and throughput respond to different GPU execution plans. Based on this
analysis, we propose a GPU scheduler, DeepRT, which provides latency guarantee
to the requests while maintaining high overall system throughput. The key
component of DeepRT, DisBatcher, batches data from different requests as much
as possible while it is proven to provide latency guarantee for requests
admitted by an Admission Control Module. DeepRT also includes an Adaptation
Module which tackles overruns. Our evaluation results show that DeepRT
outperforms state-of-the-art works in terms of the number of deadline misses
and throughput.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1903.05896v1,"Regular Expressions with Backreferences: Polynomial-Time Matching
  Techniques","Regular expressions with backreferences (regex, for short), as supported by
most modern libraries for regular expression matching, have an NP-complete
matching problem. We define a complexity parameter of regex, called active
variable degree, such that regex with this parameter bounded by a constant can
be matched in polynomial-time. Moreover, we formulate a novel type of
determinism for regex (on an automaton-theoretic level), which yields the class
of memory-deterministic regex that can be matched in time O(|w|p(|r|)) for a
polynomial p (where r is the regex and w the word). Natural extensions of these
concepts lead to properties of regex that are intractable to check.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1106.3634v1,"Strategies for Development of a Distributed Framework for Computational
  Sciences","This paper discusses some generic approach for developing grid-based
framework for enabling establishment of workflows comprising existing software
in computational sciences areas. We highlight the main requirements addressed
the developing of such framework. Some strategies for enabling interoperability
between convenient computation software in the grid environment has been shown.
The UML based instruments of graphical description of workflows for the
developing system has been suggested.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1512.03901v2,"A Probabilistic Algorithm for Computing Data-Discriminants of Likelihood
  Equations","An algebraic approach to the maximum likelihood estimation problem is to
solve a very structured parameterized polynomial system called likelihood
equations that have finitely many complex (real or non-real) solutions. The
only solutions that are statistically meaningful are the real solutions with
positive coordinates. In order to classify the parameters (data) according to
the number of real/positive solutions, we study how to efficiently compute the
discriminants, say data-discriminants (DD), of the likelihood equations. We
develop a probabilistic algorithm with three different strategies for computing
DDs. Our implemented probabilistic algorithm based on Maple and FGb is more
efficient than our previous version presented in ISSAC2015, and is also more
efficient than the standard elimination for larger benchmarks. By applying
RAGlib to a DD we compute, we give the real root classification of 3 by 3
symmetric matrix model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1909.08021v2,Strategic Formation and Reliability of Supply Chain Networks,"Supply chains are the backbone of the global economy. Disruptions to them can
be costly. Centrally managed supply chains invest in ensuring their resilience.
Decentralized supply chains, however, must rely upon the self-interest of their
individual components to maintain the resilience of the entire chain.
  We examine the incentives that independent self-interested agents have in
forming a resilient supply chain network in the face of production disruptions
and competition. In our model, competing suppliers are subject to yield
uncertainty (they deliver less than ordered) and congestion (lead time
uncertainty or, ""soft"" supply caps). Competing retailers must decide which
suppliers to link to based on both price and reliability. In the presence of
yield uncertainty only, the resulting supply chain networks are sparse.
Retailers concentrate their links on a single supplier, counter to the idea
that they should mitigate yield uncertainty by diversifying their supply base.
This happens because retailers benefit from supply variance. It suggests that
competition will amplify output uncertainty. When congestion is included as
well, the resulting networks are denser and resemble the bipartite expander
graphs that have been proposed in the supply chain literature, thereby,
providing the first example of endogenous formation of resilient supply chain
networks, without resilience being explicitly encoded in payoffs. Finally, we
show that a supplier's investments in improved yield can make it worse off.
This happens because high production output saturates the market, which, in
turn lowers prices and profits for participants.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1311.5935v2,The Simplex Algorithm is NP-mighty,"We propose to classify the power of algorithms by the complexity of the
problems that they can be used to solve. Instead of restricting to the problem
a particular algorithm was designed to solve explicitly, however, we include
problems that, with polynomial overhead, can be solved 'implicitly' during the
algorithm's execution. For example, we allow to solve a decision problem by
suitably transforming the input, executing the algorithm, and observing whether
a specific bit in its internal configuration ever switches during the
execution. We show that the Simplex Method, the Network Simplex Method (both
with Dantzig's original pivot rule), and the Successive Shortest Path Algorithm
are NP-mighty, that is, each of these algorithms can be used to solve any
problem in NP. This result casts a more favorable light on these algorithms'
exponential worst-case running times. Furthermore, as a consequence of our
approach, we obtain several novel hardness results. For example, for a given
input to the Simplex Algorithm, deciding whether a given variable ever enters
the basis during the algorithm's execution and determining the number of
iterations needed are both NP-hard problems. Finally, we close a long-standing
open problem in the area of network flows over time by showing that earliest
arrival flows are NP-hard to obtain.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.15080v3,Security Engineering for ISO 21434,"The ISO 21434 is a new standard that has been proposed to address the future
challenges of automotive cybersecurity. This white paper takes a closer look at
the ISO 21434 helping engineers to understand the ISO 21434 parts, the key
activities to be carried out and the main artefacts that shall be produced. As
any certification, obtaining the ISO 21434 certification can be daunting at
first sight. Engineers have to deploy processes that include several security
risk assessment methods to produce security arguments and evidence supporting
item security claims. In this white paper, we propose a security engineering
approach that can ease this process by relying on Rigorous Security Assessments
and Incremental Assessment Maintenance methods supported by automation. We
demonstrate by example that the proposed approach can greatly increase the
quality of the produced artefacts, the efficiency to produce them, as well as
enable continuous security assessment. Finally, we point out some key research
directions that we are investigating to fully realize the proposed approach.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.08399v3,"Big Data in Astroinformatics -- Compression of Scanned Astronomical
  Photographic Plates","Construction of Scanned Astronomical Photographic Plates(SAPPs) databases and
SVD image compression algorithm are considered. Some examples of compression
with different plates are shown.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.12692v2,k-apices of minor-closed graph classes. II. Parameterized algorithms,"Let ${\cal G}$ be a minor-closed graph class. We say that a graph $G$ is a
$k$-apex of ${\cal G}$ if $G$ contains a set $S$ of at most $k$ vertices such
that $G\setminus S$ belongs to ${\cal G}$. We denote by ${\cal A}_k ({\cal G})$
the set of all graphs that are $k$-apices of ${\cal G}.$ In the first paper of
this series we obtained upper bounds on the size of the graphs in the
minor-obstruction set of ${\cal A}_k ({\cal G})$, i.e., the minor-minimal set
of graphs not belonging to ${\cal A}_k ({\cal G}).$ In this article we provide
an algorithm that, given a graph $G$ on $n$ vertices, runs in $2^{{\sf
poly}(k)}\cdot n^3$-time and either returns a set $S$ certifying that $G \in
{\cal A}_k ({\cal G})$, or reports that $G \notin {\cal A}_k ({\cal G})$. Here
${\sf poly}$ is a polynomial function whose degree depends on the maximum size
of a minor-obstruction of ${\cal G}.$ In the special case where ${\cal G}$
excludes some apex graph as a minor, we give an alternative algorithm running
in $2^{{\sf poly}(k)}\cdot n^2$-time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0910.4052v1,Virtual-Threading: Advanced General Purpose Processors Architecture,"The paper describes the new computers architecture, the main features of
which has been claimed in the Russian Federation patent 2312388 and in the US
patent application 11/991331. This architecture is intended to effective
support of the General Purpose Parallel Computing (GPPC), the essence of which
is extremely frequent switching of threads between states of activity and
states of viewed in the paper the algorithmic latency. To emphasize the same
impact of the architectural latency and the algorithmic latency upon GPPC, is
introduced the new notion of the generalized latency and is defined its
quantitative measure - the Generalized Latency Tolerance (GLT). It is shown
that a well suited for GPPC implementation architecture should have high level
of GLT and is described such architecture, which is called the Virtual-Threaded
Machine. This architecture originates a processor virtualization in the
direction of activities virtualization, which is orthogonal to the well-known
direction of memory virtualization. The key elements of the architecture are 1)
the distributed fine grain representation of the architectural register file,
which elements are hardware swapped through levels of a microarchitectural
memory, 2) the prioritized fine grain direct hardware multiprogramming, 3) the
access controlled virtual addressing and 4) the hardware driven semaphores. The
composition of these features lets to introduce new styles of operating system
(OS) programming, which is free of interruptions, and of applied programming
with a very rare using the OS services.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.12875v2,"Perla: A Conversational Agent for Depression Screening in Digital
  Ecosystems. Design, Implementation and Validation","Most depression assessment tools are based on self-report questionnaires,
such as the Patient Health Questionnaire (PHQ-9). These psychometric
instruments can be easily adapted to an online setting by means of electronic
forms. However, this approach lacks the interacting and engaging features of
modern digital environments. With the aim of making depression screening more
available, attractive and effective, we developed Perla, a conversational agent
able to perform an interview based on the PHQ-9. We also conducted a validation
study in which we compared the results obtained by the traditional self-report
questionnaire with Perla's automated interview. Analyzing the results from this
study we draw two significant conclusions: firstly, Perla is much preferred by
Internet users, achieving more than 2.5 times more reach than a traditional
form-based questionnaire; secondly, her psychometric properties (Cronbach's
alpha of 0.81, sensitivity of 96% and specificity of 90%) are excellent and
comparable to the traditional well-established depression screening
questionnaires.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1307.1942v1,PROOFTOOL: a GUI for the GAPT Framework,"This paper introduces PROOFTOOL, the graphical user interface for the General
Architecture for Proof Theory (GAPT) framework. Its features are described with
a focus not only on the visualization but also on the analysis and
transformation of proofs and related tree-like structures, and its
implementation is explained. Finally, PROOFTOOL is compared with three other
graphical interfaces for proofs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0106009v1,Model Checking Contractual Protocols,"This paper discusses how model checking, a technique used for the
verification of behavioural requirements of dynamic systems, can be usefully
deployed for the verification of contracts. A process view of agreements
between parties is taken, whereby a contract is modelled as it evolves over
time in terms of actions or more generally events that effect changes in its
state. Modelling is done with Petri Nets in the spirit of other research work
on the representation of trade procedures. The paper illustrates all the phases
of the verification technique through an example and argues that the approach
is useful particularly in the context of pre-contractual negotiation and
contract drafting. The work reported here is part of a broader project on the
development of logic-based tools for the analysis and representation of legal
contracts.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.04135v1,"Performance Analysis of Keypoint Detectors and Binary Descriptors Under
  Varying Degrees of Photometric and Geometric Transformations","Detecting image correspondences by feature matching forms the basis of
numerous computer vision applications. Several detectors and descriptors have
been presented in the past, addressing the efficient generation of features
from interest points (keypoints) in an image. In this paper, we investigate
eight binary descriptors (AKAZE, BoostDesc, BRIEF, BRISK, FREAK, LATCH, LUCID,
and ORB) and eight interest point detector (AGAST, AKAZE, BRISK, FAST,
HarrisLapalce, KAZE, ORB, and StarDetector). We have decoupled the detection
and description phase to analyze the interest point detectors and then evaluate
the performance of the pairwise combination of different detectors and
descriptors. We conducted experiments on a standard dataset and analyzed the
comparative performance of each method under different image transformations.
We observed that: (1) the FAST, AGAST, ORB detectors were faster and detected
more keypoints, (2) the AKAZE and KAZE detectors performed better under
photometric changes while ORB was more robust against geometric changes, (3) in
general, descriptors performed better when paired with the KAZE and AKAZE
detectors, (4) the BRIEF, LUCID, ORB descriptors were relatively faster, and
(5) none of the descriptors did particularly well under geometric
transformations, only BRISK, FREAK, and AKAZE showed reasonable resiliency.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1608.03665v4,Learning Structured Sparsity in Deep Neural Networks,"High demand for computation resources severely hinders deployment of
large-scale Deep Neural Networks (DNN) in resource constrained devices. In this
work, we propose a Structured Sparsity Learning (SSL) method to regularize the
structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs.
SSL can: (1) learn a compact structure from a bigger DNN to reduce computation
cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently
accelerate the DNNs evaluation. Experimental results show that SSL achieves on
average 5.1x and 3.1x speedups of convolutional layer computation of AlexNet
against CPU and GPU, respectively, with off-the-shelf libraries. These speedups
are about twice speedups of non-structured sparsity; (3) regularize the DNN
structure to improve classification accuracy. The results show that for
CIFAR-10, regularization on layer depth can reduce 20 layers of a Deep Residual
Network (ResNet) to 18 layers while improve the accuracy from 91.25% to 92.60%,
which is still slightly higher than that of original ResNet with 32 layers. For
AlexNet, structure regularization by SSL also reduces the error by around ~1%.
Open source code is in https://github.com/wenwei202/caffe/tree/scnn",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.03311v2,Online Bin Packing with Predictions,"Bin packing is a classic optimization problem with a wide range of
applications from load balancing in networks to supply chain management. In
this work we study the online variant of the problem, in which a sequence of
items of various sizes must be placed into a minimum number of bins of uniform
capacity. The online algorithm is enhanced with a (potentially erroneous)
prediction concerning the frequency of item sizes in the sequence. We design
and analyze online algorithms with efficient tradeoffs between consistency
(i.e., the competitive ratio assuming no prediction error) and robustness
(i.e., the competitive ratio under adversarial error), and whose performance
degrades gently as a function of the prediction error. This is the first
theoretical study of online bin packing in the realistic setting of erroneous
predictions, as well as the first experimental study in the setting in which
the input is generated according to both static and evolving distributions.
Previous work on this problem has only addressed the extreme cases with respect
to the prediction error, has relied on overly powerful and error-free
prediction oracles, and has focused on experimental evaluation based on static
input distributions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.10221v1,"Expressing and Executing Informed Consent Permissions Using SWRL: The
  All of Us Use Case","The informed consent process is a complicated procedure involving permissions
as well a variety of entities and actions. In this paper, we discuss the use of
Semantic Web Rule Language (SWRL) to further extend the Informed Consent
Ontology (ICO) to allow for semantic machine-based reasoning to manage and
generate important permission-based information that can later be viewed by
stakeholders. We present four use cases of permissions from the All of Us
informed consent document and translate these permissions into SWRL expressions
to extend and operationalize ICO. Our efforts show how SWRL is able to infer
some of the implicit information based on the defined rules, and demonstrate
the utility of ICO through the use of SWRL extensions. Future work will include
developing formal and generalized rules and expressing permissions from the
entire document, as well as working towards integrating ICO into software
systems to enhance the semantic representation of informed consent for
biomedical research.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1304.3483v2,Faster sparse interpolation of straight-line programs,"We give a new probabilistic algorithm for interpolating a ""sparse"" polynomial
f given by a straight-line program. Our algorithm constructs an approximation
f* of f, such that their difference probably has at most half the number of
terms of f, then recurses on their difference. Our approach builds on previous
work by Garg and Schost (2009), and Giesbrecht and Roche (2011), and is
asymptotically more efficient in terms of the total cost of the probes required
than previous methods, in many cases.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1709.10063v1,"Finding Small Weight Isomorphisms with Additional Constraints is
  Fixed-Parameter Tractable","Lubiw showed that several variants of Graph Isomorphism are NP-complete,
where the solutions are required to satisfy certain additional constraints
[SICOMP 10, 1981]. One of these, called Isomorphism With Restrictions, is to
decide for two given graphs $X_1=(V,E_1)$ and $X_2=(V,E_2)$ and a subset
$R\subseteq V\times V$ of forbidden pairs whether there is an isomorphism $\pi$
from $X_1$ to $X_2$ such that $\pi(i)\neq j$ for all $(i,j)\in R$. We prove
that this problem and several of its generalizations are in fact in FPT:
  - The problem of deciding whether there is an isomorphism between two graphs
that moves k vertices and satisfies Lubiw-style constraints is in FPT, with k
and the size of $R$ as parameters. The problem remains in FPT if a CNF of such
constraints is allowed. It follows that the problem to decide whether there is
an isomorphism that moves exactly k vertices is in FPT. This solves a question
left open in our article on exact weight automorphisms [STACS 2017].
  - When the weight and complexity are unrestricted, finding isomorphisms that
satisfy a CNF of Lubiw-style constraints can be solved in FPT with access to a
GI oracle.
  - Checking if there is an isomorphism $\pi$ between two graphs with
complexity t is also in FPT with t as parameter, where the complexity of a
permutation is the Cayley measure defined as the minimum number t such that
$\pi$ can be expressed as a product of t transpositions.
  - We consider a more general problem in which the vertex set of a graph X is
partitioned into Red and Blue, and we are interested in an automorphism that
stabilizes Red and Blue and moves exactly k vertices in Blue, where k is the
parameter. This problem was introduced by [Downey and Fellows 1999], and we
showed [STACS 2017] that it is W[1]-hard even with color classes of size 4
inside Red. Now, for color classes of size at most 3 inside Red, we show the
problem is in FPT.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.01794v1,"Robustness Analysis of Systems' Safety through a New Notion of
  Input-to-State Safety","In this paper, we propose a new robustness notion that is applicable for
certifying systems' safety with respect to external disturbance signals. The
proposed input-to-state safety (ISSf) notion allows us to certify systems'
safety in the presence of the disturbances which is analogous to the notion of
input-to-state stability (ISS) for analyzing systems' stability.",0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.05612v3,"Are You Talking to a Machine? Dataset and Methods for Multilingual Image
  Question Answering","In this paper, we present the mQA model, which is able to answer questions
about the content of an image. The answer can be a sentence, a phrase or a
single word. Our model contains four components: a Long Short-Term Memory
(LSTM) to extract the question representation, a Convolutional Neural Network
(CNN) to extract the visual representation, an LSTM for storing the linguistic
context in an answer, and a fusing component to combine the information from
the first three components and generate the answer. We construct a Freestyle
Multilingual Image Question Answering (FM-IQA) dataset to train and evaluate
our mQA model. It contains over 150,000 images and 310,000 freestyle Chinese
question-answer pairs and their English translations. The quality of the
generated answers of our mQA model on this dataset is evaluated by human judges
through a Turing Test. Specifically, we mix the answers provided by humans and
our model. The human judges need to distinguish our model from the human. They
will also provide a score (i.e. 0, 1, 2, the larger the better) indicating the
quality of the answer. We propose strategies to monitor the quality of this
evaluation process. The experiments show that in 64.7% of cases, the human
judges cannot distinguish our model from humans. The average score is 1.454
(1.918 for human). The details of this work, including the FM-IQA dataset, can
be found on the project page: http://idl.baidu.com/FM-IQA.html",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1608.04636v4,"Linear Convergence of Gradient and Proximal-Gradient Methods Under the
  Polyak-≈Åojasiewicz Condition","In 1963, Polyak proposed a simple condition that is sufficient to show a
global linear convergence rate for gradient descent. This condition is a
special case of the \L{}ojasiewicz inequality proposed in the same year, and it
does not require strong convexity (or even convexity). In this work, we show
that this much-older Polyak-\L{}ojasiewicz (PL) inequality is actually weaker
than the main conditions that have been explored to show linear convergence
rates without strong convexity over the last 25 years. We also use the PL
inequality to give new analyses of randomized and greedy coordinate descent
methods, sign-based gradient descent methods, and stochastic gradient methods
in the classic setting (with decreasing or constant step-sizes) as well as the
variance-reduced setting. We further propose a generalization that applies to
proximal-gradient methods for non-smooth optimization, leading to simple proofs
of linear convergence of these methods. Along the way, we give simple
convergence results for a wide variety of problems in machine learning: least
squares, logistic regression, boosting, resilient backpropagation,
L1-regularization, support vector machines, stochastic dual coordinate ascent,
and stochastic variance-reduced gradient methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1605.04636v1,"Learning the Problem-Optimum Map: Analysis and Application to Global
  Optimization in Robotics","This paper describes a data-driven framework for approximate global
optimization in which precomputed solutions to a sample of problems are
retrieved and adapted during online use to solve novel problems. This approach
has promise for real-time applications in robotics, since it can produce
near-globally optimal solutions orders of magnitude faster than standard
methods. This paper establishes theoretical conditions on how many and where
samples are needed over the space of problems to achieve a given approximation
quality. The framework is applied to solve globally optimal collision-free
inverse kinematics (IK) problems, wherein large solution databases are used to
produce near-optimal solutions in sub-millisecond time on a standard PC.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.01206v3,"Local Citation Recommendation with Hierarchical-Attention Text Encoder
  and SciBERT-based Reranking","The goal of local citation recommendation is to recommend a missing reference
from the local citation context and optionally also from the global context. To
balance the tradeoff between speed and accuracy of citation recommendation in
the context of a large-scale paper database, a viable approach is to first
prefetch a limited number of relevant documents using efficient ranking methods
and then to perform a fine-grained reranking using more sophisticated models.
In that vein, BM25 has been found to be a tough-to-beat approach to
prefetching, which is why recent work has focused mainly on the reranking step.
Even so, we explore prefetching with nearest neighbor search among text
embeddings constructed by a hierarchical attention network. When coupled with a
SciBERT reranker fine-tuned on local citation recommendation tasks, our
hierarchical Attention encoder (HAtten) achieves high prefetch recall for a
given number of candidates to be reranked. Consequently, our reranker requires
fewer prefetch candidates to rerank, yet still achieves state-of-the-art
performance on various local citation recommendation datasets such as ACL-200,
FullTextPeerRead, RefSeer, and arXiv.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.11226v1,"Minimal Translations from Synchronous Communication to Synchronizing
  Locks","In order to understand the relative expressive power of larger concurrent
programming languages, we analyze translations of small process calculi which
model the communication and synchronization of concurrent processes. The source
language SYNCSIMPLE is a minimalistic model for message passing concurrency
while the target language LOCKSIMPLE is a minimalistic model for shared memory
concurrency. The former is a calculus with synchronous communication of
processes, while the latter has synchronizing mutable locations - called locks
- that behave similarly to binary semaphores. The criteria for correctness of
translations is that they preserve and reflect may-termination and
must-termination of the processes. We show that there is no correct
compositional translation from SYNCSIMPLE to LOCKSIMPLE that uses one or two
locks, independent from the initialisation of the locks. We also show that
there is a correct translation that uses three locks. Also variants of the
locks are taken into account with different blocking behavior.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1509.01338v1,"Brain Computer Interfaces for Mobile Apps: State-of-the-art and Future
  Directions","In recent times, there have been significant advancements in utilizing the
sensing capabilities of mobile devices for developing applications. The primary
objective has been to enhance the way a user interacts with the application by
making it effortless and convenient. This paper explores the capabilities of
using Brain Computer Interfaces (BCI), an evolving subset of Human Computer
Interaction (HCI) paradigms, to control mobile devices. We present a
comprehensive survey of the state-of-the-art in this area, discussing the
challenges and limitations in using BCI for mobile applications. Further we
propose possible modalities that in future can benefit with BCI applications.
This paper consolidates research directions being pursued in this domain, and
draws conclusions on feasibility and benefits of using BCI systems effectively
augmented to the mobile application development domain.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0904.1150v6,"Upper Bounds on the Capacities of Noncontrollable Finite-State Channels
  with/without Feedback","Noncontrollable finite-state channels (FSCs) are FSCs in which the channel
inputs have no influence on the channel states, i.e., the channel states evolve
freely. Since single-letter formulae for the channel capacities are rarely
available for general noncontrollable FSCs, computable bounds are usually
utilized to numerically bound the capacities. In this paper, we take the
delayed channel state as part of the channel input and then define the {\em
directed information rate} from the new channel input (including the source and
the delayed channel state) sequence to the channel output sequence. With this
technique, we derive a series of upper bounds on the capacities of
noncontrollable FSCs with/without feedback. These upper bounds can be achieved
by conditional Markov sources and computed by solving an average reward per
stage stochastic control problem (ARSCP) with a compact state space and a
compact action space. By showing that the ARSCP has a uniformly continuous
reward function, we transform the original ARSCP into a finite-state and
finite-action ARSCP that can be solved by a value iteration method. Under a
mild assumption, the value iteration algorithm is convergent and delivers a
near-optimal stationary policy and a numerical upper bound.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1209.6001v1,Bayesian Mixture Models for Frequent Itemset Discovery,"In binary-transaction data-mining, traditional frequent itemset mining often
produces results which are not straightforward to interpret. To overcome this
problem, probability models are often used to produce more compact and
conclusive results, albeit with some loss of accuracy. Bayesian statistics have
been widely used in the development of probability models in machine learning
in recent years and these methods have many advantages, including their
abilities to avoid overfitting. In this paper, we develop two Bayesian mixture
models with the Dirichlet distribution prior and the Dirichlet process (DP)
prior to improve the previous non-Bayesian mixture model developed for
transaction dataset mining. We implement the inference of both mixture models
using two methods: a collapsed Gibbs sampling scheme and a variational
approximation algorithm. Experiments in several benchmark problems have shown
that both mixture models achieve better performance than a non-Bayesian mixture
model. The variational algorithm is the faster of the two approaches while the
Gibbs sampling method achieves a more accurate results. The Dirichlet process
mixture model can automatically grow to a proper complexity for a better
approximation. Once the model is built, it can be very fast to query and run
analysis on (typically 10 times faster than Eclat, as we will show in the
experiment section). However, these approaches also show that mixture models
underestimate the probabilities of frequent itemsets. Consequently, these
models have a higher sensitivity but a lower specificity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.09007v2,A Comprehensive Analysis of Twitter Trending Topics,"In Twitter, a name, phrase, or topic that is mentioned at a greater rate than
others is called a ""trending topic"" or simply ""trend"". Twitter trends list has
a powerful ability to promote public events such as natural events, political
scandals, market changes and other types of breaking news. Nevertheless, there
have been very few works focused on the dynamics of these trending topics. In
this article, we thoroughly examined the Twitter's trending topics of 2018. To
this end, we automatically accessed Twitter's trends API and stored the
resulting 50 top trending topics in a novel dataset. We propose and analyze our
dataset according to six criteria: lexical analysis, time to reach, trend
reoccurrence, trending time, tweets count, and language analysis. Based on our
results, 77.6% of the topics that reached the Top-10 list were trending with
less than 100k tweets. More than 50% of the topics could not hold the position
for more than an hour. English and Arabic languages comprised close to 40% and
20% of the first rank topics, respectively.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.03304v1,A $2$-Approximation Algorithm for Flexible Graph Connectivity,"We present a $2$-approximation algorithm for the Flexible Graph Connectivity
problem [AHM20] via a reduction to the minimum cost $r$-out $2$-arborescence
problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0003038v1,A Splitting Set Theorem for Epistemic Specifications,"Over the past decade a considerable amount of research has been done to
expand logic programming languages to handle incomplete information. One such
language is the language of epistemic specifications. As is usual with logic
programming languages, the problem of answering queries is intractable in the
general case. For extended disjunctive logic programs, an idea that has proven
useful in simplifying the investigation of answer sets is the use of splitting
sets. In this paper we will present an extended definition of splitting sets
that will be applicable to epistemic specifications. Furthermore, an extension
of the splitting set theorem will be presented. Also, a characterization of
stratified epistemic specifications will be given in terms of splitting sets.
This characterization leads us to an algorithmic method of computing world
views of a subclass of epistemic logic programs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1204.5314v1,"A Tunable Mechanism for Identifying Trusted Nodes in Large Scale
  Distributed Networks","In this paper, we propose a simple randomized protocol for identifying
trusted nodes based on personalized trust in large scale distributed networks.
The problem of identifying trusted nodes, based on personalized trust, in a
large network setting stems from the huge computation and message overhead
involved in exhaustively calculating and propagating the trust estimates by the
remote nodes. However, in any practical scenario, nodes generally communicate
with a small subset of nodes and thus exhaustively estimating the trust of all
the nodes can lead to huge resource consumption. In contrast, our mechanism can
be tuned to locate a desired subset of trusted nodes, based on the allowable
overhead, with respect to a particular user. The mechanism is based on a simple
exchange of random walk messages and nodes counting the number of times they
are being hit by random walkers of nodes in their neighborhood. Simulation
results to analyze the effectiveness of the algorithm show that using the
proposed algorithm, nodes identify the top trusted nodes in the network with a
very high probability by exploring only around 45% of the total nodes, and in
turn generates nearly 90% less overhead as compared to an exhaustive trust
estimation mechanism, named TrustWebRank. Finally, we provide a measure of the
global trustworthiness of a node; simulation results indicate that the measures
generated using our mechanism differ by only around 0.6% as compared to
TrustWebRank.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0203005v2,A Framework for Compiling Preferences in Logic Programs,"We introduce a methodology and framework for expressing general preference
information in logic programming under the answer set semantics. An ordered
logic program is an extended logic program in which rules are named by unique
terms, and in which preferences among rules are given by a set of atoms of form
s < t where s and t are names. An ordered logic program is transformed into a
second, regular, extended logic program wherein the preferences are respected,
in that the answer sets obtained in the transformed program correspond with the
preferred answer sets of the original program. Our approach allows the
specification of dynamic orderings, in which preferences can appear arbitrarily
within a program. Static orderings (in which preferences are external to a
logic program) are a trivial restriction of the general dynamic case. First, we
develop a specific approach to reasoning with preferences, wherein the
preference ordering specifies the order in which rules are to be applied. We
then demonstrate the wide range of applicability of our framework by showing
how other approaches, among them that of Brewka and Eiter, can be captured
within our framework. Since the result of each of these transformations is an
extended logic program, we can make use of existing implementations, such as
dlv and smodels. To this end, we have developed a publicly available compiler
as a front-end for these programming systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.04085v3,Automatic Extrinsic Calibration for Lidar-Stereo Vehicle Sensor Setups,"Sensor setups consisting of a combination of 3D range scanner lasers and
stereo vision systems are becoming a popular choice for on-board perception
systems in vehicles; however, the combined use of both sources of information
implies a tedious calibration process. We present a method for extrinsic
calibration of lidar-stereo camera pairs without user intervention. Our
calibration approach is aimed to cope with the constraints commonly found in
automotive setups, such as low-resolution and specific sensor poses. To
demonstrate the performance of our method, we also introduce a novel approach
for the quantitative assessment of the calibration results, based on a
simulation environment. Tests using real devices have been conducted as well,
proving the usability of the system and the improvement over the existing
approaches. Code is available at http://wiki.ros.org/velo2cam_calibration",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0709.2962v3,Algebraic characterization of logically defined tree languages,"We give an algebraic characterization of the tree languages that are defined
by logical formulas using certain Lindstr\""om quantifiers. An important
instance of our result concerns first-order definable tree languages. Our
characterization relies on the usage of preclones, an algebraic structure
introduced by the authors in a previous paper, and of the block product
operation on preclones. Our results generalize analogous results on finite word
languages, but it must be noted that, as they stand, they do not yield an
algorithm to decide whether a given regular tree language is first-order
definable.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0508044v5,"Deciding Quantifier-Free Presburger Formulas Using Parameterized
  Solution Bounds","Given a formula in quantifier-free Presburger arithmetic, if it has a
satisfying solution, there is one whose size, measured in bits, is polynomially
bounded in the size of the formula. In this paper, we consider a special class
of quantifier-free Presburger formulas in which most linear constraints are
difference (separation) constraints, and the non-difference constraints are
sparse. This class has been observed to commonly occur in software
verification. We derive a new solution bound in terms of parameters
characterizing the sparseness of linear constraints and the number of
non-difference constraints, in addition to traditional measures of formula
size. In particular, we show that the number of bits needed per integer
variable is linear in the number of non-difference constraints and logarithmic
in the number and size of non-zero coefficients in them, but is otherwise
independent of the total number of linear constraints in the formula. The
derived bound can be used in a decision procedure based on instantiating
integer variables over a finite domain and translating the input
quantifier-free Presburger formula to an equi-satisfiable Boolean formula,
which is then checked using a Boolean satisfiability solver. In addition to our
main theoretical result, we discuss several optimizations for deriving tighter
bounds in practice. Empirical evidence indicates that our decision procedure
can greatly outperform other decision procedures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0607007v4,Theory of sexes by Geodakian as it is advanced by Iskrin,"In 1960s V.Geodakian proposed a theory that explains sexes as a mechanism for
evolutionary adaptation of the species to changing environmental conditions. In
2001 V.Iskrin refined and augmented the concepts of Geodakian and gave a new
and interesting explanation to several phenomena which involve sex, and sex
ratio, including the war-years phenomena. He also introduced a new concept of
the ""catastrophic sex ratio."" This note is an attempt to digest technical
aspects of the new ideas by Iskrin.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0404052v1,Multi-Threading And Message Communication In Qu-Prolog,"This paper presents the multi-threading and internet message communication
capabilities of Qu-Prolog. Message addresses are symbolic and the
communications package provides high-level support that completely hides
details of IP addresses and port numbers as well as the underlying TCP/IP
transport layer. The combination of the multi-threads and the high level
inter-thread message communications provide simple, powerful support for
implementing internet distributed intelligent applications.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1012.4881v1,"Generalized Delaunay Graphs with respect to any Convex Set are Plane
  Graphs","We consider two types of geometric graphs on point sets on the plane based on
a plane set C: one obtained by translates of C, another by positively scaled
translates (homothets) of C. For compact and convex C, graphs defined by scaled
translates of C, i.e., Delaunay graphs based on C, are known to be plane
graphs. We show that as long as C is convex, both types of graphs are plane
graphs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1610.05796v1,Decision Tree Classification on Outsourced Data,"This paper proposes a client-server decision tree learning method for
outsourced private data. The privacy model is anatomization/fragmentation: the
server sees data values, but the link between sensitive and identifying
information is encrypted with a key known only to clients. Clients have limited
processing and storage capability. Both sensitive and identifying information
thus are stored on the server. The approach presented also retains most
processing at the server, and client-side processing is amortized over
predictions made by the clients. Experiments on various datasets show that the
method produces decision trees approaching the accuracy of a non-private
decision tree, while substantially reducing the client's computing resource
requirements.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0105029v1,Coloring k-colorable graphs using relatively small palettes,"We obtain the following new coloring results:
  * A 3-colorable graph on $n$ vertices with maximum degree~$\Delta$ can be
colored, in polynomial time, using $O((\Delta \log\Delta)^{1/3} \cdot\log{n})$
colors. This slightly improves an $O((\Delta^{{1}/{3}}
\log^{1/2}\Delta)\cdot\log{n})$ bound given by Karger, Motwani and Sudan. More
generally, $k$-colorable graphs with maximum degree $\Delta$ can be colored, in
polynomial time, using $O((\Delta^{1-{2}/{k}}\log^{1/k}\Delta) \cdot\log{n})$
colors.
  * A 4-colorable graph on $n$ vertices can be colored, in polynomial time,
using $\Ot(n^{7/19})$ colors. This improves an $\Ot(n^{2/5})$ bound given again
by Karger, Motwani and Sudan. More generally, $k$-colorable graphs on $n$
vertices can be colored, in polynomial time, using $\Ot(n^{\alpha_k})$ colors,
where $\alpha_5=97/207$, $\alpha_6=43/79$, $\alpha_7=1391/2315$,
$\alpha_8=175/271$, ...
  The first result is obtained by a slightly more refined probabilistic
analysis of the semidefinite programming based coloring algorithm of Karger,
Motwani and Sudan. The second result is obtained by combining the coloring
algorithm of Karger, Motwani and Sudan, the combinatorial coloring algorithms
of Blum and an extension of a technique of Alon and Kahale (which is based on
the Karger, Motwani and Sudan algorithm) for finding relatively large
independent sets in graphs that are guaranteed to have very large independent
sets. The extension of the Alon and Kahale result may be of independent
interest.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1103.0318v1,Listing All Maximal Cliques in Large Sparse Real-World Graphs,"We implement a new algorithm for listing all maximal cliques in sparse graphs
due to Eppstein, L\""offler, and Strash (ISAAC 2010) and analyze its performance
on a large corpus of real-world graphs. Our analysis shows that this algorithm
is the first to offer a practical solution to listing all maximal cliques in
large sparse graphs. All other theoretically-fast algorithms for sparse graphs
have been shown to be significantly slower than the algorithm of Tomita et al.
(Theoretical Computer Science, 2006) in practice. However, the algorithm of
Tomita et al. uses an adjacency matrix, which requires too much space for large
sparse graphs. Our new algorithm opens the door for fast analysis of large
sparse graphs whose adjacency matrix will not fit into working memory.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0810.4061v1,"Locally computable approximations for spectral clustering and absorption
  times of random walks","We address the problem of determining a natural local neighbourhood or
""cluster"" associated to a given seed vertex in an undirected graph. We
formulate the task in terms of absorption times of random walks from other
vertices to the vertex of interest, and observe that these times are well
approximated by the components of the principal eigenvector of the
corresponding fundamental matrix of the graph's adjacency matrix. We further
present a locally computable gradient-descent method to estimate this
Dirichlet-Fiedler vector, based on minimising the respective Rayleigh quotient.
Experimental evaluation shows that the approximations behave well and yield
well-defined local clusters.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1409.6765v3,"A Generalization of the AL method for Fair Allocation of Indivisible
  Objects","We consider the assignment problem in which agents express ordinal
preferences over $m$ objects and the objects are allocated to the agents based
on the preferences. In a recent paper, Brams, Kilgour, and Klamler (2014)
presented the AL method to compute an envy-free assignment for two agents. The
AL method crucially depends on the assumption that agents have strict
preferences over objects. We generalize the AL method to the case where agents
may express indifferences and prove the axiomatic properties satisfied by the
algorithm. As a result of the generalization, we also get a $O(m)$ speedup on
previous algorithms to check whether a complete envy-free assignment exists or
not. Finally, we show that unless P=NP, there can be no polynomial-time
extension of GAL to the case of arbitrary number of agents.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1701.05945v1,Exploiting the Cloud Control Plane for Fun and Profit,"Cloud providers typically charge for their services. There are diverse
pricing models which often follow a pay-per-use paradigm. The consumers'
payments are expected to cover all cost which incurs to the provider for
processing, storage, bandwidth, data centre operation and engineering efforts,
among others. In contrast, the consumer management interfaces are free of
charge as they are expected to cause only a minority of the load compared to
the actual computing services. With new service models and more complex and
powerful management abilities, it is time to rethink this decision. The paper
shows how to exploit the control plane of AWS Lambda to implement stateful
services practically for free and under some circumstances even guaranteed for
free which if widely deployed would cause a monetary loss for the provider. It
also elaborates on the consistency model for AWS Lambda.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.5602v1,On Determinism and Unambiguity of Weighted Two-way Automata,"In this paper, we first study the conversion of weighted two-way automata to
one-way automata. We show that this conversion preserves the unambiguity but
does not preserve the determinism. Yet, we prove that the conversion of an
unambiguous weighted one-way automaton into a two-way automaton leads to a
deterministic two-way automaton. As a consequence, we prove that unambiguous
weighted two-way automata are equivalent to deterministic weighted two-way
automata in commutative semirings.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1212.0336v1,Analytical model of misinformation of a social network node,"This paper presents the research of the influence of cognitive, behavioral,
representational factors on the susceptibility of the participants in social
networks to misinformation, as well as on the activity of the nodes in this
regard. The importance of this research consists of method of blocking the
propaganda. This is very important because when people involuntarily acquire
information some of them experience an undesired change in their social
attitude. Such phenomena typically lead towards the information warfare. A
model was developed during this research for calculating the level of
misinformation of the social network participant (network node) based on the
model of iterative learning process.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1502.02127v2,Hyperparameter Search in Machine Learning,"We introduce the hyperparameter search problem in the field of machine
learning and discuss its main challenges from an optimization perspective.
Machine learning methods attempt to build models that capture some element of
interest based on given data. Most common learning algorithms feature a set of
hyperparameters that must be determined before training commences. The choice
of hyperparameters can significantly affect the resulting model's performance,
but determining good values can be complex; hence a disciplined, theoretically
sound search strategy is essential.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1305.4917v1,Note on Evaluation of Hierarchical Modular Systems,"This survey note describes a brief systemic view to approaches for evaluation
of hierarchical composite (modular) systems. The list of considered issues
involves the following: (i) basic assessment scales (quantitative scale,
ordinal scale, multicriteria description, two kinds of poset-like scales), (ii)
basic types of scale transformations problems, (iii) basic types of scale
integration methods. Evaluation of the modular systems is considered as
assessment of system components (and their compatibility) and integration of
the obtained local estimates into the total system estimate(s). This process is
based on the above-mentioned problems (i.e., scale transformation and
integration). Illustrations of the assessment problems and evaluation
approaches are presented (including numerical examples).",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1801.02705v3,"Analyzing Mobility-Traffic Correlations in Large WLAN Traces: Flutes vs.
  Cellos","Two major factors affecting mobile network performance are mobility and
traffic patterns. Simulations and analytical-based performance evaluations rely
on models to approximate factors affecting the network. Hence, the
understanding of mobility and traffic is imperative to the effective evaluation
and efficient design of future mobile networks. Current models target either
mobility or traffic, but do not capture their interplay. Many trace-based
mobility models have largely used pre-smartphone datasets (e.g., AP-logs), or
much coarser granularity (e.g., cell-towers) traces. This raises questions
regarding the relevance of existing models, and motivates our study to revisit
this area. In this study, we conduct a multidimensional analysis, to
quantitatively characterize mobility and traffic spatio-temporal patterns, for
laptops and smartphones, leading to a detailed integrated mobility-traffic
analysis. Our study is data-driven, as we collect and mine capacious datasets
(with 30TB, 300k devices) that capture all of these dimensions. The
investigation is performed using our systematic (FLAMeS) framework. Overall,
dozens of mobility and traffic features have been analyzed. The insights and
lessons learnt serve as guidelines and a first step towards future integrated
mobility-traffic models. In addition, our work acts as a stepping-stone towards
a richer, more-realistic suite of mobile test scenarios and benchmarks.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1701.00082v1,"A computational investigation of the relationships between single-neuron
  and network dynamics in the cerebral cortex","Functions of brain areas in complex animals are believed to rely on the
dynamics of networks of neurons rather than on single neurons. On the other
hand, the network dynamics reflect and arise from the integration and
coordination of the activity of populations of single neurons. Understanding
how single-neurons and neural-circuits dynamics complement each other to
produce brain functions is thus of paramount importance. LFPs and EEGs are good
indicators of the dynamics of mesoscopic and macroscopic populations of
neurons, while microscopic-level activities can be documented by measuring the
membrane potential, the synaptic currents or the spiking activity of individual
neurons. In this thesis we develop mathematical modelling and mathematical
analysis tools that can help the interpretation of joint measures of neural
activity at microscopic and mesoscopic or macroscopic scales. In particular, we
develop network models of recurrent cortical circuits that can clarify the
impact of several aspects of single-neuron (i.e., microscopic-level) dynamics
on the activity of the whole neural population (as measured by LFP). We then
develop statistical tools to characterize the relationship between the action
potential firing of single neurons and mass signals. We apply these latter
analysis techniques to joint recordings of the firing activity of individual
cell-type identified neurons and mesoscopic (i.e., LFP) and macroscopic (i.e.,
EEG) signals in the mouse neocortex. We identified several general aspects of
the relationship between cell-specific neural firing and mass circuit activity,
providing for example general and robust mathematical rules which infer
single-neuron firing activity from mass measures such as the LFP and the EEG.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1009.2259v1,Theory of processes,"The book gives a detailed exposition of basic concepts and results of a
theory of processes. The presentation of theoretical concepts and results is
accompanied with illustrations of their application to solving various problems
of verification of processes. Along with well-known results there are presented
author's results related to verification of processes with message passing, and
there are given examples of an application of these results.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1903.10187v6,"Designing Normative Theories for Ethical and Legal Reasoning: LogiKEy
  Framework, Methodology, and Tool Support","A framework and methodology---termed LogiKEy---for the design and engineering
of ethical reasoners, normative theories and deontic logics is presented. The
overall motivation is the development of suitable means for the control and
governance of intelligent autonomous systems. LogiKEy's unifying formal
framework is based on semantical embeddings of deontic logics, logic
combinations and ethico-legal domain theories in expressive classic
higher-order logic (HOL). This meta-logical approach enables the provision of
powerful tool support in LogiKEy: off-the-shelf theorem provers and model
finders for HOL are assisting the LogiKEy designer of ethical intelligent
agents to flexibly experiment with underlying logics and their combinations,
with ethico-legal domain theories, and with concrete examples---all at the same
time. Continuous improvements of these off-the-shelf provers, without further
ado, leverage the reasoning performance in LogiKEy. Case studies, in which the
LogiKEy framework and methodology has been applied and tested, give evidence
that HOL's undecidability often does not hinder efficient experimentation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1008.3889v1,Connectivity of Graphs Induced by Directional Antennas,"This paper addresses the problem of finding an orientation and a minimum
radius for directional antennas of a fixed angle placed at the points of a
planar set S, that induce a strongly connected communication graph. We consider
problem instances in which antenna angles are fixed at 90 and 180 degrees, and
establish upper and lower bounds for the minimum radius necessary to guarantee
strong connectivity. In the case of 90-degree angles, we establish a lower
bound of 2 and an upper bound of 7. In the case of 180-degree angles, we
establish a lower bound of sqrt(3) and an upper bound of 1+sqrt(3). Underlying
our results is the assumption that the unit disk graph for S is connected.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.14657v1,"A new system for evaluating brand importance: A use case from the
  fashion industry","Today brand managers and marketing specialists can leverage huge amount of
data to reveal patterns and trends in consumer perceptions, monitoring positive
or negative associations of brands with respect to desired topics. In this
study, we apply the Semantic Brand Score (SBS) indicator to assess brand
importance in the fashion industry. To this purpose, we measure and visualize
text data using the SBS Business Intelligence App (SBS BI), which relies on
methods and tools of text mining and social network analysis. We collected and
analyzed about 206,000 tweets that mentioned the fashion brands Fendi, Gucci
and Prada, during the period from March 5 to March 12, 2021. From the analysis
of the three SBS dimensions - prevalence, diversity and connectivity - we found
that Gucci dominated the discourse, with high values of SBS. We use this case
study as an example to present a new system for evaluating brand importance and
image, through the analysis of (big) textual data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9808004v1,"Differentiated End-to-End Internet Services using a Weighted
  Proportional Fair Sharing TCP","In this document we study the application of weighted proportional fairness
to data flows in the Internet. We let the users set the weights of their
connections in order to maximise the utility they get from the network. When
combined with a pricing scheme where connections are billed by weight and time,
such a system is known to maximise the total utility of the network. Our study
case is a national Web cache server connected to long distance links. We
propose two ways of weighting TCP connections by manipulating some parameters
of the protocol and present results from simulations and prototypes. We finally
discuss how proportional fairness could be used to implement an Internet with
differentiated services.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/cs/0011034v1,Semantic interpretation of temporal information by abductive inference,"Besides temporal information explicitly available in verbs and adjuncts, the
temporal interpretation of a text also depends on general world knowledge and
default assumptions. We will present a theory for describing the relation
between, on the one hand, verbs, their tenses and adjuncts and, on the other,
the eventualities and periods of time they represent and their relative
temporal locations.
  The theory is formulated in logic and is a practical implementation of the
concepts described in Ness Schelkens et al. We will show how an abductive
resolution procedure can be used on this representation to extract temporal
information from texts.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0803.1754v1,"A Novel Approach to Formulae Production and Overconfidence Measurement
  to Reduce Risk in Spreadsheet Modelling","Research on formulae production in spreadsheets has established the practice
as high risk yet unrecognised as such by industry. There are numerous software
applications that are designed to audit formulae and find errors. However these
are all post creation, designed to catch errors before the spreadsheet is
deployed. As a general conclusion from EuSpRIG 2003 conference it was decided
that the time has come to attempt novel solutions based on an understanding of
human factors. Hence in this paper we examine one such possibility namely a
novel example driven modelling approach. We discuss a control experiment that
compares example driven modelling against traditional approaches over several
progressively more difficult tests. The results are very interesting and
certainly point to the value of further investigation of the example driven
potential. Lastly we propose a method for statistically analysing the problem
of overconfidence in spreadsheet modellers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1
http://arxiv.org/abs/1706.09551v1,Toward Inverse Control of Physics-Based Sound Synthesis,"Long Short-Term Memory networks (LSTMs) can be trained to realize inverse
control of physics-based sound synthesizers. Physics-based sound synthesizers
simulate the laws of physics to produce output sound according to input gesture
signals. When a user's gestures are measured in real time, she or he can use
them to control physics-based sound synthesizers, thereby creating simulated
virtual instruments. An intriguing question is how to program a computer to
learn to play such physics-based models. This work demonstrates that LSTMs can
be trained to accomplish this inverse control task with four physics-based
sound synthesizers.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.06479v2,"Advances in Machine and Deep Learning for Modeling and Real-time
  Detection of Multi-Messenger Sources","We live in momentous times. The science community is empowered with an
arsenal of cosmic messengers to study the Universe in unprecedented detail.
Gravitational waves, electromagnetic waves, neutrinos and cosmic rays cover a
wide range of wavelengths and time scales. Combining and processing these
datasets that vary in volume, speed and dimensionality requires new modes of
instrument coordination, funding and international collaboration with a
specialized human and technological infrastructure. In tandem with the advent
of large-scale scientific facilities, the last decade has experienced an
unprecedented transformation in computing and signal processing algorithms. The
combination of graphics processing units, deep learning, and the availability
of open source, high-quality datasets, have powered the rise of artificial
intelligence. This digital revolution now powers a multi-billion dollar
industry, with far-reaching implications in technology and society. In this
chapter we describe pioneering efforts to adapt artificial intelligence
algorithms to address computational grand challenges in Multi-Messenger
Astrophysics. We review the rapid evolution of these disruptive algorithms,
from the first class of algorithms introduced in early 2017, to the
sophisticated algorithms that now incorporate domain expertise in their
architectural design and optimization schemes. We discuss the importance of
scientific visualization and extreme-scale computing in reducing
time-to-insight and obtaining new knowledge from the interplay between models
and data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0501044v1,Augmented Segmentation and Visualization for Presentation Videos,"We investigate methods of segmenting, visualizing, and indexing presentation
videos by separately considering audio and visual data. The audio track is
segmented by speaker, and augmented with key phrases which are extracted using
an Automatic Speech Recognizer (ASR). The video track is segmented by visual
dissimilarities and augmented by representative key frames. An interactive user
interface combines a visual representation of audio, video, text, and key
frames, and allows the user to navigate a presentation video. We also explore
clustering and labeling of speaker data and present preliminary results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0402036v1,"Towards a Model-Based Framework for Integrating Usability and Software
  Engineering Life Cycles","In this position paper we propose a process model that provides a development
infrastructure in which the usability engineering and software engineering life
cycles co-exist in complementary roles. We describe the motivation, hurdles,
rationale, arguments, and implementation plan for the need, specification, and
the usefulness of such a model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1512.01996v1,A bloated FM-index reducing the number of cache misses during the search,"The FM-index is a well-known compressed full-text index, based on the
Burrows-Wheeler transform (BWT). During a pattern search, the BWT sequence is
accessed at ""random"" locations, which is cache-unfriendly. In this paper, we
are interested in speeding up the FM-index by working on $q$-grams rather than
individual characters, at the cost of using more space. The first presented
variant is related to an inverted index on $q$-grams, yet the occurrence lists
in our solution are in the sorted suffix order rather than text order in a
traditional inverted index. This variant obtains $O(m/|CL| + \log n \log m)$
cache misses in the worst case, where $n$ and $m$ are the text and pattern
lengths, respectively, and $|CL|$ is the CPU cache line size, in symbols
(typically 64 in modern hardware). This index is often several times faster
than the fastest known FM-indexes (especially for long patterns), yet the space
requirements are enormous, $O(n\log^2 n)$ bits in theory and about $80n$-$95n$
bytes in practice. For this reason, we dub our approach FM-bloated. The second
presented variant requires $O(n\log n)$ bits of space.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0906.0049v1,"Towards Automated Deduction in Blackmail Case Analysis with Forensic
  Lucid","This work-in-progress focuses on the refinement of application of the
intensional logic to cyberforensic analysis and its benefits are compared with
the finite-state automata approach. This work extends the use of the scientific
intensional programming paradigm onto modeling and implementation of a
cyberforensics investigation process with the backtrace of event
reconstruction, modeling the evidence as multidimensional hierarchical
contexts, and proving or disproving the claims with it in the intensional
manner of evaluation. This is a practical, context-aware improvement over the
finite state automata (FSA) approach we have seen in the related works. As a
base implementation language model we use in this approach is a new dialect of
the Lucid programming language, that we call Forensic Lucid and in this paper
we focus on defining hierarchical contexts based on the intensional logic for
the evaluation of cyberforensic expressions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1003.3370v1,Adding HL7 version 3 data types to PostgreSQL,"The HL7 standard is widely used to exchange medical information
electronically. As a part of the standard, HL7 defines scalar communication
data types like physical quantity, point in time and concept descriptor but
also complex types such as interval types, collection types and probabilistic
types. Typical HL7 applications will store their communications in a database,
resulting in a translation from HL7 concepts and types into database types.
Since the data types were not designed to be implemented in a relational
database server, this transition is cumbersome and fraught with programmer
error. The purpose of this paper is two fold. First we analyze the HL7 version
3 data type definitions and define a number of conditions that must be met, for
the data type to be suitable for implementation in a relational database. As a
result of this analysis we describe a number of possible improvements in the
HL7 specification. Second we describe an implementation in the PostgreSQL
database server and show that the database server can effectively execute
scientific calculations with units of measure, supports a large number of
operations on time points and intervals, and can perform operations that are
akin to a medical terminology server. Experiments on synthetic data show that
the user defined types perform better than an implementation that uses only
standard data types from the database server.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.09797v2,"Characterizing Positively Invariant Sets: Inductive and Topological
  Methods","We present two characterizations of positive invariance of sets under the
flow of systems of ordinary differential equations. The first characterization
uses inward sets which intuitively collect those points from which the flow
evolves within the set for a short period of time, whereas the second
characterization uses the notion of exit sets, which intuitively collect those
points from which the flow immediately leaves the set. Our proofs emphasize the
use of the real induction principle as a generic and unifying proof technique
that captures the essence of the formal reasoning justifying our results and
provides cleaner alternative proofs of known results. The two characterizations
presented in this article, while essentially equivalent, lead to two rather
different decision procedures (termed respectively LZZ and ESE) for checking
whether a given semi-algebraic set is positively invariant under the flow of a
system of polynomial ordinary differential equations. The procedure LZZ
improves upon the original work by Liu, Zhan and Zhao (EMSOFT 2011). The
procedure ESE, introduced in this article, works by splitting the problem, in a
principled way, into simpler sub-problems that are easier to check, and is
shown to exhibit substantially better performance compared to LZZ on problems
featuring semi-algebraic sets described by formulas with non-trivial Boolean
structure.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1304.3121v1,Decomposing Petri nets,"In recent work, the second and third authors introduced a technique for
reachability checking in 1-bounded Petri nets, based on wiring decompositions,
which are expressions in a fragment of the compositional algebra of nets with
boundaries. Here we extend the technique to the full algebra and introduce the
related structural property of decomposition width on directed hypergraphs.
Small decomposition width is necessary for the applicability of the
reachability checking algorithm. We give examples of families of nets with
constant decomposition width and develop the underlying theory of
decompositions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.00513v1,"Optimizing Communication by Compression for Multi-GPU Scalable
  Breadth-First Searches","The Breadth First Search (BFS) algorithm is the foundation and building block
of many higher graph-based operations such as spanning trees, shortest paths
and betweenness centrality. The importance of this algorithm increases each day
due to it is a key requirement for many data structures which are becoming
popular nowadays. When the BFS algorithm is parallelized by distributing the
graph between several processors the interconnection network limits the
performance. Hence, improvements on this area may benefit the overall
performance of the algorithm.
  This work presents an alternative compression scheme for communications in
distributed BFS processing. It focuses on BFS processors using General-Purpose
Graphics Processing Units.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0709.2512v2,Quantifying Homology Classes II: Localization and Stability,"In the companion paper, we measured homology classes and computed the optimal
homology basis. This paper addresses two related problems, namely, localization
and stability. We localize a class with the cycle minimizing a certain
objective function. We explore three different objective functions, namely,
volume, diameter and radius. We show that it is NP-hard to compute the smallest
cycle using the former two. We also prove that the measurement defined in the
companion paper is stable with regard to small changes of the geometry of the
concerned space.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.06275v1,"A Restless Bandit Model for Energy-Efficient Job Assignments in Server
  Farms","We aim to maximize the energy efficiency, gauged as average energy cost per
job, in a large-scale server farm with various storage or/and computing
components, which are modeled as parallel abstracted servers. Each server works
in multiple power modes characterized by potentially different service and
energy consumption rates. The heterogeneity of servers and multiple power modes
significantly complicate the maximization problem, where optimal solutions are
generally intractable. Relying on the Whittle relaxation technique, we resort
to a near-optimal and scalable job-assignment policy. Under certain conditions
including the assumption of exponentially distributed job sizes, we prove that
our proposed policy approaches optimality as the size of the entire system
tends to infinity; that is, it is asymptotically optimal. Nevertheless, we
demonstrate by simulations that the effectiveness of our policies is not
significantly limited by the conditions used for mathematical rigor and that
our model still has wide practical applicability. In particular, the asymptotic
optimality is very much relevant for many real-world large-scale systems with
tens or hundreds of thousands of components, where conventional optimization
techniques can hardly apply. Furthermore, for non-asymptotic scenarios, we show
the effectiveness of the proposed policy through extensive numerical
simulations, where the policy substantially outperforms all the tested
baselines, and we especially demonstrate numerically its robustness against
heavy-tailed job-size distributions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1602.04257v1,Identifying Diabetic Patients with High Risk of Readmission,"Hospital readmissions are expensive and reflect the inadequacies in
healthcare system. In the United States alone, treatment of readmitted diabetic
patients exceeds 250 million dollars per year. Early identification of patients
facing a high risk of readmission can enable healthcare providers to to conduct
additional investigations and possibly prevent future readmissions. This not
only improves the quality of care but also reduces the medical expenses on
readmission. Machine learning methods have been leveraged on public health data
to build a system for identifying diabetic patients facing a high risk of
future readmission. Number of inpatient visits, discharge disposition and
admission type were identified as strong predictors of readmission. Further, it
was found that the number of laboratory tests and discharge disposition
together predict whether the patient will be readmitted shortly after being
discharged from the hospital (i.e. <30 days) or after a longer period of time
(i.e. >30 days). These insights can help healthcare providers to improve
inpatient diabetic care. Finally, the cost analysis suggests that \$252.76
million can be saved across 98,053 diabetic patient encounters by incorporating
the proposed cost sensitive analysis model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0002001v2,Computing large and small stable models,"In this paper, we focus on the problem of existence and computing of small
and large stable models. We show that for every fixed integer k, there is a
linear-time algorithm to decide the problem LSM (large stable models problem):
does a logic program P have a stable model of size at least |P|-k. In contrast,
we show that the problem SSM (small stable models problem) to decide whether a
logic program P has a stable model of size at most k is much harder. We present
two algorithms for this problem but their running time is given by polynomials
of order depending on k. We show that the problem SSM is fixed-parameter
intractable by demonstrating that it is W[2]-hard. This result implies that it
is unlikely, an algorithm exists to compute stable models of size at most k
that would run in time O(n^c), where c is a constant independent of k. We also
provide an upper bound on the fixed-parameter complexity of the problem SSM by
showing that it belongs to the class W[3].",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.05390v1,Seven challenges for harmonizing explainability requirements,"Regulators have signalled an interest in adopting explainable AI(XAI)
techniques to handle the diverse needs for model governance, operational
servicing, and compliance in the financial services industry. In this short
overview, we review the recent technical literature in XAI and argue that based
on our current understanding of the field, the use of XAI techniques in
practice necessitate a highly contextualized approach considering the specific
needs of stakeholders for particular business applications.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0
http://arxiv.org/abs/1908.09291v1,Extending TensorFlow's Semantics with Pipelined Execution,"TensorFlow is a popular cloud computing framework that targets machine
learning applications. It separates the specification of application logic (in
a dataflow graph) from the execution of the logic. TensorFlow's native runtime
executes the application with low overhead across a diverse set of hardware
including CPUs, GPUs, and ASICs. Although the underlying dataflow engine
supporting these features could be applied to computations beyond machine
learning, certain design decisions limit this broader application, such as the
inability for an application to differentiate between data items across
concurrent requests.
  This paper introduces Pipelined TensorFlow (PTF), a system that extends
TensorFlow's semantics to provide support for a broader variety of application
logic. In particular, PTF supports applications that concurrently process
finite batches of data on a single instantiation. PTF adds these semantics by
partitioning the dataflow graph into a pipeline of smaller graphs and tagging
each data item with metadata. These smaller graphs are separated by gates: new
data structures in PTF that buffer data items between graphs and interpret the
metadata to apply the new semantics. PTF's pipeline architecture executes on an
unmodified TensorFlow runtime, maintaining compatibility with many existing
TensorFlow library functions. Our evaluation shows that the pipelining
mechanism of PTF can increase the throughput of a bioinformatics application by
4$\times$ while only increasing its latency by 0.13$\times$. This results in a
sustained genome alignment and sorting rate of 321 megabases/second, using the
compute and I/O resources of 20 computers.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1502.06460v2,"Visualizing BACnet data to facilitate humans in building-security
  decision-making","Building automation systems (BAS) are interlinked networks of hardware and
software, which monitor and control events in the buildings. One of the data
communication protocols used in BAS is Building Automation and Control
networking protocol (BACnet) which is an internationally adopted ISO standard
for the communication between BAS devices. Although BAS focus on providing
safety for inhabitants, decreasing the energy consumption of buildings and
reducing their operational cost, their security suffers due to the inherent
complexity of the modern day systems. The issues such as monitoring of BAS
effectively present a significant challenge, i.e., BAS operators generally
possess only partial situation awareness. Especially in large and
inter-connected buildings, the operators face the challenge of spotting
meaningful incidents within large amounts of simultaneously occurring events,
causing the anomalies in the BAS network to go unobserved. In this paper, we
present the techniques to analyze and visualize the data for several events
from BAS devices in a way that determines the potential importance of such
unusual events and helps with the building-security decision making. We
implemented these techniques as a mobile (Android) based application for
displaying application data and as tools to analyze the communication flows
using directed graphs.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1912.10026v1,"Calibration and reference simulations for the auditory periphery model
  of Verhulst et al. 2018 version 1.2","This document describes a comprehensive procedure of how the biophysical
model published by Verhulst et al. (2018) can be calibrated on the basis of
reference auditory brainstem responses. Additionally, the filter design used in
two of the model stages, cochlear nucleus (CN) and inferior colliculus (IC), is
described in detail. These descriptions are valid for a new release of the
Verhulst et al. model, version 1.2, as well as for previous versions of the
model (version 1.1 or earlier). The differences between the model versions are
explicitly mentioned and simulations to basic auditory stimuli are shown for
model versions 1.1 and 1.2. In short, version 1.2 of the model includes a new
implementation of the CN and IC stages (Stages 5 and 6). All previous model
stages (Stages 1-4: outer and middle ear, transmission-line cochlear filter
bank, inner hair cell model, and auditory nerve model) remained unchanged. In
the new release (model version 1.2), in addition to the updated CN and IC
stages, we employed a different calibration procedure to match human reference
ABR amplitudes of waves I, III, and V more faithfully. This release note shows
the implications of these model adjustments on the simulations presented in the
original 2018 model paper. For this purpose, results from two model versions
are reported: (1) New model release (version 1.2), labelled as model v1.2; and
(2) Previous model release as used by Verhulst et al., labelled as model v1.1.
The main difference between IC model stages relates to the degree of IC
inhibition that was applied, with more inhibition in v1.2 than implemented in
v1.1. The time domain simulations presented in this document show that this
change in inhibition strength does not drastically change the results presented
in the original paper. However, v1.2 more correctly captures the
physiologically derived CN and IC inhibition/excitation strengths.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1403.7899v1,Identifying User Behavior in domain-specific Repositories,"This paper presents an analysis of the user behavior of two different
domain-specific repositories. The web analytic tool etracker was used to gain a
first overall insight into the user behavior of these repositories. Moreover,
we extended our work to describe an apache web log analysis approach which
focuses on the identification of the user behavior. Therefore the user traffic
within our systems is visualized using chord diagrams. We could find that
recommendations are used frequently and users do rarely combine searching with
faceting or filtering.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1007.1274v1,Development of a Context Aware Virtual Smart Home Simulator,"Context awareness is the most important research area in ubiquitous
computing. In particular, for smart home, context awareness attempts to bring
the best services to the home habitants. However, the implementation in the
real environment is not easy and takes a long time from building the scratch.
Thus, to support the implementation in the real smart home, it is necessary to
demonstrate that thing can be done in the simulator in which context
information can be created by virtual sensors instead of physical sensors. In
this paper, we propose ISS, an Interactive Smart home Simulator system aiming
at controlling and simulating the behavior of an intelligent house. The
developed system aims to provide architects, designers a simulation and useful
tool for understanding the interaction between environment, people and the
impact of embedded and pervasive technology on in daily life. In this research,
the smart house is considered as an environment made up of independent and
distributed devices interacting to support user's goals and tasks. Therefore,
by using ISS, the developer can realize the relationship among virtual home
space, surrounded environment, use and home appliances.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0011007v1,Tree-gram Parsing: Lexical Dependencies and Structural Relations,"This paper explores the kinds of probabilistic relations that are important
in syntactic disambiguation. It proposes that two widely used kinds of
relations, lexical dependencies and structural relations, have complementary
disambiguation capabilities. It presents a new model based on structural
relations, the Tree-gram model, and reports experiments showing that structural
relations should benefit from enrichment by lexical dependencies.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/2012.09737v2,"Model-free and Bayesian Ensembling Model-based Deep Reinforcement
  Learning for Particle Accelerator Control Demonstrated on the FERMI FEL","Reinforcement learning holds tremendous promise in accelerator controls. The
primary goal of this paper is to show how this approach can be utilised on an
operational level on accelerator physics problems. Despite the success of
model-free reinforcement learning in several domains, sample-efficiency still
is a bottle-neck, which might be encompassed by model-based methods. We compare
well-suited purely model-based to model-free reinforcement learning applied to
the intensity optimisation on the FERMI FEL system. We find that the
model-based approach demonstrates higher representational power and
sample-efficiency, while the asymptotic performance of the model-free method is
slightly superior. The model-based algorithm is implemented in a DYNA-style
using an uncertainty aware model, and the model-free algorithm is based on
tailored deep Q-learning. In both cases, the algorithms were implemented in a
way, which presents increased noise robustness as omnipresent in accelerator
control problems. Code is released in
https://github.com/MathPhysSim/FERMI_RL_Paper.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1201.3671v1,"Visualizing Flat Spacetime: Viewing Optical versus Special Relativistic
  Effects","A simple visual representation of Minkowski spacetime appropriate for a
student with a background in geometry and algebra is presented. Minkowski
spacetime can be modeled with a Euclidean 4-space to yield accurate
visualizations as predicted by special relativity theory. The contributions of
relativistic aberration as compared to classical pre-relativistic aberration to
the geometry are discussed in the context of its visual representation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1203.2031v1,Design of modular wireless sensor,"The paper addresses combinatorial approach to design of modular wireless
sensor as composition of the sensor element from its component alternatives and
aggregation of the obtained solutions into a resultant aggregated solution. A
hierarchical model is used for the wireless sensor element. The solving process
consists of three stages: (i) multicriteria ranking of design alternatives for
system components/parts, (ii) composing the selected design alternatives into
composite solution(s) while taking into account ordinal quality of the design
alternatives above and their compatibility (this stage is based on Hierarchical
Morphological Multicriteria Design - HMMD), and (iii) aggregation of the
obtained composite solutions into a resultant aggregated solution(s). A
numerical example describes the problem structuring and solving processes for
modular alarm wireless sensor element.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0406026v1,Improving Prolog Programs: Refactoring for Prolog,"Refactoring is an established technique from the OO-community to restructure
code: it aims at improving software readability, maintainability and
extensibility. Although refactoring is not tied to the OO-paradigm in
particular, its ideas have not been applied to Logic Programming until now.
  This paper applies the ideas of refactoring to Prolog programs. A catalogue
is presented listing refactorings classified according to scope. Some of the
refactorings have been adapted from the OO-paradigm, while others have been
specifically designed for Prolog. Also the discrepancy between intended and
operational semantics in Prolog is addressed by some of the refactorings.
  In addition, ViPReSS, a semi-automatic refactoring browser, is discussed and
the experience with applying \vipress to a large Prolog legacy system is
reported. Our main conclusion is that refactoring is not only a viable
technique in Prolog but also a rather desirable one.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0507064v1,Termination of rewriting strategies: a generic approach,"We propose a generic termination proof method for rewriting under strategies,
based on an explicit induction on the termination property. Rewriting trees on
ground terms are modeled by proof trees, generated by alternatively applying
narrowing and abstracting steps. The induction principle is applied through the
abstraction mechanism, where terms are replaced by variables representing any
of their normal forms. The induction ordering is not given a priori, but
defined with ordering constraints, incrementally set during the proof.
Abstraction constraints can be used to control the narrowing mechanism, well
known to easily diverge. The generic method is then instantiated for the
innermost, outermost and local strategies.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9809105v1,Hyper-Systolic Matrix Multiplication,"A novel parallel algorithm for matrix multiplication is presented. The
hyper-systolic algorithm makes use of a one-dimensional processor abstraction.
The procedure can be implemented on all types of parallel systems. It can
handle matrix-vector multiplications as well as transposed matrix products.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0909.5064v1,A Conceivable Origin of Machine Consciousness in the IDLE process,"In this short paper, we would like to call professional community's attention
to a daring idea that is surely unhelpful, but is exciting for programmers and
anyway conflicts with the trend of energy consumption in computer systems.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.10708v1,H.264/SVC Mode Decision Based on Mode Correlation and Desired Mode List,"The design of video encoders involves the implementation of fast mode
decision (FMD) algorithm to reduce computation complexity while maintaining the
performance of the coding. Although H.264/scalable video coding (SVC) achieves
high scalability and coding efficiency, it also has high complexity in
implementing its exhaustive computation. In this paper, a novel algorithm is
proposed to reduce the redundant candidate modes by making use of the
correlation among layers. The desired mode list is created based on the
probability to be the best mode for each block in the base layer and a
candidate mode selection in the enhancement layer by the correlations of modes
among the reference frame and current frame. Our algorithm is implemented in
joint scalable video model (JSVM) 9.19.15 reference software and the
performance is evaluated based on the average encoding time, peak signal to
noise ratio (PSNR) and bit rate. The experimental results show 41.89%
improvement in encoding time with minimal loss of 0.02dB in PSNR and 0.05%
increase in bit rate.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1806.03674v2,On the Covariance-Hessian Relation in Evolution Strategies,"We consider Evolution Strategies operating only with isotropic Gaussian
mutations on positive quadratic objective functions, and investigate the
covariance matrix when constructed out of selected individuals by truncation.
We prove that the covariance matrix over $(1,\lambda)$-selected decision
vectors becomes proportional to the inverse of the landscape Hessian as the
population-size $\lambda$ increases. This generalizes a previous result that
proved an equivalent phenomenon when sampling was assumed to take place in the
vicinity of the optimum. It further confirms the classical hypothesis that
statistical learning of the landscape is an inherent characteristic of standard
Evolution Strategies, and that this distinguishing capability stems only from
the usage of isotropic Gaussian mutations and rank-based selection. We provide
broad numerical validation for the proven results, and present empirical
evidence for its generalization to $(\mu,\lambda)$-selection.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1208.2768v1,Transductions Computed by One-Dimensional Cellular Automata,"Cellular automata are investigated towards their ability to compute
transductions, that is, to transform inputs into outputs. The families of
transductions computed are classified with regard to the time allowed to
process the input and to compute the output. Since there is a particular
interest in fast transductions, we mainly focus on the time complexities real
time and linear time. We first investigate the computational capabilities of
cellular automaton transducers by comparing them to iterative array
transducers, that is, we compare parallel input/output mode to sequential
input/output mode of massively parallel machines. By direct simulations, it
turns out that the parallel mode is not weaker than the sequential one.
Moreover, with regard to certain time complexities cellular automaton
transducers are even more powerful than iterative arrays. In the second part of
the paper, the model in question is compared with the sequential devices
single-valued finite state transducers and deterministic pushdown transducers.
It turns out that both models can be simulated by cellular automaton
transducers faster than by iterative array transducers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1302.5851v1,Parallel Suffix Array Construction by Accelerated Sampling,"A deterministic BSP algorithm for constructing the suffix array of a given
string is presented, based on a technique which we call accelerated sampling.
It runs in optimal O(n/p) local computation and communication, and requires a
near optimal O(log log p) synchronisation steps. The algorithm provides an
improvement over the synchronisation costs of existing algorithms, and
reinforces the importance of the sampling technique.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1605.06289v1,"Evolution Patterns: Designing and Reusing Architectural Evolution
  Knowledge to Introduce Architectural Styles","Software architectures are critical in the successful development and
evolution of software-intensive systems. While formal and automated support for
architectural descriptions has been widely addressed, their evolution is
equally crucial, but significantly less well-understood and supported. In order
to face a recurring evolution need, we introduce the concept of evolution
pattern. It formalises an architectural evolution through both a set of
concepts and a reusable evolution process. We propose it through the recurring
need of introducing an architectural style on existing software architectures.
We formally describe and analyse the feasibility of architectural evolution
patterns, and provide a practical validation by implementing them in
COSABuilder, an Eclipse plugin for the COSA architectural description language.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0103003v1,Learning Policies with External Memory,"In order for an agent to perform well in partially observable domains, it is
usually necessary for actions to depend on the history of observations. In this
paper, we explore a {\it stigmergic} approach, in which the agent's actions
include the ability to set and clear bits in an external memory, and the
external memory is included as part of the input to the agent. In this case, we
need to learn a reactive policy in a highly non-Markovian domain. We explore
two algorithms: SARSA(\lambda), which has had empirical success in partially
observable domains, and VAPS, a new algorithm due to Baird and Moore, with
convergence guarantees in partially observable domains. We compare the
performance of these two algorithms on benchmark problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1910.06954v3,"Context Matters: Recovering Human Semantic Structure from Machine
  Learning Analysis of Large-Scale Text Corpora","Applying machine learning algorithms to large-scale, text-based corpora
(embeddings) presents a unique opportunity to investigate at scale how human
semantic knowledge is organized and how people use it to judge fundamental
relationships, such as similarity between concepts. However, efforts to date
have shown a substantial discrepancy between algorithm predictions and
empirical judgments. Here, we introduce a novel approach of generating
embeddings motivated by the psychological theory that semantic context plays a
critical role in human judgments. Specifically, we train state-of-the-art
machine learning algorithms using contextually-constrained text corpora and
show that this greatly improves predictions of similarity judgments and feature
ratings. By improving the correspondence between representations derived using
embeddings generated by machine learning methods and empirical measurements of
human judgments, the approach we describe helps advance the use of large-scale
text corpora to understand the structure of human semantic representations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1111.1823v4,A generalized palindromization map in free monoids,"The palindromization map $\psi$ in a free monoid $A^*$ was introduced in 1997
by the first author in the case of a binary alphabet $A$, and later extended by
other authors to arbitrary alphabets. Acting on infinite words, $\psi$
generates the class of standard episturmian words, including standard
Arnoux-Rauzy words. In this paper we generalize the palindromization map,
starting with a given code $X$ over $A$. The new map $\psi_X$ maps $X^*$ to the
set $PAL$ of palindromes of $A^*$. In this way some properties of $\psi$ are
lost and some are saved in a weak form. When $X$ has a finite deciphering delay
one can extend $\psi_X$ to $X^{\omega}$, generating a class of infinite words
much wider than standard episturmian words. For a finite and maximal code $X$
over $A$, we give a suitable generalization of standard Arnoux-Rauzy words,
called $X$-AR words. We prove that any $X$-AR word is a morphic image of a
standard Arnoux-Rauzy word and we determine some suitable linear lower and
upper bounds to its factor complexity.
  For any code $X$ we say that $\psi_X$ is conservative when
$\psi_X(X^{*})\subseteq X^{*}$. We study conservative maps $\psi_X$ and
conditions on $X$ assuring that $\psi_X$ is conservative. We also investigate
the special case of morphic-conservative maps $\psi_{X}$, i.e., maps such that
$\phi\circ \psi = \psi_X\circ \phi$ for an injective morphism $\phi$. Finally,
we generalize $\psi_X$ by replacing palindromic closure with
$\theta$-palindromic closure, where $\theta$ is any involutory antimorphism of
$A^*$. This yields an extension of the class of $\theta$-standard words
introduced by the authors in 2006.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.15590v2,VPNet: Variable Projection Networks,"We introduce VPNet, a novel model-driven neural network architecture based on
variable projection (VP). Applying VP operators to neural networks results in
learnable features, interpretable parameters, and compact network structures.
This paper discusses the motivation and mathematical background of VPNet and
presents experiments. The VPNet approach was evaluated in the context of signal
processing, where we classified a synthetic dataset and real electrocardiogram
(ECG) signals. Compared to fully connected and one-dimensional convolutional
networks, VPNet offers fast learning ability and good accuracy at a low
computational cost of both training and inference. Based on these advantages
and the promising results obtained, we anticipate a profound impact on the
broader field of signal processing, in particular on classification, regression
and clustering problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1310.5653v1,Blind and robust images watermarking based on wavelet and edge insertion,"This paper gives a new scheme of watermarking technique related to insert the
mark by adding edge in HH sub-band of the host image after wavelet
decomposition. Contrary to most of the watermarking algorithms in wavelet
domain, our method is blind and results show that it is robust against the JPEG
and GIF compression, histogram and spectrum spreading, noise adding and small
rotation. Its robustness against compression is better than others watermarking
algorithms reported in the literature. The algorithm is flexible because its
capacity or robustness can be improved by modifying some parameters.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2003.01519v1,"Amateur Drones Detection: A machine learning approach utilizing the
  acoustic signals in the presence of strong interference","Owing to small size, sensing capabilities and autonomous nature, the Unmanned
Air Vehicles (UAVs) have enormous applications in various areas, e.g., remote
sensing, navigation, archaeology, journalism, environmental science, and
agriculture. However, the unmonitored deployment of UAVs called the amateur
drones (AmDr) can lead to serious security threats and risk to human life and
infrastructure. Therefore, timely detection of the AmDr is essential for the
protection and security of sensitive organizations, human life and other vital
infrastructure. AmDrs can be detected using different techniques based on
sound, video, thermal, and radio frequencies. However, the performance of these
techniques is limited in sever atmospheric conditions. In this paper, we
propose an efficient unsupervise machine learning approach of independent
component analysis (ICA) to detect various acoustic signals i.e., sounds of
bird, airplanes, thunderstorm, rain, wind and the UAVs in practical scenario.
After unmixing the signals, the features like Mel Frequency Cepstral
Coefficients (MFCC), the power spectral density (PSD) and the Root Mean Square
Value (RMS) of the PSD are extracted by using ICA. The PSD and the RMS of PSD
signals are extracted by first passing the signals from octave band filter
banks. Based on the above features the signals are classified using Support
Vector Machines (SVM) and K Nearest Neighbor (KNN) to detect the presence or
absence of AmDr. Unique feature of the proposed technique is the detection of a
single or multiple AmDrs at a time in the presence of multiple acoustic
interfering signals. The proposed technique is verified through extensive
simulations and it is observed that the RMS values of PSD with KNN performs
better than the MFCC with KNN and SVM.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1512.06000v1,"Privacy by design in big data: An overview of privacy enhancing
  technologies in the era of big data analytics","The extensive collection and processing of personal information in big data
analytics has given rise to serious privacy concerns, related to wide scale
electronic surveillance, profiling, and disclosure of private data. To reap the
benefits of analytics without invading the individuals' private sphere, it is
essential to draw the limits of big data processing and integrate data
protection safeguards in the analytics value chain. ENISA, with the current
report, supports this approach and the position that the challenges of
technology (for big data) should be addressed by the opportunities of
technology (for privacy).
  We first explain the need to shift from ""big data versus privacy"" to ""big
data with privacy"". In this respect, the concept of privacy by design is key to
identify the privacy requirements early in the big data analytics value chain
and in subsequently implementing the necessary technical and organizational
measures.
  After an analysis of the proposed privacy by design strategies in the
different phases of the big data value chain, we review privacy enhancing
technologies of special interest for the current and future big data landscape.
In particular, we discuss anonymization, the ""traditional"" analytics technique,
the emerging area of encrypted search and privacy preserving computations,
granular access control mechanisms, policy enforcement and accountability, as
well as data provenance issues. Moreover, new transparency and access tools in
big data are explored, together with techniques for user empowerment and
control.
  Achieving ""big data with privacy"" is no easy task and a lot of research and
implementation is still needed. Yet, it remains a possible task, as long as all
the involved stakeholders take the necessary steps to integrate privacy and
data protection safeguards in the heart of big data, by design and by default.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2112.10107v1,"Expression is enough: Improving traffic signal control with advanced
  traffic state representation","Recently, finding fundamental properties for traffic state representation is
more critical than complex algorithms for traffic signal control (TSC).In this
paper, we (1) present a novel, flexible and straightforward method advanced max
pressure (Advanced-MP), taking both running and queueing vehicles into
consideration to decide whether to change current phase; (2) novelty design the
traffic movement representation with the efficient pressure and effective
running vehicles from Advanced-MP, namely advanced traffic state (ATS); (3)
develop an RL-based algorithm template Advanced-XLight, by combining ATS with
current RL approaches and generate two RL algorithms, ""Advanced-MPLight"" and
""Advanced-CoLight"". Comprehensive experiments on multiple real-world datasets
show that: (1) the Advanced-MP outperforms baseline methods, which is efficient
and reliable for deployment; (2) Advanced-MPLight and Advanced-CoLight could
achieve new state-of-the-art. Our code is released on Github.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.07385v2,Learning-Augmented Query Policies,"We study how to utilize (possibly machine-learned) predictions in a model for
computing under uncertainty in which an algorithm can query unknown data. The
goal is to minimize the number of queries needed to solve the problem. We
consider fundamental problems such as finding the minima of intersecting sets
of elements or sorting them (these problems can also be phrased as (hyper)graph
orientation problems), as well as the minimum spanning tree problem. We discuss
different measures for the prediction accuracy and design algorithms with
performance guarantees that improve with the accuracy of predictions and that
are robust with respect to very poor prediction quality. These measures are
intuitive and might be of general interest for inputs involving uncertainty
intervals. We show that our predictions are PAC learnable. We also provide new
structural insights for the minimum spanning tree problem that might be useful
in the context of explorable uncertainty regardless of predictions. Our results
prove that untrusted predictions can circumvent known lower bounds in the model
of explorable uncertainty. We complement our results by experiments that
empirically confirm the performance of our algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.13193v2,Aggregation in non-uniform systems with advection and localized source,"We explore analytically and numerically agglomeration driven by advection and
localized source. The system is inhomogeneous in one dimension, viz. along the
direction of advection. We analyze a simplified model with mass-independent
advection velocity, diffusion coefficient, and reaction rates. We also examine
a model with mass-dependent coefficients describing aggregation with
sedimentation. For the simplified model, we obtain an exact solution for the
stationary spatially dependent agglomerate densities. In the model describing
aggregation with sedimentation, we report a new conservation law and develop a
scaling theory for the densities. For numerical efficiency we exploit the
low-rank approximation technique; this dramatically increases the computational
speed and allows simulations of large systems. The numerical results are in
excellent agreement with the predictions of our theory.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0709.1207v3,The P versus NP Brief,"This paper discusses why P and NP are likely to be different. It analyses the
essence of the concepts and points out that P and NP might be diverse by sheer
definition. It also speculates that P and NP may be unequal due to natural
laws.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.07863v5,Imperative process algebra with abstraction,"This paper introduces an imperative process algebra based on ACP (Algebra of
Communicating Processes). Like other imperative process algebras, this process
algebra deals with processes of the kind that arises from the execution of
imperative programs. It distinguishes itself from already existing imperative
process algebras among other things by supporting abstraction from actions that
are considered not to be visible. The support of abstraction of this kind opens
interesting application possibilities of the process algebra. This paper goes
briefly into the possibility of information-flow security analysis of the kind
that is concerned with the leakage of confidential data. For the presented
axiomatization, soundness and semi-completeness results with respect to a
notion of branching bisimulation equivalence are established.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0107009v1,A Blueprint for Building Serverless Applications on the Net,"A peer-to-peer application architecture is proposed that has the potential to
eliminate the back-end servers for hosting services on the Internet. The
proposed application architecture has been modeled as a distributed system for
delivering an Internet service. The service thus created, though chaotic and
fraught with uncertainties, would be highly scalable and capable of achieving
unprecedented levels of robustness and viability with the increase in the
number of the users. The core issues relating to the architecture, such as
service discovery, distributed application architecture components, and
inter-application communications, have been analysed. It is shown that the
communications for the coordination of various functions, among the cooperating
instances of the application, may be optimised using a divide-and-conquer
strategy. Finally, the areas where future work needs to be directed have been
identified.",0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1411.5611v3,FREC 14: FRontiers of RECognizability,"These proceedings are gathering twelve different research papers developping
the theory of recognizability for various kinds of discrete objects: words.
terms, graphs, etc...",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1105.3716v1,"Personal Marks and Community Certificates: Detecting Clones in Mobile
  Wireless Networks of Smart-Phones","We consider the problem of detecting clones in wireless mobile adhoc
networks. We assume that one of the devices of the network has been cloned.
Everything, including certificates and secret keys. This can happen quite
easily, because of a virus that immediately after sending all the content of
the infected device to the adversary destroys itself, or just because the owner
has left his device unattended for a few minutes in a hostile environment. The
problem is to detect this attack. We propose a solution in networks of mobile
devices carried by individuals. These networks are composed by nodes that have
the capability of using short-range communication technology like blue-tooth or
Wi-Fi, where nodes are carried by mobile users, and where links appear and
disappear according to the social relationships between the users. Our idea is
to use social physical contacts, securely collected by wireless personal
smart-phones, as a biometric way to authenticate the legitimate owner of the
device and detect the clone attack. We introduce two mechanisms: Personal Marks
and Community Certificates. Personal Marks is a simple cryptographic protocol
that works very well when the adversary is an insider, a malicious node in the
network that is part, or not very far, from the social community of the
original device that has been cloned. Community Certificates work very well
when the adversary is an outsider, a node that has the goal of using the stolen
credentials when interacting with other nodes that are far in the social
network from the original device. When combined, these mechanisms provide an
excellent protection against this very strong attack. We prove our ideas and
solutions with extensive simulations in a real world scenario-with mobility
traces collected in a real life experiment",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.00519v1,"Performance of Intelligent Reconfigurable Surface-Based Wireless
  Communications Using QAM Signaling","Intelligent reconfigurable surface (IRS) is being seen as a promising
technology for 6G wireless networks. The IRS can reconfigure the wireless
propagation environment, which results in significant performance improvement
of wireless communications. In this paper, we analyze the performance of
bandwidth-efficient quadrature amplitude modulation (QAM) techniques for
IRS-assisted wireless communications over Rayleigh fading channels. New
closed-form expressions of the generic average symbol error rate (ASER) for
rectangular QAM, square QAM and cross QAM schemes are derived. Moreover,
simplified expressions of the ASER for low signal-to-noise-ratio (SNR) and high
SNR regions are also presented, which are useful to provide insights
analytically. We comprehensively analyze the impact of modulation parameters
and the number of IRS elements employed. We also verify our theoretical results
through simulations. Our results demonstrate that employing IRS significantly
enhances the ASER performance in comparison to additive white Gaussian noise
channel at a low SNR regime. Thus, IRS-assisted wireless communications can be
a promising candidate for various low powered communication applications such
as internet-of-things (IoT).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1504.06851v1,Stable Delaunay Graphs,"Let $P$ be a set of $n$ points in $\mathrm{R}^2$, and let $\mathrm{DT}(P)$
denote its Euclidean Delaunay triangulation. We introduce the notion of an edge
of $\mathrm{DT}(P)$ being {\it stable}. Defined in terms of a parameter
$\alpha>0$, a Delaunay edge $pq$ is called $\alpha$-stable, if the (equal)
angles at which $p$ and $q$ see the corresponding Voronoi edge $e_{pq}$ are at
least $\alpha$. A subgraph $G$ of $\mathrm{DT}(P)$ is called {\it $(c\alpha,
\alpha)$-stable Delaunay graph} ($\mathrm{SDG}$ in short), for some constant $c
\ge 1$, if every edge in $G$ is $\alpha$-stable and every $c\alpha$-stable of
$\mathrm{DT}(P)$ is in $G$.
  We show that if an edge is stable in the Euclidean Delaunay triangulation of
$P$, then it is also a stable edge, though for a different value of $\alpha$,
in the Delaunay triangulation of $P$ under any convex distance function that is
sufficiently close to the Euclidean norm, and vice-versa. In particular, a
$6\alpha$-stable edge in $\mathrm{DT}(P)$ is $\alpha$-stable in the Delaunay
triangulation under the distance function induced by a regular $k$-gon for $k
\ge 2\pi/\alpha$, and vice-versa. Exploiting this relationship and the analysis
in~\cite{polydel}, we present a linear-size kinetic data structure (KDS) for
maintaining an $(8\alpha,\alpha)$-$\mathrm{SDG}$ as the points of $P$ move. If
the points move along algebraic trajectories of bounded degree, the KDS
processes nearly quadratic events during the motion, each of which can
processed in $O(\log n)$ time. Finally, we show that a number of useful
properties of $\mathrm{DT}(P)$ are retained by $\mathrm{SDG}$ of $P$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.00124v3,"Breaking (Global) Barriers in Parallel Stochastic Optimization with
  Wait-Avoiding Group Averaging","Deep learning at scale is dominated by communication time. Distributing
samples across nodes usually yields the best performance, but poses scaling
challenges due to global information dissemination and load imbalance across
uneven sample lengths. State-of-the-art decentralized optimizers mitigate the
problem, but require more iterations to achieve the same accuracy as their
globally-communicating counterparts. We present Wait-Avoiding Group Model
Averaging (WAGMA) SGD, a wait-avoiding stochastic optimizer that reduces global
communication via subgroup weight exchange. The key insight is a combination of
algorithmic changes to the averaging scheme and the use of a group allreduce
operation. We prove the convergence of WAGMA-SGD, and empirically show that it
retains convergence rates similar to Allreduce-SGD. For evaluation, we train
ResNet-50 on ImageNet; Transformer for machine translation; and deep
reinforcement learning for navigation at scale. Compared with state-of-the-art
decentralized SGD variants, WAGMA-SGD significantly improves training
throughput (e.g., 2.1x on 1,024 GPUs for reinforcement learning), and achieves
the fastest time-to-solution (e.g., the highest score using the shortest
training time for Transformer).",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.02798v5,Gradient Origin Networks,"This paper proposes a new type of generative model that is able to quickly
learn a latent representation without an encoder. This is achieved using
empirical Bayes to calculate the expectation of the posterior, which is
implemented by initialising a latent vector with zeros, then using the gradient
of the log-likelihood of the data with respect to this zero vector as new
latent points. The approach has similar characteristics to autoencoders, but
with a simpler architecture, and is demonstrated in a variational autoencoder
equivalent that permits sampling. This also allows implicit representation
networks to learn a space of implicit functions without requiring a
hypernetwork, retaining their representation advantages across datasets. The
experiments show that the proposed method converges faster, with significantly
lower reconstruction error than autoencoders, while requiring half the
parameters.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0802.3475v1,"Spreadsheet Development Methodologies using Resolver: Moving
  spreadsheets into the 21st Century","We intend to demonstrate the innate problems with existing spreadsheet
products and to show how to tackle these issues using a new type of spreadsheet
program called Resolver. It addresses the issues head-on and thereby moves the
1980's ""VisiCalc paradigm"" on to match the advances in computer languages and
user requirements. Continuous display of the spreadsheet grid and the
equivalent computer program, together with the ability to interact and add code
through either interface, provides a number of new methodologies for
spreadsheet development.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1610.05156v2,Reachability Analysis of Innermost Rewriting,"We consider the problem of inferring a grammar describing the output of a
functional program given a grammar describing its input. Solutions to this
problem are helpful for detecting bugs or proving safety properties of
functional programs, and several rewriting tools exist for solving this
problem. However, known grammar inference techniques are not able to take
evaluation strategies of the program into account. This yields very imprecise
results when the evaluation strategy matters. In this work, we adapt the Tree
Automata Completion algorithm to approximate accurately the set of terms
reachable by rewriting under the innermost strategy. We formally prove that the
proposed technique is sound and precise w.r.t. innermost rewriting. We show
that those results can be extended to the leftmost and rightmost innermost
case. The algorithms for the general innermost case have been implemented in
the Timbuk reachability tool. Experiments show that it noticeably improves the
accuracy of static analysis for functional programs using the call-by-value
evaluation strategy.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.02114v1,"Winning the War by (Strategically) Losing Battles: Settling the
  Complexity of Grundy-Values in Undirected Geography","We settle two long-standing complexity-theoretical questions-open since 1981
and 1993-in combinatorial game theory (CGT).
  We prove that the Grundy value (a.k.a. nim-value, or nimber) of Undirected
Geography is PSPACE-complete to compute. This exhibits a stark contrast with a
result from 1993 that Undirected Geography is polynomial-time solvable. By
distilling to a simple reduction, our proof further establishes a dichotomy
theorem, providing a ""phase transition to intractability"" in Grundy-value
computation, sharply characterized by a maximum degree of four: The Grundy
value of Undirected Geography over any degree-three graph is polynomial-time
computable, but over degree-four graphs-even when planar and bipartite-is
PSPACE-hard. Additionally, we show, for the first time, how to construct
Undirected Geography instances with Grundy value $\ast n$ and size polynomial
in n.
  We strengthen a result from 1981 showing that sums of tractable partisan
games are PSPACE-complete in two fundamental ways. First, since Undirected
Geography is an impartial ruleset, we extend the hardness of sums to impartial
games, a strict subset of partisan. Second, the 1981 construction is not built
from a natural ruleset, instead using a long sum of tailored short-depth game
positions. We use the sum of two Undirected Geography positions to create our
hard instances. Our result also has computational implications to
Sprague-Grundy Theory (1930s) which shows that the Grundy value of the
disjunctive sum of any two impartial games can be computed-in polynomial
time-from their Grundy values. In contrast, we prove that assuming PSPACE
$\neq$ P, there is no general polynomial-time method to summarize two
polynomial-time solvable impartial games to efficiently solve their disjunctive
sum.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1407.7841v3,Caching and Auditing in the RPPM Model,"Crampton and Sellwood recently introduced a variant of relationship-based
access control based on the concepts of relationships, paths and principal
matching, to which we will refer as the RPPM model. In this paper, we show that
the RPPM model can be extended to provide support for caching of authorization
decisions and enforcement of separation of duty policies. We show that these
extensions are natural and powerful. Indeed, caching provides far greater
advantages in RPPM than it does in most other access control models and we are
able to support a wide range of separation of duty policies.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1801.03326v2,Expected Policy Gradients for Reinforcement Learning,"We propose expected policy gradients (EPG), which unify stochastic policy
gradients (SPG) and deterministic policy gradients (DPG) for reinforcement
learning. Inspired by expected sarsa, EPG integrates (or sums) across actions
when estimating the gradient, instead of relying only on the action in the
sampled trajectory. For continuous action spaces, we first derive a practical
result for Gaussian policies and quadratic critics and then extend it to a
universal analytical method, covering a broad class of actors and critics,
including Gaussian, exponential families, and policies with bounded support.
For Gaussian policies, we introduce an exploration method that uses covariance
proportional to the matrix exponential of the scaled Hessian of the critic with
respect to the actions. For discrete action spaces, we derive a variant of EPG
based on softmax policies. We also establish a new general policy gradient
theorem, of which the stochastic and deterministic policy gradient theorems are
special cases. Furthermore, we prove that EPG reduces the variance of the
gradient estimates without requiring deterministic policies and with little
computational overhead. Finally, we provide an extensive experimental
evaluation of EPG and show that it outperforms existing approaches on multiple
challenging control domains.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.03719v2,The Mathematical Syntax of Architectures,"Despite several (accepted) standards, core notions typically employed in
information technology or system engineering architectures lack the precise and
exact foundations encountered in logic, algebra, and other branches of
mathematics.
  In this contribution we define the syntactical aspects of the term
architecture in a mathematically rigorous way. We motivate our particular
choice by demonstrating (i) how commonly understood and expected properties of
an architecture--as defined by various standards--can be suitably identified or
derived within our formalization, (ii) how our concept is fully compatible with
real life (business) architectures, and (iii) how our definition complements
recent foundational work in this area (Wilkinson 2018, Dickersen 2020).
  We furthermore develop a rigorous notion of architectural similarity based on
the notion of homomorphisms allowing the class of architectures to be regarded
as a category, Arch. We demonstrate the applicability of our concepts to theory
by deriving theorems on the classification of certain types of architectures.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1706.02526v1,Conditional Transition Systems with Upgrades,"We introduce a variant of transition systems, where activation of transitions
depends on conditions of the environment and upgrades during runtime
potentially create additional transitions. Using a cornerstone result in
lattice theory, we show that such transition systems can be modelled in two
ways: as conditional transition systems (CTS) with a partial order on
conditions, or as lattice transition systems (LaTS), where transitions are
labelled with the elements from a distributive lattice. We define equivalent
notions of bisimilarity for both variants and characterise them via a
bisimulation game.
  We explain how conditional transition systems are related to featured
transition systems for the modelling of software product lines. Furthermore, we
show how to compute bisimilarity symbolically via BDDs by defining an operation
on BDDs that approximates an element of a Boolean algebra into a lattice. We
have implemented our procedure and provide runtime results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.07009v1,"Gender Balance in Computer Science and Engineering in Italian
  Universities","Multiple studies have shown that gender balance in the fields of Science,
Technology, Engineering and Maths -- and in particular in ICT -- is still far
to be achieved. Several initiatives have been recently taken to increase the
women participation, but it is difficult, at present, to evaluate their impact
and their potential of changing the situation. This paper contributes to the
discussion by presenting a descriptive analysis of the gender balance in
Computer Science and Computer Engineering in Italian Universities.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0
http://arxiv.org/abs/1504.04208v1,"Contextualization of topics - browsing through terms, authors, journals
  and cluster allocations","This paper builds on an innovative Information Retrieval tool, Ariadne. The
tool has been developed as an interactive network visualization and browsing
tool for large-scale bibliographic databases. It basically allows to gain
insights into a topic by contextualizing a search query (Koopman et al., 2015).
In this paper, we apply the Ariadne tool to a far smaller dataset of 111,616
documents in astronomy and astrophysics. Labeled as the Berlin dataset, this
data have been used by several research teams to apply and later compare
different clustering algorithms. The quest for this team effort is how to
delineate topics. This paper contributes to this challenge in two different
ways. First, we produce one of the different cluster solution and second, we
use Ariadne (the method behind it, and the interface - called LittleAriadne) to
display cluster solutions of the different group members. By providing a tool
that allows the visual inspection of the similarity of article clusters
produced by different algorithms, we present a complementary approach to other
possible means of comparison. More particular, we discuss how we can - with
LittleAriadne - browse through the network of topical terms, authors, journals
and cluster solutions in the Berlin dataset and compare cluster solutions as
well as see their context.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.09356v3,Decentralized Deep Learning with Arbitrary Communication Compression,"Decentralized training of deep learning models is a key element for enabling
data privacy and on-device learning over networks, as well as for efficient
scaling to large compute clusters. As current approaches suffer from limited
bandwidth of the network, we propose the use of communication compression in
the decentralized training context. We show that Choco-SGD $-$ recently
introduced and analyzed for strongly-convex objectives only $-$ converges under
arbitrary high compression ratio on general non-convex functions at the rate
$O\bigl(1/\sqrt{nT}\bigr)$ where $T$ denotes the number of iterations and $n$
the number of workers. The algorithm achieves linear speedup in the number of
workers and supports higher compression than previous state-of-the art methods.
We demonstrate the practical performance of the algorithm in two key scenarios:
the training of deep learning models (i) over distributed user devices,
connected by a social network and (ii) in a datacenter (outperforming
all-reduce time-wise).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1508.00142v2,Online Contention Resolution Schemes,"We introduce a new rounding technique designed for online optimization
problems, which is related to contention resolution schemes, a technique
initially introduced in the context of submodular function maximization. Our
rounding technique, which we call online contention resolution schemes (OCRSs),
is applicable to many online selection problems, including Bayesian online
selection, oblivious posted pricing mechanisms, and stochastic probing models.
It allows for handling a wide set of constraints, and shares many strong
properties of offline contention resolution schemes. In particular, OCRSs for
different constraint families can be combined to obtain an OCRS for their
intersection. Moreover, we can approximately maximize submodular functions in
the online settings we consider.
  We, thus, get a broadly applicable framework for several online selection
problems, which improves on previous approaches in terms of the types of
constraints that can be handled, the objective functions that can be dealt
with, and the assumptions on the strength of the adversary. Furthermore, we
resolve two open problems from the literature; namely, we present the first
constant-factor constrained oblivious posted price mechanism for matroid
constraints, and the first constant-factor algorithm for weighted stochastic
probing with deadlines.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1503.01913v1,"Terminating Distributed Construction of Shapes and Patterns in a Fair
  Solution of Automata","We consider a solution of automata similar to Population Protocols and
Network Constructors. The automata (or nodes) move passively in a well-mixed
solution and can cooperate by interacting in pairs. Every such interaction may
result in an update of the local states of the nodes. Additionally, the nodes
may also choose to connect to each other in order to start forming some
required structure. We may think of such nodes as the smallest possible
programmable pieces of matter. The model that we introduce here is a more
applied version of Network Constructors, imposing physical (or geometrical)
constraints on the connections. Each node can connect to other nodes only via a
very limited number of local ports, therefore at any given time it has only a
bounded number of neighbors. Connections are always made at unit distance and
are perpendicular to connections of neighboring ports. We show that this
restricted model is still capable of forming very practical 2D or 3D shapes. We
provide direct constructors for some basic shape construction problems. We then
develop new techniques for determining the constructive capabilities of our
model. One of the main novelties of our approach, concerns our attempt to
overcome the inability of such systems to detect termination. In particular, we
exploit the assumptions that the system is well-mixed and has a unique leader,
in order to give terminating protocols that are correct with high probability
(w.h.p.). This allows us to develop terminating subroutines that can be
sequentially composed to form larger modular protocols. One of our main results
is a terminating protocol counting the size $n$ of the system w.h.p.. We then
use this protocol as a subroutine in order to develop our universal
constructors, establishing that the nodes can self-organize w.h.p. into
arbitrarily complex shapes while still detecting termination of the
construction.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1502.02290v1,How Hard is Computing Parity with Noisy Communications?,"We show a tight lower bound of $\Omega(N \log\log N)$ on the number of
transmissions required to compute the parity of $N$ input bits with constant
error in a noisy communication network of $N$ randomly placed sensors, each
having one input bit and communicating with others using local transmissions
with power near the connectivity threshold. This result settles the lower bound
question left open by Ying, Srikant and Dullerud (WiOpt 06), who showed how the
sum of all the $N$ bits can be computed using $O(N \log\log N)$ transmissions.
The same lower bound has been shown to hold for a host of other functions
including majority by Dutta and Radhakrishnan (FOCS 2008).
  Most works on lower bounds for communication networks considered mostly the
full broadcast model without using the fact that the communication in real
networks is local, determined by the power of the transmitters. In fact, in
full broadcast networks computing parity needs $\theta(N)$ transmissions. To
obtain our lower bound we employ techniques developed by Goyal, Kindler and
Saks (FOCS 05), who showed lower bounds in the full broadcast model by reducing
the problem to a model of noisy decision trees. However, in order to capture
the limited range of transmissions in real sensor networks, we adapt their
definition of noisy decision trees and allow each node of the tree access to
only a limited part of the input. Our lower bound is obtained by exploiting
special properties of parity computations in such noisy decision trees.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0711.0784v1,Addendum to Research MMMCV; A Man/Microbio/Megabio/Computer Vision,"In October 2007, a Research Proposal for the University of Sydney, Australia,
the author suggested that biovie-physical phenomenon as `electrodynamic
dependant biological vision', is governed by relativistic quantum laws and
biovision. The phenomenon on the basis of `biovielectroluminescence', satisfies
man/microbio/megabio/computer vision (MMMCV), as a robust candidate for
physical and visual sciences. The general aim of this addendum is to present a
refined text of Sections 1-3 of that proposal and highlighting the contents of
its Appendix in form of a `Mechanisms' Section. We then briefly remind in an
article aimed for December 2007, by appending two more equations into Section
3, a theoretical II-time scenario as a time model well-proposed for the
phenomenon. The time model within the core of the proposal, plays a significant
role in emphasizing the principle points on Objectives no. 1-8, Sub-hypothesis
3.1.2, mentioned in Article [arXiv:0710.0410]. It also expresses the time
concept in terms of causing quantized energy f(|E|) of time |t|, emit in regard
to shortening the probability of particle loci as predictable patterns of
particle's un-occurred motion, a solution to Heisenberg's uncertainty principle
(HUP) into a simplistic manner. We conclude that, practical frames via a time
algorithm to this model, fixates such predictable patterns of motion of scenery
bodies onto recordable observation points of a MMMCV system. It even
suppresses/predicts superposition phenomena coming from a human subject and/or
other bio-subjects for any decision making event, e.g., brainwave quantum
patterns based on vision. Maintaining the existential probability of Riemann
surfaces of II-time scenarios in the context of biovielectroluminescence, makes
motion-prediction a possibility.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.08365v1,"Counting Objects by Diffused Index: geometry-free and training-free
  approach","Counting objects is a fundamental but challenging problem. In this paper, we
propose diffusion-based, geometry-free, and learning-free methodologies to
count the number of objects in images. The main idea is to represent each
object by a unique index value regardless of its intensity or size, and to
simply count the number of index values. First, we place different vectors,
refer to as seed vectors, uniformly throughout the mask image. The mask image
has boundary information of the objects to be counted. Secondly, the seeds are
diffused using an edge-weighted harmonic variational optimization model within
each object. We propose an efficient algorithm based on an operator splitting
approach and alternating direction minimization method, and theoretical
analysis of this algorithm is given. An optimal solution of the model is
obtained when the distributed seeds are completely diffused such that there is
a unique intensity within each object, which we refer to as an index. For
computational efficiency, we stop the diffusion process before a full
convergence, and propose to cluster these diffused index values. We refer to
this approach as Counting Objects by Diffused Index (CODI). We explore scalar
and multi-dimensional seed vectors. For Scalar seeds, we use Gaussian fitting
in histogram to count, while for vector seeds, we exploit a high-dimensional
clustering method for the final step of counting via clustering. The proposed
method is flexible even if the boundary of the object is not clear nor fully
enclosed. We present counting results in various applications such as
biological cells, agriculture, concert crowd, and transportation. Some
comparisons with existing methods are presented.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0805.0577v2,Infinity-Norm Sphere-Decoding,"The most promising approaches for efficient detection in multiple-input
multiple-output (MIMO) wireless systems are based on sphere-decoding (SD). The
conventional (and optimum) norm that is used to conduct the tree traversal step
in SD is the l-2 norm. It was, however, recently observed that using the
l-infinity norm instead reduces the hardware complexity of SD considerably at
only a marginal performance loss. These savings result from a reduction in the
length of the critical path in the circuit and the silicon area required for
metric computation, but are also, as observed previously through simulation
results, a consequence of a reduction in the computational (i.e., algorithmic)
complexity. The aim of this paper is an analytical performance and
computational complexity analysis of l-infinity norm SD. For i.i.d. Rayleigh
fading MIMO channels, we show that l-infinity norm SD achieves full diversity
order with an asymptotic SNR gap, compared to l-2 norm SD, that increases at
most linearly in the number of receive antennas. Moreover, we provide a
closed-form expression for the computational complexity of l-infinity norm SD
based on which we establish that its complexity scales exponentially in the
system size. Finally, we characterize the tree pruning behavior of l-infinity
norm SD and show that it behaves fundamentally different from that of l-2 norm
SD.",0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.07351v1,"A Relevant Content Filtering Based Framework For Data Stream
  Summarization","Social media platforms are a rich source of information these days, however,
of all the available information, only a small fraction is of users' interest.
To help users catch up with the latest topics of their interests from the large
amount of information available in social media, we present a relevant content
filtering based framework for data stream summarization. More specifically,
given the topic or event of interest, this framework can dynamically discover
and filter out relevant information from irrelevant information in the stream
of text provided by social media platforms. It then further captures the most
representative and up-to-date information to generate a sequential summary or
event story line along with the evolution of the topic or event. Our framework
does not depend on any labeled data, it instead uses the weak supervision
provided by the user, which matches the real scenarios of users searching for
information about an ongoing event. We experimented on two real events traced
by a Twitter dataset from TREC 2011. The results verified the effectiveness of
relevant content filtering and sequential summary generation of the proposed
framework. It also shows its robustness of using the most easy-to-obtain weak
supervision, i.e., trending topic or hashtag. Thus, this framework can be
easily integrated into social media platforms such as Twitter to generate
sequential summaries for the events of interest. We also make the manually
generated gold-standard sequential summaries of the two test events publicly
available for future use in the community.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1904.10108v1,Intersection Subtyping with Constructors,"We study the question of extending the BCD intersection type system with
additional type constructors. On the typing side, we focus on adding the usual
rules for product types. On the subtyping side, we consider a generic way of
defining a subtyping relation on families of types which include intersection
types. We find back the BCD subtyping relation by considering the particular
case where the type constructors are intersection, omega and arrow. We obtain
an extension of BCD subtyping to product types as another instance. We show how
the preservation of typing by both reduction and expansion is satisfied in all
the considered cases. Our approach takes benefits from a ""subformula property""
of the proposed presentation of the subtyping relation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1809.05467v1,"Discovering Reliable Dependencies from Data: Hardness and Improved
  Algorithms","The reliable fraction of information is an attractive score for quantifying
(functional) dependencies in high-dimensional data. In this paper, we
systematically explore the algorithmic implications of using this measure for
optimization. We show that the problem is NP-hard, which justifies the usage of
worst-case exponential-time as well as heuristic search methods. We then
substantially improve the practical performance for both optimization styles by
deriving a novel admissible bounding function that has an unbounded potential
for additional pruning over the previously proposed one. Finally, we
empirically investigate the approximation ratio of the greedy algorithm and
show that it produces highly competitive results in a fraction of time needed
for complete branch-and-bound style search.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.00125v1,Situation Calculus by Term Rewriting,"A version of the situation calculus in which situations are represented as
first-order terms is presented. Fluents can be computed from the term
structure, and actions on the situations correspond to rewrite rules on the
terms. Actions that only depend on or influence a subset of the fluents can be
described as rewrite rules that operate on subterms of the terms in some cases.
If actions are bidirectional then efficient completion methods can be used to
solve planning problems. This representation for situations and actions is most
similar to the fluent calculus of Thielscher \cite{Thielscher98}, except that
this representation is more flexible and more use is made of the subterm
structure. Some examples are given, and a few general methods for constructing
such sets of rewrite rules are presented. This paper was submitted to FSCD 2020
on December 23, 2019.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1502.02265v2,"A geometric approach for the upper bound theorem for Minkowski sums of
  convex polytopes","We derive tight expressions for the maximum number of $k$-faces,
$0\le{}k\le{}d-1$, of the Minkowski sum, $P_1+...+P_r$, of $r$ convex
$d$-polytopes $P_1,...,P_r$ in $\mathbb{R}^d$, where $d\ge{}2$ and $r<d$, as a
(recursively defined) function on the number of vertices of the polytopes.
  Our results coincide with those recently proved by Adiprasito and Sanyal [2].
In contrast to Adiprasito and Sanyal's approach, which uses tools from
Combinatorial Commutative Algebra, our approach is purely geometric and uses
basic notions such as $f$- and $h$-vector calculus and shellings, and
generalizes the methodology used in [15] and [14] for proving upper bounds on
the $f$-vector of the Minkowski sum of two and three convex polytopes,
respectively.
  The key idea behind our approach is to express the Minkowski sum
$P_1+...+P_r$ as a section of the Cayley polytope $\mathcal{C}$ of the
summands; bounding the $k$-faces of $P_1+...+P_r$ reduces to bounding the
subset of the $(k+r-1)$-faces of $\mathcal{C}$ that contain vertices from each
of the $r$ polytopes.
  We end our paper with a sketch of an explicit construction that establishes
the tightness of the upper bounds.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1607.04765v1,"Design and implementation of audio communication system for
  social-humanoid robot Lumen as an exhibition guide in Electrical Engineering
  Days 2015","Social Robot Lumen is a humanoid robot created to act like human and be human
friend. In this study, Lumen scenario is limited on Lumen as an exhibition
guide in Electrical Engineering Days 2015, a seminar and exhibition of
electrical engineering undergraduate and graduate student of Bandung Institute
of Technology. To be an exhibition guide, Lumen is equipped by Nao robot, a
server, and processing applications. Audio communication system is one of the
processing applications. The purpose of the system is to create verbal
communication that allow Lumen to receive human voice and respond naturally to
it. To be able to communicate like a human, audio communication system is built
with speech recognition module to transform speech data into text, speech
synthesizer module to transform text data into speech, and gender
identification module to distinguish adult female and male voice. Speech
recognition module is implemented using Google Speech Recognition API, speech
synthesizer module is implemented using Acapela engine, and gender
identification module implemented by utilizing speech signal feature that is
extracted using Fast Fourier Transform algorithm. Hardware used for
implementation are Nao robot, computer, and wireless modem.
  -----
  Lumen Robot Sosial Robot merupakan robot humanoid yang diciptakan agar dapat
bersikap seperti manusia dan menjadi teman bagi manusia. Sistem komunikasi
audio merupakan salah satu aplikasi pengolah yang bertujuan agar Lumen dapat
menerima suara manusia dan meresponnya dengan natural, yaitu seperti cara
manusia merespon manusia lainnya. Untuk dapat berkomunikasi seperti manusia,
sistem komunikasi audio dilengkapi dengan tiga buah modul: speech recognition
untuk mengubah data suara menjadi teks, speech synthesizer untuk mengubah data
teks menjadi suara, dan gender identification untuk membedakan suara wanita dan
pria.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1603.08648v3,A Comparison of NOOP to Structural Domain-Theoretic Models of OOP,"Mainstream object-oriented programming languages such as Java, C#, C++ and
Scala are all almost entirely nominally-typed. NOOP is a recently developed
domain-theoretic model of OOP that was designed to include full nominal
information found in nominally-typed OOP. This paper compares NOOP to the most
widely known domain-theoretic models of OOP, namely, the models developed by
Cardelli and Cook, which were structurally-typed models. Leveraging the
development of NOOP, the comparison presented in this paper provides a clear
and precise mathematical account for the relation between nominal and
structural OO type systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.05772v1,"On Œª-backbone coloring of cliques with tree backbones in linear
  time","A $\lambda$-backbone coloring of a graph $G$ with its subgraph (also called a
backbone) $H$ is a function $c \colon V(G) \rightarrow \{1,\dots, k\}$ ensuring
that $c$ is a proper coloring of $G$ and for each $\{u,v\} \in E(H)$ it holds
that $|c(u) - c(v)| \ge \lambda$. In this paper we propose a way to color
cliques with tree and forest backbones in linear time that the largest color
does not exceed $\max\{n, 2 \lambda\} + \Delta(H)^2 \lceil\log{n} \rceil$. This
result improves on the previously existing approximation algorithms as it is
$(\Delta(H)^2 \lceil\log{n} \rceil)$-absolutely approximate, i.e. with an
additive error over the optimum. We also present an infinite family of trees
$T$ with $\Delta(T) = 3$ for which the coloring of cliques with backbones $T$
require to use at least $\max\{n, 2 \lambda\} + \Omega(\log{n})$ colors for
$\lambda$ close to $\frac{n}{2}$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.00724v1,"A lemma on closures and its application to modularity in logic
  programming semantics","This note points out a lemma on closures of monotonic increasing functions
and shows how it is applicable to decomposition and modularity for semantics
defined as the least fixedpoint of some monotonic function. In particular it
applies to numerous semantics of logic programs. An appendix addresses the
fixedpoints of (possibly non-monotonic) functions that are sandwiched between
functions with the same fixedpoints.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0004015v2,"Introduction to the GiNaC Framework for Symbolic Computation within the
  C++ Programming Language","The traditional split-up into a low level language and a high level language
in the design of computer algebra systems may become obsolete with the advent
of more versatile computer languages. We describe GiNaC, a special-purpose
system that deliberately denies the need for such a distinction. It is entirely
written in C++ and the user can interact with it directly in that language. It
was designed to provide efficient handling of multivariate polynomials,
algebras and special functions that are needed for loop calculations in
theoretical quantum field theory. It also bears some potential to become a more
general purpose symbolic package.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1406.3561v1,"When relevance is not Enough: Promoting Visual Attractiveness for
  Fashion E-commerce","Fashion, and especially apparel, is the fastest-growing category in online
shopping. As consumers requires sensory experience especially for apparel goods
for which their appearance matters most, images play a key role not only in
conveying crucial information that is hard to express in text, but also in
affecting consumer's attitude and emotion towards the product. However,
research related to e-commerce product image has mostly focused on quality at
perceptual level, but not the quality of content, and the way of presenting.
This study aims to address the effectiveness of types of image in showcasing
fashion apparel in terms of its attractiveness, i.e. the ability to draw
consumer's attention, interest, and in return their engagement. We apply
advanced vision technique to quantize attractiveness using three common display
types in fashion filed, i.e. human model, mannequin, and flat. We perform
two-stage study by starting with large scale behavior data from real online
market, then moving to well designed user experiment to further deepen our
understandings on consumer's reasoning logic behind the action. We propose a
Fisher noncentral hypergeometric distribution based user choice model to
quantitatively evaluate user's preference. Further, we investigate the
potentials to leverage visual impact for a better search that caters to user's
preference. A visual attractiveness based re-ranking model that incorporates
both presentation efficacy and user preference is proposed. We show
quantitative improvement by promoting visual attractiveness into search on top
of relevance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2010.00543v1,"Artificial Creations: Ascription, Ownership, Time-Specific Monopolies","Creativity has always been synonymous with humans. No other living species
could boast of creativity as humans could. Even the smartest computers thrived
only on the ingenious imaginations of its coders. However, that is steadily
changing with highly advanced artificially intelligent systems that demonstrate
incredible capabilities to autonomously (i.e., with minimal or no human input)
produce creative products that would ordinarily deserve intellectual property
status if created by a human. These systems could be called artificial creators
and their creative products artificial creations. The use of artificial
creators is likely to become a part of mainstream production practices in the
creative and innovation industries sooner than we realize. When they do,
intellectual property regimes (that are inherently designed to reward human
creativity) must be sufficiently prepared to aptly respond to the phenomenon of
what could be called artificial creativity. Needless to say, any such response
must be guided by considerations of public welfare. This study analyzes what
that response ought to look like by revisiting the determinants of intellectual
property and critiquing its nature and modes. This understanding of
intellectual property is then applied to investigate the determinants of
intellectual property in artificial creations so as to determine the intrinsic
justifications for intellectual property rewards for artificial creativity, and
accordingly, develop general modalities for granting intellectual property
status to artificial creations. Finally, the treatment of artificial works
(i.e., copyrightable artificial creations) and artificial inventions (i.e.,
patentable artificial creations) by current intellectual property regimes is
critiqued, and specific modalities for granting intellectual property status to
artificial works and artificial inventions are developed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0
http://arxiv.org/abs/cs/9908002v1,"After Compilers and Operating Systems : The Third Advance in Application
  Support","After compilers and operating systems, TSIAs are the third advance in
application support. A compiler supports a high level application definition in
a programming language. An operating system supports a high level interface to
the resources used by an application execution. A Task System and Item
Architecture (TSIA) provides an application with a transparent reliable,
distributed, heterogeneous, adaptive, dynamic, real-time, interactive,
parallel, secure or other execution. In addition to supporting the application
execution, a TSIA also supports the application definition. This run-time
support for the definition is complementary to the compile-time support of a
compiler. For example, this allows a language similar to Fortran or C to
deliver features promised by functional computing. While many TSIAs exist, they
previously have not been recognized as such and have served only a particular
type of application. Existing TSIAs and other projects demonstrate that TSIAs
are feasible for most applications. As the next paradigm for application
support, the TSIA simplifies and unifies existing computing practice and
research. By solving many outstanding problems, the TSIA opens many, many new
opportunities for computing.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.06377v2,Geometry-Based Next Frame Prediction from Monocular Video,"We consider the problem of next frame prediction from video input. A
recurrent convolutional neural network is trained to predict depth from
monocular video input, which, along with the current video image and the camera
trajectory, can then be used to compute the next frame. Unlike prior next-frame
prediction approaches, we take advantage of the scene geometry and use the
predicted depth for generating the next frame prediction. Our approach can
produce rich next frame predictions which include depth information attached to
each pixel. Another novel aspect of our approach is that it predicts depth from
a sequence of images (e.g. in a video), rather than from a single still image.
We evaluate the proposed approach on the KITTI dataset, a standard dataset for
benchmarking tasks relevant to autonomous driving. The proposed method produces
results which are visually and numerically superior to existing methods that
directly predict the next frame. We show that the accuracy of depth prediction
improves as more prior frames are considered.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1201.4179v1,Applications of topology in computer algorithms,"The aim of this paper is to discuss some applications of general topology in
computer algorithms including modeling and simulation, and also in computer
graphics and image processing. While the progress in these areas heavily
depends on advances in computing hardware, the major intellectual achievements
are the algorithms. The applications of general topology in other branches of
mathematics are not discussed, since they are not applications of mathematics
outside of mathematics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0504050v2,"Mapping Fusion and Synchronized Hyperedge Replacement into Logic
  Programming","In this paper we compare three different formalisms that can be used in the
area of models for distributed, concurrent and mobile systems. In particular we
analyze the relationships between a process calculus, the Fusion Calculus,
graph transformations in the Synchronized Hyperedge Replacement with Hoare
synchronization (HSHR) approach and logic programming. We present a translation
from Fusion Calculus into HSHR (whereas Fusion Calculus uses Milner
synchronization) and prove a correspondence between the reduction semantics of
Fusion Calculus and HSHR transitions. We also present a mapping from HSHR into
a transactional version of logic programming and prove that there is a full
correspondence between the two formalisms. The resulting mapping from Fusion
Calculus to logic programming is interesting since it shows the tight analogies
between the two formalisms, in particular for handling name generation and
mobility. The intermediate step in terms of HSHR is convenient since graph
transformations allow for multiple, remote synchronizations, as required by
Fusion Calculus semantics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.05357v1,"Hate versus Politics: Detection of Hate against Policy makers in Italian
  tweets","Accurate detection of hate speech against politicians, policy making and
political ideas is crucial to maintain democracy and free speech.
Unfortunately, the amount of labelled data necessary for training models to
detect hate speech are limited and domain-dependent. In this paper, we address
the issue of classification of hate speech against policy makers from Twitter
in Italian, producing the first resource of this type in this language. We
collected and annotated 1264 tweets, examined the cases of disagreements
between annotators, and performed in-domain and cross-domain hate speech
classifications with different features and algorithms. We achieved a
performance of ROC AUC 0.83 and analyzed the most predictive attributes, also
finding the different language features in the anti-policymakers and
anti-immigration domains. Finally, we visualized networks of hashtags to
capture the topics used in hateful and normal tweets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2104.06529v1,BERT Embeddings Can Track Context in Conversational Search,"The use of conversational assistants to search for information is becoming
increasingly more popular among the general public, pushing the research
towards more advanced and sophisticated techniques. In the last few years, in
particular, the interest in conversational search is increasing, not only
because of the generalization of conversational assistants but also because
conversational search is a step forward in allowing a more natural interaction
with the system.
  In this work, the focus is on exploring the context present of the
conversation via the historical utterances and respective embeddings with the
aim of developing a conversational search system that helps people search for
information in a natural way. In particular, this system must be able to
understand the context where the question is posed, tracking the current state
of the conversation and detecting mentions to previous questions and answers.
We achieve this by using a context-tracking component based on neural
query-rewriting models. Another crucial aspect of the system is to provide the
most relevant answers given the question and the conversational history. To
achieve this objective, we used a Transformer-based re-ranking method and
expanded this architecture to use the conversational context.
  The results obtained with the system developed showed the advantages of using
the context present in the natural language utterances and in the neural
embeddings generated throughout the conversation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0101030v1,Tree Contractions and Evolutionary Trees,"An evolutionary tree is a rooted tree where each internal vertex has at least
two children and where the leaves are labeled with distinct symbols
representing species. Evolutionary trees are useful for modeling the
evolutionary history of species. An agreement subtree of two evolutionary trees
is an evolutionary tree which is also a topological subtree of the two given
trees. We give an algorithm to determine the largest possible number of leaves
in any agreement subtree of two trees T_1 and T_2 with n leaves each. If the
maximum degree d of these trees is bounded by a constant, the time complexity
is O(n log^2(n)) and is within a log(n) factor of optimal. For general d, this
algorithm runs in O(n d^2 log(d) log^2(n)) time or alternatively in O(n d
sqrt(d) log^3(n)) time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1906.11237v1,"Making a Sieve Random: Improved Semi-Streaming Algorithm for Submodular
  Maximization under a Cardinality Constraint","In this paper we consider the problem of maximizing a non-negative submodular
function subject to a cardinality constraint in the data stream model.
Previously, the best known algorithm for this problem was a
$5.828$-approximation semi-streaming algorithm based on a local search
technique (Feldman et al., 2018). For the special case of this problem in which
the objective function is also monotone, the state-of-the-art semi-streaming
algorithm is an algorithm known as Sieve-Streaming, which is based on a
different technique (Badanidiyuru, 2014). Adapting the technique of
Sieve-Streaming to non-monotone objective functions has turned out to be a
challenging task, which has so far prevented an improvement over the local
search based $5.828$-approximation. In this work, we overcome the above
challenge, and manage to adapt Sieve-Streaming to non-monotone objective
functions by introducing a ""just right"" amount of randomness into it.
Consequently, we get a semi-streaming polynomial time $4.282$-approximation
algorithm for non-monotone objectives. Moreover, if one allows our algorithm to
run in super-polynomial time, then its approximation ratio can be further
improved to $3 + \varepsilon$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9907010v1,Language Identification With Confidence Limits,"A statistical classification algorithm and its application to language
identification from noisy input are described. The main innovation is to
compute confidence limits on the classification, so that the algorithm
terminates when enough evidence to make a clear decision has been made, and so
avoiding problems with categories that have similar characteristics. A second
application, to genre identification, is briefly examined. The results show
that some of the problems of other language identification techniques can be
avoided, and illustrate a more important point: that a statistical language
process can be used to provide feedback about its own success rate.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0212039v1,"Low Size-Complexity Inductive Logic Programming: The East-West Challenge
  Considered as a Problem in Cost-Sensitive Classification","The Inductive Logic Programming community has considered proof-complexity and
model-complexity, but, until recently, size-complexity has received little
attention. Recently a challenge was issued ""to the international computing
community"" to discover low size-complexity Prolog programs for classifying
trains. The challenge was based on a problem first proposed by Ryszard
Michalski, 20 years ago. We interpreted the challenge as a problem in
cost-sensitive classification and we applied a recently developed
cost-sensitive classifier to the competition. Our algorithm was relatively
successful (we won a prize). This paper presents our algorithm and analyzes the
results of the competition.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1803.02088v1,"Explain Yourself: A Natural Language Interface for Scrutable Autonomous
  Robots","Autonomous systems in remote locations have a high degree of autonomy and
there is a need to explain what they are doing and why in order to increase
transparency and maintain trust. Here, we describe a natural language chat
interface that enables vehicle behaviour to be queried by the user. We obtain
an interpretable model of autonomy through having an expert 'speak out-loud'
and provide explanations during a mission. This approach is agnostic to the
type of autonomy model and as expert and operator are from the same user-group,
we predict that these explanations will align well with the operator's mental
model, increase transparency and assist with operator training.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.01199v1,Tractable Combinations of Theories via Sampling,"For a first-order theory $T$, the Constraint Satisfaction Problem of $T$ is
the computational problem of deciding whether a given conjunction of atomic
formulas is satisfiable in some model of $T$. In this article we develop
sufficient conditions for polynomial-time tractability of the constraint
satisfaction problem for the union of two theories with disjoint relational
signatures. To this end, we introduce the concept of sampling for theories and
show that samplings can be applied to examples which are not covered by the
seminal result of Nelson and Oppen.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.0773v2,Simplification of Training Data for Cross-Project Defect Prediction,"Cross-project defect prediction (CPDP) plays an important role in estimating
the most likely defect-prone software components, especially for new or
inactive projects. To the best of our knowledge, few prior studies provide
explicit guidelines on how to select suitable training data of quality from a
large number of public software repositories. In this paper, we have proposed a
training data simplification method for practical CPDP in consideration of
multiple levels of granularity and filtering strategies for data sets. In
addition, we have also provided quantitative evidence on the selection of a
suitable filter in terms of defect-proneness ratio. Based on an empirical study
on 34 releases of 10 open-source projects, we have elaborately compared the
prediction performance of different defect predictors built with five
well-known classifiers using training data simplified at different levels of
granularity and with two popular filters. The results indicate that when using
the multi-granularity simplification method with an appropriate filter, the
prediction models based on Naive Bayes can achieve fairly good performance and
outperform the benchmark method.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1811.01001v1,On Evaluating the Generalization of LSTM Models in Formal Languages,"Recurrent Neural Networks (RNNs) are theoretically Turing-complete and
established themselves as a dominant model for language processing. Yet, there
still remains an uncertainty regarding their language learning capabilities. In
this paper, we empirically evaluate the inductive learning capabilities of Long
Short-Term Memory networks, a popular extension of simple RNNs, to learn simple
formal languages, in particular $a^nb^n$, $a^nb^nc^n$, and $a^nb^nc^nd^n$. We
investigate the influence of various aspects of learning, such as training data
regimes and model capacity, on the generalization to unobserved samples. We
find striking differences in model performances under different training
settings and highlight the need for careful analysis and assessment when making
claims about the learning capabilities of neural network models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1407.6350v1,Symmetric Disclosure: a Fresh Look at k-Anonymity,"We analyze how the sparsity of a typical aggregate social relation impacts
the network overhead of online communication systems designed to provide
k-anonymity. Once users are grouped in anonymity sets there will likely be few
related pairs of users between any two particular sets, and so the sets need to
be large in order to provide cover traffic between them. We can reduce the
associated overhead by having both parties in a communication specify both the
origin and the target sets of the communication. We propose to call this
communication primitive ""symmetric disclosure."" If in order to retrieve
messages a user specifies a group from which he expects to receive them, the
negative impact of the sparsity is offset.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1301.2650v3,Krylov Subspace Recycling for Sequences of Shifted Linear Systems,"We study the use of Krylov subspace recycling for the solution of a sequence
of slowly-changing families of linear systems, where each family consists of
shifted linear systems that differ in the coefficient matrix only by multiples
of the identity. Our aim is to explore the simultaneous solution of each family
of shifted systems within the framework of subspace recycling, using one
augmented subspace to extract candidate solutions for all the shifted systems.
The ideal method would use the same augmented subspace for all systems and have
fixed storage requirements, independent of the number of shifted systems per
family. We show that a method satisfying both requirements cannot exist in this
framework.
  As an alternative, we introduce two schemes. One constructs a separate
deflation space for each shifted system but solves each family of shifted
systems simultaneously. The other builds only one recycled subspace and
constructs approximate corrections to the solutions of the shifted systems at
each cycle of the iterative linear solver while only minimizing the base system
residual. At convergence of the base system solution, we apply the method
recursively to the remaining unconverged systems. We present numerical examples
involving systems arising in lattice quantum chromodynamics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1807.11052v3,Trust-Based Identity Sharing For Token Grants,"Authentication and authorization are two key elements of a software
application. In modern day, OAuth 2.0 framework and OpenID Connect protocol are
widely adopted standards fulfilling these requirements. These protocols are
implemented into authorization servers. It is common to call these
authorization servers as identity servers or identity providers since they hold
user identity information. Applications registered to an identity provider can
use OpenID Connect to retrieve ID token for authentication. Access token
obtained along with ID token allows the application to consume OAuth 2.0
protected resources. In this approach, the client application is bound to a
single identity provider. If the client needs to consume a protected resource
from a different domain, which only accepts tokens of a defined identity
provider, then the client must again follow OpenID Connect protocol to obtain
new tokens. This requires user identity details to be stored in the second
identity provider as well. This paper proposes an extension to OpenID Connect
protocol to overcome this issue. It proposes a client-centric mechanism to
exchange identity information as token grants against a trusted identity
provider. Once a grant is accepted, resulting token response contains an access
token, which is good enough to access protected resources from token issuing
identity provider's domain.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1206.4224v3,Factoring bivariate lacunary polynomials without heights,"We present an algorithm which computes the multilinear factors of bivariate
lacunary polynomials. It is based on a new Gap Theorem which allows to test
whether a polynomial of the form P(X,X+1) is identically zero in time
polynomial in the number of terms of P(X,Y). The algorithm we obtain is more
elementary than the one by Kaltofen and Koiran (ISSAC'05) since it relies on
the valuation of polynomials of the previous form instead of the height of the
coefficients. As a result, it can be used to find some linear factors of
bivariate lacunary polynomials over a field of large finite characteristic in
probabilistic polynomial time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1308.2149v2,"Frameworks for Reasoning about Syntax that Utilize Quotation and
  Evaluation","It is often useful, if not necessary, to reason about the syntactic structure
of an expression in an interpreted language (i.e., a language with a
semantics). This paper introduces a mathematical structure called a syntax
framework that is intended to be an abstract model of a system for reasoning
about the syntax of an interpreted language. Like many concrete systems for
reasoning about syntax, a syntax framework contains a mapping of expressions in
the interpreted language to syntactic values that represent the syntactic
structures of the expressions; a language for reasoning about the syntactic
values; a mechanism called quotation to refer to the syntactic value of an
expression; and a mechanism called evaluation to refer to the value of the
expression represented by a syntactic value. A syntax framework provides a
basis for integrating reasoning about the syntax of the expressions with
reasoning about what the expressions mean. The notion of a syntax framework is
used to discuss how quotation and evaluation can be built into a language and
to define what quasiquotation is. Several examples of syntax frameworks are
presented.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1011.5808v1,Dotted Version Vectors: Logical Clocks for Optimistic Replication,"In cloud computing environments, a large number of users access data stored
in highly available storage systems. To provide good performance to
geographically disperse users and allow operation even in the presence of
failures or network partitions, these systems often rely on optimistic
replication solutions that guarantee only eventual consistency. In this
scenario, it is important to be able to accurately and efficiently identify
updates executed concurrently. In this paper, first we review, and expose
problems with current approaches to causality tracking in optimistic
replication: these either lose information about causality or do not scale, as
they require replicas to maintain information that grows linearly with the
number of clients or updates. Then, we propose a novel solution that fully
captures causality while being very concise in that it maintains information
that grows linearly only with the number of servers that register updates for a
given data element, bounded by the degree of replication.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0710.0410v1,"The Theory of Unified Relativity for a Biovielectroluminescence
  Phenomenon via Fly's Visual and Imaging System","The elucidation upon fly's neuronal patterns as a link to computer graphics
and memory cards I/O's, is investigated for the phenomenon by propounding a
unified theory of Einstein's two known relativities. It is conclusive that
flies could contribute a certain amount of neuromatrices indicating an imagery
function of a visual-computational system into computer graphics and storage
systems. The visual system involves the time aspect, whereas flies possess
faster pulses compared to humans' visual ability due to the E-field state on an
active fly's eye surface. This behaviour can be tested on a dissected fly
specimen at its ommatidia. Electro-optical contacts and electrodes are wired
through the flesh forming organic emitter layer to stimulate light emission,
thereby to a computer circuit. The next step is applying a threshold voltage
with secondary voltages to the circuit denoting an array of essential
electrodes for bit switch. As a result, circuit's dormant pulses versus active
pulses at the specimen's area are recorded. The outcome matrix possesses a
construction of RGB and time radicals expressing the time problem in
consumption, allocating time into computational algorithms, enhancing the
technology far beyond. The obtained formulation generates consumed distance
cons(x), denoting circuital travel between data source/sink for pixel data and
bendable wavelengths. Once 'image logic' is in place, incorporating this point
of graphical acceleration permits one to enhance graphics and optimize
immensely central processing, data transmissions between memory and computer
visual system. The phenomenon can be mainly used in 360-deg. display/viewing,
3D scanning techniques, military and medicine, a robust and cheap substitution
for e.g. pre-motion pattern analysis, real-time rendering and LCDs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1511.09325v1,"Scaling to 1024 software processes and hardware cores of the distributed
  simulation of a spiking neural network including up to 20G synapses","This short report describes the scaling, up to 1024 software processes and
hardware cores, of a distributed simulator of plastic spiking neural networks.
A previous report demonstrated good scalability of the simulator up to 128
processes. Herein we extend the speed-up measurements and strong and weak
scaling analysis of the simulator to the range between 1 and 1024 software
processes and hardware cores. We simulated two-dimensional grids of cortical
columns including up to ~20G synapses connecting ~11M neurons. The neural
network was distributed over a set of MPI processes and the simulations were
run on a server platform composed of up to 64 dual-socket nodes, each socket
equipped with Intel Haswell E5-2630 v3 processors (8 cores @ 2.4 GHz clock).
All nodes are interconned through an InfiniBand network. The DPSNN simulator
has been developed by INFN in the framework of EURETILE and CORTICONIC European
FET Project and will be used by the WaveScalEW tem in the framework of the
Human Brain Project (HBP), SubProject 2 - Cognitive and Systems Neuroscience.
This report lays the groundwork for a more thorough comparison with the neural
simulation tool NEST.",0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0911.4178v1,Folksonomic Tag Clouds as an Aid to Content Indexing,"Social tagging systems have recently developed as a popular method of data
organisation on the Internet. These systems allow users to organise their
content in a way that makes sense to them, rather than forcing them to use a
pre-determined and rigid set of categorisations. These folksonomies provide
well populated sources of unstructured tags describing web resources which
could potentially be used as semantic index terms for these resources. However
getting people to agree on what tags best describe a resource is a difficult
problem, therefore any feature which increases the consistency and stability of
terms chosen would be extremely beneficial. We investigate how the provision of
a tag cloud, a weighted list of terms commonly used to assist in browsing a
folksonomy, during the tagging process itself influences the tags produced and
how difficult the user perceived the task to be. We show that illustrating the
most popular tags to users assists in the tagging process and encourages a
stable and consistent folksonomy to form.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.09994v1,"GL-Coarsener: A Graph representation learning framework to construct
  coarse grid hierarchy for AMG solvers","In many numerical schemes, the computational complexity scales non-linearly
with the problem size. Solving a linear system of equations using direct
methods or most iterative methods is a typical example. Algebraic multi-grid
(AMG) methods are numerical methods used to solve large linear systems of
equations efficiently. One of the main differences between AMG methods is how
the coarser grid is constructed from a given fine grid. There are two main
classes of AMG methods; graph and aggregation based coarsening methods. Here we
propose an aggregation-based coarsening framework leveraging graph
representation learning and clustering algorithms. Our method introduces the
power of machine learning into the AMG research field and opens a new
perspective for future researches. The proposed method uses graph
representation learning techniques to learn latent features of the graph
obtained from the underlying matrix of coefficients. Using these extracted
features, we generated a coarser grid from the fine grid. The proposed method
is highly capable of parallel computations. Our experiments show that the
proposed method's efficiency in solving large systems is closely comparable
with other aggregation-based methods, demonstrating the high capability of
graph representation learning in designing multi-grid solvers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1101.5046v1,"Jancar's formal system for deciding bisimulation of first-order grammars
  and its non-soundness","We construct an example of proof within the main formal system from
arXiv:1010.4760v3, which is intended to capture the bisimulation equivalence
for non-deterministic first-order grammars, and show that its conclusion is
semantically false. We then locate and analyze the flawed argument in the
soundness (meta)-proof of the above reference.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1912.09274v1,Neural network based limiter with transfer learning,"A neural network is trained using simulation data from a Runge Kutta
discontinuous Galerkin (RKDG) method and a modal high order limiter. With this
methodology, we design one and two-dimensional black-box shock detection
functions. Furthermore, we describe a strategy to adapt the shock detection
function to different numerical schemes without the need of a full training
cycle and large dataset. We evaluate the performance of the neural network on a
RKDG scheme for validation. To evaluate the domain adaptation properties of
this neural network limiter, our methodology is verified on a residual
distribution scheme (RDS), both in one and two-dimensional problems, and on
Cartesian and unstructured meshes. Lastly, we report on the quality of the
numerical solutions when using a neural based shock detection method, in
comparison to more traditional limiters, as well as on the computational impact
of using this method in existing codes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1309.5442v1,Operating the Cloud from Inside Out,"Virtual machine images and instances (VMs) in cloud computing centres are
typically designed as isolation containers for applications, databases and
networking functions. In order to build complex distributed applications,
multiple virtual machines must be connected, orchestrated and combined with
platform and infrastructure services from the hosting environment. There are
several reasons why sometimes it is beneficial to introduce a new layer,
Cloud-in-a-VM, which acts as a portable management interface to a cluster of
VMs. We reason about the benefits and present our Cloud-in-a-VM implementation
called Nested Cloud which allows consumers to become light-weight cloud
operators on demand and reap multiple advantages, including fully utilised
resource allocations. The practical usefulness and the performance of the
intermediate cloud stack VM are evaluated in a marketplace scenario.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1701.06823v1,"Graph Analytics for anomaly detection in homogeneous wireless networks -
  A Simulation Approach","In the Internet of Things (IoT) devices are exposed to various kinds of
attacks when connected to the Internet. An attack detection mechanism that
understands the limitations of these severely resource-constrained devices is
necessary. This is important since current approaches are either customized for
wireless networks or for the conventional Internet with heavy data
transmission. Also, the detection mechanism need not always be as
sophisticated. Simply signaling that an attack is taking place may be enough in
some situations, for example in NIDS using anomaly detection. In graph
networks, central nodes are the nodes that bear the most influence in the
network. The purpose of this research is to explore experimentally the
relationship between the behavior of central nodes and anomaly detection when
an attack spreads through a network. As a result, we propose a novel anomaly
detection approach using this unique methodology which has been unexplored so
far in communication networks. Also, in the experiment, we identify presence of
an attack originating and propagating throughout a network of IoT using our
methodology.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1607.05065v1,Processing In-memory realization using Quantum Dot Cellular Automata,"The present manuscript deals with the realization of Processing In-memory
(PIM) computing architecture using Quantum Dot Cellular Automata (QCA) and
Akers array. The PIM computing architecture becomes popular due to its
effective framework for storage and computation of data in a single unit. Here,
we illustrate two input NAND and NOR gate with the help of QCA based Akers
Array as a case study. The QCA flip-flop is used as a primitive cell to design
PIM architecture. The results suggested that both the gate have minimum power
dissipation. The polarization results of proposed architecture suggested that
the signals are in good control. The foot print of the primitive cell equals to
0.04 micron^2, which is smaller than conventional CMOS primitive cell. The
combination of QCA and Akers array provides many additional benefits over the
conventional architecture like reduction in the power consumption and feature
size, furthermore, it also improves the computational speed.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1508.05804v2,A note on the complexity of the causal ordering problem,"In this note we provide a concise report on the complexity of the causal
ordering problem, originally introduced by Simon to reason about causal
dependencies implicit in systems of mathematical equations. We show that
Simon's classical algorithm to infer causal ordering is NP-Hard---an
intractability previously guessed but never proven. We present then a detailed
account based on Nayak's suggested algorithmic solution (the best available),
which is dominated by computing transitive closure---bounded in time by
$O(|\mathcal V|\cdot |\mathcal S|)$, where $\mathcal S(\mathcal E, \mathcal V)$
is the input system structure composed of a set $\mathcal E$ of equations over
a set $\mathcal V$ of variables with number of variable appearances (density)
$|\mathcal S|$. We also comment on the potential of causal ordering for
emerging applications in large-scale hypothesis management and analytics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1308.6166v1,Bidimensionality of Geometric Intersection Graphs,"Let B be a finite collection of geometric (not necessarily convex) bodies in
the plane. Clearly, this class of geometric objects naturally generalizes the
class of disks, lines, ellipsoids, and even convex polygons. We consider
geometric intersection graphs GB where each body of the collection B is
represented by a vertex, and two vertices of GB are adjacent if the
intersection of the corresponding bodies is non-empty. For such graph classes
and under natural restrictions on their maximum degree or subgraph exclusion,
we prove that the relation between their treewidth and the maximum size of a
grid minor is linear. These combinatorial results vastly extend the
applicability of all the meta-algorithmic results of the bidimensionality
theory to geometrically defined graph classes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.02322v2,"Efficient refinements on YOLOv3 for real-time detection and assessment
  of diabetic foot Wagner grades","Currently, the screening of Wagner grades of diabetic feet (DF) still relies
on professional podiatrists. However, in less-developed countries, podiatrists
are scarce, which led to the majority of undiagnosed patients. In this study,
we proposed the real-time detection and location method for Wagner grades of DF
based on refinements on YOLOv3. We collected 2,688 data samples and implemented
several methods, such as a visual coherent image mixup, label smoothing, and
training scheduler revamping, based on the ablation study. The experimental
results suggested that the refinements on YOLOv3 achieved an accuracy of 91.95%
and the inference speed of a single picture reaches 31ms with the NVIDIA Tesla
V100. To test the performance of the model on a smartphone, we deployed the
refinements on YOLOv3 models on an Android 9 system smartphone. This work has
the potential to lead to a paradigm shift for clinical treatment of the DF in
the future, to provide an effective healthcare solution for DF tissue analysis
and healing status.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1101.0605v1,"High Performance Gravitational N-body Simulations on a Planet-wide
  Distributed Supercomputer","We report on the performance of our cold-dark matter cosmological N-body
simulation which was carried out concurrently using supercomputers across the
globe. We ran simulations on 60 to 750 cores distributed over a variety of
supercomputers in Amsterdam (the Netherlands, Europe), in Tokyo (Japan, Asia),
Edinburgh (UK, Europe) and Espoo (Finland, Europe). Regardless the network
latency of 0.32 seconds and the communication over 30.000 km of optical network
cable we are able to achieve about 87% of the performance compared to an equal
number of cores on a single supercomputer. We argue that using widely
distributed supercomputers in order to acquire more compute power is
technically feasible, and that the largest obstacle is introduced by local
scheduling and reservation policies.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.14621v1,Fair Classification via Unconstrained Optimization,"Achieving the Bayes optimal binary classification rule subject to group
fairness constraints is known to be reducible, in some cases, to learning a
group-wise thresholding rule over the Bayes regressor. In this paper, we extend
this result by proving that, in a broader setting, the Bayes optimal fair
learning rule remains a group-wise thresholding rule over the Bayes regressor
but with a (possible) randomization at the thresholds. This provides a stronger
justification to the post-processing approach in fair classification, in which
(1) a predictor is learned first, after which (2) its output is adjusted to
remove bias. We show how the post-processing rule in this two-stage approach
can be learned quite efficiently by solving an unconstrained optimization
problem. The proposed algorithm can be applied to any black-box machine
learning model, such as deep neural networks, random forests and support vector
machines. In addition, it can accommodate many fairness criteria that have been
previously proposed in the literature, such as equalized odds and statistical
parity. We prove that the algorithm is Bayes consistent and motivate it,
furthermore, via an impossibility result that quantifies the tradeoff between
accuracy and fairness across multiple demographic groups. Finally, we conclude
by validating the algorithm on the Adult benchmark dataset.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.03756v1,"Understanding Barriers and Design Opportunities to Improve Healthcare
  and QOL for Older Adults through Voice Assistants","Voice based Intelligent Virtual Assistants (IVAs) promise to improve
healthcare management and Quality of Life (QOL) by introducing the paradigm of
hands free and eye free interactions. However, there has been little
understanding regarding the challenges for designing such systems for older
adults, especially when it comes to healthcare related tasks. To tackle this,
we consider the processes of care delivery and QOL enhancements for older
adults as a collaborative task between patients and providers. By interviewing
16 older adults living independently or semi independently and 5 providers, we
identified 12 barriers that older adults might encounter during daily routine
and while managing health. We ultimately highlighted key design challenges and
opportunities that might be introduced when integrating voice based IVAs into
the life of older adults. Our work will benefit practitioners who study and
attempt to create full fledged IVA powered smart devices to deliver better care
and support an increased QOL for aging populations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0
http://arxiv.org/abs/1902.05172v1,Computability logic: Giving Caesar what belongs to Caesar,"The present article is a brief informal survey of computability logic --- the
game-semantically conceived formal theory of computational resources and tasks.
This relatively young nonclassical logic is a conservative extension of
classical first order logic but is much more expressive than the latter,
yielding a wide range of new potential application areas. In a reasonable (even
if not strict) sense the same holds for intuitionistic and linear logics, which
allows us to say that CoL reconciles and unifies the three traditions of
logical thought (and beyond) on the basis of its natural and ""universal"" game
semantics. A comprehensive online survey of the subject can be found at
http://www.csc.villanova.edu/~japaridz/CL/ .",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1207.7148v1,"A Formalization and Proof of the Extended Church-Turing Thesis -Extended
  Abstract-","We prove the Extended Church-Turing Thesis: Every effective algorithm can be
efficiently simulated by a Turing machine. This is accomplished by emulating an
effective algorithm via an abstract state machine, and simulating such an
abstract state machine by a random access machine, representing data as a
minimal term graph.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1612.04590v1,"A literature survey of the quality economics of defect-detection
  techniques","Over the last decades, a considerable amount of empirical knowledge about the
efficiency of defect-detection techniques has been accumulated. Also a few
surveys have summarised those studies with different focuses, usually for a
specific type of technique. This work reviews the results of empirical studies
and associates them with a model of software quality economics. This allows a
better comparison of the different techniques and supports the application of
the model in practice as several parameters can be approximated with typical
average values. The main contributions are the provision of average values of
several interesting quantities w.r.t. defect detection and the identification
of areas that need further research because of the limited knowledge available.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1911.09150v1,"Polylogarithmic Approximation Algorithm for k-Connected Directed Steiner
  Tree on Quasi-Bipartite Graphs","In the k-Connected Directed Steiner Tree problem (k-DST), we are given a
directed graph G=(V, E) with edge (or vertex) costs, a root vertex r, a set of
q terminals T, and a connectivity requirement k>0; the goal is to find a
minimum-cost subgraph H of G such that H has k internally disjoint paths from
the root r to each terminal t . The k-DST problem is a natural generalization
of the classical Directed Steiner Tree problem (DST) in the fault-tolerant
setting in which the solution subgraph is required to have an r,t-path, for
every terminal t, even after removing k-1 edges or vertices.
  Despite being a classical problem, there are not many positive results on the
problem, especially for the case k >= 3. In this paper, we will present an
O(log k log q)-approximation algorithm for k-DST when an input graph is
quasi-bipartite, i.e., when there is no edge joining two non-terminal vertices.
To the best of our knowledge, our algorithm is the only known non-trivial
approximation algorithm for k-DST, for k >= 3, that runs in polynomial-time
regardless of the structure of the optimal solution. In addition, our algorithm
is tight for every constant k, due to the hardness result inherited from the
Set Cover problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1001.3464v1,Formalizing cCSP Synchronous Semantics in PVS,"Compensating CSP (cCSP) is a language defined to model long running business
transactions within the framework of standard CSP process algebra. In earlier
work, we have defined both traces and operational semantics of the language. We
have shown the consistency between the two semantic models by defining a
relationship between them. Synchronization was missing from the earlier
semantic definitions which is an important feature for any process algebra. In
this paper, we address this issue by extending the syntax and semantics to
support synchronization and define a relationship between the semantic models.
Moreover, we improve the scalability of our proof technique by mechanically
verifying the semantic relationship using theorem prover PVS. We show how to
embed process algebra terms and semantics into PVS and to use these embeddings
to prove the semantic relationship.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.02086v2,"DeepBrain: Towards Personalized EEG Interaction through Attentional and
  Embedded LSTM Learning","The ""mind-controlling"" capability has always been in mankind's fantasy. With
the recent advancements of electroencephalograph (EEG) techniques,
brain-computer interface (BCI) researchers have explored various solutions to
allow individuals to perform various tasks using their minds. However, the
commercial off-the-shelf devices to run accurate EGG signal collection are
usually expensive and the comparably cheaper devices can only present coarse
results, which prevents the practical application of these devices in domestic
services. To tackle this challenge, we propose and develop an end-to-end
solution that enables fine brain-robot interaction (BRI) through embedded
learning of coarse EEG signals from the low-cost devices, namely DeepBrain, so
that people having difficulty to move, such as the elderly, can mildly command
and control a robot to perform some basic household tasks. Our contributions
are two folds: 1) We present a stacked long short term memory (Stacked LSTM)
structure with specific pre-processing techniques to handle the time-dependency
of EEG signals and their classification. 2) We propose personalized design to
capture multiple features and achieve accurate recognition of individual EEG
signals by enhancing the signal interpretation of Stacked LSTM with attention
mechanism. Our real-world experiments demonstrate that the proposed end-to-end
solution with low cost can achieve satisfactory run-time speed, accuracy and
energy-efficiency.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1608.00889v2,Approximating the Maximum Number of Synchronizing States in Automata,"We consider the problem {\sc Max Sync Set} of finding a maximum synchronizing
set of states in a given automaton. We show that the decision version of this
problem is PSPACE-complete and investigate the approximability of {\sc Max Sync
Set} for binary and weakly acyclic automata (an automaton is called weakly
acyclic if it contains no cycles other than self-loops). We prove that,
assuming $P \ne NP$, for any $\varepsilon > 0$, the {\sc Max Sync Set} problem
cannot be approximated in polynomial time within a factor of $O(n^{1 -
\varepsilon})$ for weakly acyclic $n$-state automata with alphabet of linear
size, within a factor of $O(n^{\frac{1}{2} - \varepsilon})$ for binary
$n$-state automata, and within a factor of $O(n^{\frac{1}{3} - \varepsilon})$
for binary weakly acyclic $n$-state automata. Finally, we prove that for unary
automata the problem becomes solvable in polynomial time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.04845v1,High performance computing on Android devices -- a case study,"High performance computing for low power devices can be useful to speed up
calculations on processors that use a lower clock rate than computers for which
energy efficiency is not an issue. In this trial, different high performance
techniques for Android devices have been compared, with a special focus on the
use of the GPU. Although not officially supported, the OpenCL framework can be
used on Android tablets. For the comparison of the different parallel
programming paradigms, a benchmark was chosen that could be implemented easily
with all frameworks. The Mandelbrot algorithm is computationally intensive and
has very few input and output operations. The algorithm has been implemented in
Java, C, C with assembler, C with SIMD assembler, C with OpenCL and scalar
instructions and C with OpenCL and vector instructions. The implementations
have been tested for all architectures currently supported by Android. High
speedups can be achieved using SIMD and OpenCL, although the implementation is
not straightforward for either one. Apps that use the GPU must account for the
fact that they can be suspended by the user at any moment. In using the OpenCL
framework on the GPU of Android devices, a computational power comparable to
those of modern high speed CPUs can be made available to the software
developer.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.00356v3,Committees providing EJR can be computed efficiently,"We identify a whole family of approval-based multi-winner voting rules that
satisfy PJR. Moreover, we identify a subfamily of voting rules within this
family that satisfy EJR. All these voting rules can be computed in polynomial
time as long as the subalgorithms that characterize each rule within the family
are polynomial. One of the voting rules that satisfy EJR can be computed in
$O(n m k)$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.01611v2,"When in Doubt, Ask: Generating Answerable and Unanswerable Questions,
  Unsupervised","Question Answering (QA) is key for making possible a robust communication
between human and machine. Modern language models used for QA have surpassed
the human-performance in several essential tasks; however, these models require
large amounts of human-generated training data which are costly and
time-consuming to create. This paper studies augmenting human-made datasets
with synthetic data as a way of surmounting this problem. A state-of-the-art
model based on deep transformers is used to inspect the impact of using
synthetic answerable and unanswerable questions to complement a well-known
human-made dataset. The results indicate a tangible improvement in the
performance of the language model (measured in terms of F1 and EM scores)
trained on the mixed dataset. Specifically, unanswerable question-answers prove
more effective in boosting the model: the F1 score gain from adding to the
original dataset the answerable, unanswerable, and combined question-answers
were 1.3%, 5.0%, and 6.7%, respectively. [Link to the Github repository:
https://github.com/lnikolenko/EQA]",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1406.6037v1,"Preemptive Thread Block Scheduling with Online Structural Runtime
  Prediction for Concurrent GPGPU Kernels","Recent NVIDIA Graphics Processing Units (GPUs) can execute multiple kernels
concurrently. On these GPUs, the thread block scheduler (TBS) uses the FIFO
policy to schedule their thread blocks. We show that FIFO leaves performance to
chance, resulting in significant loss of performance and fairness. To improve
performance and fairness, we propose use of the preemptive Shortest Remaining
Time First (SRTF) policy instead. Although SRTF requires an estimate of runtime
of GPU kernels, we show that such an estimate of the runtime can be easily
obtained using online profiling and exploiting a simple observation on GPU
kernels' grid structure. Specifically, we propose a novel Structural Runtime
Predictor. Using a simple Staircase model of GPU kernel execution, we show that
the runtime of a kernel can be predicted by profiling only the first few thread
blocks. We evaluate an online predictor based on this model on benchmarks from
ERCBench, and find that it can estimate the actual runtime reasonably well
after the execution of only a single thread block. Next, we design a thread
block scheduler that is both concurrent kernel-aware and uses this predictor.
We implement the SRTF policy and evaluate it on two-program workloads from
ERCBench. SRTF improves STP by 1.18x and ANTT by 2.25x over FIFO. When compared
to MPMax, a state-of-the-art resource allocation policy for concurrent kernels,
SRTF improves STP by 1.16x and ANTT by 1.3x. To improve fairness, we also
propose SRTF/Adaptive which controls resource usage of concurrently executing
kernels to maximize fairness. SRTF/Adaptive improves STP by 1.12x, ANTT by
2.23x and Fairness by 2.95x compared to FIFO. Overall, our implementation of
SRTF achieves system throughput to within 12.64% of Shortest Job First (SJF, an
oracle optimal scheduling policy), bridging 49% of the gap between FIFO and
SJF.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9902004v1,"The Alex Catalogue, A Collection of Digital Texts with Automatic Methods
  for Acquisition and Cataloging, User-Defined Typography, Cross-searching of
  Indexed Content, and a Sense of Community","This paper describes the Alex Catalogue of Electronic Texts, the only
Internet-accessible collection of digital documents allowing the user to 1)
dynamically create customized, typographically readable documents on demand, 2)
search the content of one or more documents from the collection simultaneously,
3) create sets of documents from the collection for review and annotation, and
4) publish these sets of annotated documents in turn fostering a sense of
community around the Catalogue. More than a just a collection of links that
will break over time, Alex is an archive of electronic texts providing
unprecedented access to its content and features allowing it to meet the needs
of a wide variety of users and settings. Furthermore, the process of
maintaining the Catalogue is streamlined with tools for automatic acquisition
and cataloging making it possible to sustain the service with a minimum of
personnel.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.16353v1,"Human Trust-based Feedback Control: Dynamically varying automation
  transparency to optimize human-machine interactions","Human trust in automation plays an essential role in interactions between
humans and automation. While a lack of trust can lead to a human's disuse of
automation, over-trust can result in a human trusting a faulty autonomous
system which could have negative consequences for the human. Therefore, human
trust should be calibrated to optimize human-machine interactions with respect
to context-specific performance objectives. In this article, we present a
probabilistic framework to model and calibrate a human's trust and workload
dynamics during his/her interaction with an intelligent decision-aid system.
This calibration is achieved by varying the automation's transparency---the
amount and utility of information provided to the human. The parameterization
of the model is conducted using behavioral data collected through human-subject
experiments, and three feedback control policies are experimentally validated
and compared against a non-adaptive decision-aid system. The results show that
human-automation team performance can be optimized when the transparency is
dynamically updated based on the proposed control policy. This framework is a
first step toward widespread design and implementation of real-time adaptive
automation for use in human-machine interactions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1007.3341v1,Simulation technique for available bandwidth estimation,"The paper proposes a method for measuring available bandwidth, based on
testing network packets of various sizes (Variable Packet Size method, VPS).
The boundaries of applicability of the model have been found, which are based
on the accuracy of measurements of packet delays, also we have derived a
formula of measuring the upper limit of bandwidth. The computer simulation has
been performed and relationship between the measurement error of available
bandwidth and the number of measurements has been found. Experimental
verification with the use of RIPE Test Box measuring system has shown that the
suggested method has advantages over existing measurement techniques. Pathload
utility has been chosen as an alternative technique of measurement, and to
ensure reliable results statistics by SNMP agent has been withdrawn directly
from the router.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.12909v1,Formal Verification of the Ethereum 2.0 Beacon Chain,"We report our experience in the formal verification of the reference
implementation of the Beacon Chain. The Beacon Chain is the backbone component
of the new Proof-of-Stake Ethereum 2.0 network: it is in charge of tracking
information about the validators, their stakes, their attestations (votes) and
if some validators are found to be dishonest, to slash them (they lose some of
their stakes). The Beacon Chain is mission-critical and any bug in it could
compromise the whole network. The Beacon Chain reference implementation
developed by the Ethereum Foundation is written in Python, and provides a
detailed operational description of the state machine each Beacon Chain's
network participant (node) must implement. We have formally specified and
verified the absence of runtime errors in (a large and critical part of) the
Beacon Chain reference implementation using the verification-friendly language
Dafny. During the course of this work, we have uncovered several issues,
proposed verified fixes. We have also synthesised functional correctness
specifications that enable us to provide guarantees beyond runtime errors. Our
software artefact is available at https://github.com/ConsenSys/eth2.0-dafny.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1204.2342v2,Towards A Generic Formal Framework for Access Control Systems,"There have been many proposals for access control models and authorization
policy languages, which are used to inform the design of access control
systems. Most, if not all, of these proposals impose restrictions on the
implementation of access control systems, thereby limiting the type of
authorization requests that can be processed or the structure of the
authorization policies that can be specified. In this paper, we develop a
formal characterization of the features of an access control model that imposes
few restrictions of this nature. Our characterization is intended to be a
generic framework for access control, from which we may derive access control
models and reason about the properties of those models. In this paper, we
consider the properties of monotonicity and completeness, the first being
particularly important for attribute-based access control systems. XACML, an
XML-based language and architecture for attribute-based access control, is
neither monotonic nor complete. Using our framework, we define attribute-based
access control models, in the style of XACML, that are, respectively, monotonic
and complete.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/0906.3423v1,ModelTalk: A Framework for Developing Domain Specific Executable Models,"Developing and maintaining complex, large-scale, product line of highly
customized software systems is difficult and costly. Part of the difficulty is
due to the need to communicate business knowledge between domain experts and
application programmers. Domain specific model driven development (MDD)
addresses this difficulty by providing domain experts and developers with
domain specific abstractions for communicating designs. Most MDD
implementations take a generative approach. In contrast, we adopt an
interpretive approach to domain specific model driven development. We present a
framework, named ModelTalk, that integrates MDD, dependency injection and
meta-modeling to form an interpretive, domain specific modeling framework. The
framework is complemented by tool support that provides developers with the
same advanced level of usability for modeling as they are accustomed to in
programming environments. ModelTalk is used in a commercial setting for
developing a product line of Telco grade business support systems (BSS).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.07625v1,Hybrid dynamical type theories for navigation,"We present a hybrid dynamical type theory equipped with useful primitives for
organizing and proving safety of navigational control algorithms. This type
theory combines the framework of Fu--Kishida--Selinger for constructing linear
dependent type theories from state-parameter fibrations with previous work on
categories of hybrid systems under sequential composition. We also define a
conjectural embedding of a fragment of linear-time temporal logic within our
type theory, with the goal of obtaining interoperability with existing
state-of-the-art tools for automatic controller synthesis from formal task
specifications. As a case study, we use the type theory to organize and prove
safety properties for an obstacle-avoiding navigation algorithm of
Arslan--Koditschek as implemented by Vasilopoulos. Finally, we speculate on
extensions of the type theory to deal with conjugacies between model and
physical spaces, as well as hierarchical template-anchor relationships.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0707.1099v3,"Worst-Case Interactive Communication and Enhancing Sensor Network
  Lifetime","We are concerned with the problem of maximizing the worst-case lifetime of a
data-gathering wireless sensor network consisting of a set of sensor nodes
directly communicating with a base-station.We propose to solve this problem by
modeling sensor node and base-station communication as the interactive
communication between multiple correlated informants (sensor nodes) and a
recipient (base-station). We provide practical and scalable interactive
communication protocols for data gathering in sensor networks and demonstrate
their efficiency compared to traditional approaches.
  In this paper, we first develop a formalism to address the problem of
worst-case interactive communication between a set of multiple correlated
informants and a recipient. We realize that there can be different objectives
to achieve in such a communication scenario and compute the optimal number of
messages and bits exchanged to realize these objectives. Then, we propose to
adapt these results in the context of single-hop data-gathering sensor
networks. Finally, based on this proposed formalism, we propose a clustering
based communication protocol for large sensor networks and demonstrate its
superiority over a traditional clustering protocol.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1612.08199v1,A Simple Semantics for Haskell Overloading,"As originally proposed, type classes provide overloading and ad-hoc
definition, but can still be understood (and implemented) in terms of strictly
parametric calculi. This is not true of subsequent extensions of type classes.
Functional dependencies and equality constraints allow the satisfiability of
predicates to refine typing; this means that the interpretations of equivalent
qualified types may not be interconvertible. Overlapping instances and instance
chains allow predicates to be satisfied without determining the implementations
of their associated class methods, introducing truly non-parametric behavior.
We propose a new approach to the semantics of type classes, interpreting
polymorphic expressions by the behavior of each of their ground instances, but
without requiring that those behaviors be parametrically determined. We argue
that this approach both matches the intuitive meanings of qualified types and
accurately models the behavior of programs",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1102.0969v2,"On the Complexity of Newman's Community Finding Approach for Biological
  and Social Networks","Given a graph of interactions, a module (also called a community or cluster)
is a subset of nodes whose fitness is a function of the statistical
significance of the pairwise interactions of nodes in the module. The topic of
this paper is a model-based community finding approach, commonly referred to as
modularity clustering, that was originally proposed by Newman and has
subsequently been extremely popular in practice. Various heuristic methods are
currently employed for finding the optimal solution. However, the exact
computational complexity of this approach is still largely unknown.
  To this end, we initiate a systematic study of the computational complexity
of modularity clustering. Due to the specific quadratic nature of the
modularity function, it is necessary to study its value on sparse graphs and
dense graphs separately. Our main results include a (1+\eps)-inapproximability
for dense graphs and a logarithmic approximation for sparse graphs. We make use
of several combinatorial properties of modularity to get these results. These
are the first non-trivial approximability results beyond the previously known
NP-hardness results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1507.05841v3,On the GI-Completeness of a Sorting Networks Isomorphism,"The subitemset isomorphism problem is really important and there are
excellent practical solutions described in the literature. However, the
computational complexity analysis and classification of the BZ (Bundala and
Zavodny) subitemset isomorphism problem is currently an open problem. In this
paper we prove that checking whether two sorting networks are BZ isomorphic to
each other is GI-Complete; the general GI (Graph Isomorphism) problem is known
to be in NP and LWPP, but widely believed to be neither P nor NP-Complete;
recent research suggests that the problem is in QP. Moreover, we state the BZ
sorting network isomorphism problem as a general isomorphism problem on
itemsets --- because every sorting network is represented by Bundala and
Zavodny as an itemset. The complexity classification presented in this paper
applies sorting networks, as well as the general itemset isomorphism problem.
The main consequence of our work is that currently no polynomial-time algorithm
exists for solving the BZ sorting network subitemset isomorphism problem;
however the CM (Choi and Moon) sorting network isomorphism problem can be
efficiently solved in polynomial time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.08211v3,SSN: Soft Shadow Network for Image Compositing,"We introduce an interactive Soft Shadow Network (SSN) to generates
controllable soft shadows for image compositing. SSN takes a 2D object mask as
input and thus is agnostic to image types such as painting and vector art. An
environment light map is used to control the shadow's characteristics, such as
angle and softness. SSN employs an Ambient Occlusion Prediction module to
predict an intermediate ambient occlusion map, which can be further refined by
the user to provides geometric cues to modulate the shadow generation. To train
our model, we design an efficient pipeline to produce diverse soft shadow
training data using 3D object models. In addition, we propose an inverse shadow
map representation to improve model training. We demonstrate that our model
produces realistic soft shadows in real-time. Our user studies show that the
generated shadows are often indistinguishable from shadows calculated by a
physics-based renderer and users can easily use SSN through an interactive
application to generate specific shadow effects in minutes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.03393v1,"MudrockNet: Semantic Segmentation of Mudrock SEM Images through Deep
  Learning","Segmentation and analysis of individual pores and grains of mudrocks from
scanning electron microscope images is non-trivial because of noise, imaging
artifacts, variation in pixel grayscale values across images, and overlaps in
grayscale values among different physical features such as silt grains, clay
grains, and pores in an image, which make their identification difficult.
Moreover, because grains and pores often have overlapping grayscale values,
direct application of threshold-based segmentation techniques is not
sufficient. Recent advances in the field of computer vision have made it easier
and faster to segment images and identify multiple occurrences of such features
in an image, provided that ground-truth data for training the algorithm is
available. Here, we propose a deep learning SEM image segmentation model,
MudrockNet based on Google's DeepLab-v3+ architecture implemented with the
TensorFlow library. The ground-truth data was obtained from an image-processing
workflow applied to scanning electron microscope images of uncemented muds from
the Kumano Basin offshore Japan at depths < 1.1 km. The trained deep learning
model obtained a pixel-accuracy about 90%, and predictions for the test data
obtained a mean intersection over union (IoU) of 0.6591 for silt grains and
0.6642 for pores. We also compared our model with the random forest classifier
using trainable Weka segmentation in ImageJ, and it was observed that
MudrockNet gave better predictions for both silt grains and pores. The size,
concentration, and spatial arrangement of the silt and clay grains can affect
the petrophysical properties of a mudrock, and an automated method to
accurately identify the different grains and pores in mudrocks can help improve
reservoir and seal characterization for petroleum exploration and anthropogenic
waste sequestration.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.08128v1,Tangent-Chebyshev rational maps and Redei functions,"Recently Lima and Campello de Souza introduced a new class of rational
functions over odd-order finite fields, and explained their potential
usefulness in cryptography. We show that these new functions are conjugate to
the classical family of Redei rational functions, so that the properties of the
new functions follow from properties of Redei functions. We also prove new
properties of these functions, and introduce analogous functions in
characteristic 2, while also introducing a new version of trigonometry over
finite fields of even order, which is of independent interest.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1601.05833v1,Executing Arbitrary Code in the Context of the Smartcard System Service,"This report summarizes our findings regarding a severe weakness in
implementations of the Open Mobile API deployed on several Android devices. The
vulnerability allows arbitrary code coming from a specially crafted Android
application package (APK) to be injected into and executed by the smartcard
system service component (the middleware component of the Open Mobile API
implementation). This can be exploited to gain elevated capabilities, such as
privileges protected by signature- and system-level permissions assigned to
this service. The affected source code seems to originate from the
SEEK-for-Android open-source project and was adopted by various vendor-specific
implementations of the Open Mobile API, including the one that is used on the
Nexus 6 (as of Android version 5.1).",0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1302.4957v3,"Learning Bayesian Networks: A Unification for Discrete and Gaussian
  Domains","We examine Bayesian methods for learning Bayesian networks from a combination
of prior knowledge and statistical data. In particular, we unify the approaches
we presented at last year's conference for discrete and Gaussian domains. We
derive a general Bayesian scoring metric, appropriate for both domains. We then
use this metric in combination with well-known statistical facts about the
Dirichlet and normal--Wishart distributions to derive our metrics for discrete
and Gaussian domains.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.03728v2,Flexible Networks for Learning Physical Dynamics of Deformable Objects,"Learning the physical dynamics of deformable objects with particle-based
representation has been the objective of many computational models in machine
learning. While several state-of-the-art models have achieved this objective in
simulated environments, most existing models impose a precondition, such that
the input is a sequence of ordered point sets. That is, the order of the points
in each point set must be the same across the entire input sequence. This
precondition restrains the model from generalizing to real-world data, which is
considered to be a sequence of unordered point sets. In this paper, we propose
a model named time-wise PointNet (TP-Net) that solves this problem by directly
consuming a sequence of unordered point sets to infer the future state of a
deformable object with particle-based representation. Our model consists of a
shared feature extractor that extracts global features from each input point
set in parallel and a prediction network that aggregates and reasons on these
features for future prediction. The key concept of our approach is that we use
global features rather than local features to achieve invariance to input
permutations and ensure the stability and scalability of our model. Experiments
demonstrate that our model achieves state-of-the-art performance with real-time
prediction speed in both synthetic dataset and real-world dataset. In addition,
we provide quantitative and qualitative analysis on why our approach is more
effective and efficient than existing approaches.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1502.06318v1,On the Susceptibility of the Deferred Acceptance Algorithm,"The Deferred Acceptance Algorithm (DAA) is the most widely accepted and used
algorithm to match students, workers, or residents to colleges, firms or
hospitals respectively. In this paper, we consider for the first time, the
complexity of manipulating DAA by agents such as colleges that have capacity
more than one. For such agents, truncation is not an exhaustive strategy. We
present efficient algorithms to compute a manipulation for the colleges when
the colleges are proposing or being proposed to. We then conduct detailed
experiments on the frequency of manipulable instances in order to get better
insight into strategic aspects of two-sided matching markets. Our results bear
somewhat negative news: assuming that agents have information other agents'
preference, they not only often have an incentive to misreport but there exist
efficient algorithms to find such a misreport.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1004.5424v1,"Graphic Symbol Recognition using Graph Based Signature and Bayesian
  Network Classifier","We present a new approach for recognition of complex graphic symbols in
technical documents. Graphic symbol recognition is a well known challenge in
the field of document image analysis and is at heart of most graphic
recognition systems. Our method uses structural approach for symbol
representation and statistical classifier for symbol recognition. In our system
we represent symbols by their graph based signatures: a graphic symbol is
vectorized and is converted to an attributed relational graph, which is used
for computing a feature vector for the symbol. This signature corresponds to
geometry and topology of the symbol. We learn a Bayesian network to encode
joint probability distribution of symbol signatures and use it in a supervised
learning scenario for graphic symbol recognition. We have evaluated our method
on synthetically deformed and degraded images of pre-segmented 2D architectural
and electronic symbols from GREC databases and have obtained encouraging
recognition rates.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1802.03292v1,Mathematical Logic in Computer Science,"The article retraces major events and milestones in the mutual influences
between mathematical logic and computer science since the 1950s.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
http://arxiv.org/abs/1211.0330v1,"Improved rank bounds for design matrices and a new proof of Kelly's
  theorem","We study the rank of complex sparse matrices in which the supports of
different columns have small intersections. The rank of these matrices, called
design matrices, was the focus of a recent work by Barak et. al. (BDWY11) in
which they were used to answer questions regarding point configurations. In
this work we derive near-optimal rank bounds for these matrices and use them to
obtain asymptotically tight bounds in many of the geometric applications. As a
consequence of our improved analysis, we also obtain a new, linear algebraic,
proof of Kelly's theorem, which is the complex analog of the Sylvester-Gallai
theorem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.03384v2,AutoML-Zero: Evolving Machine Learning Algorithms From Scratch,"Machine learning research has advanced in multiple aspects, including model
structures and learning methods. The effort to automate such research, known as
AutoML, has also made significant progress. However, this progress has largely
focused on the architecture of neural networks, where it has relied on
sophisticated expert-designed layers as building blocks---or similarly
restrictive search spaces. Our goal is to show that AutoML can go further: it
is possible today to automatically discover complete machine learning
algorithms just using basic mathematical operations as building blocks. We
demonstrate this by introducing a novel framework that significantly reduces
human bias through a generic search space. Despite the vastness of this space,
evolutionary search can still discover two-layer neural networks trained by
backpropagation. These simple neural networks can then be surpassed by evolving
directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques
emerge in the top algorithms, such as bilinear interactions, normalized
gradients, and weight averaging. Moreover, evolution adapts algorithms to
different task types: e.g., dropout-like techniques appear when little data is
available. We believe these preliminary successes in discovering machine
learning algorithms from scratch indicate a promising new direction for the
field.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.00180v2,Incorporating Domain Knowledge into Deep Neural Networks,"We present a survey of ways in which domain-knowledge has been included when
constructing models with neural networks. The inclusion of domain-knowledge is
of special interest not just to constructing scientific assistants, but also,
many other areas that involve understanding data using human-machine
collaboration. In many such instances, machine-based model construction may
benefit significantly from being provided with human-knowledge of the domain
encoded in a sufficiently precise form. This paper examines two broad
approaches to encode such knowledge--as logical and numerical constraints--and
describes techniques and results obtained in several sub-categories under each
of these approaches.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1312.6808v1,Socially-Aware Venue Recommendation for Conference Participants,"Current research environments are witnessing high enormities of presentations
occurring in different sessions at academic conferences. This situation makes
it difficult for researchers (especially juniors) to attend the right
presentation session(s) for effective collaboration. In this paper, we propose
an innovative venue recommendation algorithm to enhance smart conference
participation. Our proposed algorithm, Social Aware Recommendation of Venues
and Environments (SARVE), computes the Pearson Correlation and social
characteristic information of conference participants. SARVE further
incorporates the current context of both the smart conference community and
participants in order to model a recommendation process using distributed
community detection. Through the integration of the above computations and
techniques, we are able to recommend presentation sessions of active
participant presenters that may be of high interest to a particular
participant. We evaluate SARVE using a real world dataset. Our experimental
results demonstrate that SARVE outperforms other state-of-the-art methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0510073v1,Semantic Embedding of Petri Nets into Event-B,"We present an embedding of Petri nets into B abstract systems. The embedding
is achieved by translating both the static structure (modelling aspect) and the
evolution semantics of Petri nets. The static structure of a Petri-net is
captured within a B abstract system through a graph structure. This abstract
system is then included in another abstract system which captures the evolution
semantics of Petri-nets. The evolution semantics results in some B events
depending on the chosen policies: basic nets or high level Petri nets. The
current embedding enables one to use conjointly Petri nets and Event-B in the
same system development, but at different steps and for various analysis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.07745v1,Crowdsourcing Cybersecurity: Cyber Attack Detection using Social Media,"Social media is often viewed as a sensor into various societal events such as
disease outbreaks, protests, and elections. We describe the use of social media
as a crowdsourced sensor to gain insight into ongoing cyber-attacks. Our
approach detects a broad range of cyber-attacks (e.g., distributed denial of
service (DDOS) attacks, data breaches, and account hijacking) in an
unsupervised manner using just a limited fixed set of seed event triggers. A
new query expansion strategy based on convolutional kernels and dependency
parses helps model reporting structure and aids in identifying key event
characteristics. Through a large-scale analysis over Twitter, we demonstrate
that our approach consistently identifies and encodes events, outperforming
existing methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.11688v2,"Hadamard Extensions and the Identification of Mixtures of Product
  Distributions","The Hadamard Extension of a matrix is the matrix consisting of all Hadamard
products of subsets of its rows. This construction arises in the context of
identifying a mixture of product distributions on binary random variables: full
column rank of such extensions is a necessary ingredient of identification
algorithms. We provide several results concerning when a Hadamard Extension has
full column rank.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.04097v1,On Quantified Propositional Logics and the Exponential Time Hierarchy,"We study quantified propositional logics from the complexity theoretic point
of view. First we introduce alternating dependency quantified boolean formulae
(ADQBF) which generalize both quantified and dependency quantified boolean
formulae. We show that the truth evaluation for ADQBF is
AEXPTIME(poly)-complete. We also identify fragments for which the problem is
complete for the levels of the exponential hierarchy. Second we study
propositional team-based logics. We show that DQBF formulae correspond
naturally to quantified propositional dependence logic and present a general
NEXPTIME upper bound for quantified propositional logic with a large class of
generalized dependence atoms. Moreover we show AEXPTIME(poly)-completeness for
extensions of propositional team logic with generalized dependence atoms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1504.05515v2,"Parameterized complexity dichotomy for $(r,\ell)$-Vertex Deletion","For two integers $r, \ell \geq 0$, a graph $G = (V, E)$ is an
$(r,\ell)$-graph if $V$ can be partitioned into $r$ independent sets and $\ell$
cliques. In the parameterized $(r,\ell)$-Vertex Deletion problem, given a graph
$G$ and an integer $k$, one has to decide whether at most $k$ vertices can be
removed from $G$ to obtain an $(r,\ell)$-graph. This problem is NP-hard if
$r+\ell \geq 1$ and encompasses several relevant problems such as Vertex Cover
and Odd Cycle Transversal. The parameterized complexity of $(r,\ell)$-Vertex
Deletion was known for all values of $(r,\ell)$ except for $(2,1)$, $(1,2)$,
and $(2,2)$. We prove that each of these three cases is FPT and, furthermore,
solvable in single-exponential time, which is asymptotically optimal in terms
of $k$. We consider as well the version of $(r,\ell)$-Vertex Deletion where the
set of vertices to be removed has to induce an independent set, and provide
also a parameterized complexity dichotomy for this problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0405092v1,Learning Hybrid Algorithms for Vehicle Routing Problems,"This paper presents a generic technique for improving hybrid algorithms
through the discovery of and tuning of meta-heuristics. The idea is to
represent a family of push/pull heuristics that are based upon inserting and
removing tasks in a current solution, with an algebra. We then let a learning
algorithm search for the best possible algebraic term, which represents a
hybrid algorithm for a given set of problems and an optimization criterion. In
a previous paper, we described this algebra in detail and provided a set of
preliminary results demonstrating the utility of this approach, using vehicle
routing with time windows (VRPTW) as a domain example. In this paper we expand
upon our results providing a more robust experimental framework and learning
algorithms, and report on some new results using the standard Solomon
benchmarks. In particular, we show that our learning algorithm is able to
achieve results similar to the best-published algorithms using only a fraction
of the CPU time. We also show that the automatic tuning of the best hybrid
combination of such techniques yields a better solution than hand tuning, with
considerably less effort.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.09525v1,"To Automatically Map Source Code Entities to Architectural Modules with
  Naive Bayes","Background: The process of mapping a source code entity onto an architectural
module is to a large degree a manual task. Automating this process could
increase the use of static architecture conformance checking methods, such as
reflexion modeling, in industry. Current techniques rely on user
parameterization and a highly cohesive design. A machine learning approach
would potentially require fewer parameters and better use of the available
information to aid in automatic mapping. Aim: We investigate how a classifier
can be trained to map from source code to architecture modules automatically.
This classifier is trained with semantic and syntactic dependency information
extracted from the source code and from architecture descriptions. The
classifier is implemented using multinomial naive Bayes and evaluated. Method:
We perform experiments and compare the classifier with three state-of-the-art
mapping functions in eight open-source Java systems with known
ground-truth-mappings. Results: We find that the classifier outperforms the
state-of-the-art in all cases and that it provides a useful baseline for
further research in the area of semi-automatic incremental clustering.
Conclusions: We conclude that machine learning is a useful approach that
performs better and with less need for parameterization compared to other
approaches. Future work includes investigating problematic mappings and a more
diverse set of subject systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1011.6223v3,Just-In-Time compilation of OCaml byte-code,"This paper presents various improvements that were applied to OCamlJIT2, a
Just-In-Time compiler for the OCaml byte-code virtual machine. OCamlJIT2
currently runs on various Unix-like systems with x86 or x86-64 processors. The
improvements, including the new x86 port, are described in detail, and
performance measures are given, including a direct comparison of OCamlJIT2 to
OCamlJIT.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1511.09230v1,A Type Theory for Probabilistic and Bayesian Reasoning,"This paper introduces a novel type theory and logic for probabilistic
reasoning. Its logic is quantitative, with fuzzy predicates. It includes
normalisation and conditioning of states. This conditioning uses a key aspect
that distinguishes our probabilistic type theory from quantum type theory,
namely the bijective correspondence between predicates and side-effect free
actions (called instrument, or assert, maps). The paper shows how suitable
computation rules can be derived from this predicate-action correspondence, and
uses these rules for calculating conditional probabilities in two well-known
examples of Bayesian reasoning in (graphical) models. Our type theory may thus
form the basis for a mechanisation of Bayesian inference.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1611.00532v1,"An asymptotically optimal, online algorithm for weighted random sampling
  with replacement","This paper presents a novel algorithm solving the classic problem of
generating a random sample of size s from population of size n with non-uniform
probabilities. The sampling is done with replacement. The algorithm requires
constant additional memory, and works in O(n) time (even when s >> n, in which
case the algorithm produces a list containing, for every population member, the
number of times it has been selected for sample). The algorithm works online,
and as such is well-suited to processing streams. In addition, a novel method
of mass-sampling from any discrete distribution using the algorithm is
presented.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1111.0735v1,"Using Automated Dependency Analysis To Generate Representation
  Information","To preserve access to digital content, we must preserve the representation
information that captures the intended interpretation of the data. In
particular, we must be able to capture performance dependency requirements,
i.e. to identify the other resources that are required in order for the
intended interpretation to be constructed successfully. Critically, we must
identify the digital objects that are only referenced in the source data, but
are embedded in the performance, such as fonts. This paper describes a new
technique for analysing the dynamic dependencies of digital media, focussing on
analysing the process that underlies the performance, rather than parsing and
deconstructing the source data. This allows the results of format-specific
characterisation tools to be verified independently, and facilitates the
generation of representation information for any digital media format, even
when no suitable characterisation tool exists.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.05289v1,TSSort: Probabilistic Noise Resistant Sorting,"In this paper we present TSSort, a probabilistic, noise resistant, quickly
converging comparison sort algorithm based on Microsoft TrueSkill. The
algorithm combines TrueSkill's updating rules with a newly developed next item
pair selection strategy, enabling it to beat standard sorting algorithms w.r.t.
convergence speed and noise resistance, as shown in simulations. TSSort is
useful if comparisons of items are expensive or noisy, or if intermediate
results shall be approximately ordered.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1105.5924v1,"Reconstruction of Fractional Brownian Motion Signals From Its Sparse
  Samples Based on Compressive Sampling","This paper proposes a new fBm (fractional Brownian motion)
interpolation/reconstruction method from partially known samples based on CS
(Compressive Sampling). Since 1/f property implies power law decay of the fBm
spectrum, the fBm signals should be sparse in frequency domain. This property
motivates the adoption of CS in the development of the reconstruction method.
Hurst parameter H that occurs in the power law determines the sparsity level,
therefore the CS reconstruction quality of an fBm signal for a given number of
known subsamples will depend on H. However, the proposed method does not
require the information of H to reconstruct the fBm signal from its partial
samples. The method employs DFT (Discrete Fourier Transform) as the sparsity
basis and a random matrix derived from known samples positions as the
projection basis. Simulated fBm signals with various values of H are used to
show the relationship between the Hurst parameter and the reconstruction
quality. Additionally, US-DJIA (Dow Jones Industrial Average) stock index
monthly values time-series are also used to show the applicability of the
proposed method to reconstruct a real-world data.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1707.04867v1,"Near Optimal Sized Weight Tolerant Subgraph for Single Source Shortest
  Path","In this paper we address the problem of computing a sparse subgraph of a
weighted directed graph such that the exact distances from a designated source
vertex to all other vertices are preserved under bounded weight increment.
Finding a small sized subgraph that preserves distances between any pair of
vertices is a well studied problem. Since in the real world any network is
prone to failures, it is natural to study the fault tolerant version of the
above problem. Unfortunately, it turns out that there may not always exist such
a sparse subgraph even under single edge failure [Demetrescu \emph{et al.}
'08]. However in real applications it is not always the case that a link (edge)
in a network becomes completely faulty. Instead, it can happen that some links
become more congested which can easily be captured by increasing weight on the
corresponding edges. Thus it makes sense to try to construct a sparse distance
preserving subgraph under the above weight increment model. To the best of our
knowledge this problem has not been studied so far. In this paper we show that
given any weighted directed graph with $n$ vertices and a source vertex, one
can construct a subgraph that contains at most $e \cdot (k-1)!2^kn$ many edges
such that it preserves distances between the source and all other vertices as
long as the total weight increment is bounded by $k$ and we are allowed to have
only integer valued (can be negative) weight on each edge and also weight of an
edge can only be increased by some positive integer. Next we show a lower bound
of $c\cdot 2^kn$, for some constant $c \ge 5/4$, on the size of the subgraph.
We also argue that restriction of integer valued weight and integer valued
weight increment are actually essential by showing that if we remove any one of
these two restrictions we may need to store $\Omega(n^2)$ edges to preserve
distances.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1809.05408v4,Socially Aware Kalman Neural Networks for Trajectory Prediction,"Trajectory prediction is a critical technique in the navigation of robots and
autonomous vehicles. However, the complex traffic and dynamic uncertainties
yield challenges in the effectiveness and robustness in modeling. We purpose a
data-driven approach socially aware Kalman neural networks (SAKNN) where the
interaction layer and the Kalman layer are embedded in the architecture,
resulting in a class of architectures with huge potential to directly learn
from high variance sensor input and robustly generate low variance outcomes.
The evaluation of our approach on NGSIM dataset demonstrates that SAKNN
performs state-of-the-art on prediction effectiveness in a relatively long-term
horizon and significantly improves the signal-to-noise ratio of the predicted
signal.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.06116v1,"A Data-Driven Reinforcement Learning Solution Framework for Optimal and
  Adaptive Personalization of a Hip Exoskeleton","Robotic exoskeletons are exciting technologies for augmenting human mobility.
However, designing such a device for seamless integration with the human user
and to assist human movement still is a major challenge. This paper aims at
developing a novel data-driven solution framework based on reinforcement
learning (RL), without first modeling the human-robot dynamics, to provide
optimal and adaptive personalized torque assistance for reducing human efforts
during walking. Our automatic personalization solution framework includes the
assistive torque profile with two control timing parameters (peak and offset
timings), the least square policy iteration (LSPI) for learning the parameter
tuning policy, and a cost function based on transferred work ratio. The
proposed controller was successfully validated on a healthy human subject to
assist unilateral hip extension in walking. The results showed that the optimal
and adaptive RL controller as a new approach was feasible for tuning assistive
torque profile of the hip exoskeleton that coordinated with human actions and
reduced activation level of hip extensor muscle in human.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1612.00670v1,"How Are Programs Found? Speculating About Language Ergonomics With
  Curry-Howard","Functional languages with strong static type systems have beneficial
properties to help ensure program correctness and reliability. Surprisingly,
their practical significance in applications is low relative to other languages
lacking in those dimensions. In this paper, the programs-as-proofs analogy is
taken seriously to gain speculative insights by analysis of creation habits in
the proof-centric discipline of mathematics. Viewed in light of this analogy, a
sampling of mathematicians' attitudes towards formal proof suggests that the
crucial role of intuition and experimentation in programming tasks may be under
appreciated, hinting at a possible explanation of the challenges rigorously
disciplined languages face in practical applications.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
http://arxiv.org/abs/2102.00212v1,Monitoring the Impacts of a Tailings Dam Failure Using Satellite Images,"Monitoring dam failures using satellite images provides first responders with
efficient management of early interventions. It is also equally important to
monitor spatial and temporal changes in the inundation area to track the
post-disaster recovery. On January 25th, 2019, the tailings dam of the
C\'orrego do Feij\~ao iron ore mine, located in Brumadinho, Brazil, collapsed.
This disaster caused more than 230 fatalities and 30 missing people leading to
damage on the order of multiple billions of dollars. This study uses Sentinel-2
satellite images to map the inundation area and assess and delineate the land
use and land cover impacted by the dam failure. The images correspond to data
captures from January 22nd (3 days before), and February 02 (7 days after the
collapse). Satellite images of the region were classified for before and
aftermath of the disaster implementing a machine learning algorithm. In order
to have sufficient land cover types to validate the quality and accuracy of the
algorithm, 7 classes were defined: mine, forest, build-up, river, agricultural,
clear water, and grassland. The developed classification algorithm yielded a
high accuracy (99%) for the image before the collapse. This paper determines
land cover impact using two different models, 1) by using the trained network
in the ""after"" image, and 2) by creating a second network, trained in a subset
of points of the ""after"" image, and then comparing the land cover results of
the two trained networks. In the first model, applying the trained network to
the ""after"" image, the accuracy is still high (86%), but lower than using the
second model (98%). This strategy can be applied at a low cost for monitoring
and assessment by using openly available satellite information and, in case of
dam collapse or with a larger budget, higher resolution and faster data can be
obtained by fly-overs on the area of concern.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1204.2588v1,"Probabilistic Latent Tensor Factorization Model for Link Pattern
  Prediction in Multi-relational Networks","This paper aims at the problem of link pattern prediction in collections of
objects connected by multiple relation types, where each type may play a
distinct role. While common link analysis models are limited to single-type
link prediction, we attempt here to capture the correlations among different
relation types and reveal the impact of various relation types on performance
quality. For that, we define the overall relations between object pairs as a
\textit{link pattern} which consists in interaction pattern and connection
structure in the network, and then use tensor formalization to jointly model
and predict the link patterns, which we refer to as \textit{Link Pattern
Prediction} (LPP) problem. To address the issue, we propose a Probabilistic
Latent Tensor Factorization (PLTF) model by introducing another latent factor
for multiple relation types and furnish the Hierarchical Bayesian treatment of
the proposed probabilistic model to avoid overfitting for solving the LPP
problem. To learn the proposed model we develop an efficient Markov Chain Monte
Carlo sampling method. Extensive experiments are conducted on several real
world datasets and demonstrate significant improvements over several existing
state-of-the-art methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1001.5134v4,Towards Network Games with Social Preferences,"Many distributed systems can be modeled as network games: a collection of
selfish players that communicate in order to maximize their individual
utilities. The performance of such games can be evaluated through the costs of
the system equilibria: the system states in which no player can increase her
utility by unilaterally changing her behavior. However, assuming that all
players are selfish and in particular that all players have the same utility
function may not always be appropriate. Hence, several extensions to
incorporate also altruistic and malicious behavior in addition to selfishness
have been proposed over the last years. In this paper, we seek to go one step
further and study arbitrary relationships between participants. In particular,
we introduce the notion of the social range matrix and explore the effects of
the social range matrix on the equilibria in a network game. In order to derive
concrete results, we propose a simplistic network creation game that captures
the effect of social relationships among players.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1008.0201v2,Novel Modifications of Parallel Jacobi Algorithms,"We describe two main classes of one-sided trigonometric and hyperbolic
Jacobi-type algorithms for computing eigenvalues and eigenvectors of Hermitian
matrices. These types of algorithms exhibit significant advantages over many
other eigenvalue algorithms. If the matrices permit, both types of algorithms
compute the eigenvalues and eigenvectors with high relative accuracy.
  We present novel parallelization techniques for both trigonometric and
hyperbolic classes of algorithms, as well as some new ideas on how pivoting in
each cycle of the algorithm can improve the speed of the parallel one-sided
algorithms. These parallelization approaches are applicable to both
distributed-memory and shared-memory machines.
  The numerical testing performed indicates that the hyperbolic algorithms may
be superior to the trigonometric ones, although, in theory, the latter seem
more natural.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1312.6585v4,Explicit linear kernels via dynamic programming,"Several algorithmic meta-theorems on kernelization have appeared in the last
years, starting with the result of Bodlaender et al. [FOCS 2009] on graphs of
bounded genus, then generalized by Fomin et al. [SODA 2010] to graphs excluding
a fixed minor, and by Kim et al. [ICALP 2013] to graphs excluding a fixed
topological minor. Typically, these results guarantee the existence of linear
or polynomial kernels on sparse graph classes for problems satisfying some
generic conditions but, mainly due to their generality, it is not clear how to
derive from them constructive kernels with explicit constants. In this paper we
make a step toward a fully constructive meta-kernelization theory on sparse
graphs. Our approach is based on a more explicit protrusion replacement
machinery that, instead of expressibility in CMSO logic, uses dynamic
programming, which allows us to find an explicit upper bound on the size of the
derived kernels. We demonstrate the usefulness of our techniques by providing
the first explicit linear kernels for $r$-Dominating Set and $r$-Scattered Set
on apex-minor-free graphs, and for Planar-\mathcal{F}-Deletion on graphs
excluding a fixed (topological) minor in the case where all the graphs in
\mathcal{F} are connected.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1005.2072v1,A UI Design Case Study and a Prototype of a Travel Search Engine,"We review a case study of a UI design project for a complete travel search
engine system prototype for regular and corporate users. We discuss various
usage scenarios, guidelines, and so for, and put them into a web-based
prototype with screenshots and the like. We combined into our prototype the
best features found at the time (2002) on most travel-like sites and added more
to them as a part of our research. We conducted feasibility studies, review
common design guidelines and Nelson's heuristics while constructing this work.
The prototype is itself open-source, but has no backend functionality, as the
focus is the user-centered design of such a system. While the prototype is
mostly static, some dynamic activity is present through the use of PHP.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1909.07656v1,Resource-Aware Automata and Games for Optimal Synthesis,"We consider quantitative notions of parity automaton and parity game aimed at
modelling resource-aware behaviour, and study (memory-full) strategies for
exhibiting accepting runs that require a minimum amount of initial resources,
respectively for winning a game with minimum initial resources. We also show
how such strategies can be simplified to consist of only two types of moves:
the former aimed at increasing resources, the latter aimed at satisfying the
acceptance condition.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1706.05374v6,Expected Policy Gradients,"We propose expected policy gradients (EPG), which unify stochastic policy
gradients (SPG) and deterministic policy gradients (DPG) for reinforcement
learning. Inspired by expected sarsa, EPG integrates across the action when
estimating the gradient, instead of relying only on the action in the sampled
trajectory. We establish a new general policy gradient theorem, of which the
stochastic and deterministic policy gradient theorems are special cases. We
also prove that EPG reduces the variance of the gradient estimates without
requiring deterministic policies and, for the Gaussian case, with no
computational overhead. Finally, we show that it is optimal in a certain sense
to explore with a Gaussian policy such that the covariance is proportional to
the exponential of the scaled Hessian of the critic with respect to the
actions. We present empirical results confirming that this new form of
exploration substantially outperforms DPG with the Ornstein-Uhlenbeck heuristic
in four challenging MuJoCo domains.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1407.1328v1,"Toward Software Measurement and Quality Analysis of MARF and GIPSY Case
  Studies, a Team 8 SOEN6611-S14 Project Report","Measurement is an important criterion to improve the performance of a
product. This paper presents a comparative study involving measurements between
two frameworks MARF and GIPSY. Initially it establishes a thorough
understanding of these frameworks and their applications. MARF comprises of a
number of algorithms for voice and speech processing etc. GIPSY on the contrary
provides a multi lingual platform for developing compiler components. These
frameworks are meant to provide an open source environment for the programmers
or users and implement them in applications. Several metrics are used for
object-oriented design quality assessment. We use these metrics to evaluate the
code quality of both MARF and GIPSY. We describe how tools can be used to
analyze these metric values and categorize the quality of the code as excellent
or worse. Based on these values we interpret the results in terms of quality
attributes achieved. Quantitative and qualitative analysis of metric values is
made in this regard to elaborate the impact of design parameters on the quality
of the code.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/cs/0402016v1,Perspects in astrophysical databases,"Astrophysics has become a domain extremely rich of scientific data. Data
mining tools are needed for information extraction from such large datasets.
This asks for an approach to data management emphasizing the efficiency and
simplicity of data access; efficiency is obtained using multidimensional access
methods and simplicity is achieved by properly handling metadata. Moreover,
clustering and classification techniques on large datasets pose additional
requirements in terms of computation and memory scalability and
interpretability of results. In this study we review some possible solutions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1201.0397v1,Hiding Malicious Content in PDF Documents,"This paper is a proof-of-concept demonstration for a specific digital
signatures vulnerability that shows the ineffectiveness of the WYSIWYS (What
You See Is What You Sign) concept. The algorithm is fairly simple: the attacker
generates a polymorphic file that has two different types of content (text, as
a PDF document for example, and image: TIFF - two of the most widely used file
formats). When the victim signs the dual content file, he/ she only sees a PDF
document and is unaware of the hidden content inside the file. After obtaining
the legally signed document from the victim, the attacker simply has to change
the extension to the other file format. This will not invalidate the digital
signature, as no bits were altered. The destructive potential of the attack is
considerable, as the Portable Document Format (PDF) is widely used in
e-government and in e-business contexts.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2007.02669v1,"Separating Positive and Negative Data Examples by Concepts and Formulas:
  The Case of Restricted Signatures","We study the separation of positive and negative data examples in terms of
description logic (DL) concepts and formulas of decidable FO fragments, in the
presence of an ontology. In contrast to previous work, we add a signature that
specifies a subset of the symbols from the data and ontology that can be used
for separation. We consider weak and strong versions of the resulting problem
that differ in how the negative examples are treated. Our main results are that
(a projective form of) the weak version is decidable in $\mathcal{ALCI}$ while
it is undecidable in the guarded fragment GF, the guarded negation fragment
GNF, and the DL $\mathcal{ALCFIO}$, and that strong separability is decidable
in $\mathcal{ALCI}$, GF, and GNF. We also provide (mostly tight) complexity
bounds.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1808.05077v1,Exploiting Deep Learning for Persian Sentiment Analysis,"The rise of social media is enabling people to freely express their opinions
about products and services. The aim of sentiment analysis is to automatically
determine subject's sentiment (e.g., positive, negative, or neutral) towards a
particular aspect such as topic, product, movie, news etc. Deep learning has
recently emerged as a powerful machine learning technique to tackle a growing
demand of accurate sentiment analysis. However, limited work has been conducted
to apply deep learning algorithms to languages other than English, such as
Persian. In this work, two deep learning models (deep autoencoders and deep
convolutional neural networks (CNNs)) are developed and applied to a novel
Persian movie reviews dataset. The proposed deep learning models are analyzed
and compared with the state-of-the-art shallow multilayer perceptron (MLP)
based machine learning model. Simulation results demonstrate the enhanced
performance of deep learning over state-of-the-art MLP.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.00872v2,"Efficient, high-performance pancreatic segmentation using multi-scale
  feature extraction","For artificial intelligence-based image analysis methods to reach clinical
applicability, the development of high-performance algorithms is crucial. For
example, existent segmentation algorithms based on natural images are neither
efficient in their parameter use nor optimized for medical imaging. Here we
present MoNet, a highly optimized neural-network-based pancreatic segmentation
algorithm focused on achieving high performance by efficient multi-scale image
feature utilization.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0112014v5,Guaranteeing the diversity of number generators,"A major problem in using iterative number generators of the form
x_i=f(x_{i-1}) is that they can enter unexpectedly short cycles. This is hard
to analyze when the generator is designed, hard to detect in real time when the
generator is used, and can have devastating cryptanalytic implications. In this
paper we define a measure of security, called_sequence_diversity_, which
generalizes the notion of cycle-length for non-iterative generators. We then
introduce the class of counter assisted generators, and show how to turn any
iterative generator (even a bad one designed or seeded by an adversary) into a
counter assisted generator with a provably high diversity, without reducing the
quality of generators which are already cryptographically strong.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0602031v1,Classifying Signals with Local Classifiers,"This paper deals with the problem of classifying signals. The new method for
building so called local classifiers and local features is presented. The
method is a combination of the lifting scheme and the support vector machines.
Its main aim is to produce effective and yet comprehensible classifiers that
would help in understanding processes hidden behind classified signals. To
illustrate the method we present the results obtained on an artificial and a
real dataset.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0207050v2,"Value withdrawal explanations: a theoretical tool for programming
  environments","Constraint logic programming combines declarativity and efficiency thanks to
constraint solvers implemented for specific domains. Value withdrawal
explanations have been efficiently used in several constraints programming
environments but there does not exist any formalization of them. This paper is
an attempt to fill this lack. Furthermore, we hope that this theoretical tool
could help to validate some programming environments. A value withdrawal
explanation is a tree describing the withdrawal of a value during a domain
reduction by local consistency notions and labeling. Domain reduction is
formalized by a search tree using two kinds of operators: operators for local
consistency notions and operators for labeling. These operators are defined by
sets of rules. Proof trees are built with respect to these rules. For each
removed value, there exists such a proof tree which is the withdrawal
explanation of this value.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1708.06467v1,"CD Grammar Systems with Two Propagating Scattered Context Components
  Characterize the Family of Context Sensitive Languages","The L(PSCG)=L(CS) problem asks whether propagating scattered context grammars
and context sensitive grammars are equivalent. The presented paper reformulates
and answers this problem in terms of CD grammar systems. More specifically, it
characterizes the family of context sensitive languages by two-component CD
grammar systems with propagating scattered context rules.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0511027v1,Discrete Network Dynamics. Part 1: Operator Theory,"An operator algebra implementation of Markov chain Monte Carlo algorithms for
simulating Markov random fields is proposed. It allows the dynamics of networks
whose nodes have discrete state spaces to be specified by the action of an
update operator that is composed of creation and annihilation operators. This
formulation of discrete network dynamics has properties that are similar to
those of a quantum field theory of bosons, which allows reuse of many
conceptual and theoretical structures from QFT. The equilibrium behaviour of
one of these generalised MRFs and of the adaptive cluster expansion network
(ACEnet) are shown to be equivalent, which provides a way of unifying these two
theories.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1006.5099v1,Stochastic Calculus of Wrapped Compartments,"The Calculus of Wrapped Compartments (CWC) is a variant of the Calculus of
Looping Sequences (CLS). While keeping the same expressiveness, CWC strongly
simplifies the development of automatic tools for the analysis of biological
systems. The main simplification consists in the removal of the sequencing
operator, thus lightening the formal treatment of the patterns to be matched in
a term (whose complexity in CLS is strongly affected by the variables matching
in the sequences).
  We define a stochastic semantics for this new calculus. As an application we
model the interaction between macrophages and apoptotic neutrophils and a
mechanism of gene regulation in E.Coli.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.11225v2,Randomized fast no-loss expert system to play tic tac toe like a human,"This paper introduces a blazingly fast, no-loss expert system for Tic Tac Toe
using Decision Trees called T3DT, that tries to emulate human gameplay as
closely as possible. It does not make use of any brute force, minimax or
evolutionary techniques, but is still always unbeatable. In order to make the
gameplay more human-like, randomization is prioritized and T3DT randomly
chooses one of the multiple optimal moves at each step. Since it does not need
to analyse the complete game tree at any point, T3DT is exceptionally faster
than any brute force or minimax algorithm, this has been shown theoretically as
well as empirically from clock-time analyses in this paper. T3DT also doesn't
need the data sets or the time to train an evolutionary model, making it a
practical no-loss approach to play Tic Tac Toe.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0202036v1,Equivalence and Isomorphism for Boolean Constraint Satisfaction,"A Boolean constraint satisfaction instance is a conjunction of constraint
applications, where the allowed constraints are drawn from a fixed set B of
Boolean functions. We consider the problem of determining whether two given
constraint satisfaction instances are equivalent and prove a Dichotomy Theorem
by showing that for all sets C of allowed constraints, this problem is either
polynomial-time solvable or coNP-complete, and we give a simple criterion to
determine which case holds.
  A more general problem addressed in this paper is the isomorphism problem,
the problem of determining whether there exists a renaming of the variables
that makes two given constraint satisfaction instances equivalent in the above
sense. We prove that this problem is coNP-hard if the corresponding equivalence
problem is coNP-hard, and polynomial-time many-one reducible to the graph
isomorphism problem in all other cases.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0305043v1,"Modeling of aerodynamic Space-to-Surface flight with optimal trajectory
  for targeting","Modeling has been created for a Space-to-Surface system defined for an
optimal trajectory for targeting in terminal phase. The modeling includes
models for simulation atmosphere, speed of sound, aerodynamic flight and
navigation by an infrared system. The modeling simulation includes statistical
analysis of the modeling results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0205041v1,Faster Parametric Shortest Path and Minimum Balance Algorithms,"The parametric shortest path problem is to find the shortest paths in graph
where the edge costs are of the form w_ij+lambda where each w_ij is constant
and lambda is a parameter that varies. The problem is to find shortest path
trees for every possible value of lambda.
  The minimum-balance problem is to find a ``weighting'' of the vertices so
that adjusting the edge costs by the vertex weights yields a graph in which,
for every cut, the minimum weight of any edge crossing the cut in one direction
equals the minimum weight of any edge crossing the cut in the other direction.
  The paper presents fast algorithms for both problems. The algorithms run in
O(nm+n^2 log n) time. The paper also describes empirical studies of the
algorithms on random graphs, suggesting that the expected time for finding a
minimum-mean cycle (an important special case of both problems) is O(n log(n) +
m).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.10030v1,"Accelerating Framework of Transformer by Hardware Design and Model
  Compression Co-Optimization","State-of-the-art Transformer-based models, with gigantic parameters, are
difficult to be accommodated on resource constrained embedded devices.
Moreover, with the development of technology, more and more embedded devices
are available to run a Transformer model. For a Transformer model with
different constraints (tight or loose), it can be deployed onto devices with
different computing power. However, in previous work, designers did not choose
the best device among multiple devices. Instead, they just used an existing
device to deploy model, which was not necessarily the best fit and may lead to
underutilization of resources. To address the deployment challenge of
Transformer and the problem to select the best device, we propose an algorithm
& hardware closed-loop acceleration framework. Given a dataset, a model,
latency constraint LC and accuracy constraint AC, our framework can provide a
best device satisfying both constraints. In order to generate a compressed
model with high sparsity ratio, we propose a novel pruning technique,
hierarchical pruning (HP). We optimize the sparse matrix storage format for HP
matrix to further reduce memory usage for FPGA implementation. We design a
accelerator that takes advantage of HP to solve the problem of concurrent
random access. Experiments on Transformer and TinyBert model show that our
framework can find different devices for various LC and AC, covering from
low-end devices to high-end devices. Our HP can achieve higher sparsity ratio
and is more flexible than other sparsity pattern. Our framework can achieve
37x, 1.9x, 1.7x speedup compared to CPU, GPU and FPGA, respectively.",0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.01828v1,Delaunay Triangulation on Skeleton of Flowers for Classification,"In this work, we propose a Triangle based approach to classify flower images.
Initially, flowers are segmented using whorl based region merging segmentation.
Skeleton of a flower is obtained from the segmented flower using a skeleton
pruning method. The Delaunay triangulation is obtained from the endpoints and
junction points detected on the skeleton. The length and angle features are
extracted from the obtained Delaunay triangles and then are aggregated to
represent in the form of interval-valued type data. A suitable classifier has
been explored for the purpose of classification. To corroborate the efficacy of
the proposed method, an experiment is conducted on our own data set of 30
classes of flowers, containing 3000 samples.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.08417v2,Reinforcement Learning with a Corrupted Reward Channel,"No real-world reward function is perfect. Sensory errors and software bugs
may result in RL agents observing higher (or lower) rewards than they should.
For example, a reinforcement learning agent may prefer states where a sensory
error gives it the maximum reward, but where the true reward is actually small.
We formalise this problem as a generalised Markov Decision Problem called
Corrupt Reward MDP. Traditional RL methods fare poorly in CRMDPs, even under
strong simplifying assumptions and when trying to compensate for the possibly
corrupt rewards. Two ways around the problem are investigated. First, by giving
the agent richer data, such as in inverse reinforcement learning and
semi-supervised reinforcement learning, reward corruption stemming from
systematic sensory errors may sometimes be completely managed. Second, by using
randomisation to blunt the agent's optimisation, reward corruption can be
partially managed under some assumptions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.12853v2,"Optimising HEP parameter fits via Monte Carlo weight derivative
  regression","HEP event selection is traditionally considered a binary classification
problem, involving the dichotomous categories of signal and background. In
distribution fits for particle masses or couplings, however, signal events are
not all equivalent, as the signal differential cross section has different
sensitivities to the measured parameter in different regions of phase space. In
this paper, I describe a mathematical framework for the evaluation and
optimization of HEP parameter fits, where this sensitivity is defined on an
event-by-event basis, and for MC events it is modeled in terms of their MC
weight derivatives with respect to the measured parameter. Minimising the
statistical error on a measurement implies the need to resolve (i.e. separate)
events with different sensitivities, which ultimately represents a
non-dichotomous classification problem. Since MC weight derivatives are not
available for real data, the practical strategy I suggest consists in training
a regressor of weight derivatives against MC events, and then using it as an
optimal partitioning variable for 1-dimensional fits of data events. This
CHEP2019 paper is an extension of the study presented at CHEP2018: in
particular, event-by-event sensitivities allow the exact computation of the
""FIP"" ratio between the Fisher information obtained from an analysis and the
maximum information that could possibly be obtained with an ideal detector.
Using this expression, I discuss the relationship between FIP and two metrics
commonly used in Meteorology (Brier score and MSE), and the importance of
""sharpness"" both in HEP and in that domain. I finally point out that HEP
distribution fits should be optimized and evaluated using probabilistic metrics
(like FIP or MSE), whereas ranking metrics (like AUC) or threshold metrics
(like accuracy) are of limited relevance for these specific problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1904.07136v3,"Specifying Concurrent Programs in Separation Logic: Morphisms and
  Simulations","In addition to pre- and postconditions, program specifications in recent
separation logics for concurrency have employed an algebraic structure of
resources---a form of state transition system---to describe the state-based
program invariants that must be preserved, and to record the permissible atomic
changes to program state. In this paper we introduce a novel notion of resource
morphism, i.e. structure-preserving function on resources, and show how to
effectively integrate it into separation logic, using an associated notion of
morphism-specific simulation. We apply morphisms and simulations to programs
verified under one resource, to compositionally adapt them to operate under
another resource, thus facilitating proof reuse.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.11844v1,"AI at work -- Mitigating safety and discriminatory risk with technical
  standards","The use of artificial intelligence (AI) and AI methods in the workplace holds
both great opportunities as well as risks to occupational safety and
discrimination. In addition to legal regulation, technical standards will play
a key role in mitigating such risk by defining technical requirements for
development and testing of AI systems. This paper provides an overview and
assessment of existing international, European and German standards as well as
those currently under development. The paper is part of the research project
""ExamAI - Testing and Auditing of AI systems"" and focusses on the use of AI in
an industrial production environment as well as in the realm of human resource
management (HR).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0
http://arxiv.org/abs/2101.11824v5,"Exploring Lightweight Interventions at Posting Time to Reduce the
  Sharing of Misinformation on Social Media","When users on social media share content without considering its veracity,
they may unwittingly be spreading misinformation. In this work, we investigate
the design of lightweight interventions that nudge users to assess the accuracy
of information as they share it. Such assessment may deter users from posting
misinformation in the first place, and their assessments may also provide
useful guidance to friends aiming to assess those posts themselves. In support
of lightweight assessment, we first develop a taxonomy of the reasons why
people believe a news claim is or is not true; this taxonomy yields a checklist
that can be used at posting time. We conduct evaluations to demonstrate that
the checklist is an accurate and comprehensive encapsulation of people's
free-response rationales. In a second experiment, we study the effects of three
behavioral nudges -- 1) checkboxes indicating whether headings are accurate, 2)
tagging reasons (from our taxonomy) that a post is accurate via a checklist and
3) providing free-text rationales for why a headline is or is not accurate --
on people's intention of sharing the headline on social media. From an
experiment with 1668 participants, we find that both providing accuracy
assessment and rationale reduce the sharing of false content. They also reduce
the sharing of true content, but to a lesser degree that yields an overall
decrease in the fraction of shared content that is false. Our findings have
implications for designing social media and news sharing platforms that draw
from richer signals of content credibility contributed by users. In addition,
our validated taxonomy can be used by platforms and researchers as a way to
gather rationales in an easier fashion than free-response.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0911.3352v2,Counting Triangulations of Planar Point Sets,"We study the maximal number of triangulations that a planar set of $n$ points
can have, and show that it is at most $30^n$. This new bound is achieved by a
careful optimization of the charging scheme of Sharir and Welzl (2006), which
has led to the previous best upper bound of $43^n$ for the problem.
  Moreover, this new bound is useful for bounding the number of other types of
planar (i.e., crossing-free) straight-line graphs on a given point set.
Specifically, we derive new upper bounds for the number of planar graphs
($o(239.4^n)$), spanning cycles ($O(70.21^n)$), and spanning trees ($160^n$).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1908.06098v1,"Projections of achievable performance for Weather & Climate Dwarfs, and
  for entire NWP applications, on hybrid architectures","This document is one of the deliverable reports created for the ESCAPE
project. ESCAPE stands for Energy-efficient Scalable Algorithms for Weather
Prediction at Exascale. The project develops world-class, extreme-scale
computing capabilities for European operational numerical weather prediction
and future climate models. This is done by identifying Weather & Climate dwarfs
which are key patterns in terms of computation and communication (in the spirit
of the Berkeley dwarfs). These dwarfs are then optimised for different hardware
architectures (single and multi-node) and alternative algorithms are explored.
Performance portability is addressed through the use of domain specific
languages.
  This deliverable contains the description of the performance and energy
models for the selected Weather & Climate dwarfs for different hardware
architectures, multinode with GPU accelerators in particular. Presented
performance models are extension to model provided in Deliverable 3.2. With
some further enhancements, they are incorporated in the DCworms simulator. In
particular, extended models allow to predict computational and energy
performance on different architectures: single and multinodes, equipped with
CPUs and GPUs accelerators. This allows to provide feasible performance
projection at system scale.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1901.05372v2,"The notion of ""Unimaginable Numbers"" in computational number theory","Literature considers under the name \emph{unimaginable numbers} any positive
integer going beyond any physical application, with this being more of a vague
description of what we are talking about rather than an actual mathematical
definition. This simply means that research in this topic must always consider
shortened representations, usually involving \emph{recursion}, to even being
able to describe such numbers.\par\medskip One of the most known methodologies
to conceive such numbers is using \emph{hyperoperations}, that is a sequence of
binary functions defined recursively starting from the usual chain: addition -
multiplication - exponentiation. The most important notations to represent such
hyperoperations have been considered by Knuth, Goodstein, Ackermann and Conway
as described in this work's introduction.\par\medskip Within this work we will
give an axiomatic set for this topic, and then try to find on one hand other
ways to represent unimaginable numbers, as well as on the other hand
applications to computer science, where the algorithmic nature of
representations and the increased computation capabilities of computers give
the perfect field to develop further the topic.\par\medskip After the
introduction, we will give axioms and generalizations for the up-arrow
notation; in the subsequent section we consider a representation via rooted
trees of the \emph{hereditary base-$n$ notation} involved in Goodstein's
theorem, which can be used efficiently to represent some defective unimaginable
numbers, and in the last section we will analyse some methods to compare big
numbers, proving specifically a theorem about approximation using scientific
notation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1911.08595v1,Probabilistic Properties of GIG Digraphs,"We study the probabilistic properties of the Greatest Increase Grid (GIG)
digraph. We compute the probability of a particular sequence of directed edges
connecting two random vertices. We compute the joint probability that a set of
vertices are all sinks, and derive the mean and variance in the number of sinks
in a randomly labeled GIG digraph. Finally, we show that the expected size of
the maximum component of vertices converges.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1802.00362v1,"When Scientists Become Social Scientists: How Citizen Science Projects
  Learn About Volunteers","Online citizen science projects involve recruitment of volunteers to assist
researchers with the creation, curation, and analysis of large datasets.
Enhancing the quality of these data products is a fundamental concern for teams
running citizen science projects. Decisions about a project's design and
operations have a critical effect both on whether the project recruits and
retains enough volunteers, and on the quality of volunteers' work. The
processes by which the team running a project learn about their volunteers play
a critical role in these decisions. Improving these processes will enhance
decision-making, resulting in better quality datasets, and more successful
outcomes for citizen science projects. This paper presents a qualitative case
study, involving interviews and long-term observation, of how the team running
Galaxy Zoo, a major citizen science project in astronomy, came to know their
volunteers and how this knowledge shaped their decision-making processes. This
paper presents three instances that played significant roles in shaping Galaxy
Zoo team members' understandings of volunteers. Team members integrated
heterogeneous sources of information to derive new insights into the
volunteers. Project metrics and formal studies of volunteers combined with
tacit understandings gained through on- and offline interactions with
volunteers. This paper presents a number of recommendations for practice. These
recommendations include strategies for improving how citizen science project
team members learn about volunteers, and how teams can more effectively
circulate among themselves what they learn.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/cs/0501094v2,Corpus based Enrichment of GermaNet Verb Frames,"Lexical semantic resources, like WordNet, are often used in real applications
of natural language document processing. For example, we integrated GermaNet in
our document suite XDOC of processing of German forensic autopsy protocols. In
addition to the hypernymy and synonymy relation, we want to adapt GermaNet's
verb frames for our analysis. In this paper we outline an approach for the
domain related enrichment of GermaNet verb frames by corpus based syntactic and
co-occurred data analyses of real documents.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0109077v2,"Coase's Penguin, or Linux and the Nature of the Firm","The paper explains why open source software is an instance of a potentially
broader phenomenon. Specifically, I suggest that nonproprietary peer-production
of information and cultural materials will likely be a ubiquitous phenomenon in
a pervasively networked society. I describe a number of such enterprises, at
various stages of the information production value chain. These enterprises
suggest that incentives to engage in nonproprietary peer production are trivial
as long as enough contributors can be organized to contribute. This implies
that the limit on the reach of peer production efforts is the modularity,
granularity, and cost of integration of a good produced, not its total cost. I
also suggest reasons to think that peer-production can have systematic
advantages over both property-based markets and corporate managerial
hierarchies as a method of organizing information and cultural production in a
networked environment, because it is a better mechanism for clearing
information about human capital available to work on existing information
inputs to produce new outputs, and because it permits largers sets of agents to
use larger sets of resources where there are increasing returns to the scale of
both the set of agents and the set of resources available for work on projects.
As capital costs and communications costs decrease in importance as factors of
information production, the relative advantage of peer production in clearing
human capital becomes more salient.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1908.00418v1,"MIN: Co-Governing Multi-Identifier Network Architecture and its
  Prototype on Operator's Network","IP protocol is the core of TCP/IP network layer. However, since IP address
and its Domain Name are allocated and managed by a single agency, there are
risks of centralization. The semantic overload of IP address also reduces its
scalability and mobility, which further hinders the security.
  This paper proposes a co-governing Multi-Identifier Network (MIN)
architecture that constructs a network layer with parallel coexistence of
multiple identifiers, including identity, content, geographic information, and
IP address. On the management plane, we develop an efficient management system
using consortium blockchain with voting consensus, so the network can
simultaneously manage and support by hundreds or thousands of nodes with high
throughput. On the data plane, we propose an algorithm merging hash table and
prefix tree (HTP) for FIB, which avoids the false-negative error and can
inter-translate different identifiers with tens of billions of entries.
Further, we propose a scheme to transport IP packets using CCN as a tunnel for
supporting progressive deployment. We deployed the prototype of MIN to the
largest operators' network in Mainland China, Hongkong and Macao, and
demonstrated that the network can register identifier under co-governing
consensus algorithm, support VoD service very well.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1012.5664v2,Bounds on the maximum multiplicity of some common geometric graphs,"We obtain new lower and upper bounds for the maximum multiplicity of some
weighted and, respectively, non-weighted common geometric graphs drawn on n
points in the plane in general position (with no three points collinear):
perfect matchings, spanning trees, spanning cycles (tours), and triangulations.
  (i) We present a new lower bound construction for the maximum number of
triangulations a set of n points in general position can have. In particular,
we show that a generalized double chain formed by two almost convex chains
admits {\Omega}(8.65^n) different triangulations. This improves the bound
{\Omega}(8.48^n) achieved by the double zig-zag chain configuration studied by
Aichholzer et al.
  (ii) We present a new lower bound of {\Omega}(12.00^n) for the number of
non-crossing spanning trees of the double chain composed of two convex chains.
The previous bound, {\Omega}(10.42^n), stood unchanged for more than 10 years.
  (iii) Using a recent upper bound of 30^n for the number of triangulations,
due to Sharir and Sheffer, we show that n points in the plane in general
position admit at most O(68.62^n) non-crossing spanning cycles.
  (iv) We derive lower bounds for the number of maximum and minimum weighted
geometric graphs (matchings, spanning trees, and tours). We show that the
number of shortest non-crossing tours can be exponential in n. Likewise, we
show that both the number of longest non-crossing tours and the number of
longest non-crossing perfect matchings can be exponential in n. Moreover, we
show that there are sets of n points in convex position with an exponential
number of longest non-crossing spanning trees. For points in convex position we
obtain tight bounds for the number of longest and shortest tours. We give a
combinatorial characterization of the longest tours, which leads to an O(nlog
n) time algorithm for computing them.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1812.02412v3,Euler Transformation of Polyhedral Complexes,"We propose an Euler transformation that transforms a given $d$-dimensional
cell complex $K$ for $d=2,3$ into a new $d$-complex $\hat{K}$ in which every
vertex is part of a uniform even number of edges. Hence every vertex in the
graph $\hat{G}$ that is the $1$-skeleton of $\hat{K}$ has an even degree, which
makes $\hat{G}$ Eulerian, i.e., it is guaranteed to contain an Eulerian tour.
Meshes whose edges admit Eulerian tours are crucial in coverage problems
arising in several applications including 3D printing and robotics.
  For $2$-complexes in $\mathbb{R}^2$ ($d=2$) under mild assumptions (that no
two adjacent edges of a $2$-cell in $K$ are boundary edges), we show that the
Euler transformed $2$-complex $\hat{K}$ has a geometric realization in
$\mathbb{R}^2$, and that each vertex in its $1$-skeleton has degree $4$. We
bound the numbers of vertices, edges, and $2$-cells in $\hat{K}$ as small
scalar multiples of the corresponding numbers in $K$. We prove corresponding
results for $3$-complexes in $\mathbb{R}^3$ under an additional assumption that
the degree of a vertex in each $3$-cell containing it is $3$. In this setting,
every vertex in $\hat{G}$ is shown to have a degree of $6$.
  We also present bounds on parameters measuring geometric quality (aspect
ratios, minimum edge length, and maximum angle) of $\hat{K}$ in terms of the
corresponding parameters of $K$ (for $d=2, 3$). Finally, we illustrate a direct
application of the proposed Euler transformation in additive manufacturing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.09726v1,"Training future teachers in natural sciences and mathematics by means of
  computer simulation: a social constructivist approach","The monograph defines the conditions of training of future teachers in
natural sciences and mathematics by means of computer simulation, developed a
structural-functional model of training, selected socio-constructivist forms of
organization, methods and learning tools of computer modeling for future
teachers of natural and mathematical disciplines.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1605.00317v4,Performance of LDPC Decoders with Missing Connections,"Due to process variation in nanoscale manufacturing, there may be permanently
missing connections in information processing hardware. Due to timing errors in
circuits, there may be missed messages in intra-chip communications, equivalent
to transiently missing connections. In this work, we investigate the
performance of message-passing LDPC decoders in the presence of missing
connections. We prove concentration and convergence theorems that validate the
use of density evolution performance analysis. Arbitrarily small error
probability is not possible with missing connections, but we find suitably
defined decoding thresholds for communication systems with binary erasure
channels under peeling decoding, as well as binary symmetric channels under
Gallager A and B decoding. We see that decoding is robust to missing wires, as
decoding thresholds degrade smoothly. Moreover, there is a stochastic
facilitation effect in Gallager B decoders with missing connections. We also
compare the decoding sensitivity with respect to channel noise and missing
wiring.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.04905v2,i-SpaSP: Structured Neural Pruning via Sparse Signal Recovery,"We propose a novel, structured pruning algorithm for neural networks -- the
iterative, Sparse Structured Pruning algorithm, dubbed as i-SpaSP. Inspired by
ideas from sparse signal recovery, i-SpaSP operates by iteratively identifying
a larger set of important parameter groups (e.g., filters or neurons) within a
network that contribute most to the residual between pruned and dense network
output, then thresholding these groups based on a smaller, pre-defined pruning
ratio. For both two-layer and multi-layer network architectures with ReLU
activations, we show the error induced by pruning with i-SpaSP decays
polynomially, where the degree of this polynomial becomes arbitrarily large
based on the sparsity of the dense network's hidden representations. In our
experiments, i-SpaSP is evaluated across a variety of datasets (i.e., MNIST,
ImageNet, and XNLI) and architectures (i.e., feed forward networks, ResNet34,
MobileNetV2, and BERT), where it is shown to discover high-performing
sub-networks and improve upon the pruning efficiency of provable baseline
methodologies by several orders of magnitude. Put simply, i-SpaSP is easy to
implement with automatic differentiation, achieves strong empirical results,
comes with theoretical convergence guarantees, and is efficient, thus
distinguishing itself as one of the few computationally efficient, practical,
and provable pruning algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1509.06233v1,Tree Automata,"This is a reissue of the book Tree Automata by F. G\'ecseg and M. Steinby
originally published in 1984 by Akad\'emiai Kiad\'o, Budapest. Some mistakes
have been corrected and a few obscure passages have been clarified. Moreover,
some more recent contributions and current lines of research are reviewed in an
appendix that also contains several new references.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.2329v1,A Proof Theoretic Study of Soft Concurrent Constraint Programming,"Concurrent Constraint Programming (CCP) is a simple and powerful model for
concurrency where agents interact by telling and asking constraints. Since
their inception, CCP-languages have been designed for having a strong
connection to logic. In fact, the underlying constraint system can be built
from a suitable fragment of intuitionistic (linear) logic --ILL-- and processes
can be interpreted as formulas in ILL. Constraints as ILL formulas fail to
represent accurately situations where ""preferences"" (called soft constraints)
such as probabilities, uncertainty or fuzziness are present. In order to
circumvent this problem, c-semirings have been proposed as algebraic structures
for defining constraint systems where agents are allowed to tell and ask soft
constraints. Nevertheless, in this case, the tight connection to logic and
proof theory is lost. In this work, we give a proof theoretical interpretation
to soft constraints: they can be defined as formulas in a suitable fragment of
ILL with subexponentials (SELL) where subexponentials, ordered in a c-semiring
structure, are interpreted as preferences. We hence achieve two goals: (1)
obtain a CCP language where agents can tell and ask soft constraints and (2)
prove that the language in (1) has a strong connection with logic. Hence we
keep a declarative reading of processes as formulas while providing a logical
framework for soft-CCP based systems. An interesting side effect of (1) is that
one is also able to handle probabilities (and other modalities) in SELL, by
restricting the use of the promotion rule for non-idempotent c-semirings.This
finer way of controlling subexponentials allows for considering more
interesting spaces and restrictions, and it opens the possibility of specifying
more challenging computational systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1812.07277v1,The Prefetch Aggressiveness Tradeoff in 360$^{\circ}$ Video Streaming,"With 360$^{\circ}$ video, only a limited fraction of the full view is
displayed at each point in time. This has prompted the design of streaming
delivery techniques that allow alternative playback qualities to be delivered
for each candidate viewing direction. However, while prefetching based on the
user's expected viewing direction is best done close to playback deadlines,
large buffers are needed to protect against shortfalls in future available
bandwidth. This results in conflicting goals and an important prefetch
aggressiveness tradeoff problem regarding how far ahead in time from the
current playpoint prefetching should be done. This paper presents the first
characterization of this tradeoff. The main contributions include an empirical
characterization of head movement behavior based on data from viewing sessions
of four different categories of 360$^{\circ}$ video, an optimization-based
comparison of the prefetch aggressiveness tradeoffs seen for these video
categories, and a data-driven discussion of further optimizations, which
include a novel system design that allows both tradeoff objectives to be
targeted simultaneously. By qualitatively and quantitatively analyzing the
above tradeoffs, we provide insights into how to best design tomorrow's
delivery systems for 360$^{\circ}$ videos, allowing content providers to reduce
bandwidth costs and improve users' playback experiences.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.08686v1,Fully General Online Imitation Learning,"In imitation learning, imitators and demonstrators are policies for picking
actions given past interactions with the environment. If we run an imitator, we
probably want events to unfold similarly to the way they would have if the
demonstrator had been acting the whole time. No existing work provides formal
guidance in how this might be accomplished, instead restricting focus to
environments that restart, making learning unusually easy, and conveniently
limiting the significance of any mistake. We address a fully general setting,
in which the (stochastic) environment and demonstrator never reset, not even
for training purposes. Our new conservative Bayesian imitation learner
underestimates the probabilities of each available action, and queries for more
data with the remaining probability. Our main result: if an event would have
been unlikely had the demonstrator acted the whole time, that event's
likelihood can be bounded above when running the (initially totally ignorant)
imitator instead. Meanwhile, queries to the demonstrator rapidly diminish in
frequency.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.12723v1,On Interpretability and Similarity in Concept-Based Machine Learning,"Machine Learning (ML) provides important techniques for classification and
predictions. Most of these are black-box models for users and do not provide
decision-makers with an explanation. For the sake of transparency or more
validity of decisions, the need to develop explainable/interpretable ML-methods
is gaining more and more importance. Certain questions need to be addressed:
  How does an ML procedure derive the class for a particular entity? Why does a
particular clustering emerge from a particular unsupervised ML procedure? What
can we do if the number of attributes is very large? What are the possible
reasons for the mistakes for concrete cases and models?
  For binary attributes, Formal Concept Analysis (FCA) offers techniques in
terms of intents of formal concepts, and thus provides plausible reasons for
model prediction. However, from the interpretable machine learning viewpoint,
we still need to provide decision-makers with the importance of individual
attributes to the classification of a particular object, which may facilitate
explanations by experts in various domains with high-cost errors like medicine
or finance.
  We discuss how notions from cooperative game theory can be used to assess the
contribution of individual attributes in classification and clustering
processes in concept-based machine learning. To address the 3rd question, we
present some ideas on how to reduce the number of attributes using similarities
in large contexts.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0805.1296v1,"A Simple Dynamic Mind-map Framework To Discover Associative
  Relationships in Transactional Data Streams","In this paper, we informally introduce dynamic mind-maps that represent a new
approach on the basis of a dynamic construction of connectionist structures
during the processing of a data stream. This allows the representation and
processing of recursively defined structures and avoids the problem of a more
traditional, fixed-size architecture with the processing of input structures of
unknown size. For a data stream analysis with association discovery, the
incremental analysis of data leads to results on demand. Here, we describe a
framework that uses symbolic cells to calculate associations based on
transactional data streams as it exists in e.g. bibliographic databases. We
follow a natural paradigm of applying simple operations on cells yielding on a
mind-map structure that adapts over time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0204012v1,Exploiting Synergy Between Ontologies and Recommender Systems,"Recommender systems learn about user preferences over time, automatically
finding things of similar interest. This reduces the burden of creating
explicit queries. Recommender systems do, however, suffer from cold-start
problems where no initial information is available early on upon which to base
recommendations. Semantic knowledge structures, such as ontologies, can provide
valuable domain knowledge and user information. However, acquiring such
knowledge and keeping it up to date is not a trivial task and user interests
are particularly difficult to acquire and maintain. This paper investigates the
synergy between a web-based research paper recommender system and an ontology
containing information automatically extracted from departmental databases
available on the web. The ontology is used to address the recommender systems
cold-start problem. The recommender system addresses the ontology's
interest-acquisition problem. An empirical evaluation of this approach is
conducted and the performance of the integrated systems measured.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1003.0773v1,S-Program Calculus,"This paper presents a special subset of the first-order predicate logic named
S-program calculus (briefly S-calculus). The S-calculus is a calculus
consisting of so-called S-formulas that are defined over the abstract state
space of a virtual machine. We show that S-formulas are a highly general tool
for analyzing program semantics inasmuch as Hoare triplets of total and partial
correctness are not more than two S-formulas. Moreover, all the rules of Hoare
logic can be derived using S-formulas and axioms/theorems of first-order
predicate calculus. The S-calculus is a powerful mechanism for proving program
correctness as well as for building additional proving tools using theorems of
the predicate logic. Every proof is based on deriving the validity of some
S-formula, so the procedure may be automated using automatic theorem provers
(we will use Coq in this paper). As an example of the use of S-calculus, we
will prove the four basic properties of Dijsktra's operator wp. The proofs
given by Dijkstra are not completely formalized and we will show that a full
formalization can be achieved using S-calculus. Finally, we add one more
theorem to the above-mentioned four, namely the law of negation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.00242v1,"Crime Scene Re-investigation: A Postmortem Analysis of Game Account
  Stealers' Behaviors","As item trading becomes more popular, users can change their game items or
money into real money more easily. At the same time, hackers turn their eyes on
stealing other users game items or money because it is much easier to earn
money than traditional gold-farming by running game bots. Game companies
provide various security measures to block account- theft attempts, but many
security measures on the user-side are disregarded by users because of lack of
usability. In this study, we propose a server-side account theft detection
system base on action sequence analysis to protect game users from malicious
hackers. We tested this system in the real Massively Multiplayer Online Role
Playing Game (MMORPG). By analyzing users full game play log, our system can
find the particular action sequences of hackers with high accuracy. Also, we
can trace where the victim accounts stolen money goes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1
http://arxiv.org/abs/1909.09357v1,"Locality, Statefulness, and Causality in Distributed Information Systems
  (Concerning the Scale Dependence Of System Promises)","Several popular best-practice manifestos for IT design and architecture use
terms like `stateful', `stateless', `shared nothing', etc, and describe `fact
based' or `functional' descriptions of causal evolution to describe computer
processes, especially in cloud computing. The concepts are used ambiguously and
sometimes in contradictory ways, which has led to many imprecise beliefs about
their implications. This paper outlines the simple view of state and causation
in Promise Theory, which accounts for the scaling of processes and the
relativity of different observers in a natural way. It's shown that the
concepts of statefulness or statelessness are artifacts of observational scale
and causal bias towards functional evaluation. If we include feedback loops,
recursion, and process convergence, which appear acausal to external observers,
the arguments about (im)mutable state need to be modified in a scale-dependent
way. In most cases the intended focus of such remarks is not terms like
`statelessness' but process predictability. A simple principle may be
substituted in most cases as a guide to system design: the principle the
separation of dynamic scales.
  Understanding data reliance and the ability to keep stable promises is of
crucial importance to the consistency of data pipelines, and distributed
client-server interactions, albeit in different ways. With increasingly data
intensive processes over widely separated distributed deployments, e.g. in the
Internet of Things and AI applications, the effects of instability need a more
careful treatment.
  These notes are part of an initiative to engage with thinkers and
practitioners towards a more rational and disciplined language for systems
engineering for era of ubiquitous extended-cloud computing.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1707.06211v1,"Wireless enabled clothing: New modular technologies and overall supply
  chain impact","The paper is devoted to the realization of wireless enabled clothing,
employing recent new technologies in electronics, textile, and renewable power.
This new wireless enabled clothing architecture is modular and
distributed,allowing for customization in functionality and clothing designs.
Are studied the implications for supply chains,distribution channels, and cost
benefits. Modular wireless enabled clothing offers significant personalization
opportunities at costs comparable with mobile terminals.",0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1
http://arxiv.org/abs/cs/0612106v1,On Completeness of Logical Relations for Monadic Types,"Software security can be ensured by specifying and verifying security
properties of software using formal methods with strong theoretical bases. In
particular, programs can be modeled in the framework of lambda-calculi, and
interesting properties can be expressed formally by contextual equivalence
(a.k.a. observational equivalence). Furthermore, imperative features, which
exist in most real-life software, can be nicely expressed in the so-called
computational lambda-calculus. Contextual equivalence is difficult to prove
directly, but we can often use logical relations as a tool to establish it in
lambda-calculi. We have already defined logical relations for the computational
lambda-calculus in previous work. We devote this paper to the study of their
completeness w.r.t. contextual equivalence in the computational
lambda-calculus.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.03521v1,"Responsive Graphical User Interface (ReGUI) and its Implementation in
  MATLAB","In this paper we introduce the responsive graphical user interface (ReGUI)
approach to creating applications, and demonstrate how this approach can be
implemented in MATLAB. The same general technique can be used in other
programming languages.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1601.07279v1,Myopic Policy Bounds for Information Acquisition POMDPs,"This paper addresses the problem of optimal control of robotic sensing
systems aimed at autonomous information gathering in scenarios such as
environmental monitoring, search and rescue, and surveillance and
reconnaissance. The information gathering problem is formulated as a partially
observable Markov decision process (POMDP) with a reward function that captures
uncertainty reduction. Unlike the classical POMDP formulation, the resulting
reward structure is nonlinear in the belief state and the traditional
approaches do not apply directly. Instead of developing a new approximation
algorithm, we show that if attention is restricted to a class of problems with
certain structural properties, one can derive (often tight) upper and lower
bounds on the optimal policy via an efficient myopic computation. These policy
bounds can be applied in conjunction with an online branch-and-bound algorithm
to accelerate the computation of the optimal policy. We obtain informative
lower and upper policy bounds with low computational effort in a target
tracking domain. The performance of branch-and-bounding is demonstrated and
compared with exact value iteration.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1211.0729v8,A Simple Algorithm for Computing BOCP,"In this article, we devise a concise algorithm for computing BOCP. Our method
is simple, easy-to-implement but without loss of efficiency. Given two
circular-arc polygons with $m$ and $n$ edges respectively, our method runs in
$O(m+n+(l+k)\log l)$ time, using $O(m+n+k)$ space, where $k$ is the number of
intersections, and $l$ is the number of {edge}s. Our algorithm has the power to
approximate to linear complexity when $k$ and $l$ are small. The superiority of
the proposed algorithm is also validated through empirical study.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1802.07932v1,Faster integer multiplication using short lattice vectors,"We prove that $n$-bit integers may be multiplied in $O(n \log n \, 4^{\log^*
n})$ bit operations. This complexity bound had been achieved previously by
several authors, assuming various unproved number-theoretic hypotheses. Our
proof is unconditional, and depends in an essential way on Minkowski's theorem
concerning lattice vectors in symmetric convex sets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0911.1851v3,Functional units for natural numbers,"Interaction with services provided by an execution environment forms part of
the behaviours exhibited by instruction sequences under execution. Mechanisms
related to the kind of interaction in question have been proposed in the
setting of thread algebra. Like thread, service is an abstract behavioural
concept. The concept of a functional unit is similar to the concept of a
service, but more concrete. A state space is inherent in the concept of a
functional unit, whereas it is not inherent in the concept of a service. In
this paper, we establish the existence of a universal computable functional
unit for natural numbers and related results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.08288v1,"On computations with Double Schubert Automaton and stable maps of
  Multivariate Cryptography","The families of bijective transformations $G_n$ of affine space $K^n$ over
general commutative ring $K$ of increasing order with the property of stability
will be constructed. Stability means that maximal degree of elements of cyclic
subgroup generated by the transformation of degree $d$ is bounded by $d$. In
the case $K=F_q$ these transformations of $K^n$ can be of an exponential order.
We introduce large groups formed by quadratic transformations and numerical
encryption algorithm protected by secure protocol of Noncommutative
Cryptography. The construction of transformations is presented in terms of
walks on Double Schubert Graphs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.03443v2,"Group Scissor: Scaling Neuromorphic Computing Design to Large Neural
  Networks","Synapse crossbar is an elementary structure in Neuromorphic Computing Systems
(NCS). However, the limited size of crossbars and heavy routing congestion
impedes the NCS implementations of big neural networks. In this paper, we
propose a two-step framework (namely, group scissor) to scale NCS designs to
big neural networks. The first step is rank clipping, which integrates low-rank
approximation into the training to reduce total crossbar area. The second step
is group connection deletion, which structurally prunes connections to reduce
routing congestion between crossbars. Tested on convolutional neural networks
of LeNet on MNIST database and ConvNet on CIFAR-10 database, our experiments
show significant reduction of crossbar area and routing area in NCS designs.
Without accuracy loss, rank clipping reduces total crossbar area to 13.62\% and
51.81\% in the NCS designs of LeNet and ConvNet, respectively. Following rank
clipping, group connection deletion further reduces the routing area of LeNet
and ConvNet to 8.1\% and 52.06\%, respectively.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1409.0093v2,"Proceedings of the 1st OMNeT++ Community Summit, Hamburg, Germany,
  September 2, 2014","This is the Proceedings of the 1st OMNeT++ Community Summit, which was held
in Hamburg, Germany, September 2, 2014.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.04140v1,"Characterization of Time-variant and Time-invariant Assessment of
  Suicidality on Reddit using C-SSRS","Suicide is the 10th leading cause of death in the U.S (1999-2019). However,
predicting when someone will attempt suicide has been nearly impossible. In the
modern world, many individuals suffering from mental illness seek emotional
support and advice on well-known and easily-accessible social media platforms
such as Reddit. While prior artificial intelligence research has demonstrated
the ability to extract valuable information from social media on suicidal
thoughts and behaviors, these efforts have not considered both severity and
temporality of risk. The insights made possible by access to such data have
enormous clinical potential - most dramatically envisioned as a trigger to
employ timely and targeted interventions (i.e., voluntary and involuntary
psychiatric hospitalization) to save lives. In this work, we address this
knowledge gap by developing deep learning algorithms to assess suicide risk in
terms of severity and temporality from Reddit data based on the Columbia
Suicide Severity Rating Scale (C-SSRS). In particular, we employ two deep
learning approaches: time-variant and time-invariant modeling, for user-level
suicide risk assessment, and evaluate their performance against a
clinician-adjudicated gold standard Reddit corpus annotated based on the
C-SSRS. Our results suggest that the time-variant approach outperforms the
time-invariant method in the assessment of suicide-related ideations and
supportive behaviors (AUC:0.78), while the time-invariant model performed
better in predicting suicide-related behaviors and suicide attempt (AUC:0.64).
The proposed approach can be integrated with clinical diagnostic interviews for
improving suicide risk assessments.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0609062v2,Nominal Logic Programming,"Nominal logic is an extension of first-order logic which provides a simple
foundation for formalizing and reasoning about abstract syntax modulo
consistent renaming of bound names (that is, alpha-equivalence). This article
investigates logic programming based on nominal logic. We describe some typical
nominal logic programs, and develop the model-theoretic, proof-theoretic, and
operational semantics of such programs. Besides being of interest for ensuring
the correct behavior of implementations, these results provide a rigorous
foundation for techniques for analysis and reasoning about nominal logic
programs, as we illustrate via examples.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.08730v1,"Customized Video QoE Estimation with Algorithm-Agnostic Transfer
  Learning","The development of QoE models by means of Machine Learning (ML) is
challenging, amongst others due to small-size datasets, lack of diversity in
user profiles in the source domain, and too much diversity in the target
domains of QoE models. Furthermore, datasets can be hard to share between
research entities, as the machine learning models and the collected user data
from the user studies may be IPR- or GDPR-sensitive. This makes a decentralized
learning-based framework appealing for sharing and aggregating learned
knowledge in-between the local models that map the obtained metrics to the user
QoE, such as Mean Opinion Scores (MOS). In this paper, we present a transfer
learning-based ML model training approach, which allows decentralized local
models to share generic indicators on MOS to learn a generic base model, and
then customize the generic base model further using additional features that
are unique to those specific localized (and potentially sensitive) QoE nodes.
We show that the proposed approach is agnostic to specific ML algorithms,
stacked upon each other, as it does not necessitate the collaborating localized
nodes to run the same ML algorithm. Our reproducible results reveal the
advantages of stacking various generic and specific models with corresponding
weight factors. Moreover, we identify the optimal combination of algorithms and
weight factors for the corresponding localized QoE nodes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.00865v1,Neural Differential Equations for Single Image Super-resolution,"Although Neural Differential Equations have shown promise on toy problems
such as MNIST, they have yet to be successfully applied to more challenging
tasks. Inspired by variational methods for image restoration relying on partial
differential equations, we choose to benchmark several forms of Neural DEs and
backpropagation methods on single image super-resolution. The adjoint method
previously proposed for gradient estimation has no theoretical stability
guarantees; we find a practical case where this makes it unusable, and show
that discrete sensitivity analysis has better stability. In our experiments,
differential models match the performance of a state-of-the art
super-resolution model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1307.7430v2,Holographic Algorithms Beyond Matchgates,"Holographic algorithms introduced by Valiant are composed of two ingredients:
matchgates, which are gadgets realizing local constraint functions by weighted
planar perfect matchings, and holographic reductions, which show equivalences
among problems with different descriptions via certain basis transformations.
In this paper, we replace matchgates in the paradigm above by the affine type
and the product type constraint functions, which are known to be tractable in
general (not necessarily planar) graphs. More specifically, we present
polynomial-time algorithms to decide if a given counting problem has a
holographic reduction to another problem defined by the affine or product-type
functions. Our algorithms also find a holographic transformation when one
exists. We further present polynomial-time algorithms of the same decision and
search problems for symmetric functions, where the complexity is measured in
terms of the (exponentially more) succinct representations. The algorithm for
the symmetric case also shows that the recent dichotomy theorem for Holant
problems with symmetric constraints is efficiently decidable. Our proof
techniques are mainly algebraic, e.g., using stabilizers and orbits of group
actions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1603.07466v1,Semantics and Analysis of DMN Decision Tables,"The Decision Model and Notation (DMN) is a standard notation to capture
decision logic in business applications in general and business processes in
particular. A central construct in DMN is that of a decision table. The
increasing use of DMN decision tables to capture critical business knowledge
raises the need to support analysis tasks on these tables such as correctness
and completeness checking. This paper provides a formal semantics for DMN
tables, a formal definition of key analysis tasks and scalable algorithms to
tackle two such tasks, i.e., detection of overlapping rules and of missing
rules. The algorithms are based on a geometric interpretation of decision
tables that can be used to support other analysis tasks by tapping into
geometric algorithms. The algorithms have been implemented in an open-source
DMN editor and tested on large decision tables derived from a credit lending
dataset.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0201013v1,"Computing Preferred Answer Sets by Meta-Interpretation in Answer Set
  Programming","Most recently, Answer Set Programming (ASP) is attracting interest as a new
paradigm for problem solving. An important aspect which needs to be supported
is the handling of preferences between rules, for which several approaches have
been presented. In this paper, we consider the problem of implementing
preference handling approaches by means of meta-interpreters in Answer Set
Programming. In particular, we consider the preferred answer set approaches by
Brewka and Eiter, by Delgrande, Schaub and Tompits, and by Wang, Zhou and Lin.
We present suitable meta-interpreters for these semantics using DLV, which is
an efficient engine for ASP. Moreover, we also present a meta-interpreter for
the weakly preferred answer set approach by Brewka and Eiter, which uses the
weak constraint feature of DLV as a tool for expressing and solving an
underlying optimization problem. We also consider advanced meta-interpreters,
which make use of graph-based characterizations and often allow for more
efficient computations. Our approach shows the suitability of ASP in general
and of DLV in particular for fast prototyping. This can be fruitfully exploited
for experimenting with new languages and knowledge-representation formalisms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.05758v2,"It Means More if It Sounds Good: Yet Another Hypothesis Concerning the
  Evolution of Polysemous Words","This position paper looks into the formation of language and shows ties
between structural properties of the words in the English language and their
polysemy. Using Ollivier-Ricci curvature over a large graph of synonyms to
estimate polysemy it shows empirically that the words that arguably are easier
to pronounce also tend to have multiple meanings.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0110034v2,Inference of termination conditions for numerical loops in Prolog,"We present a new approach to termination analysis of numerical computations
in logic programs. Traditional approaches fail to analyse them due to non
well-foundedness of the integers. We present a technique that allows overcoming
these difficulties. Our approach is based on transforming a program in a way
that allows integrating and extending techniques originally developed for
analysis of numerical computations in the framework of query-mapping pairs with
the well-known framework of acceptability. Such an integration not only
contributes to the understanding of termination behaviour of numerical
computations, but also allows us to perform a correct analysis of such
computations automatically, by extending previous work on a constraint-based
approach to termination. Finally, we discuss possible extensions of the
technique, including incorporating general term orderings.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1811.00454v1,"Referenceless Performance Evaluation of Audio Source Separation using
  Deep Neural Networks","Current performance evaluation for audio source separation depends on
comparing the processed or separated signals with reference signals. Therefore,
common performance evaluation toolkits are not applicable to real-world
situations where the ground truth audio is unavailable. In this paper, we
propose a performance evaluation technique that does not require reference
signals in order to assess separation quality. The proposed technique uses a
deep neural network (DNN) to map the processed audio into its quality score.
Our experiment results show that the DNN is capable of predicting the
sources-to-artifacts ratio from the blind source separation evaluation toolkit
without the need for reference signals.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1703.09726v1,Ruling out FPT algorithms for Weighted Coloring on forests,"Given a graph $G$, a proper $k$-coloring of $G$ is a partition $c =
(S_i)_{i\in [1,k]}$ of $V(G)$ into $k$ stable sets $S_1,\ldots, S_{k}$. Given a
weight function $w: V(G) \to \mathbb{R}^+$, the weight of a color $S_i$ is
defined as $w(i) = \max_{v \in S_i} w(v)$ and the weight of a coloring $c$ as
$w(c) = \sum_{i=1}^{k}w(i)$. Guan and Zhu [Inf. Process. Lett., 1997] defined
the weighted chromatic number of a pair $(G,w)$, denoted by $\sigma(G,w)$, as
the minimum weight of a proper coloring of $G$. For a positive integer $r$,
they also defined $\sigma(G,w;r)$ as the minimum of $w(c)$ among all proper
$r$-colorings $c$ of $G$.
  The complexity of determining $\sigma(G,w)$ when $G$ is a tree was open for
almost 20 years, until Ara\'ujo et al. [SIAM J. Discrete Math., 2014] recently
proved that the problem cannot be solved in time $n^{o(\log n)}$ on $n$-vertex
trees unless the Exponential Time Hypothesis (ETH) fails.
  The objective of this article is to provide hardness results for computing
$\sigma(G,w)$ and $\sigma(G,w;r)$ when $G$ is a tree or a forest, relying on
complexity assumptions weaker than the ETH. Namely, we study the problem from
the viewpoint of parameterized complexity, and we assume the weaker hypothesis
$FPT \neq W[1]$. Building on the techniques of Ara\'ujo et al., we prove that
when $G$ is a forest, computing $\sigma(G,w)$ is $W[1]$-hard parameterized by
the size of a largest connected component of $G$, and that computing
$\sigma(G,w;r)$ is $W[2]$-hard parameterized by $r$. Our results rule out the
existence of $FPT$ algorithms for computing these invariants on trees or
forests for many natural choices of the parameter.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.10293v1,Decision Questions for Probabilistic Automata on Small Alphabets,"We study the emptiness and $\lambda$-reachability problems for unary and
binary Probabilistic Finite Automata (PFA) and characterise the complexity of
these problems in terms of the degree of ambiguity of the automaton and the
size of its alphabet. Our main result is that emptiness and
$\lambda$-reachability are solvable in EXPTIME for polynomially ambiguous unary
PFA and if, in addition, the transition matrix is over $\{0, 1\}$, we show they
are in NP. In contrast to the Skolem-hardness of the $\lambda$-reachability and
emptiness problems for exponentially ambiguous unary PFA, we show that these
problems are NP-hard even for finitely ambiguous unary PFA. For binary
polynomially ambiguous PFA with fixed and commuting transition matrices, we
prove NP-hardness of the $\lambda$-reachability (dimension $9$), nonstrict
emptiness (dimension $37$) and strict emptiness (dimension $40$) problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0908.2793v2,Applications of Metric Coinduction,"Metric coinduction is a form of coinduction that can be used to establish
properties of objects constructed as a limit of finite approximations. One can
prove a coinduction step showing that some property is preserved by one step of
the approximation process, then automatically infer by the coinduction
principle that the property holds of the limit object. This can often be used
to avoid complicated analytic arguments involving limits and convergence,
replacing them with simpler algebraic arguments. This paper examines the
application of this principle in a variety of areas, including infinite
streams, Markov chains, Markov decision processes, and non-well-founded sets.
These results point to the usefulness of coinduction as a general proof
technique.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1208.4945v2,Parallel ACO with a Ring Neighborhood for Dynamic TSP,"The current paper introduces a new parallel computing technique based on ant
colony optimization for a dynamic routing problem. In the dynamic traveling
salesman problem the distances between cities as travel times are no longer
fixed. The new technique uses a parallel model for a problem variant that
allows a slight movement of nodes within their Neighborhoods. The algorithm is
tested with success on several large data sets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0404046v1,"Visualising the structure of architectural open spaces based on shape
  analysis","This paper proposes the application of some well known two-dimensional
geometrical shape descriptors for the visualisation of the structure of
architectural open spaces. The paper demonstrates the use of visibility
measures such as distance to obstacles and amount of visible space to calculate
shape descriptors such as convexity and skeleton of the open space. The aim of
the paper is to indicate a simple, objective and quantifiable approach to
understand the structure of open spaces otherwise impossible due to the complex
construction of built structures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0507071v1,"Security for Distributed Web-Applications via Aspect-Oriented
  Programming","Identity Management is becoming more and more important in business systems
as they are opened for third parties including trading partners, consumers and
suppliers. This paper presents an approach securing a system without any
knowledge of the system source code. The security module adds to the existing
system authentication and authorisation based on aspect oriented programming
and the liberty alliance framework, an upcoming industrie standard providing
single sign on. In an initial training phase the module is adapted to the
application which is to be secured. Moreover the use of hardware tokens and
proactive computing is demonstrated. The high modularisation is achived through
use of AspectJ, a programming language extension of Java.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1704.02253v3,"Formalizing Mathematical Knowledge as a Biform Theory Graph: A Case
  Study","A biform theory is a combination of an axiomatic theory and an algorithmic
theory that supports the integration of reasoning and computation. These are
ideal for formalizing algorithms that manipulate mathematical expressions. A
theory graph is a network of theories connected by meaning-preserving theory
morphisms that map the formulas of one theory to the formulas of another
theory. Theory graphs are in turn well suited for formalizing mathematical
knowledge at the most convenient level of abstraction using the most convenient
vocabulary. We are interested in the problem of whether a body of mathematical
knowledge can be effectively formalized as a theory graph of biform theories.
As a test case, we look at the graph of theories encoding natural number
arithmetic. We used two different formalisms to do this, which we describe and
compare. The first is realized in ${\rm CTT}_{\rm uqe}$, a version of Church's
type theory with quotation and evaluation, and the second is realized in Agda,
a dependently typed programming language.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0603121v1,"minimUML: A Minimalist Approach to UML Diagraming for Early Computer
  Science Education","The Unified Modeling Language (UML) is commonly used in introductory Computer
Science to teach basic object-oriented design. However, there appears to be a
lack of suitable software to support this task. Many of the available programs
that support UML focus on developing code and not on enhancing learning. Those
that were designed for educational use sometimes have poor interfaces or are
missing common and important features, such as multiple selection and
undo/redo. There is a need for software that is tailored to an instructional
environment and has all the useful and needed functionality for that specific
task. This is the purpose of minimUML. minimUML provides a minimum amount of
UML, just what is commonly used in beginning programming classes, while
providing a simple, usable interface. In particular, minimUML was designed to
support abstract design while supplying features for exploratory learning and
error avoidance. In addition, it allows for the annotation of diagrams, through
text or freeform drawings, so students can receive feedback on their work.
minimUML was developed with the goal of supporting ease of use, supporting
novice students, and a requirement of no prior-training for its use.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/cs/0012010v1,The Role of Commutativity in Constraint Propagation Algorithms,"Constraint propagation algorithms form an important part of most of the
constraint programming systems. We provide here a simple, yet very general
framework that allows us to explain several constraint propagation algorithms
in a systematic way. In this framework we proceed in two steps. First, we
introduce a generic iteration algorithm on partial orderings and prove its
correctness in an abstract setting. Then we instantiate this algorithm with
specific partial orderings and functions to obtain specific constraint
propagation algorithms.
  In particular, using the notions commutativity and semi-commutativity, we
show that the {\tt AC-3}, {\tt PC-2}, {\tt DAC} and {\tt DPC} algorithms for
achieving (directional) arc consistency and (directional) path consistency are
instances of a single generic algorithm. The work reported here extends and
simplifies that of Apt \citeyear{Apt99b}.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0111029v1,Versatile Data Acquisition and Controls for Epics Using Vme-Based Fpgas,"Field-Programmable Gate Arrays (FPGAs) have provided Thomas Jefferson
National Accelerator Facility (Jefferson Lab) with versatile VME-based data
acquisition and control interfaces with minimal development times. FPGA designs
have been used to interface to VME and provide control logic for numerous
systems. The building blocks of these logic designs can be tailored to the
individual needs of each system and provide system operators with read-backs
and controls via a VME interface to an EPICS based computer. This versatility
allows the system developer to choose components and define operating
parameters and options that are not readily available commercially. Jefferson
Lab has begun developing standard FPGA libraries that result in quick turn
around times and inexpensive designs.",0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.10878v1,Enabling NAS with Automated Super-Network Generation,"Recent Neural Architecture Search (NAS) solutions have produced impressive
results training super-networks and then deriving subnetworks, a.k.a. child
models that outperform expert-crafted models from a pre-defined search space.
Efficient and robust subnetworks can be selected for resource-constrained edge
devices, allowing them to perform well in the wild. However, constructing
super-networks for arbitrary architectures is still a challenge that often
prevents the adoption of these approaches. To address this challenge, we
present BootstrapNAS, a software framework for automatic generation of
super-networks for NAS. BootstrapNAS takes a pre-trained model from a popular
architecture, e.g., ResNet- 50, or from a valid custom design, and
automatically creates a super-network out of it, then uses state-of-the-art NAS
techniques to train the super-network, resulting in subnetworks that
significantly outperform the given pre-trained model. We demonstrate the
solution by generating super-networks from arbitrary model repositories and
make available the resulting super-networks for reproducibility of the results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1707.05101v2,"On consistency of optimal pricing algorithms in repeated posted-price
  auctions with strategic buyer","We study revenue optimization learning algorithms for repeated posted-price
auctions where a seller interacts with a single strategic buyer that holds a
fixed private valuation for a good and seeks to maximize his cumulative
discounted surplus. For this setting, first, we propose a novel algorithm that
never decreases offered prices and has a tight strategic regret bound in
$\Theta(\log\log T)$ under some mild assumptions on the buyer surplus
discounting. This result closes the open research question on the existence of
a no-regret horizon-independent weakly consistent pricing. The proposed
algorithm is inspired by our observation that a double decrease of offered
prices in a weakly consistent algorithm is enough to cause a linear regret.
This motivates us to construct a novel transformation that maps a
right-consistent algorithm to a weakly consistent one that never decreases
offered prices.
  Second, we outperform the previously known strategic regret upper bound of
the algorithm PRRFES, where the improvement is achieved by means of a finer
constant factor $C$ of the principal term $C\log\log T$ in this upper bound.
Finally, we generalize results on strategic regret previously known for
geometric discounting of the buyer's surplus to discounting of other types,
namely: the optimality of the pricing PRRFES to the case of geometrically
concave decreasing discounting; and linear lower bound on the strategic regret
of a wide range of horizon-independent weakly consistent algorithms to the case
of arbitrary discounts.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.03972v2,Register automata with linear arithmetic,"We propose a novel automata model over the alphabet of rational numbers,
which we call register automata over the rationals (RA-Q). It reads a sequence
of rational numbers and outputs another rational number. RA-Q is an extension
of the well-known register automata (RA) over infinite alphabets, which are
finite automata equipped with a finite number of registers/variables for
storing values. Like in the standard RA, the RA-Q model allows both equality
and ordering tests between values. It, moreover, allows to perform linear
arithmetic between certain variables. The model is quite expressive: in
addition to the standard RA, it also generalizes other well-known models such
as affine programs and arithmetic circuits.
  The main feature of RA-Q is that despite the use of linear arithmetic, the
so-called invariant problem---a generalization of the standard non-emptiness
problem---is decidable. We also investigate other natural decision problems,
namely, commutativity, equivalence, and reachability. For deterministic RA-Q,
commutativity and equivalence are polynomial-time inter-reducible with the
invariant problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1402.2190v1,"Surfaces Representation with Sharp Features Using Sqrt(3) and Loop
  Subdivision Schemes","This paper presents a hybrid algorithm that combines features form both
Sqrt(3) and Loop Subdivision schemes. The algorithm aims at preserving sharp
features and trim regions, during the surfaces subdivision, using a set of
rules. The implementation is nontrivial due to the computational, topological,
and smoothness constraints, which should be satisfied by the underlying
surface. The fundamental innovation, in this research work, is the ability to
preserve sharp features anywhere on a surface. In addition, the resulting
representation remains within the multiresolution subdivision framework.
Preserving the original representation has a core advantage that all the
applicable operations to the multiresolution subdivision surfaces can
subsequently be applied to the edited model. Experimental results, including
surfaces coarsening and smoothing, were performed using the proposed algorithm
for validation purposes, and the results revealed that the proposed algorithm
outperforms the other recent state of the art algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.13638v1,EDLaaS; Fully Homomorphic Encryption Over Neural Network Graphs,"We present automatically parameterised Fully Homomorphic Encryption (FHE),
for encrypted neural network inference. We present and exemplify our inference
over FHE compatible neural networks with our own open-source framework and
reproducible step-by-step examples. We use the 4th generation Cheon, Kim, Kim
and Song (CKKS) FHE scheme over fixed points provided by the Microsoft Simple
Encrypted Arithmetic Library (MS-SEAL). We significantly enhance the usability
and applicability of FHE in deep learning contexts, with a focus on the
constituent graphs, traversal, and optimisation. We find that FHE is not a
panacea for all privacy preserving machine learning (PPML) problems, and that
certain limitations still remain, such as model training. However we also find
that in certain contexts FHE is well suited for computing completely private
predictions with neural networks. We focus on convolutional neural networks
(CNNs), fashion-MNIST, and levelled FHE operations. The ability to privately
compute sensitive problems more easily, while lowering the barriers to entry,
can allow otherwise too-sensitive fields to begin advantaging themselves of
performant third-party neural networks. Lastly we show encrypted deep learning,
applied to a sensitive real world problem in agri-food, and how this can have a
large positive impact on food-waste and encourage much-needed data sharing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1201.4754v1,Existence of Stability in Hedonic Coalition Formation Games,"In this paper, we examine \emph{hedonic coalition formation games} in which
each player's preferences over partitions of players depend only on the members
of his coalition. We present three main results in which restrictions on the
preferences of the players guarantee the existence of stable partitions for
various notions of stability. The preference restrictions pertain to \emph{top
responsiveness} and \emph{bottom responsiveness} which model optimistic and
pessimistic behavior of players respectively. The existence results apply to
natural subclasses of \emph{additive separable hedonic games} and \emph{hedonic
games with \B-preferences}. It is also shown that our existence results cannot
be strengthened to the case of stronger known stability concepts.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.12987v1,"Overview of the CLEF--2021 CheckThat! Lab on Detecting Check-Worthy
  Claims, Previously Fact-Checked Claims, and Fake News","We describe the fourth edition of the CheckThat! Lab, part of the 2021
Conference and Labs of the Evaluation Forum (CLEF). The lab evaluates
technology supporting tasks related to factuality, and covers Arabic,
Bulgarian, English, Spanish, and Turkish. Task 1 asks to predict which posts in
a Twitter stream are worth fact-checking, focusing on COVID-19 and politics (in
all five languages). Task 2 asks to determine whether a claim in a tweet can be
verified using a set of previously fact-checked claims (in Arabic and English).
Task 3 asks to predict the veracity of a news article and its topical domain
(in English). The evaluation is based on mean average precision or precision at
rank k for the ranking tasks, and macro-F1 for the classification tasks. This
was the most popular CLEF-2021 lab in terms of team registrations: 132 teams.
Nearly one-third of them participated: 15, 5, and 25 teams submitted official
runs for tasks 1, 2, and 3, respectively.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.08247v2,"Probabilistic Localization of Insect-Scale Drones on Floating-Gate
  Inverter Arrays","We propose a novel compute-in-memory (CIM)-based ultra-low-power framework
for probabilistic localization of insect-scale drones. The conventional
probabilistic localization approaches rely on the three-dimensional (3D)
Gaussian Mixture Model (GMM)-based representation of a 3D map. A GMM model with
hundreds of mixture functions is typically needed to adequately learn and
represent the intricacies of the map. Meanwhile, localization using complex GMM
map models is computationally intensive. Since insect-scale drones operate
under extremely limited area/power budget, continuous localization using GMM
models entails much higher operating energy -- thereby, limiting flying
duration and/or size of the drone due to a larger battery. Addressing the
computational challenges of localization in an insect-scale drone using a CIM
approach, we propose a novel framework of 3D map representation using a
harmonic mean of ""Gaussian-like"" mixture (HMGM) model. The likelihood function
useful for drone localization can be efficiently implemented by connecting many
multi-input inverters in parallel, each programmed with the parameters of the
3D map model represented as HMGM. When the depth measurements are projected to
the input of the implementation, the summed current of the inverters emulates
the likelihood of the measurement. We have characterized our approach on an
RGB-D indoor localization dataset. The average localization error in our
approach is $\sim$0.1125 m which is only slightly degraded than software-based
evaluation ($\sim$0.08 m). Meanwhile, our localization framework is
ultra-low-power, consuming as little as $\sim$17 $\mu$W power while processing
a depth frame in 1.33 ms over hundred pose hypotheses in the particle-filtering
(PF) algorithm used to localize the drone.",0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1211.5227v1,"Service Composition Design Pattern for Autonomic Computing Systems using
  Association Rule based Learning and Service-Oriented Architecture","In this paper we present a Service Injection and composition Design Pattern
for Unstructured Peer-to-Peer networks, which is designed with Aspect-oriented
design patterns, and amalgamation of the Strategy, Worker Object, and
Check-List Design Patterns used to design the Self-Adaptive Systems. It will
apply self reconfiguration planes dynamically without the interruption or
intervention of the administrator for handling service failures at the servers.
When a client requests for a complex service, Service Composition should be
done to fulfil the request. If a service is not available in the memory, it
will be injected as Aspectual Feature Module code. We used Service Oriented
Architecture (SOA) with Web Services in Java to Implement the composite Design
Pattern. As far as we know, there are no studies on composition of design
patterns for Peer-to-peer computing domain. The pattern is described using a
java-like notation for the classes and interfaces. A simple UML class and
Sequence diagrams are depicted.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0003016v1,"Abductive and Consistency-Based Diagnosis Revisited: a Modeling
  Perspective","Diagnostic reasoning has been characterized logically as consistency-based
reasoning or abductive reasoning. Previous analyses in the literature have
shown, on the one hand, that choosing the (in general more restrictive)
abductive definition may be appropriate or not, depending on the content of the
knowledge base [Console&Torasso91], and, on the other hand, that, depending on
the choice of the definition the same knowledge should be expressed in
different form [Poole94].
  Since in Model-Based Diagnosis a major problem is finding the right way of
abstracting the behavior of the system to be modeled, this paper discusses the
relation between modeling, and in particular abstraction in the model, and the
notion of diagnosis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0106058v1,"Enabling the Long-Term Archival of Signed Documents through Time
  Stamping","In this paper we describe how to build a trusted reliable distributed service
across administrative domains in a peer-to-peer network. The application we use
to motivate our work is a public key time stamping service called Prokopius.
The service provides a secure, verifiable but distributable stable archive that
maintains time stamped snapshots of public keys over time. This in turn allows
clients to verify time stamped documents or certificates that rely on formerly
trusted public keys that are no longer in service or where the signer no longer
exists. We find that such a service can time stamp the snapshots of public keys
in a network of 148 nodes at the granularity of a couple of days, even in the
worst case where an adversary causes the maximal amount of damage allowable
within our fault model.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2003.11139v2,Cheating in online gaming spreads through observation and victimization,"Antisocial behavior can be contagious, spreading from individual to
individual and rippling through social networks. Moreover, it can spread not
only through third-party influence from observation, just like innovations or
individual behavior do, but also through direct experience, via
""pay-it-forward"" retaliation. Here, we distinguish between the effects of
observation and victimization for the contagion of antisocial behavior by
analyzing large-scale digital-trace data. We study the spread of cheating in
more than a million matches of an online multiplayer first-person shooter game,
in which up to 100 players compete individually or in teams against strangers.
We identify event sequences in which a player who observes or is killed by a
certain number of cheaters starts cheating, and evaluate the extent to which
these sequences would appear if we preserve the team and interaction structure
but assume alternative gameplay scenarios. The results reveal that social
contagion is only likely to exist for those who both observe and experience
cheating, suggesting that third-party influence and ""pay-it-forward""
reciprocity interact positively. In addition, the effect is present only for
those who both observe and experience more than once, suggesting that cheating
is more likely to spread after repeated or multi-source exposure. Approaching
online games as models of social systems, we use the findings to discuss
strategies for targeted interventions to stem the spread of cheating and
antisocial behavior more generally in online communities, schools,
organizations, and sports.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1411.5003v3,"Survey of End-to-End Mobile Network Measurement Testbeds, Tools, and
  Services","Mobile (cellular) networks enable innovation, but can also stifle it and lead
to user frustration when network performance falls below expectations. As
mobile networks become the predominant method of Internet access, developer,
research, network operator, and regulatory communities have taken an increased
interest in measuring end-to-end mobile network performance to, among other
goals, minimize negative impact on application responsiveness. In this survey
we examine current approaches to end-to-end mobile network performance
measurement, diagnosis, and application prototyping. We compare available tools
and their shortcomings with respect to the needs of researchers, developers,
regulators, and the public. We intend for this survey to provide a
comprehensive view of currently active efforts and some auspicious directions
for future work in mobile network measurement and mobile application
performance evaluation.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.08265v1,"Mining for Process Improvements: Analyzing Software Repositories in
  Agile Retrospectives","Software Repositories contain knowledge on how software engineering teams
work, communicate, and collaborate. It can be used to develop a data-informed
view of a team's development process, which in turn can be employed for process
improvement initiatives. In modern, Agile development methods, process
improvement takes place in Retrospective meetings, in which the last
development iteration is discussed. However, previously proposed activities
that take place in these meetings often do not rely on project data, instead
depending solely on the perceptions of team members. We propose new
Retrospective activities, based on mining the software repositories of
individual teams, to complement existing approaches with more objective,
data-informed process views.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1810.09390v1,A minimax near-optimal algorithm for adaptive rejection sampling,"Rejection Sampling is a fundamental Monte-Carlo method. It is used to sample
from distributions admitting a probability density function which can be
evaluated exactly at any given point, albeit at a high computational cost.
However, without proper tuning, this technique implies a high rejection rate.
Several methods have been explored to cope with this problem, based on the
principle of adaptively estimating the density by a simpler function, using the
information of the previous samples. Most of them either rely on strong
assumptions on the form of the density, or do not offer any theoretical
performance guarantee. We give the first theoretical lower bound for the
problem of adaptive rejection sampling and introduce a new algorithm which
guarantees a near-optimal rejection rate in a minimax sense.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1811.01158v1,Boosted Sparse and Low-Rank Tensor Regression,"We propose a sparse and low-rank tensor regression model to relate a
univariate outcome to a feature tensor, in which each unit-rank tensor from the
CP decomposition of the coefficient tensor is assumed to be sparse. This
structure is both parsimonious and highly interpretable, as it implies that the
outcome is related to the features through a few distinct pathways, each of
which may only involve subsets of feature dimensions. We take a
divide-and-conquer strategy to simplify the task into a set of sparse unit-rank
tensor regression problems. To make the computation efficient and scalable, for
the unit-rank tensor regression, we propose a stagewise estimation procedure to
efficiently trace out its entire solution path. We show that as the step size
goes to zero, the stagewise solution paths converge exactly to those of the
corresponding regularized regression. The superior performance of our approach
is demonstrated on various real-world and synthetic examples.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1701.02571v2,Stack Semantics of Type Theory,"We give a model of dependent type theory with one univalent universe and
propositional truncation interpreting a type as a stack, generalising the
groupoid model of type theory. As an application, we show that countable choice
cannot be proved in dependent type theory with one univalent universe and
propositional truncation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1302.1920v1,"SWATI: Synthesizing Wordlengths Automatically Using Testing and
  Induction","In this paper, we present an automated technique SWATI: Synthesizing
Wordlengths Automatically Using Testing and Induction, which uses a combination
of Nelder-Mead optimization based testing, and induction from examples to
automatically synthesize optimal fixedpoint implementation of numerical
routines. The design of numerical software is commonly done using
floating-point arithmetic in design-environments such as Matlab. However, these
designs are often implemented using fixed-point arithmetic for speed and
efficiency reasons especially in embedded systems. The fixed-point
implementation reduces implementation cost, provides better performance, and
reduces power consumption. The conversion from floating-point designs to
fixed-point code is subject to two opposing constraints: (i) the word-width of
fixed-point types must be minimized, and (ii) the outputs of the fixed-point
program must be accurate. In this paper, we propose a new solution to this
problem. Our technique takes the floating-point program, specified accuracy and
an implementation cost model and provides the fixed-point program with
specified accuracy and optimal implementation cost. We demonstrate the
effectiveness of our approach on a set of examples from the domain of automated
control, robotics and digital signal processing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1201.5365v2,Fast Computation of Smith Forms of Sparse Matrices Over Local Rings,"We present algorithms to compute the Smith Normal Form of matrices over two
families of local rings.
  The algorithms use the \emph{black-box} model which is suitable for sparse
and structured matrices. The algorithms depend on a number of tools, such as
matrix rank computation over finite fields, for which the best-known time- and
memory-efficient algorithms are probabilistic.
  For an $\nxn$ matrix $A$ over the ring $\Fzfe$, where $f^e$ is a power of an
irreducible polynomial $f \in \Fz$ of degree $d$, our algorithm requires
$\bigO(\eta de^2n)$ operations in $\F$, where our black-box is assumed to
require $\bigO(\eta)$ operations in $\F$ to compute a matrix-vector product by
a vector over $\Fzfe$ (and $\eta$ is assumed greater than $\Pden$). The
algorithm only requires additional storage for $\bigO(\Pden)$ elements of $\F$.
In particular, if $\eta=\softO(\Pden)$, then our algorithm requires only
$\softO(n^2d^2e^3)$ operations in $\F$, which is an improvement on known dense
methods for small $d$ and $e$.
  For the ring $\ZZ/p^e\ZZ$, where $p$ is a prime, we give an algorithm which
is time- and memory-efficient when the number of nontrivial invariant factors
is small. We describe a method for dimension reduction while preserving the
invariant factors. The time complexity is essentially linear in $\mu n r e \log
p,$ where $\mu$ is the number of operations in $\ZZ/p\ZZ$ to evaluate the
black-box (assumed greater than $n$) and $r$ is the total number of non-zero
invariant factors.
  To avoid the practical cost of conditioning, we give a Monte Carlo
certificate, which at low cost, provides either a high probability of success
or a proof of failure. The quest for a time- and memory-efficient solution
without restrictions on the number of nontrivial invariant factors remains
open. We offer a conjecture which may contribute toward that end.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0710.3332v4,Model and Program Repair via SAT Solving,"We consider the following \emph{model repair problem}: given a finite Kripke
structure $M$ and a specification formula $\eta$ in some modal or temporal
logic, determine if $M$ contains a substructure $M'$ (with the same initial
state) that satisfies $\eta$. Thus, $M$ can be ``repaired'' to satisfy the
specification $\eta$ by deleting some transitions.
  We map an instance $(M, \eta)$ of model repair to a boolean formula
$\repfor(M,\eta)$ such that $(M, \eta)$ has a solution iff $\repfor(M,\eta)$ is
satisfiable. Furthermore, a satisfying assignment determines which transitions
must be removed from $M$ to generate a model $M'$ of $\eta$. Thus, we can use
any SAT solver to repair Kripke structures. Furthermore, using a complete SAT
solver yields a complete algorithm: it always finds a repair if one exists.
  We extend our method to repair finite-state shared memory concurrent
programs, to solve the discrete event supervisory control problem
\cite{RW87,RW89}, to check for the existence of symmettric solutions
\cite{ES93}, and to accomodate any boolean constraint on the existence of
states and transitions in the repaired model.
  Finally, we show that model repair is NP-complete for CTL, and logics with
polynomial model checking algorithms to which CTL can be reduced in polynomial
time. A notable example of such a logic is Alternating-Time Temporal Logic
(ATL).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1307.6272v4,On Axiomatization of Inconsistency Indicators for Pairwise Comparisons,"We examine the notion of inconsistency in pairwise comparisons and propose an
axiomatization which is independent of any method of approximation or the
inconsistency indicator definition (e.g., Analytic Hierarchy Process, AHP). It
has been proven that the eigenvalue-based inconsistency (proposed as a part of
AHP) is incorrect.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1406.4712v3,"An algorithm for Boolean satisfiability based on generalized orthonormal
  expansion","This paper proposes an algorithm for deciding consistency of systems of
Boolean equations in several variables with co-efficients in the two element
Boolean algebra $B_{0}=\{0,1\}$ and find all satisfying assignments. The
algorithm is based on the application of a well known generalized Boole-Shannon
orthonormal (ON) expansion of Boolean functions. A necessary and sufficient
consistency condition for a special class of functions was developed in
\cite{sule} using such an expansion. Paper \cite{sule} develops a condition for
consistency of the equation $f(X)=0$ for the special classes of Boolean
functions 1) $f$ in $B(\Phi(X))$ for an ON set $\Phi$ of Boolean functions in
$X$ over a general Boolean algebra $B$ and 2) $f$ in $B(X_{2})(\Phi(X_{1}))$.
The present paper addresses the problem of obtaining the consistency conditions
for arbitrary Boolean functions in $B_{0}(X)$. Next, the consistency for a
single equation is shown equivalent to another system of Boolean equations
which involves the ON functions and characterizes all solutions. This result is
then extended for Boolean systems in several variables over the algebra
$B_{0}=\{0,1\}$ which does not convert the system into a single equation. This
condition leads to the algorithm for computing all solutions of the Boolean
system without using analogous resolution and determine satisfiability. For
special systems defined by CNF formulas this algorithm results into an
extension of the DPLL algorithm in which the \emph{splitting rule} is
generalized to several variables in terms of ON terms in the sense that
splitting of CNF set in a single variable $x$ is equivalent to ON terms $x,x'$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1709.03754v1,Transform Invariant Auto-encoder,"The auto-encoder method is a type of dimensionality reduction method. A
mapping from a vector to a descriptor that represents essential information can
be automatically generated from a set of vectors without any supervising
information. However, an image and its spatially shifted version are encoded
into different descriptors by an existing ordinary auto-encoder because each
descriptor includes a spatial subpattern and its position. To generate a
descriptor representing a spatial subpattern in an image, we need to normalize
its spatial position in the images prior to training an ordinary auto-encoder;
however, such a normalization is generally difficult for images without obvious
standard positions. We propose a transform invariant auto-encoder and an
inference model of transform parameters. By the proposed method, we can
separate an input into a transform invariant descriptor and transform
parameters. The proposed method can be applied to various auto-encoders without
requiring any special modules or labeled training samples. By applying it to
shift transforms, we can achieve a shift invariant auto-encoder that can
extract a typical spatial subpattern independent of its relative position in a
window. In addition, we can achieve a model that can infer shift parameters
required to restore the input from the typical subpattern. As an example of the
proposed method, we demonstrate that a descriptor generated by a shift
invariant auto-encoder can represent a typical spatial subpattern. In addition,
we demonstrate the imitation of a human hand by a robot hand as an example of a
regression based on spatial subpatterns.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1002.3303v4,"Cryptanalysis of an Efficient Signcryption Scheme with Forward Secrecy
  Based on Elliptic Curve","The signcryption is a relatively new cryptographic technique that is supposed
to fulfill the functionalities of encryption and digital signature in a single
logical step. Several signcryption schemes are proposed throughout the years,
each of them having its own problems and limitations. In this paper, the
security of a recent signcryption scheme, i.e. Hwang et al.'s scheme is
analyzed, and it is proved that it involves several security flaws and
shortcomings. Several devastating attacks are also introduced to the mentioned
scheme whereby it fails all the desired and essential security attributes of a
signcryption scheme.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1806.04996v1,The Intersection Problem for Finite Semigroups,"We investigate the intersection problem for finite semigroups, which asks for
a given set of regular languages, represented by recognizing morphisms to
finite semigroups, whether there exists a word contained in their intersection.
We introduce compressibility measures as a useful tool to classify the
intersection problem for certain classes of finite semigroups into circuit
complexity classes and Turing machine complexity classes. Using this framework,
we obtain a new and simple proof that for groups and commutative semigroups,
the problem is contained in NP. We uncover certain structural and
non-structural properties determining the complexity of the intersection
problem for varieties of semigroups containing only trivial submonoids. More
specifically, we prove NP-hardness for classes of semigroups having a property
called unbounded order and for the class of all nilpotent semigroups of bounded
order. On the contrary, we show that bounded order and commutativity imply
containment in the circuit complexity class qAC^k (for some k) and decidability
in quasi-polynomial time. We also establish connections to the monoid variant
of the problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.05443v1,Approximations from Anywhere and General Rough Sets,"Not all approximations arise from information systems. The problem of fitting
approximations, subjected to some rules (and related data), to information
systems in a rough scheme of things is known as the \emph{inverse problem}. The
inverse problem is more general than the duality (or abstract representation)
problems and was introduced by the present author in her earlier papers. From
the practical perspective, a few (as opposed to one) theoretical frameworks may
be suitable for formulating the problem itself. \emph{Granular operator spaces}
have been recently introduced and investigated by the present author in her
recent work in the context of antichain based and dialectical semantics for
general rough sets. The nature of the inverse problem is examined from
number-theoretic and combinatorial perspectives in a higher order variant of
granular operator spaces and some necessary conditions are proved. The results
and the novel approach would be useful in a number of unsupervised and semi
supervised learning contexts and algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1708.00129v1,"Deep Generative Adversarial Neural Networks for Realistic Prostate
  Lesion MRI Synthesis","Generative Adversarial Neural Networks (GANs) are applied to the synthetic
generation of prostate lesion MRI images. GANs have been applied to a variety
of natural images, is shown show that the same techniques can be used in the
medical domain to create realistic looking synthetic lesion images. 16mm x 16mm
patches are extracted from 330 MRI scans from the SPIE ProstateX Challenge 2016
and used to train a Deep Convolutional Generative Adversarial Neural Network
(DCGAN) utilizing cutting edge techniques. Synthetic outputs are compared to
real images and the implicit latent representations induced by the GAN are
explored. Training techniques and successful neural network architectures are
explained in detail.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1203.5683v1,Time-Constrained Temporal Logic Control of Multi-Affine Systems,"In this paper, we consider the problem of controlling a dynamical system such
that its trajectories satisfy a temporal logic property in a given amount of
time. We focus on multi-affine systems and specifications given as
syntactically co-safe linear temporal logic formulas over rectangular regions
in the state space. The proposed algorithm is based on the estimation of time
bounds for facet reachability problems and solving a time optimal reachability
problem on the product between a weighted transition system and an automaton
that enforces the satisfaction of the specification. A random optimization
algorithm is used to iteratively improve the solution.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1811.01162v1,"Improved approximation algorithms for path vertex covers in regular
  graphs","Given a simple graph $G = (V, E)$ and a constant integer $k \ge 2$, the
$k$-path vertex cover problem ({\sc P$k$VC}) asks for a minimum subset $F
\subseteq V$ of vertices such that the induced subgraph $G[V - F]$ does not
contain any path of order $k$. When $k = 2$, this turns out to be the classic
vertex cover ({\sc VC}) problem, which admits a $\left(2 - {\rm
\Theta}\left(\frac 1{\log|V|}\right)\right)$-approximation. The general {\sc
P$k$VC} admits a trivial $k$-approximation; when $k = 3$ and $k = 4$, the best
known approximation results for {\sc P$3$VC} and {\sc P$4$VC} are a
$2$-approximation and a $3$-approximation, respectively. On $d$-regular graphs,
the approximation ratios can be reduced to $\min\left\{2 - \frac 5{d+3} +
\epsilon, 2 - \frac {(2 - o(1))\log\log d}{\log d}\right\}$ for {\sc VC} ({\it
i.e.}, {\sc P$2$VC}), $2 - \frac 1d + \frac {4d - 2}{3d |V|}$ for {\sc P$3$VC},
$\frac {\lfloor d/2\rfloor (2d - 2)}{(\lfloor d/2\rfloor + 1) (d - 2)}$ for
{\sc P$4$VC}, and $\frac {2d - k + 2}{d - k + 2}$ for {\sc P$k$VC} when $1 \le
k-2 < d \le 2(k-2)$. By utilizing an existing algorithm for graph defective
coloring, we first present a $\frac {\lfloor d/2\rfloor (2d - k + 2)}{(\lfloor
d/2\rfloor + 1) (d - k + 2)}$-approximation for {\sc P$k$VC} on $d$-regular
graphs when $1 \le k - 2 < d$. This beats all the best known approximation
results for {\sc P$k$VC} on $d$-regular graphs for $k \ge 3$, except for {\sc
P$4$VC} it ties with the best prior work and in particular they tie at $2$ on
cubic graphs and $4$-regular graphs. We then propose a $1.875$-approximation
and a $1.852$-approximation for {\sc P$4$VC} on cubic graphs and $4$-regular
graphs, respectively. We also present a better approximation algorithm for {\sc
P$4$VC} on $d$-regular bipartite graphs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0603069v1,"The neighbor-scattering number can be computed in polynomial time for
  interval graphs","Neighbor-scattering number is a useful measure for graph vulnerability. For
some special kinds of graphs, explicit formulas are given for this number.
However, for general graphs it is shown that to compute this number is
NP-complete. In this paper, we prove that for interval graphs this number can
be computed in polynomial time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1608.03677v1,Differential Privacy as a Mutual Information Constraint,"Differential privacy is a precise mathematical constraint meant to ensure
privacy of individual pieces of information in a database even while queries
are being answered about the aggregate. Intuitively, one must come to terms
with what differential privacy does and does not guarantee. For example, the
definition prevents a strong adversary who knows all but one entry in the
database from further inferring about the last one. This strong adversary
assumption can be overlooked, resulting in misinterpretation of the privacy
guarantee of differential privacy.
  Herein we give an equivalent definition of privacy using mutual information
that makes plain some of the subtleties of differential privacy. The
mutual-information differential privacy is in fact sandwiched between
$\epsilon$-differential privacy and $(\epsilon,\delta)$-differential privacy in
terms of its strength. In contrast to previous works using unconditional mutual
information, differential privacy is fundamentally related to conditional
mutual information, accompanied by a maximization over the database
distribution. The conceptual advantage of using mutual information, aside from
yielding a simpler and more intuitive definition of differential privacy, is
that its properties are well understood. Several properties of differential
privacy are easily verified for the mutual information alternative, such as
composition theorems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1409.7240v1,Optimal decremental connectivity in planar graphs,"We show an algorithm for dynamic maintenance of connectivity information in
an undirected planar graph subject to edge deletions. Our algorithm may answer
connectivity queries of the form `Are vertices $u$ and $v$ connected with a
path?' in constant time. The queries can be intermixed with any sequence of
edge deletions, and the algorithm handles all updates in $O(n)$ time. This
results improves over previously known $O(n \log n)$ time algorithm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1807.08937v1,"The model of methodical system and learning objectives of the
  foundations of mathematical informatics for students of technical
  universities","Introduction. Development of methodical system for training course
""Foundations of mathematical informatics"" plays a key role in forming the
students' of technical universities competencies in mathematical informatics.
So it is very important to analyze of the components of methodical system,
identify the weaknesses and problems that can significantly impair its quality
and which can not be overcome without its further development. Purpose. Develop
a model of methodical system of training course ""Foundations of mathematical
informatics"" for students of technical universities and specify its target
component. Methods. Using cloud technologies at learning the foundations of
mathematical informatics requires the construction technology training, which
results in the selection of appropriate cloudoriented forms of organization and
teaching methods. On the other hand, the theory, methods and tools for cloud
significantly affect the primary content of learning and its goals. Thus, the
cloud technology theory, methods and tools are the basis for constructing
methodical system of training course ""Foundations of mathematical informatics"".
Results. Development of methodical system of training course ""Foundations of
mathematical informatics"" plays a leading role in forming the students' of
technical universities competencies in mathematical informatics due to its
fundamental impact and technology improvement. Conclusion. The model of
methodical system of training course ""Foundations of mathematical informatics""
includes content, objectives and learning technology. The last one contains
forms of organization, methods and teaching tools, including leading cloud
technologies. The main purpose of training course ""Foundations of mathematical
informatics"" is developing of students competencies of technical universities
in mathematical informatics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1807.00785v4,Rule Algebras for Adhesive Categories,"We demonstrate that the most well-known approach to rewriting graphical
structures, the Double-Pushout (DPO) approach, possesses a notion of sequential
compositions of rules along an overlap that is associative in a natural sense.
Notably, our results hold in the general setting of $\mathcal{M}$-adhesive
categories. This observation complements the classical Concurrency Theorem of
DPO rewriting. We then proceed to define rule algebras in both settings, where
the most general categories permissible are the finitary (or finitary
restrictions of) $\mathcal{M}$-adhesive categories with $\mathcal{M}$-effective
unions. If in addition a given such category possess an $\mathcal{M}$-initial
object, the resulting rule algebra is unital (in addition to being
associative). We demonstrate that in this setting a canonical representation of
the rule algebras is obtainable, which opens the possibility of applying the
concept to define and compute the evolution of statistical moments of
observables in stochastic DPO rewriting systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.07692v1,"A Hybrid Objective Function for Robustness of Artificial Neural Networks
  -- Estimation of Parameters in a Mechanical System","In several studies, hybrid neural networks have proven to be more robust
against noisy input data compared to plain data driven neural networks. We
consider the task of estimating parameters of a mechanical vehicle model based
on acceleration profiles. We introduce a convolutional neural network
architecture that is capable to predict the parameters for a family of vehicle
models that differ in the unknown parameters. We introduce a convolutional
neural network architecture that given sequential data predicts the parameters
of the underlying data's dynamics. This network is trained with two objective
functions. The first one constitutes a more naive approach that assumes that
the true parameters are known. The second objective incorporates the knowledge
of the underlying dynamics and is therefore considered as hybrid approach. We
show that in terms of robustness, the latter outperforms the first objective on
noisy input data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0802.2258v1,Using Alloy to model-check visual design notations,"This paper explores the process of validation for the abstract syntax of a
graphical notation. We define an unified specification for five of the UML
diagrams used by the Discovery Method and, in this document, we illustrate how
diagrams can be represented in Alloy and checked against our specification in
order to know if these are valid under the Discovery notation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0602048v1,On the Optimality of the ARQ-DDF Protocol,"The performance of the automatic repeat request-dynamic decode and forward
(ARQ-DDF) cooperation protocol is analyzed in two distinct scenarios. The first
scenario is the multiple access relay (MAR) channel where a single relay is
dedicated to simultaneously help several multiple access users. For this setup,
it is shown that the ARQ-DDF protocol achieves the optimal diversity
multiplexing tradeoff (DMT) of the channel. The second scenario is the
cooperative vector multiple access (CVMA) channel where the users cooperate in
delivering their messages to a destination equipped with multiple receiving
antennas. For this setup, we develop a new variant of the ARQ-DDF protocol
where the users are purposefully instructed not to cooperate in the first round
of transmission. Lower and upper bounds on the achievable DMT are then derived.
These bounds are shown to converge to the optimal tradeoff as the number of
transmission rounds increases.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.04442v2,Hitting minors on bounded treewidth graphs. IV. An optimal algorithm,"For a fixed finite collection of graphs ${\cal F}$, the ${\cal F}$-M-DELETION
problem asks, given an $n$-vertex input graph $G,$ for the minimum number of
vertices that intersect all minor models in $G$ of the graphs in ${\cal F}$. by
Courcelle Theorem, this problem can be solved in time $f_{{\cal F}}(tw)\cdot
n^{O(1)},$ where $tw$ is the treewidth of $G$, for some function $f_{{\cal F}}$
depending on ${\cal F}$ In a recent series of articles, we have initiated the
programme of optimizing asymptotically the function $f_{{\cal F}}$. Here we
provide an algorithm showing that $f_{{\cal F}}(tw) = 2^{O(tw\cdot \log tw)}$
for every collection ${\cal F}$. Prior to this work, the best known function
$f_{{\cal F}}$ was double-exponential in $tw$. In particular, our algorithm
vastly extends the results of Jansen et al. [SODA 2014] for the particular case
${\cal F}=\{K_5,K_{3,3}\}$ and of Kociumaka and Pilipczuk [Algorithmica 2019]
for graphs of bounded genus, and answers an open problem posed by Cygan et al.
[Inf Comput 2017]. We combine several ingredients such as the machinery of
boundaried graphs in dynamic programming via representatives, the Flat Wall
Theorem, Bidimensionality, the irrelevant vertex technique, treewidth
modulators, and protrusion replacement. Together with our previous results
providing single-exponential algorithms for particular collections ${\cal F}$
[Theor Comput Sci 2020] and general lower bounds [J Comput Syst Sci 2020], our
algorithm yields the following complexity dichotomy when ${\cal F} = \{H\}$
contains a single connected graph $H,$ assuming the Exponential Time
Hypothesis: $f_H(tw)=2^{\Theta(tw)}$ if $H$ is a contraction of the chair or
the banner, and $f_H(tw)=2^{\Theta(tw\cdot \log tw)}$ otherwise.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1305.4367v1,Parallelizing Stream with Future,"Stream is re-interpreted in terms of a Lazy monad. Future is substituted for
Lazy in the obtained construct, resulting in possible parallelization of any
algorithm expressible as a Stream computation. The principle is tested against
two example algorithms. Performance is evaluated, and a way to improve it
briefly discussed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1906.01618v2,"Scene Representation Networks: Continuous 3D-Structure-Aware Neural
  Scene Representations","Unsupervised learning with generative models has the potential of discovering
rich representations of 3D scenes. While geometric deep learning has explored
3D-structure-aware representations of scene geometry, these models typically
require explicit 3D supervision. Emerging neural scene representations can be
trained only with posed 2D images, but existing methods ignore the
three-dimensional structure of scenes. We propose Scene Representation Networks
(SRNs), a continuous, 3D-structure-aware scene representation that encodes both
geometry and appearance. SRNs represent scenes as continuous functions that map
world coordinates to a feature representation of local scene properties. By
formulating the image formation as a differentiable ray-marching algorithm,
SRNs can be trained end-to-end from only 2D images and their camera poses,
without access to depth or shape. This formulation naturally generalizes across
scenes, learning powerful geometry and appearance priors in the process. We
demonstrate the potential of SRNs by evaluating them for novel view synthesis,
few-shot reconstruction, joint shape and appearance interpolation, and
unsupervised discovery of a non-rigid face model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0110004v1,"Embedding conditional event algebras into temporal calculus of
  conditionals","In this paper we prove that all the existing conditional event algebras embed
into a three-valued extension of temporal logic of discrete past time, which
the authors of this paper have proposed in anothe paper as a general model of
conditional events.
  First of all, we discuss the descriptive incompleteness of the cea's. In this
direction, we show that some important notions, like independence of
conditional events, cannot be properly addressed in the framework of
conditional event algebras, while they can be precisely formulated and analyzed
in the temporal setting.
  We also demonstrate that the embeddings allow one to use Markov chain
algorithms (suitable for the temporal calculus) for computing probabilities of
complex conditional expressions of the embedded conditional event algebras, and
that these algorithms can outperform those previously known.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.06490v3,Greedy Spanners in Euclidean Spaces Admit Sublinear Separators,"The greedy spanner in a low dimensional Euclidean space is a fundamental
geometric construction that has been extensively studied over three decades as
it possesses the two most basic properties of a good spanner: constant maximum
degree and constant lightness. Recently, Eppstein and Khodabandeh showed that
the greedy spanner in $\mathbb{R}^2$ admits a sublinear separator in a strong
sense: any subgraph of $k$ vertices of the greedy spanner in $\mathbb{R}^2$ has
a separator of size $O(\sqrt{k})$. Their technique is inherently planar and is
not extensible to higher dimensions. They left showing the existence of a small
separator for the greedy spanner in $\mathbb{R}^d$ for any constant $d\geq 3$
as an open problem. In this paper, we resolve the problem of Eppstein and
Khodabandeh by showing that any subgraph of $k$ vertices of the greedy spanner
in $\mathbb{R}^d$ has a separator of size $O(k^{1-1/d})$. We introduce a new
technique that gives a simple characterization for any geometric graph to have
a sublinear separator that we dub $\tau$-lanky: a geometric graph is
$\tau$-lanky if any ball of radius $r$ cuts at most $\tau$ edges of length at
least $r$ in the graph. We show that any $\tau$-lanky geometric graph of $n$
vertices in $\mathbb{R}^d$ has a separator of size $O(\tau n^{1-1/d})$. We then
derive our main result by showing that the greedy spanner is $O(1)$-lanky. We
indeed obtain a more general result that applies to unit ball graphs and point
sets of low fractal dimensions in $\mathbb{R}^d$. Our technique naturally
extends to doubling metrics. We use the $\tau$-lanky characterization to show
that there exists a $(1+\epsilon)$-spanner for doubling metrics of dimension
$d$ with a constant maximum degree and a separator of size
$O(n^{1-\frac{1}{d}})$; this result resolves an open problem posed by Abam and
Har-Peled a decade ago.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1309.6772v1,The Multiple-orientability Thresholds for Random Hypergraphs,"A $k$-uniform hypergraph $H = (V, E)$ is called $\ell$-orientable, if there
is an assignment of each edge $e\in E$ to one of its vertices $v\in e$ such
that no vertex is assigned more than $\ell$ edges. Let $H_{n,m,k}$ be a
hypergraph, drawn uniformly at random from the set of all $k$-uniform
hypergraphs with $n$ vertices and $m$ edges. In this paper we establish the
threshold for the $\ell$-orientability of $H_{n,m,k}$ for all $k\ge 3$ and
$\ell \ge 2$, i.e., we determine a critical quantity $c_{k, \ell}^*$ such that
with probability $1-o(1)$ the graph $H_{n,cn,k}$ has an $\ell$-orientation if
$c < c_{k, \ell}^*$, but fails doing so if $c > c_{k, \ell}^*$.
  Our result has various applications including sharp load thresholds for
cuckoo hashing, load balancing with guaranteed maximum load, and massive
parallel access to hard disk arrays.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.07073v1,Feedback Clustering for Online Travel Agencies Searches: a Case Study,"Understanding choices performed by online customers is a growing need in the
travel industry. In many practical situations, the only available information
is the flight search query performed by the customer with no additional profile
knowledge. In general, customer flight bookings are driven by prices, duration,
number of connections, and so on. However, not all customers might assign the
same importance to each of those criteria. Here comes the need of grouping
together all flight searches performed by the same kind of customer, that is
having the same booking criteria. The effectiveness of some set of
recommendations, for a single cluster, can be measured in terms of the number
of bookings historically performed. This effectiveness measure plays the role
of a feedback, that is an external knowledge which can be recombined to
iteratively obtain a final segmentation. In this paper, we describe our Online
Travel Agencies (OTA) flight search use case and highlight its specific
features. We address the flight search segmentation problem motivated above by
proposing a novel algorithm called Split-or-Merge (S/M). This algorithm is a
variation of the Split-Merge-Evolve (SME) method. The SME method has already
been introduced in the community as an iterative process updating a clustering
given by the K-means algorithm by splitting and merging clusters subject to
feedback independent evaluations. No previous application of the SME method to
the real-word data is reported in literature to the best of our knowledge.
Here, we provide experimental evaluations over real-world data to the SME and
the S/M methods. The impact on our domain-specific metrics obtained under the
SME and the S/M methods suggests that feedback clustering techniques can be
very promising in the handling of the domain of OTA flight searches.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.6029v1,"Pareto efficiency in synthesizing shared autonomy policies with temporal
  logic constraints","In systems in which control authority is shared by an autonomous controller
and a human operator, it is important to find solutions that achieve a
desirable system performance with a reasonable workload for the human operator.
We formulate a shared autonomy system capable of capturing the interaction and
switching control between an autonomous controller and a human operator, as
well as the evolution of the operator's cognitive state during control
execution. To trade-off human's effort and the performance level, e.g.,
measured by the probability of satisfying the underlying temporal logic
specification, a two-stage policy synthesis algorithm is proposed for
generating Pareto efficient coordination and control policies with respect to
user specified weights. We integrate the Tchebychev scalarization method for
multi-objective optimization methods to obtain a better coverage of the set of
Pareto efficient solutions than linear scalarization methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.09418v1,Method to Classify Skin Lesions using Dermoscopic images,"Skin cancer is the most common cancer in the existing world constituting
one-third of the cancer cases. Benign skin cancers are not fatal, can be cured
with proper medication. But it is not the same as the malignant skin cancers.
In the case of malignant melanoma, in its peak stage, the maximum life
expectancy is less than or equal to 5 years. But, it can be cured if detected
in early stages. Though there are numerous clinical procedures, the accuracy of
diagnosis falls between 49% to 81% and is time-consuming. So, dermoscopy has
been brought into the picture. It helped in increasing the accuracy of
diagnosis but could not demolish the error-prone behaviour. A quick and less
error-prone solution is needed to diagnose this majorly growing skin cancer.
This project deals with the usage of deep learning in skin lesion
classification. In this project, an automated model for skin lesion
classification using dermoscopic images has been developed with CNN(Convolution
Neural Networks) as a training model. Convolution neural networks are known for
capturing features of an image. So, they are preferred in analyzing medical
images to find the characteristics that drive the model towards success.
Techniques like data augmentation for tackling class imbalance, segmentation
for focusing on the region of interest and 10-fold cross-validation to make the
model robust have been brought into the picture. This project also includes
usage of certain preprocessing techniques like brightening the images using
piece-wise linear transformation function, grayscale conversion of the image,
resize the image. This project throws a set of valuable insights on how the
accuracy of the model hikes with the bringing of new input strategies,
preprocessing techniques. The best accuracy this model could achieve is 0.886",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0110001v2,Computer Security: Competing Concepts,"This paper focuses on a tension we discovered in the philosophical part of
our multidisciplinary project on values in web-browser security. Our project
draws on the methods and perspectives of empirical social science, computer
science, and philosophy to identify values embodied in existing web-browser
security and also to prescribe changes to existing systems (in particular,
Mozilla) so that values relevant to web-browser systems are better served than
presently they are. The tension, which we had not seen explicitly addressed in
any other work on computer security, emerged when we set out to extract from
the concept of security the set values that ought to guide the shape of
web-browser security. We found it impossible to construct an internally
consistent set of values until we realized that two robust -- and in places
competing -- conceptions of computer security were influencing our thinking. We
needed to pry these apart and make a primary commitment to one. One conception
of computer security invokes the ordinary meaning of security. According to it,
computer security should protect people -- computer users -- against dangers,
harms, and threats. Clearly this ordinary conception of security is already
informing much of the work and rhetoric surrounding computer security. But
another, substantively richer conception, also defines the aims and trajectory
of computer security -- computer security as an element of national security.
Although, like the ordinary conception, this one is also concerned with
protection against threats, its primary subject is the state, not the
individual. The two conceptions suggest divergent system-specifications, not
for all mechanisms but a significant few.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1505.01306v1,Understanding Graph Structure of Wikipedia for Query Expansion,"Knowledge bases are very good sources for knowledge extraction, the ability
to create knowledge from structured and unstructured sources and use it to
improve automatic processes as query expansion. However, extracting knowledge
from unstructured sources is still an open challenge. In this respect,
understanding the structure of knowledge bases can provide significant benefits
for the effectiveness of such purpose. In particular, Wikipedia has become a
very popular knowledge base in the last years because it is a general
encyclopedia that has a large amount of information and thus, covers a large
amount of different topics. In this piece of work, we analyze how articles and
categories of Wikipedia relate to each other and how these relationships can
support a query expansion technique. In particular, we show that the structures
in the form of dense cycles with a minimum amount of categories tend to
identify the most relevant information.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.02902v1,"An Ontology-Based, Fully Probabilistic, Scalable Method for Human
  Activity Recognition","Efficiency and scalability are obstacles that have not yet received a viable
response from the human activity recognition research community. This paper
proposes an activity recognition method. The knowledge model is in the form of
ontology, the state-of-the-art in knowledge representation and reasoning. The
ontology starts with probabilistic information about subjects' low-level
activities and location and then is populated with the assertion axioms learned
from data or defined by the user. Unlike methods that choose only the most
probable candidate from sensor readings, the proposed method keeps multiple
candidates with the known degree of confidence for each one and involves them
in decision making. Using this method, the system is more flexible to deal with
unreliable readings from sensors, and the final recognition rate is improved.
Besides, to resolve the scalability problem, a system is designed and
implemented to do reasoning and storing in a relational database management
system. Numerical evaluation and conceptual benchmarking prove the proposed
system feasibility.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.06060v1,"Exploiting the Power of Levenberg-Marquardt Optimizer with Anomaly
  Detection in Time Series","The Levenberg-Marquardt (LM) optimization algorithm has been widely used for
solving machine learning problems. Literature reviews have shown that the LM
can be very powerful and effective on moderate function approximation problems
when the number of weights in the network is not more than a couple of hundred.
In contrast, the LM does not seem to perform as well when dealing with pattern
recognition or classification problems, and inefficient when networks become
large (e.g. with more than 500 weights). In this paper, we exploit the true
power of LM algorithm using some real world aircraft datasets. On these
datasets most other commonly used optimizers are unable to detect the anomalies
caused by the changing conditions of the aircraft engine. The challenging
nature of the datasets are the abrupt changes in the time series data. We find
that the LM optimizer has a much better ability to approximate abrupt changes
and detect anomalies than other optimizers. We compare the performance, in
addressing this anomaly/change detection problem, of the LM and several other
optimizers. We assess the relative performance based on a range of measures
including network complexity (i.e. number of weights), fitting accuracy, over
fitting, training time, use of GPUs and memory requirement etc. We also discuss
the issue of robust LM implementation in MATLAB and Tensorflow for promoting
more popular usage of the LM algorithm and potential use of LM optimizer for
large-scale problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.06186v2,Tracelet Hopf algebras and decomposition spaces,"Tracelets are the intrinsic carriers of causal information in categorical
rewriting systems. In this work, we assemble tracelets into a symmetric
monoidal decomposition space, inducing a cocommutative Hopf algebra of
tracelets. This Hopf algebra captures important combinatorial and algebraic
aspects of rewriting theory, and is motivated by applications of its
representation theory to stochastic rewriting systems such as chemical reaction
networks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.14549v1,"Latent Equilibrium: A unified learning theory for arbitrarily fast
  computation with arbitrarily slow neurons","The response time of physical computational elements is finite, and neurons
are no exception. In hierarchical models of cortical networks each layer thus
introduces a response lag. This inherent property of physical dynamical systems
results in delayed processing of stimuli and causes a timing mismatch between
network output and instructive signals, thus afflicting not only inference, but
also learning. We introduce Latent Equilibrium, a new framework for inference
and learning in networks of slow components which avoids these issues by
harnessing the ability of biological neurons to phase-advance their output with
respect to their membrane potential. This principle enables quasi-instantaneous
inference independent of network depth and avoids the need for phased
plasticity or computationally expensive network relaxation phases. We jointly
derive disentangled neuron and synapse dynamics from a prospective energy
function that depends on a network's generalized position and momentum. The
resulting model can be interpreted as a biologically plausible approximation of
error backpropagation in deep cortical networks with continuous-time, leaky
neuronal dynamics and continuously active, local plasticity. We demonstrate
successful learning of standard benchmark datasets, achieving competitive
performance using both fully-connected and convolutional architectures, and
show how our principle can be applied to detailed models of cortical
microcircuitry. Furthermore, we study the robustness of our model to
spatio-temporal substrate imperfections to demonstrate its feasibility for
physical realization, be it in vivo or in silico.",0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0309007v1,"ROC Curves Within the Framework of Neural Network Assembly Memory Model:
  Some Analytic Results","On the basis of convolutional (Hamming) version of recent Neural Network
Assembly Memory Model (NNAMM) for intact two-layer autoassociative Hopfield
network optimal receiver operating characteristics (ROCs) have been derived
analytically. A method of taking into account explicitly a priori probabilities
of alternative hypotheses on the structure of information initiating memory
trace retrieval and modified ROCs (mROCs, a posteriori probabilities of correct
recall vs. false alarm probability) are introduced. The comparison of empirical
and calculated ROCs (or mROCs) demonstrates that they coincide quantitatively
and in this way intensities of cues used in appropriate experiments may be
estimated. It has been found that basic ROC properties which are one of
experimental findings underpinning dual-process models of recognition memory
can be explained within our one-factor NNAMM.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0405098v3,A Logic for Reasoning about Evidence,"We introduce a logic for reasoning about evidence that essentially views
evidence as a function from prior beliefs (before making an observation) to
posterior beliefs (after making the observation). We provide a sound and
complete axiomatization for the logic, and consider the complexity of the
decision problem. Although the reasoning in the logic is mainly propositional,
we allow variables representing numbers and quantification over them. This
expressive power seems necessary to capture important properties of evidence.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0204043v1,Learning from Scarce Experience,"Searching the space of policies directly for the optimal policy has been one
popular method for solving partially observable reinforcement learning
problems. Typically, with each change of the target policy, its value is
estimated from the results of following that very policy. This requires a large
number of interactions with the environment as different polices are
considered. We present a family of algorithms based on likelihood ratio
estimation that use data gathered when executing one policy (or collection of
policies) to estimate the value of a different policy. The algorithms combine
estimation and optimization stages. The former utilizes experience to build a
non-parametric representation of an optimized function. The latter performs
optimization on this estimate. We show positive empirical results and provide
the sample complexity bound.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1305.5800v1,"Lightweight Contention Management for Efficient Compare-and-Swap
  Operations","Many concurrent data-structure implementations use the well-known
compare-and-swap (CAS) operation, supported in hardware by most modern
multiprocessor architectures for inter-thread synchronization. A key weakness
of the CAS operation is the degradation in its performance in the presence of
memory contention.
  In this work we study the following question: can software-based contention
management improve the efficiency of hardware-provided CAS operations? Our
performance evaluation establishes that lightweight contention management
support can greatly improve performance under medium and high contention levels
while typically incurring only small overhead when contention is low.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1011.2922v1,Emoticonsciousness,"A temporal analysis of emoticon use in Swedish, Italian, German and English
asynchronous electronic communication is reported. Emoticons are classified as
positive, negative and neutral. Postings to newsgroups over a 66 week period
are considered. The aggregate analysis of emoticon use in newsgroups for
science and politics tend on the whole to be consistent over the entire time
period. Where possible, events that coincide with divergences from trends in
language-subject pairs are noted. Political discourse in Italian over the
period shows marked use of negative emoticons, and in Swedish, positive
emoticons.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1001.0529v2,"On Iterated Dominance, Matrix Elimination, and Matched Paths","We study computational problems arising from the iterated removal of weakly
dominated actions in anonymous games. Our main result shows that it is
NP-complete to decide whether an anonymous game with three actions can be
solved via iterated weak dominance. The two-action case can be reformulated as
a natural elimination problem on a matrix, the complexity of which turns out to
be surprisingly difficult to characterize and ultimately remains open. We
however establish connections to a matching problem along paths in a directed
graph, which is computationally hard in general but can also be used to
identify tractable cases of matrix elimination. We finally identify different
classes of anonymous games where iterated dominance is in P and NP-complete,
respectively.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0301036v2,Algorithms using Java for Spreadsheet Dependent Cell Recomputation,"Java implementations of algorithms used by spreadsheets to automatically
recompute the set of cells dependent on a changed cell are described using a
mathematical model for spreadsheets based on graph theory. These solutions
comprise part of a Java API that allows a client application to read, modify,
and maintain spreadsheet data without using the spreadsheet application program
that produced it. Features of the Java language that successfully improve the
running time performance of the algorithms are also described.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.10498v3,Weighted automata are compact and actively learnable,"We show that weighted automata over the field of two elements can be
exponentially more compact than non-deterministic finite state automata. To
show this, we combine ideas from automata theory and communication complexity.
However, weighted automata are also efficiently learnable in Angluin's minimal
adequate teacher model in a number of queries that is polynomial in the size of
the minimal weighted automaton.. We include an algorithm for learning WAs over
any field based on a linear algebraic generalization of the Angluin-Schapire
algorithm. Together, this produces a surprising result: weighted automata over
fields are structured enough that even though they can be very compact, they
are still efficiently learnable.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1906.00110v1,"The Evolutionary Price of Anarchy: Locally Bounded Agents in a Dynamic
  Virus Game","The Price of Anarchy (PoA) is a well-established game-theoretic concept to
shed light on coordination issues arising in open distributed systems. Leaving
agents to selfishly optimize comes with the risk of ending up in sub-optimal
states (in terms of performance and/or costs), compared to a centralized system
design. However, the PoA relies on strong assumptions about agents' rationality
(e.g., resources and information) and interactions, whereas in many distributed
systems agents interact locally with bounded resources. They do so repeatedly
over time (in contrast to ""one-shot games""), and their strategies may evolve.
Using a more realistic evolutionary game model, this paper introduces a
realized evolutionary Price of Anarchy (ePoA). The ePoA allows an exploration
of equilibrium selection in dynamic distributed systems with multiple
equilibria, based on local interactions of simple memoryless agents.
Considering a fundamental game related to virus propagation on networks, we
present analytical bounds on the ePoA in basic network topologies and for
different strategy update dynamics. In particular, deriving stationary
distributions of the stochastic evolutionary process, we find that the Nash
equilibria are not always the most abundant states, and that different
processes can feature significant off-equilibrium behavior, leading to a
significantly higher ePoA compared to the PoA studied traditionally in the
literature.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1706.05748v1,"A Space-Time Approach for the Time-Domain Simulation in a Rotating
  Reference Frame","We approach the discretisation of Maxwell's equations directly in space-time
without making any non-relativistic assumptions with the particular focus on
simulations in rotating reference frames. As a research example we study
Sagnac's effect in a rotating ring resonator. After the discretisation, we
express the numerical scheme in a form resembling 3D FIT with leapfrog. We
compare the stability and convergence properties of two 4D approaches, namely
FIT and FEM, both using Whitney interpolation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.10682v3,"Hybrid deep learning methods for phenotype prediction from clinical
  notes","Identifying patient cohorts from clinical notes in secondary electronic
health records is a fundamental task in clinical information management.
However, with the growing number of clinical notes, it becomes challenging to
analyze the data manually for phenotype detection. Automatic extraction of
clinical concepts would helps to identify the patient phenotypes correctly.
This paper proposes a novel hybrid model for automatically extracting patient
phenotypes using natural language processing and deep learning models to
determine the patient phenotypes without dictionaries and human intervention.
The model is based on a neural bidirectional sequence model (BiLSTM or BiGRU)
and a CNN layer for phenotypes identification. An extra CNN layer is run
parallel to the hybrid model to extract more features related to each
phenotype. We used pre-trained embeddings such as FastText and Word2vec
separately as the input layers to evaluate other embedding's performance.
Experimental results using MIMIC III database in internal comparison
demonstrate that the proposed model achieved significant performance
improvement over existing models. The enhanced version of our model with an
extra CNN layer obtained a relatively higher F1-score than the original hybrid
model. We also showed that BiGRU layer with FastText embedding had better
performance than BiLSTM layer to identify patient phenotypes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1810.11665v1,"Towards Smart City Innovation Under the Perspective of Software-Defined
  Networking, Artificial Intelligence and Big Data","Smart city projects address many of the current problems afflicting high
populated areas and cities and, as such, are a target for government,
institutions and private organizations that plan to explore its foreseen
advantages. In technical terms, smart city projects present a complex set of
requirements including a large number users with highly different and
heterogeneous requirements. In this scenario, this paper proposes and analyses
the impact and perspectives on adopting software-defined networking and
artificial intelligence as innovative approaches for smart city project
development and deployment. Big data is also considered as an inherent element
of most smart city project that must be tackled. A framework layered view is
proposed with a discussion about software-defined networking and machine
learning impacts on innovation followed by a use case that demonstrates the
potential benefits of cognitive learning for smart cities. It is argued that
the complexity of smart city projects do require new innovative approaches that
potentially result in more efficient and intelligent systems.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0410037v1,Hardware-Oriented Group Solutions for Hard Problems,"Group and individual solutions are considered for hard problems such as
satisfiability problem. Time-space trade-off in a structured active memory
provides means to achieve lower time complexity for solutions of these
problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1210.2316v1,"Disjunctive Datalog with Existential Quantifiers: Semantics,
  Decidability, and Complexity Issues","Datalog is one of the best-known rule-based languages, and extensions of it
are used in a wide context of applications. An important Datalog extension is
Disjunctive Datalog, which significantly increases the expressivity of the
basic language. Disjunctive Datalog is useful in a wide range of applications,
ranging from Databases (e.g., Data Integration) to Artificial Intelligence
(e.g., diagnosis and planning under incomplete knowledge). However, in recent
years an important shortcoming of Datalog-based languages became evident, e.g.
in the context of data-integration (consistent query-answering, ontology-based
data access) and Semantic Web applications: The language does not permit any
generation of and reasoning with unnamed individuals in an obvious way. In
general, it is weak in supporting many cases of existential quantification. To
overcome this problem, Datalogex has recently been proposed, which extends
traditional Datalog by existential quantification in rule heads. In this work,
we propose a natural extension of Disjunctive Datalog and Datalogex, called
Datalogexor, which allows both disjunctions and existential quantification in
rule heads and is therefore an attractive language for knowledge representation
and reasoning, especially in domains where ontology-based reasoning is needed.
We formally define syntax and semantics of the language Datalogexor, and
provide a notion of instantiation, which we prove to be adequate for
Datalogexor. A main issue of Datalogex and hence also of Datalogexor is that
decidability is no longer guaranteed for typical reasoning tasks. In order to
address this issue, we identify many decidable fragments of the language, which
extend, in a natural way, analog classes defined in the non-disjunctive case.
Moreover, we carry out an in-depth complexity analysis, deriving interesting
results which range from Logarithmic Space to Exponential Time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.02936v1,Adaptive Accessible AR/VR Systems,"Augmented, virtual and mixed reality technologies offer new ways of
interacting with digital media. However, such technologies are not well
explored for people with different ranges of abilities beyond a few specific
navigation and gaming applications. While new standardization activities are
investigating accessibility issues with existing AR/VR systems, commercial
systems are still confined to specialized hardware and software limiting their
widespread adoption among people with disabilities as well as seniors. This
proposal takes a novel approach by exploring the application of user
model-based personalization for AR/VR systems to improve accessibility. The
workshop will be organized by experienced researchers in the field of human
computer interaction, robotics control, assistive technology, and AR/VR
systems, and will consist of peer reviewed papers and hands-on demonstrations.
Keynote speeches and demonstrations will cover latest accessibility research at
Microsoft, Google, Verizon and leading universities.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.10252v2,CoRT: Complementary Rankings from Transformers,"Many recent approaches towards neural information retrieval mitigate their
computational costs by using a multi-stage ranking pipeline. In the first
stage, a number of potentially relevant candidates are retrieved using an
efficient retrieval model such as BM25. Although BM25 has proven decent
performance as a first-stage ranker, it tends to miss relevant passages. In
this context we propose CoRT, a simple neural first-stage ranking model that
leverages contextual representations from pretrained language models such as
BERT to complement term-based ranking functions while causing no significant
delay at query time. Using the MS MARCO dataset, we show that CoRT
significantly increases the candidate recall by complementing BM25 with missing
candidates. Consequently, we find subsequent re-rankers achieve superior
results with less candidates. We further demonstrate that passage retrieval
using CoRT can be realized with surprisingly low latencies.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.07982v1,"Feature Selection for Learning to Predict Outcomes of Compute Cluster
  Jobs with Application to Decision Support","We present a machine learning framework and a new test bed for data mining
from the Slurm Workload Manager for high-performance computing (HPC) clusters.
The focus was to find a method for selecting features to support decisions:
helping users decide whether to resubmit failed jobs with boosted CPU and
memory allocations or migrate them to a computing cloud. This task was cast as
both supervised classification and regression learning, specifically,
sequential problem solving suitable for reinforcement learning. Selecting
relevant features can improve training accuracy, reduce training time, and
produce a more comprehensible model, with an intelligent system that can
explain predictions and inferences. We present a supervised learning model
trained on a Simple Linux Utility for Resource Management (Slurm) data set of
HPC jobs using three different techniques for selecting features: linear
regression, lasso, and ridge regression. Our data set represented both HPC jobs
that failed and those that succeeded, so our model was reliable, less likely to
overfit, and generalizable. Our model achieved an R^2 of 95\% with 99\%
accuracy. We identified five predictors for both CPU and memory properties.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0801.0398v3,On the graph isomorphism problem,"We relate the graph isomorphism problem to the solvability of certain systems
of linear equations with nonnegative variables. This version replaces the two
previous versions of this paper.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1701.05011v1,Assessing User Expertise in Spoken Dialog System Interactions,"Identifying the level of expertise of its users is important for a system
since it can lead to a better interaction through adaptation techniques.
Furthermore, this information can be used in offline processes of root cause
analysis. However, not much effort has been put into automatically identifying
the level of expertise of an user, especially in dialog-based interactions. In
this paper we present an approach based on a specific set of task related
features. Based on the distribution of the features among the two classes -
Novice and Expert - we used Random Forests as a classification approach.
Furthermore, we used a Support Vector Machine classifier, in order to perform a
result comparison. By applying these approaches on data from a real system,
Let's Go, we obtained preliminary results that we consider positive, given the
difficulty of the task and the lack of competing approaches for comparison.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9904008v1,Transducers from Rewrite Rules with Backreferences,"Context sensitive rewrite rules have been widely used in several areas of
natural language processing, including syntax, morphology, phonology and speech
processing. Kaplan and Kay, Karttunen, and Mohri & Sproat have given various
algorithms to compile such rewrite rules into finite-state transducers. The
present paper extends this work by allowing a limited form of backreferencing
in such rules. The explicit use of backreferencing leads to more elegant and
general solutions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1203.4642v1,Why Watching Movie Tweets Won't Tell the Whole Story?,"Data from Online Social Networks (OSNs) are providing analysts with an
unprecedented access to public opinion on elections, news, movies etc. However,
caution must be taken to determine whether and how much of the opinion
extracted from OSN user data is indeed reflective of the opinion of the larger
online population. In this work we study this issue in the context of movie
reviews on Twitter and compare the opinion of Twitter users with that of the
online population of IMDb and Rotten Tomatoes. We introduce new metrics to show
that the Twitter users can be characteristically different from general users,
both in their rating and their relative preference for Oscar-nominated and
non-nominated movies. Additionally, we investigate whether such data can truly
predict a movie's box-office success.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.05776v1,Terminating cases of flooding,"Basic synchronous flooding proceeds in rounds. Given a finite undirected
(network) graph $G$, a set of sources $I \subseteq G$ initiate flooding in the
first round by every node in $I$ sending the same message to all of its
neighbours. In each subsequent round, nodes send the message to all of their
neighbours from which they did not receive the message in the previous round.
Flooding terminates when no node in $G$ sends a message in a round. The
question of termination has not been settled - rather, non-termination is
implicitly assumed to be possible.
  We show that flooding terminates on every finite graph. In the case of a
single source $g_0$, flooding terminates in $e$ rounds if $G$ is bipartite and
$j$ rounds with $e < j \leq e+d+1$ otherwise, where $e$ and $d$ are the
eccentricity of $g_0$ and diameter of $G$ respectively. For
communication/broadcast to all nodes, this is asymptotically time optimal and
obviates the need for construction and maintenance of spanning structures. We
extend to dynamic flooding initiated in multiple rounds with possibly multiple
messages. The cases where a node only sends a message to neighbours from which
it did not receive {\it any} message in the previous round, and where a node
sends some highest ranked message to all neighbours from which it did not
receive {\it that} message in the previous round, both terminate. All these
cases also hold if the network graph loses edges over time. Non-terminating
cases include asynchronous flooding, flooding where messages have fixed delays
at edges, cases of multiple-message flooding and cases where the network graph
acquires edges over time.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0910.3848v1,"Equivalence Classes of Optimal Structures in HP Protein Models Including
  Side Chains","Lattice protein models, as the Hydrophobic-Polar (HP) model, are a common
abstraction to enable exhaustive studies on structure, function, or evolution
of proteins. A main issue is the high number of optimal structures, resulting
from the hydrophobicity-based energy function applied. We introduce an
equivalence relation on protein structures that correlates to the energy
function. We discuss the efficient enumeration of optimal representatives of
the corresponding equivalence classes and the application of the results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.03044v2,"A First Look into the Structural Properties and Resilience of Blockchain
  Overlays","Blockchain (BC) systems are highly distributed peer-to-peer networks that
offer an alternative to centralized services and promise robustness to
coordinated attacks. However, the resilience and overall security of a BC
system rests heavily on the structural properties of its underlying
peer-to-peer overlay. Despite their success, BC overlay networks' critical
design aspects, connectivity properties and network-layer inter-dependencies
are still poorly understood. In this work, we set out to fill this gap and
study the most important overlay network structural properties and robustness
to targeted attacks of seven distinct BC networks. In particular, we probe and
crawl these BC networks every two hours to gather information about all their
available peers, over a duration of 28 days. We analyze 335 network snapshots
per BC network, for a total of 2345 snapshots. We construct, at frequent
intervals, connectivity graphs for each BC network, consisting of all potential
connections between peers. We analyze the structural graph properties of these
networks and compare them across the seven BC networks. We also study how these
properties associate with the resilience of each network to partitioning
attacks, i.e., when peers are selected, attacked and taken offline, using
different selection strategies driven by the aforementioned structural
properties. In fact, we show that by targeting fewer than 10 highly-connected
peers, major BCs such as Bitcoin can be partitioned into disjoint, i.e.,
disconnected, components. Finally, we uncover a hidden interconnection between
different BC networks, where certain peers participate in more than one BC
network, which has serious implications for the robustness of the overall BC
network ecosystem.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1501.04877v1,"Towards Gathering Robots with Limited View in Linear Time: The Closed
  Chain Case","In the gathering problem, n autonomous robots have to meet on a single point.
We consider the gathering of a closed chain of point-shaped, anonymous robots
on a grid. The robots only have local knowledge about a constant number of
neighboring robots along the chain in both directions. Actions are performed in
the fully synchronous time model FSYNC. Every robot has a limited memory that
may contain one timestamp of the global clock, also visible to its direct
neighbors. In this synchronous time model, there is no limited view gathering
algorithm known to perform better than in quadratic runtime. The configurations
that show the quadratic lower bound are closed chains. In this paper, we
present the first sub-quadratic---in fact linear time---gathering algorithm for
closed chains on a grid.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0710.1484v1,"The structure and modeling results of the parallel spatial switching
  system","Problems of the switching parallel system designing provided spatial
switching of packets from random time are discussed. Results of modeling of
switching system as systems of mass service are resulted.",0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1904.06198v1,Cyclic Coding Algorithms under MorphoSys Reconfigurable Computing System,"This paper introduces reconfigurable computing (RC) and specifically chooses
one of the prototypes in this field, MorphoSys (M1) [1 - 5]. The paper
addresses the results obtained when using RC in mapping algorithms pertaining
to digital coding in relation to previous research [6 - 10]. The chosen
algorithms relate to cyclic coding techniques, namely the CCITT CRC-16 and the
CRC-16. A performance analysis study of the M1 RC system is also presented to
evaluate the efficiency of the algorithm execution on the M1 system. For
comparison purposes, three other systems where used to map the same algorithms
showing the advantages and disadvantages of each compared with the M1 system.
The algorithms were run on the 8x8 RC (reconfigurable) array of the M1
(MorphoSys) system; numerical examples were simulated to validate our results,
using the MorphoSys mULATE program, which simulates MorphoSys operations.",0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1201.5346v1,"Tableau-based decision procedure for the multi-agent epistemic logic
  with all coalitional operators for common and distributed knowledge","We develop a conceptually clear, intuitive, and feasible decision procedure
for testing satisfiability in the full multi-agent epistemic logic CMAEL(CD)
with operators for common and distributed knowledge for all coalitions of
agents mentioned in the language. To that end, we introduce Hintikka structures
for CMAEL(CD) and prove that satisfiability in such structures is equivalent to
satisfiability in standard models. Using that result, we design an incremental
tableau-building procedure that eventually constructs a satisfying Hintikka
structure for every satisfiable input set of formulae of CMAEL(CD) and closes
for every unsatisfiable input set of formulae.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1411.0440v8,Modelling serendipity in a computational context,"The term serendipity describes a creative process that develops, in context,
with the active participation of a creative agent, but not entirely within that
agent's control. While a system cannot be made to perform serendipitously on
demand, we argue that its $\mathit{serendipity\ potential}$ can be increased by
means of a suitable system architecture and other design choices. We distil a
unified description of serendipitous occurrences from historical theorisations
of serendipity and creativity. This takes the form of a framework with six
phases: $\mathit{perception}$, $\mathit{attention}$, $\mathit{interest}$,
$\mathit{explanation}$, $\mathit{bridge}$, and $\mathit{valuation}$. We then
use this framework to organise a survey of literature in cognitive science,
philosophy, and computing, which yields practical definitions of the six
phases, along with heuristics for implementation. We use the resulting model to
evaluate the serendipity potential of four existing systems developed by
others, and two systems previously developed by two of the authors. Most
existing research that considers serendipity in a computing context deals with
serendipity as a service; here we relate theories of serendipity to the
development of autonomous systems and computational creativity practice. We
argue that serendipity is not teleologically blind, and outline representative
directions for future applications of our model. We conclude that it is
feasible to equip computational systems with the potential for serendipity, and
that this could be beneficial in varied computational creativity/AI
applications, particularly those designed to operate responsively in real-world
contexts.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1510.08435v5,Private Webmail 2.0: Simple and Easy-to-Use Secure Email,"Private Webmail 2.0 (Pwm 2.0) improves upon the current state of the art by
increasing the usability and practical security of secure email for ordinary
users. More users are able to send and receive encrypted emails without
mistakenly revealing sensitive information. In this paper we describe user
interface traits that positively affect the usability and security of Pwm 2.0:
(1) an artificial delay to encryption that enhances user confidence in Pwm 2.0
while simultaneously instructing users on who can read their encrypted
messages; (2) a modified composition interface that helps protect users from
mistakenly sending sensitive information in the clear; (3) an annotated secure
email composition interface that instructs users on how to correctly use secure
email; and (4) inline, context-sensitive tutorials, which improved view rates
for tutorials from less than 10% in earlier systems to over 90% for Pwm 2.0. In
a user study involving 51 participants we validate these interface
modifications, and also show that the use of manual encryption has no effect on
usability or security.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0212024v1,Unsupervised Language Acquisition: Theory and Practice,"In this thesis I present various algorithms for the unsupervised machine
learning of aspects of natural languages using a variety of statistical models.
The scientific object of the work is to examine the validity of the so-called
Argument from the Poverty of the Stimulus advanced in favour of the proposition
that humans have language-specific innate knowledge. I start by examining an a
priori argument based on Gold's theorem, that purports to prove that natural
languages cannot be learned, and some formal issues related to the choice of
statistical grammars rather than symbolic grammars. I present three novel
algorithms for learning various parts of natural languages: first, an algorithm
for the induction of syntactic categories from unlabelled text using
distributional information, that can deal with ambiguous and rare words;
secondly, a set of algorithms for learning morphological processes in a variety
of languages, including languages such as Arabic with non-concatenative
morphology; thirdly an algorithm for the unsupervised induction of a
context-free grammar from tagged text. I carefully examine the interaction
between the various components, and show how these algorithms can form the
basis for a empiricist model of language acquisition. I therefore conclude that
the Argument from the Poverty of the Stimulus is unsupported by the evidence.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0711.0348v1,Compiling ER Specifications into Declarative Programs,"This paper proposes an environment to support high-level database programming
in a declarative programming language. In order to ensure safe database
updates, all access and update operations related to the database are generated
from high-level descriptions in the entity- relationship (ER) model. We propose
a representation of ER diagrams in the declarative language Curry so that they
can be constructed by various tools and then translated into this
representation. Furthermore, we have implemented a compiler from this
representation into a Curry program that provides access and update operations
based on a high-level API for database programming.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1703.06021v7,Causal Consistency for Reversible Multiparty Protocols,"In programming models with a reversible semantics, computational steps can be
undone. This paper addresses the integration of reversible semantics into
process languages for communication-centric systems equipped with behavioral
types. In prior work, we introduced a monitors-as-memories approach to
seamlessly integrate reversible semantics into a process model in which
concurrency is governed by session types (a class of behavioral types),
covering binary (two-party) protocols with synchronous communication. The
applicability and expressiveness of the binary setting, however, is limited.
  Here we extend our approach, and use it to define reversible semantics for an
expressive process model that accounts for multiparty (n-party) protocols,
asynchronous communication, decoupled rollbacks, and abstraction passing. As
main result, we prove that our reversible semantics for multiparty protocols is
causally-consistent. A key technical ingredient in our developments is an
alternative reversible semantics with atomic rollbacks, which is conceptually
simple and is shown to characterize decoupled rollbacks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.15246v1,Correctness by construction for probabilistic programs,"The ""correct by construction"" paradigm is an important component of modern
Formal Methods, and here we use the probabilistic Guarded-Command Language
$\mathit{pGCL}$ to illustrate its application to $\mathit{probabilistic}$
programming. $\mathit{pGCL}$ extends Dijkstra's guarded-command language
$\mathit{GCL}$ with probabilistic choice, and is equipped with a
correctness-preserving refinement relation $(\sqsubseteq)$ that enables
compact, abstract specifications of probabilistic properties to be transformed
gradually to concrete, executable code by applying mathematical insights in a
systematic and layered way. Characteristically for ""correctness by
construction"", as far as possible the reasoning in each refinement-step layer
does not depend on earlier layers, and does not affect later ones. We
demonstrate the technique by deriving a fair-coin implementation of any given
discrete probability distribution. In the special case of simulating a fair
die, our correct-by-construction algorithm turns out to be ""within spitting
distance"" of Knuth and Yao's optimal solution.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.02040v1,"Three-dimensional Cooperative Localization of Commercial-Off-The-Shelf
  Sensors","Many location-based services use Received Signal Strength (RSS) measurements
due to their universal availability. In this paper, we study the association of
a large number of low-cost Internet-of-Things (IoT) sensors and their possible
installation locations, which can enable various sensing and automation-related
applications. We propose an efficient approach to solve the corresponding
permutation combinatorial optimization problem, which integrates continuous
space cooperative localization and permutation space likelihood ascent search.
A convex relaxation-based optimization is designed to estimate the coarse
locations of blindfolded devices in continuous 3D spaces, which are then
projected to the feasible permutation space. An efficient Cram\'er-Rao Lower
Bound based likelihood ascent search algorithm is proposed to refine the
solution. Extensive experiments were conducted to evaluate the performance of
the proposed approach, which show that the proposed approach significantly
outperforms state-of-the-art combinatorial optimization algorithms and achieves
close-to-100% accuracy with affordable execution time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1109.4240v1,"Actor Continuation Passing: Efficient and Extensible Request Routing for
  Event-Driven Architectures","The logic for handling of application requests to a staged, event-driven
architecture is often distributed over different portions of the source code.
This complicates changing and understanding the flow of events in the system.
The article presents an approach that extracts request handling logic from
regular stage functionality into a set of request scripts. These scripts are
executed step-wise by sending continuations that encapsulate their request's
current execution state to stages for local processing and optional forwarding
of follow-up continuations. A new internal domain specific language (DSL) that
aims to simplify writing of request scripts is described along with its
implementation for the scala actors library. Evaluation results indicate that
request handling with actor continuations performs about equally or better
compared to using separate stages for request handling logic for scripts of at
least 3 sequential steps.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1906.10005v1,From Quantified CTL to QBF,"QCTL extends the temporal logic CTL with quantifications over atomic
propositions. This extension is known to be very expressive: QCTL allows us to
express complex properties over Kripke structures (it is as expressive as MSO).
Several semantics exist for the quantifications: here, we work with the
structure semantics, where the extra propositions label the Kripke structure
(and not its execution tree), and the model-checking problem is known to be
PSPACE-complete in this framework. We propose a model-checking algorithm for
QCTL based on a reduction to QBF. We consider several reduction strategies, and
we compare them with a prototype (based on the SMT-solver Z3) on several
examples.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0606001v1,"Tight Bounds for the Min-Max Boundary Decomposition Cost of Weighted
  Graphs","Many load balancing problems that arise in scientific computing applications
ask to partition a graph with weights on the vertices and costs on the edges
into a given number of almost equally-weighted parts such that the maximum
boundary cost over all parts is small.
  Here, this partitioning problem is considered for bounded-degree graphs
G=(V,E) with edge costs c: E->R+ that have a p-separator theorem for some p>1,
i.e., any (arbitrarily weighted) subgraph of G can be separated into two parts
of roughly the same weight by removing a vertex set S such that the edges
incident to S in the subgraph have total cost at most proportional to (SUM_e
c^p_e)^(1/p), where the sum is over all edges e in the subgraph.
  We show for all positive integers k and weights w that the vertices of G can
be partitioned into k parts such that the weight of each part differs from the
average weight by less than MAX{w_v; v in V}, and the boundary edges of each
part have cost at most proportional to (SUM_e c_e^p/k)^(1/p) + MAX_e c_e. The
partition can be computed in time nearly proportional to the time for computing
a separator S of G.
  Our upper bound on the boundary costs is shown to be tight up to a constant
factor for infinitely many instances with a broad range of parameters. Previous
results achieved this bound only if one has c=1, w=1, and one allows parts with
weight exceeding the average by a constant fraction.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.12528v3,"Towards Dynamic-Point Systems on Metric Graphs with Longest
  Stabilization Time","A dynamical system of points moving along the edges of a graph could be
considered as a geometrical discrete dynamical system or as a discrete version
of a quantum graph with localized wave packets. We study the set of such
systems over metric graphs that can be constructed from a given set of
commensurable edges with fixed lengths. It is shown that there always exists a
system consisting of a bead graph with vertex degrees not greater than three
that demonstrates the longest stabilization time in such a set. The results are
extended to graphs with incommensurable edges using the notion of
$\varepsilon$-nets and, also, it is shown that dynamical systems of points on
linear graphs have the slowest growth of the number of dynamic points",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.13893v1,Explainable Machine Larning for liver transplantation,"In this work, we present a flexible method for explaining, in human readable
terms, the predictions made by decision trees used as decision support in liver
transplantation. The decision trees have been obtained through machine learning
applied on a dataset collected at the liver transplantation unit at the
Coru\~na University Hospital Center and are used to predict long term (five
years) survival after transplantation. The method we propose is based on the
representation of the decision tree as a set of rules in a logic program (LP)
that is further annotated with text messages. This logic program is then
processed using the tool xclingo (based on Answer Set Programming) that allows
building compound explanations depending on the annotation text and the rules
effectively fired when a given input is provided. We explore two alternative LP
encodings: one in which rules respect the tree structure (more convenient to
reflect the learning process) and one where each rule corresponds to a
(previously simplified) tree path (more readable for decision making).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.10863v2,Outer approximations of core points for integer programming,"For several decades the dominant techniques for integer linear programming
have been branching and cutting planes. Recently, several authors have
developed core point methods for solving symmetric integer linear programs
(ILPs). An integer point is called a core point if its orbit polytope is
lattice-free. It has been shown that for symmetric ILPs, optimizing over the
set of core points gives the same answer as considering the entire space.
Existing core point techniques rely on the number of core points (or
equivalence classes) being finite, which requires special symmetry groups. In
this paper we develop some new methods for solving symmetric ILPs (based on
outer approximations of core points) that do not depend on finiteness but are
more efficient if the group has large disjoint cycles in its set of generators.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.08197v1,Open-Domain Conversational Search Assistant with Transformers,"Open-domain conversational search assistants aim at answering user questions
about open topics in a conversational manner. In this paper we show how the
Transformer architecture achieves state-of-the-art results in key IR tasks,
leveraging the creation of conversational assistants that engage in open-domain
conversational search with single, yet informative, answers. In particular, we
propose an open-domain abstractive conversational search agent pipeline to
address two major challenges: first, conversation context-aware search and
second, abstractive search-answers generation. To address the first challenge,
the conversation context is modeled with a query rewriting method that unfolds
the context of the conversation up to a specific moment to search for the
correct answers. These answers are then passed to a Transformer-based re-ranker
to further improve retrieval performance. The second challenge, is tackled with
recent Abstractive Transformer architectures to generate a digest of the top
most relevant passages. Experiments show that Transformers deliver a solid
performance across all tasks in conversational search, outperforming the best
TREC CAsT 2019 baseline.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1904.08770v1,"An Empirical Evaluation of Text Representation Schemes on Multilingual
  Social Web to Filter the Textual Aggression","This paper attempt to study the effectiveness of text representation schemes
on two tasks namely: User Aggression and Fact Detection from the social media
contents. In User Aggression detection, The aim is to identify the level of
aggression from the contents generated in the Social media and written in the
English, Devanagari Hindi and Romanized Hindi. Aggression levels are
categorized into three predefined classes namely: `Non-aggressive`, `Overtly
Aggressive`, and `Covertly Aggressive`. During the disaster-related incident,
Social media like, Twitter is flooded with millions of posts. In such emergency
situations, identification of factual posts is important for organizations
involved in the relief operation. We anticipated this problem as a combination
of classification and Ranking problem. This paper presents a comparison of
various text representation scheme based on BoW techniques, distributed
word/sentence representation, transfer learning on classifiers. Weighted $F_1$
score is used as a primary evaluation metric. Results show that text
representation using BoW performs better than word embedding on machine
learning classifiers. While pre-trained Word embedding techniques perform
better on classifiers based on deep neural net. Recent transfer learning model
like ELMO, ULMFiT are fine-tuned for the Aggression classification task.
However, results are not at par with pre-trained word embedding model. Overall,
word embedding using fastText produce best weighted $F_1$-score than Word2Vec
and Glove. Results are further improved using pre-trained vector model.
Statistical significance tests are employed to ensure the significance of the
classification results. In the case of lexically different test Dataset, other
than training Dataset, deep neural models are more robust and perform
substantially better than machine learning classifiers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0303016v1,Fast Parallel I/O on Cluster Computers,"Today's cluster computers suffer from slow I/O, which slows down
I/O-intensive applications. We show that fast disk I/O can be achieved by
operating a parallel file system over fast networks such as Myrinet or Gigabit
Ethernet.
  In this paper, we demonstrate how the ParaStation3 communication system helps
speed-up the performance of parallel I/O on clusters using the open source
parallel virtual file system (PVFS) as testbed and production system. We will
describe the set-up of PVFS on the Alpha-Linux-Cluster-Engine (ALiCE) located
at Wuppertal University, Germany. Benchmarks on ALiCE achieve
write-performances of up to 1 GB/s from a 32-processor compute-partition to a
32-processor PVFS I/O-partition, outperforming known benchmark results for PVFS
on the same network by more than a factor of 2. Read-performance from
buffer-cache reaches up to 2.2 GB/s. Our benchmarks are giant, I/O-intensive
eigenmode problems from lattice quantum chromodynamics, demonstrating stability
and performance of PVFS over Parastation in large-scale production runs.",0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1204.1393v1,Continuous Markov Random Fields for Robust Stereo Estimation,"In this paper we present a novel slanted-plane MRF model which reasons
jointly about occlusion boundaries as well as depth. We formulate the problem
as the one of inference in a hybrid MRF composed of both continuous (i.e.,
slanted 3D planes) and discrete (i.e., occlusion boundaries) random variables.
This allows us to define potentials encoding the ownership of the pixels that
compose the boundary between segments, as well as potentials encoding which
junctions are physically possible. Our approach outperforms the
state-of-the-art on Middlebury high resolution imagery as well as in the more
challenging KITTI dataset, while being more efficient than existing slanted
plane MRF-based methods, taking on average 2 minutes to perform inference on
high resolution imagery.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1701.00773v1,"Security-related Research in Ubiquitous Computing -- Results of a
  Systematic Literature Review","In an endeavor to reach the vision of ubiquitous computing where users are
able to use pervasive services without spatial and temporal constraints, we are
witnessing a fast growing number of mobile and sensor-enhanced devices becoming
available. However, in order to take full advantage of the numerous benefits
offered by novel mobile devices and services, we must address the related
security issues. In this paper, we present results of a systematic literature
review (SLR) on security-related topics in ubiquitous computing environments.
In our study, we found 5165 scientific contributions published between 2003 and
2015. We applied a systematic procedure to identify the threats,
vulnerabilities, attacks, as well as corresponding defense mechanisms that are
discussed in those publications. While this paper mainly discusses the results
of our study, the corresponding SLR protocol which provides all details of the
SLR is also publicly available for download.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0
http://arxiv.org/abs/1710.11395v1,The Slashdot Zoo: Mining a Social Network with Negative Edges,"We analyse the corpus of user relationships of the Slashdot technology news
site. The data was collected from the Slashdot Zoo feature where users of the
website can tag other users as friends and foes, providing positive and
negative endorsements. We adapt social network analysis techniques to the
problem of negative edge weights. In particular, we consider signed variants of
global network characteristics such as the clustering coefficient, node-level
characteristics such as centrality and popularity measures, and link-level
characteristics such as distances and similarity measures. We evaluate these
measures on the task of identifying unpopular users, as well as on the task of
predicting the sign of links and show that the network exhibits multiplicative
transitivity which allows algebraic methods based on matrix multiplication to
be used. We compare our methods to traditional methods which are only suitable
for positively weighted edges.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0908.3459v1,"An Algorithmic Perspective on Some Network Design, Construction and
  Analysis Problems","Efficient network design, construction and analysis are important topics,
considering the highly dynamic environment in which data communication occurs
nowadays. In this paper we address several problems concerning these topics
from an algorithmic point of view.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.05178v1,"Software Process Improvement Based on Defect Prevention Using Capability
  and Testing Model Integration in Extreme Programming","Nowadays, Software Process Improvement popularly known as SPI has been able
to receive an immense concern in the continuous process to purify software
quality. Several Agile methodologies previously have worked with Extreme
programming (XP). Before improving the process, defect prevention (DP) is
inevitable. In addition, DP largely depends on defect detection either found
earlier in the design and implementation stages or held in the testing phases.
However, testing maturity model integration (TMMI) has a crucial aspect in DP
as well as process improvement of the software. In particular, when software
gets validated by being tested and fixed the defects up, it achieves the
maximum capability maturity model integration (CMMI) aiming the process
improvement. Here, the article has proposed an improved defect detection and
prevention model to enhance the software process following the approach of XP.
Besides, as a unique contribution, we have united the capability and testing
model integration to ensure better SPI.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0306102v1,"Prototyping Virtual Data Technologies in ATLAS Data Challenge 1
  Production","For efficiency of the large production tasks distributed worldwide, it is
essential to provide shared production management tools comprised of
integratable and interoperable services. To enhance the ATLAS DC1 production
toolkit, we introduced and tested a Virtual Data services component. For each
major data transformation step identified in the ATLAS data processing pipeline
(event generation, detector simulation, background pile-up and digitization,
etc) the Virtual Data Cookbook (VDC) catalogue encapsulates the specific data
transformation knowledge and the validated parameters settings that must be
provided before the data transformation invocation. To provide for local-remote
transparency during DC1 production, the VDC database server delivered in a
controlled way both the validated production parameters and the templated
production recipes for thousands of the event generation and detector
simulation jobs around the world, simplifying the production management
solutions.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1605.06868v1,"Decidable models of integer-manipulating programs with recursive
  parallelism (technical report)","We study safety verification for multithreaded programs with recursive
parallelism (i.e. unbounded thread creation and recursion) as well as unbounded
integer variables. Since the threads in each program configuration are
structured in a hierarchical fashion, our model is state-extended ground-tree
rewrite systems equipped with shared unbounded integer counters that can be
incremented, decremented, and compared against an integer constant. Since the
model is Turing-complete, we propose a decidable underapproximation. First,
using a restriction similar to context-bounding, we underapproximate the global
control by a weak global control (i.e. DAGs possibly with self-loops), thereby
limiting the number of synchronisations between different threads. Second, we
bound the number of reversals between non-decrementing and non-incrementing
modes of the counters. Under this restriction, we show that reachability
becomes NP-complete. In fact, it is poly-time reducible to satisfaction over
existential Presburger formulas, which allows one to tap into highly optimised
SMT solvers. Our decidable approximation strictly generalises known decidable
models including (i) weakly-synchronised ground-tree rewrite systems, and (ii)
synchronisation/reversal-bounded concurrent pushdown systems systems with
counters. Finally, we show that, when equipped with reversal-bounded counters,
relaxing the weak control restriction by the notion of senescence results in
undecidability.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1812.04911v2,The Crossing Tverberg Theorem,"Tverberg's theorem is one of the cornerstones of discrete geometry. It states
that, given a set $X$ of at least $(d+1)(r-1)+1$ points in $\mathbb R^d$, one
can find a partition $X=X_1\cup \ldots \cup X_r$ of $X$, such that the convex
hulls of the $X_i$, $i=1,\ldots,r$, all share a common point. In this paper, we
prove a strengthening of this theorem that guarantees a partition which, in
addition to the above, has the property that the boundaries of full-dimensional
convex hulls have pairwise nonempty intersections. Possible generalizations and
algorithmic aspects are also discussed.
  As a concrete application, we show that any $n$ points in the plane in
general position span $\lfloor n/3\rfloor$ vertex-disjoint triangles that are
pairwise crossing, meaning that their boundaries have pairwise nonempty
intersections; this number is clearly best possible. A previous result of
Rebollar et al.\ guarantees $\lfloor n/6\rfloor$ pairwise crossing triangles.
Our result generalizes to a result about simplices in $\mathbb R^d,d\ge2$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1901.08969v1,"A Zero-Shot Learning application in Deep Drawing process using
  Hyper-Process Model","One of the consequences of passing from mass production to mass customization
paradigm in the nowadays industrialized world is the need to increase
flexibility and responsiveness of manufacturing companies. The high-mix /
low-volume production forces constant accommodations of unknown product
variants, which ultimately leads to high periods of machine calibration. The
difficulty related with machine calibration is that experience is required
together with a set of experiments to meet the final product quality.
Unfortunately, all possible combinations of machine parameters is so high that
is difficult to build empirical knowledge. Due to this fact, normally trial and
error approaches are taken making one-of-a-kind products not viable. Therefore,
a Zero-Shot Learning (ZSL) based approach called hyper-process model (HPM) to
learn the relation among multiple tasks is used as a way to shorten the
calibration phase. Assuming each product variant is a task to solve, first, a
shape analysis on data to learn common modes of deformation between tasks is
made, and secondly, a mapping between these modes and task descriptions is
performed. Ultimately, the present work has two main contributions: 1)
Formulation of an industrial problem into a ZSL setting where new process
models can be generated for process optimization and 2) the definition of a
regression problem in the domain of ZSL. For that purpose, a 2-d deep drawing
simulated process was used based on data collected from the Abaqus simulator,
where a significant number of process models were collected to test the
effectiveness of the approach. The obtained results show that is possible to
learn new tasks without any available data (both labeled and unlabeled) by
leveraging information about already existing tasks, allowing to speed up the
calibration phase and make a quicker integration of new products into
manufacturing systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0305063v2,"McRunjob: A High Energy Physics Workflow Planner for Grid Production
  Processing","McRunjob is a powerful grid workflow manager used to manage the generation of
large numbers of production processing jobs in High Energy Physics. In use at
both the DZero and CMS experiments, McRunjob has been used to manage large
Monte Carlo production processing since 1999 and is being extended to uses in
regular production processing for analysis and reconstruction. Described at
CHEP 2001, McRunjob converts core metadata into jobs submittable in a variety
of environments. The powerful core metadata description language includes
methods for converting the metadata into persistent forms, job descriptions,
multi-step workflows, and data provenance information. The language features
allow for structure in the metadata by including full expressions, namespaces,
functional dependencies, site specific parameters in a grid environment, and
ontological definitions. It also has simple control structures for
parallelization of large jobs. McRunjob features a modular design which allows
for easy expansion to new job description languages or new application level
tasks.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0405035v3,"Security-Performance Tradeoffs of Inheritance based Key Predistribution
  for Wireless Sensor Networks","Key predistribution is a well-known technique for ensuring secure
communication via encryption among sensors deployed in an ad-hoc manner to form
a sensor network. In this paper, we propose a novel 2-Phase technique for key
predistribution based on a combination of inherited and random key assignments
from the given key pool to individual sensor nodes. We also develop an
analytical framework for measuring security-performance tradeoffs of different
key distribution schemes by providing metrics for measuring sensornet
connectivity and resiliency to enemy attacks. In particular, we show
analytically that the 2-Phase scheme provides better average connectivity and
superior $q$-composite connectivity than the random scheme. We then prove that
the invulnerability of a communication link under arbitrary number of node
captures by an adversary is higher under the 2-Phase scheme. The probability of
a communicating node pair having an exclusive key also scales better with
network size under the 2-Phase scheme. We also show analytically that the
vulnerability of an arbitrary communication link in the sensornet to single
node capture is lower under 2-Phase assuming both network-wide as well as
localized capture. Simulation results also show that the number of exclusive
keys shared between any two nodes is higher while the number of $q$-composite
links compromised when a given number of nodes are captured by the enemy is
smaller under the 2-Phase scheme as compared to the random one.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2102.03148v1,Securing emergent behaviour in swarm robotics,"Swarm robotics is the study of how a large number of relatively simple robots
can be designed so that a desired collective behaviour emerges from the local
interactions among robots and between the robots and their environment. While
many aspects of a swarm may be modelled as various types of ad hoc networks,
and accordingly many aspects of security of the swarm may be achieved by
conventional means, here we will focus on swarm emergent behaviour as something
that most distinguishes swarm robotics from ad hoc networks. We discuss the
challenges emergent behaviour poses on communications security, and by
classifying a swarm by types of robots, types of communication channels, and
types of adversaries, we examine what classes may be secured by traditional
methods and focus on aspects that are most relevant to allowing emergent
behaviour. We will examine how this can be secured by ensuring that
communication is secure. We propose a simple solution using hash chains, and by
modelling swarm communications using a series of random graphs, we show that
this allows us to identify rogue robots with a high probability.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.15634v4,On the Nature and Types of Anomalies: A Review of Deviations in Data,"Anomalies are occurrences in a dataset that are in some way unusual and do
not fit the general patterns. The concept of the anomaly is typically
ill-defined and perceived as vague and domain-dependent. Moreover, despite some
250 years of publications on the topic, no comprehensive and concrete overviews
of the different types of anomalies have hitherto been published. By means of
an extensive literature review this study therefore offers the first
theoretically principled and domain-independent typology of data anomalies and
presents a full overview of anomaly types and subtypes. To concretely define
the concept of the anomaly and its different manifestations, the typology
employs five dimensions: data type, cardinality of relationship, anomaly level,
data structure, and data distribution. These fundamental and data-centric
dimensions naturally yield 3 broad groups, 9 basic types, and 63 subtypes of
anomalies. The typology facilitates the evaluation of the functional
capabilities of anomaly detection algorithms, contributes to explainable data
science, and provides insights into relevant topics such as local versus global
anomalies.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0905.2367v1,A Language-theoretic View on Guidelines and Consistency Rules of UML,"Guidelines and consistency rules of UML are used to control the degrees of
freedom provided by the language to prevent faults. Guidelines are used in
specific domains (e.g., avionics) to recommend the proper use of technologies.
Consistency rules are used to deal with inconsistencies in models. However,
guidelines and consistency rules use informal restrictions on the uses of
languages, which makes checking difficult. In this paper, we consider these
problems from a language-theoretic view. We propose the formalism of C-Systems,
short for ""formal language control systems"". A C-System consists of a
controlled grammar and a controlling grammar. Guidelines and consistency rules
are formalized as controlling grammars that control the uses of UML, i.e. the
derivations using the grammar of UML. This approach can be implemented as a
parser, which can automatically verify the rules on a UML user model in XMI
format. A comparison to related work shows our contribution: a generic top-down
and syntax-based approach that checks language level constraints at
compile-time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1001.0338v3,"Optimal Homologous Cycles, Total Unimodularity, and Linear Programming","Given a simplicial complex with weights on its simplices, and a nontrivial
cycle on it, we are interested in finding the cycle with minimal weight which
is homologous to the given one. Assuming that the homology is defined with
integer coefficients, we show the following : For a finite simplicial complex
$K$ of dimension greater than $p$, the boundary matrix $[\partial_{p+1}]$ is
totally unimodular if and only if $H_p(L, L_0)$ is torsion-free, for all pure
subcomplexes $L_0, L$ in $K$ of dimensions $p$ and $p+1$ respectively, where
$L_0$ is a subset of $L$. Because of the total unimodularity of the boundary
matrix, we can solve the optimization problem, which is inherently an integer
programming problem, as a linear program and obtain integer solution. Thus the
problem of finding optimal cycles in a given homology class can be solved in
polynomial time. This result is surprising in the backdrop of a recent result
which says that the problem is NP-hard under $\mathbb{Z}_2$ coefficients which,
being a field, is in general easier to deal with. One consequence of our
result, among others, is that one can compute in polynomial time an optimal
2-cycle in a given homology class for any finite simplicial complex embedded in
$\mathbb{R}^3$. Our optimization approach can also be used for various related
problems, such as finding an optimal chain homologous to a given one when these
are not cycles.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1406.5550v2,"ViDaExpert: user-friendly tool for nonlinear visualization and analysis
  of multidimensional vectorial data","ViDaExpert is a tool for visualization and analysis of multidimensional
vectorial data. ViDaExpert is able to work with data tables of ""object-feature""
type that might contain numerical feature values as well as textual labels for
rows (objects) and columns (features). ViDaExpert implements several
statistical methods such as standard and weighted Principal Component Analysis
(PCA) and the method of elastic maps (non-linear version of PCA), Linear
Discriminant Analysis (LDA), multilinear regression, K-Means clustering, a
variant of decision tree construction algorithm. Equipped with several
user-friendly dialogs for configuring data point representations (size, shape,
color) and fast 3D viewer, ViDaExpert is a handy tool allowing to construct an
interactive 3D-scene representing a table of data in multidimensional space and
perform its quick and insightfull statistical analysis, from basic to advanced
methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.08394v2,"Convergence of Artificial Intelligence and High Performance Computing on
  NSF-supported Cyberinfrastructure","Significant investments to upgrade and construct large-scale scientific
facilities demand commensurate investments in R&D to design algorithms and
computing approaches to enable scientific and engineering breakthroughs in the
big data era. Innovative Artificial Intelligence (AI) applications have powered
transformational solutions for big data challenges in industry and technology
that now drive a multi-billion dollar industry, and which play an ever
increasing role shaping human social patterns. As AI continues to evolve into a
computing paradigm endowed with statistical and mathematical rigor, it has
become apparent that single-GPU solutions for training, validation, and testing
are no longer sufficient for computational grand challenges brought about by
scientific facilities that produce data at a rate and volume that outstrip the
computing capabilities of available cyberinfrastructure platforms. This
realization has been driving the confluence of AI and high performance
computing (HPC) to reduce time-to-insight, and to enable a systematic study of
domain-inspired AI architectures and optimization schemes to enable data-driven
discovery. In this article we present a summary of recent developments in this
field, and describe specific advances that authors in this article are
spearheading to accelerate and streamline the use of HPC platforms to design
and apply accelerated AI algorithms in academia and industry.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.04953v1,Designing Recommender Systems to Depolarize,"Polarization is implicated in the erosion of democracy and the progression to
violence, which makes the polarization properties of large algorithmic content
selection systems (recommender systems) a matter of concern for peace and
security. While algorithm-driven social media does not seem to be a primary
driver of polarization at the country level, it could be a useful intervention
point in polarized societies. This paper examines algorithmic depolarization
interventions with the goal of conflict transformation: not suppressing or
eliminating conflict but moving towards more constructive conflict. Algorithmic
intervention is considered at three stages: which content is available
(moderation), how content is selected and personalized (ranking), and content
presentation and controls (user interface). Empirical studies of online
conflict suggest that the exposure diversity intervention proposed as an
antidote to ""filter bubbles"" can be improved and can even worsen polarization
under some conditions. Using civility metrics in conjunction with diversity in
content selection may be more effective. However, diversity-based interventions
have not been tested at scale and may not work in the diverse and dynamic
contexts of real platforms. Instead, intervening in platform polarization
dynamics will likely require continuous monitoring of polarization metrics,
such as the widely used ""feeling thermometer."" These metrics can be used to
evaluate product features, and potentially engineered as algorithmic
objectives. It may further prove necessary to include polarization measures in
the objective functions of recommender algorithms to prevent optimization
processes from creating conflict as a side effect.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2105.07968v2,A New Vertex Connectivity Metric,"A new metric for quantifying pairwise vertex connectivity in graphs is
defined and an implementation presented. While general in nature, it features a
combination of input features well-suited for social networks, including
applicability to directed or undirected graphs, weighted edges, and computes
using the impact from all-paths between the vertices. Moreover, the $O(V+E)$
method is applicable to large graphs. Comparisons with other techniques are
included.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0107011v1,Distributed Broadcast in Wireless Networks with Unknown Topology,"A multi-hop synchronous wirelss network is said to be unknown if the nodes
have no knowledge of the topology. A basic task in wireless network is that of
broadcasting a message (created by a fixed source node) to all nodes of the
network. The multi-broadcast that consists in performing a set of r independent
broadcasts. In this paper, we study the completion and the termination time of
distributed protocols for both the (single) broadcast and the multi-broadcast
operations on unknown networks as functions of the number of nodes n, the
maximum eccentricity D, the maximum in-degree Delta, and the congestion c of
the networks. We establish new connections between these operations and some
combinatorial concepts, such as selective families, strongly-selective families
(also known as superimposed codes), and pairwise r-different families. Such
connections, combined with a set of new lower and upper bounds on the size of
the above families, allow us to derive new lower bounds and new distributed
protocols for the broadcast and multi-broadcast operations. In particular, our
upper bounds are almost tight and improve exponentially over the previous
bounds when D and Delta are polylogarithmic in n. Network topologies having
``small'' eccentricity and ``small'' degree (such as bounded-degree expanders)
are often used in practice to achieve efficient communication.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1810.08188v1,Web Application for Collaborative Semantic Web Information Architecture,"In this paper is analyzed the prototyping of the information visualization on
a Web Application for community purposes in a collaborative environment
representing an evolution of the actual social networks like Facebook,
Instagram, Twitter, Linkedin, VirgilioPeople,... The intent of this work is to
identify the most common features of Web App for the information visualization
based on the Semantic Web and discuss how they support the user's requirements
in a ""collaborative"" environment. A solution for the context-aware development
of UI is based on ""joint meaning"" understood as a joint construal of the
creator of the community contents and the user of the community contents thanks
to the context and interface adaptation using the faced taxonomy with the
Semantic Web. A proof-of concept prototype allows showing that the proposed
methodological approach can also easily be applied to existing presentation
components, built with different languages and/or component technologies.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1904.06847v1,On the Lambek Calculus with an Exchange Modality,"In this paper we introduce Commutative/Non-Commutative Logic (CNC logic) and
two categorical models for CNC logic. This work abstracts Benton's
Linear/Non-Linear Logic by removing the existence of the exchange structural
rule. One should view this logic as composed of two logics; one sitting to the
left of the other. On the left, there is intuitionistic linear logic, and on
the right is a mixed commutative/non-commutative formalization of the Lambek
calculus. Then both of these logics are connected via a pair of monoidal
adjoint functors. An exchange modality is then derivable within the logic using
the adjunction between both sides. Thus, the adjoint functors allow one to pull
the exchange structural rule from the left side to the right side. We then give
a categorical model in terms of a monoidal adjunction, and then a concrete
model in terms of dialectica Lambek spaces.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.05213v1,Exploiting Knowledge Graphs for Facilitating Product/Service Discovery,"Most of the existing techniques to product discovery rely on syntactic
approaches, thus ignoring valuable and specific semantic information of the
underlying standards during the process. The product data comes from different
heterogeneous sources and formats giving rise to the problem of
interoperability. Above all, due to the continuously increasing influx of data,
the manual labeling is getting costlier. Integrating the descriptions of
different products into a single representation requires organizing all the
products across vendors in a single taxonomy. Practically relevant and quality
product categorization standards are still limited in number; and that too in
academic research projects where we can majorly see only prototypes as compared
to industry. This work presents a cost-effective solution for e-commerce on the
Data Web by employing an unsupervised approach for data classification and
exploiting the knowledge graphs for matching. The proposed architecture
describes available products in web ontology language OWL and stores them in a
triple store. User input specifications for certain products are matched
against the available product categories to generate a knowledge graph. This
mullti-phased top-down approach to develop and improve existing, if any,
tailored product recommendations will be able to connect users with the exact
product/service of their choice.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0311014v1,"Optimality of Universal Bayesian Sequence Prediction for General Loss
  and Alphabet","Various optimality properties of universal sequence predictors based on
Bayes-mixtures in general, and Solomonoff's prediction scheme in particular,
will be studied. The probability of observing $x_t$ at time $t$, given past
observations $x_1...x_{t-1}$ can be computed with the chain rule if the true
generating distribution $\mu$ of the sequences $x_1x_2x_3...$ is known. If
$\mu$ is unknown, but known to belong to a countable or continuous class $\M$
one can base ones prediction on the Bayes-mixture $\xi$ defined as a
$w_\nu$-weighted sum or integral of distributions $\nu\in\M$. The cumulative
expected loss of the Bayes-optimal universal prediction scheme based on $\xi$
is shown to be close to the loss of the Bayes-optimal, but infeasible
prediction scheme based on $\mu$. We show that the bounds are tight and that no
other predictor can lead to significantly smaller bounds. Furthermore, for
various performance measures, we show Pareto-optimality of $\xi$ and give an
Occam's razor argument that the choice $w_\nu\sim 2^{-K(\nu)}$ for the weights
is optimal, where $K(\nu)$ is the length of the shortest program describing
$\nu$. The results are applied to games of chance, defined as a sequence of
bets, observations, and rewards. The prediction schemes (and bounds) are
compared to the popular predictors based on expert advice. Extensions to
infinite alphabets, partial, delayed and probabilistic prediction,
classification, and more active systems are briefly discussed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0803.4354v6,"A O(n^8) X O(n^7) Linear Programming Model of the Traveling Salesman
  Problem","In this paper, we present a new linear programming (LP) formulation of the
Traveling Salesman Problem (TSP). The proposed model has O(n^8) variables and
O(n^7) constraints, where n is the number of cities. Our numerical
experimentation shows that computational times for the proposed linear program
are several orders of magnitude smaller than those for the existing model [3].",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0110057v1,"Generating Multilingual Personalized Descriptions of Museum Exhibits -
  The M-PIRO Project","This paper provides an overall presentation of the M-PIRO project. M-PIRO is
developing technology that will allow museums to generate automatically textual
or spoken descriptions of exhibits for collections available over the Web or in
virtual reality environments. The descriptions are generated in several
languages from information in a language-independent database and small
fragments of text, and they can be tailored according to the backgrounds of the
users, their ages, and their previous interaction with the system. An authoring
tool allows museum curators to update the system's database and to control the
language and content of the resulting descriptions. Although the project is
still in progress, a Web-based demonstrator that supports English, Greek and
Italian is already available, and it is used throughout the paper to highlight
the capabilities of the emerging technology.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1912.12043v1,"Implementation of XTEA Encryption Protocol based on IEEE 802.15.4
  Wireless Systems","The problem of data security in IEEE 802.15.4 systems on Pololu Wixel devices
is solved, examples of hardware and software implementation of encryption and
decryption of different devices on the same platform are given. The proposed
approaches can be used in the development, implementation and operation of
wireless enterprise, industrial, and personal systems. Possible areas of
development for this work are related to research on improving encryption
algorithms (increasing key length, using asymmetric ciphers, etc.), comparing
their performance, and implementing a complete data exchange protocol. During
the work there were problems in the implementation of encryption algorithms on
low-power processors. During the work, a number of issues were resolved
regarding type reduction, addressing, memory space, buffer overflow, and more.
Issues resolved with reconciliation of receiver and transmitter operation.
Examples of hardware and software implementation of encryption and decryption
of different devices based on Pololu Wixel are given in the paper. The basic
task of building a secure communication channel by encrypting data in the
channel was solved and firmware and application software were obtained to fully
validate the devices. In addition, this work has great application potential,
since the implementation of encryption in existing systems will have a small
impact on implementation and will not affect the project budget, but will
dramatically improve the security of data transmission in these networks. The
proposed approaches can be used in the development, implementation and
operation of wireless enterprise, industrial, and personal systems. Continuing
this work may be to test the performance of other protocols on this and similar
hardware for systems that may be embedded in short-range wireless communication
projects of short-range standards.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.04056v1,"On the exact learnability of graph parameters: The case of partition
  functions","We study the exact learnability of real valued graph parameters $f$ which are
known to be representable as partition functions which count the number of
weighted homomorphisms into a graph $H$ with vertex weights $\alpha$ and edge
weights $\beta$. M. Freedman, L. Lov\'asz and A. Schrijver have given a
characterization of these graph parameters in terms of the $k$-connection
matrices $C(f,k)$ of $f$. Our model of learnability is based on D. Angluin's
model of exact learning using membership and equivalence queries. Given such a
graph parameter $f$, the learner can ask for the values of $f$ for graphs of
their choice, and they can formulate hypotheses in terms of the connection
matrices $C(f,k)$ of $f$. The teacher can accept the hypothesis as correct, or
provide a counterexample consisting of a graph. Our main result shows that in
this scenario, a very large class of partition functions, the rigid partition
functions, can be learned in time polynomial in the size of $H$ and the size of
the largest counterexample in the Blum-Shub-Smale model of computation over the
reals with unit cost.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.02198v1,Quantum Hoare Type Theory: Extended Abstract,"As quantum computers become real, it is high time we come up with effective
techniques that help programmers write correct quantum programs. In classical
computing, formal verification and sound static type systems prevent several
classes of bugs from being introduced. There is a need for similar techniques
in the quantum regime. Inspired by Hoare Type Theory in the classical paradigm,
we propose Quantum Hoare Types by extending the Quantum IO Monad by indexing it
with pre- and post-conditions that serve as program specifications. In this
paper, we introduce Quantum Hoare Type Theory (QHTT), present its syntax and
typing rules, and demonstrate its effectiveness with the help of examples.
  QHTT has the potential to be a unified system for programming, specifying,
and reasoning about quantum programs. This is a work in progress.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.10240v1,Automated Aggregator -- Rewriting with the Counting Aggregate,"Answer set programming is a leading declarative constraint programming
paradigm with wide use for complex knowledge-intensive applications. Modern
answer set programming languages support many equivalent ways to model
constraints and specifications in a program. However, so far answer set
programming has failed to develop systematic methodologies for building
representations that would uniformly lend well to automated processing. This
suggests that encoding selection, in the same way as algorithm selection and
portfolio solving, may be a viable direction for improving performance of
answer-set solving. The necessary precondition is automating the process of
generating possible alternative encodings. Here we present an automated
rewriting system, the Automated Aggregator or AAgg, that given a non-ground
logic program, produces a family of equivalent programs with complementary
performance when run under modern answer set programming solvers. We
demonstrate this behavior through experimental analysis and propose the
system's use in automated answer set programming solver selection tools.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.02288v1,The Gotsman-Linial Conjecture is False,"In 1991, Craig Gotsman and Nathan Linial conjectured that for all $n$ and
$d$, the average sensitivity of a degree-$d$ polynomial threshold function on
$n$ variables is maximized by the degree-$d$ symmetric polynomial which
computes the parity function on the $d$ layers of the hypercube with Hamming
weight closest to $n/2$. We refute the conjecture for almost all $d$ and for
almost all $n$, and we confirm the conjecture in many of the remaining cases.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0610036v3,Optimization of Memory Usage in Tardos's Fingerprinting Codes,"It is known that Tardos's collusion-secure probabilistic fingerprinting code
(Tardos code; STOC'03) has length of theoretically minimal order with respect
to the number of colluding users. However, Tardos code uses certain continuous
probability distribution in codeword generation, which creates some problems
for practical use, in particular, it requires large extra memory. A solution
proposed so far is to use some finite probability distributions instead. In
this paper, we determine the optimal finite distribution in order to decrease
extra memory amount. By our result, the extra memory is reduced to 1/32 of the
original, or even becomes needless, in some practical setting. Moreover, the
code length is also reduced, e.g. to about 20.6% of Tardos code asymptotically.
Finally, we address some other practical issues such as approximation errors
which are inevitable in any real implementation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1209.2229v4,"A Direct Version of Veldman's Proof of Open Induction on Cantor Space
  via Delimited Control Operators","First, we reconstruct Wim Veldman's result that Open Induction on Cantor
space can be derived from Double-negation Shift and Markov's Principle. In
doing this, we notice that one has to use a countable choice axiom in the proof
and that Markov's Principle is replaceable by slightly strengthening the
Double-negation Shift schema. We show that this strengthened version of
Double-negation Shift can nonetheless be derived in a constructive intermediate
logic based on delimited control operators, extended with axioms for
higher-type Heyting Arithmetic. We formalize the argument and thus obtain a
proof term that directly derives Open Induction on Cantor space by the shift
and reset delimited control operators of Danvy and Filinski.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.00321v1,A Way to a Universal VR Accessibility Toolkit,"Virtual Reality (VR) has become more and more popular with dropping prices
for systems and a growing number of users. However, the issue of accessibility
in VR has been hardly addressed so far and no uniform approach or standard
exists at this time. In this position paper, we propose a customisable toolkit
implemented at the system-level and discuss the potential benefits of this
approach and challenges that will need to be overcome for a successful
implementation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1704.02145v1,A Fine-Grained Hierarchy of Hard Problems in the Separated Fragment,"Recently, the separated fragment (SF) has been introduced and proved to be
decidable. Its defining principle is that universally and existentially
quantified variables may not occur together in atoms. The known upper bound on
the time required to decide SF's satisfiability problem is formulated in terms
of quantifier alternations: Given an SF sentence $\exists \vec{z} \forall
\vec{x}_1 \exists \vec{y}_1 \ldots \forall \vec{x}_n \exists \vec{y}_n . \psi$
in which $\psi$ is quantifier free, satisfiability can be decided in
nondeterministic $n$-fold exponential time. In the present paper, we conduct a
more fine-grained analysis of the complexity of SF-satisfiability. We derive an
upper and a lower bound in terms of the degree of interaction of existential
variables (short: degree)}---a novel measure of how many separate existential
quantifier blocks in a sentence are connected via joint occurrences of
variables in atoms. Our main result is the $k$-NEXPTIME-completeness of the
satisfiability problem for the set $SF_{\leq k}$ of all SF sentences that have
degree $k$ or smaller. Consequently, we show that SF-satisfiability is
non-elementary in general, since SF is defined without restrictions on the
degree. Beyond trivial lower bounds, nothing has been known about the hardness
of SF-satisfiability so far.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1810.10257v1,A general proof certification framework for modal logic,"One of the main issues in proof certification is that different theorem
provers, even when designed for the same logic, tend to use different proof
formalisms and produce outputs in different formats. The project ProofCert
promotes the usage of a common specification language and of a small and
trusted kernel in order to check proofs coming from different sources and for
different logics. By relying on that idea and by using a classical focused
sequent calculus as a kernel, we propose here a general framework for checking
modal proofs. We present the implementation of the framework in a Prolog-like
language and show how it is possible to specialize it in a simple and modular
way in order to cover different proof formalisms, such as labeled systems,
tableaux, sequent calculi and nested sequent calculi. We illustrate the method
for the logic K by providing several examples and discuss how to further extend
the approach.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
http://arxiv.org/abs/1503.04069v2,LSTM: A Search Space Odyssey,"Several variants of the Long Short-Term Memory (LSTM) architecture for
recurrent neural networks have been proposed since its inception in 1995. In
recent years, these networks have become the state-of-the-art models for a
variety of machine learning problems. This has led to a renewed interest in
understanding the role and utility of various computational components of
typical LSTM variants. In this paper, we present the first large-scale analysis
of eight LSTM variants on three representative tasks: speech recognition,
handwriting recognition, and polyphonic music modeling. The hyperparameters of
all LSTM variants for each task were optimized separately using random search,
and their importance was assessed using the powerful fANOVA framework. In
total, we summarize the results of 5400 experimental runs ($\approx 15$ years
of CPU time), which makes our study the largest of its kind on LSTM networks.
Our results show that none of the variants can improve upon the standard LSTM
architecture significantly, and demonstrate the forget gate and the output
activation function to be its most critical components. We further observe that
the studied hyperparameters are virtually independent and derive guidelines for
their efficient adjustment.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1303.0382v1,Network algebra for synchronous dataflow,"We develop an algebraic theory of synchronous dataflow networks. First, a
basic algebraic theory of networks, called BNA (Basic Network Algebra), is
introduced. This theory captures the basic algebraic properties of networks.
For synchronous dataflow networks, it is subsequently extended with additional
constants for the branching connections that occur between the cells of
synchronous dataflow networks and axioms for these additional constants. We
also give two models of the resulting theory, the one based on stream
transformers and the other based on processes as considered in process algebra.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0003068v1,A Polyvariant Binding-Time Analysis for Off-line Partial Deduction,"We study the notion of binding-time analysis for logic programs. We formalise
the unfolding aspect of an on-line partial deduction system as a Prolog
program. Using abstract interpretation, we collect information about the
run-time behaviour of the program. We use this information to make the control
decisions about the unfolding at analysis time and to turn the on-line system
into an off-line system. We report on some initial experiments.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1001.4427v1,Extensional and Intensional Strategies,"This paper is a contribution to the theoretical foundations of strategies. We
first present a general definition of abstract strategies which is extensional
in the sense that a strategy is defined explicitly as a set of derivations of
an abstract reduction system. We then move to a more intensional definition
supporting the abstract view but more operational in the sense that it
describes a means for determining such a set. We characterize the class of
extensional strategies that can be defined intensionally. We also give some
hints towards a logical characterization of intensional strategies and propose
a few challenging perspectives.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0803.2365v1,SAFIUS - A secure and accountable filesystem over untrusted storage,"We describe SAFIUS, a secure accountable file system that resides over an
untrusted storage. SAFIUS provides strong security guarantees like
confidentiality, integrity, prevention from rollback attacks, and
accountability. SAFIUS also enables read/write sharing of data and provides the
standard UNIX-like interface for applications. To achieve accountability with
good performance, it uses asynchronous signatures; to reduce the space required
for storing these signatures, a novel signature pruning mechanism is used.
SAFIUS has been implemented on a GNU/Linux based system modifying OpenGFS.
Preliminary performance studies show that SAFIUS has a tolerable overhead for
providing secure storage: while it has an overhead of about 50% of OpenGFS in
data intensive workloads (due to the overhead of performing
encryption/decryption in software), it is comparable (or better in some cases)
to OpenGFS in metadata intensive workloads.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.06138v1,Hybrid Model with Time Modeling for Sequential Recommender Systems,"Deep learning based methods have been used successfully in recommender system
problems. Approaches using recurrent neural networks, transformers, and
attention mechanisms are useful to model users' long- and short-term
preferences in sequential interactions. To explore different session-based
recommendation solutions, Booking.com recently organized the WSDM WebTour 2021
Challenge, which aims to benchmark models to recommend the final city in a
trip. This study presents our approach to this challenge. We conducted several
experiments to test different state-of-the-art deep learning architectures for
recommender systems. Further, we proposed some changes to Neural Attentive
Recommendation Machine (NARM), adapted its architecture for the challenge
objective, and implemented training approaches that can be used in any
session-based model to improve accuracy. Our experimental result shows that the
improved NARM outperforms all other state-of-the-art benchmark methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.01809v1,"DA-DRN: Degradation-Aware Deep Retinex Network for Low-Light Image
  Enhancement","Images obtained in real-world low-light conditions are not only low in
brightness, but they also suffer from many other types of degradation, such as
color distortion, unknown noise, detail loss and halo artifacts. In this paper,
we propose a Degradation-Aware Deep Retinex Network (denoted as DA-DRN) for
low-light image enhancement and tackle the above degradation. Based on Retinex
Theory, the decomposition net in our model can decompose low-light images into
reflectance and illumination maps and deal with the degradation in the
reflectance during the decomposition phase directly. We propose a
Degradation-Aware Module (DA Module) which can guide the training process of
the decomposer and enable the decomposer to be a restorer during the training
phase without additional computational cost in the test phase. DA Module can
achieve the purpose of noise removal while preserving detail information into
the illumination map as well as tackle color distortion and halo artifacts. We
introduce Perceptual Loss to train the enhancement network to generate the
brightness-improved illumination maps which are more consistent with human
visual perception. We train and evaluate the performance of our proposed model
over the LOL real-world and LOL synthetic datasets, and we also test our model
over several other frequently used datasets without Ground-Truth (LIME, DICM,
MEF and NPE datasets). We conduct extensive experiments to demonstrate that our
approach achieves a promising effect with good rubustness and generalization
and outperforms many other state-of-the-art methods qualitatively and
quantitatively. Our method only takes 7 ms to process an image with 600x400
resolution on a TITAN Xp GPU.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0204050v1,Computing Homotopic Shortest Paths Efficiently,"This paper addresses the problem of finding shortest paths homotopic to a
given disjoint set of paths that wind amongst point obstacles in the plane. We
present a faster algorithm than previously known.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.15590v1,"Enjeux √©thiques de l'IA en sant√© : une humanisation du parcours de
  soin par l'intelligence artificielle ?","Considering the use of artificial intelligence for greater personalization of
patient care and better management of human and material resources may seem
like an opportunity not to be missed. In order to offer a better humanization
of the care pathway, artificial intelligence is a tool that decision-makers in
the hospital sector must appropriate by taking care of the new ethical issues
and conflicts of values that this technology generates.
  Envisager le recours \`a l'intelligence artificielle pour une plus grande
personnalisation de la prise en charge du patient et une meilleure gestion des
ressources humaines et mat\'erielles peut sembler une opportunit\'e \`a ne pas
manquer. Afin de proposer une meilleure humanisation du parcours de soin,
l'intelligence artificielle est un outil que les d\'ecideurs du milieu
hospitalier doivent s'approprier en veillant aux nouveaux enjeux \'ethiques et
conflits de valeurs que cette technologie engendre.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1001.3251v2,The Recognition of Tolerance and Bounded Tolerance Graphs,"Tolerance graphs model interval relations in such a way that intervals can
tolerate a certain degree of overlap without being in conflict. This subclass
of perfect graphs has been extensively studied, due to both its interesting
structure and its numerous applications. Several efficient algorithms for
optimization problems that are NP-hard on general graphs have been designed for
tolerance graphs. In spite of this, the recognition of tolerance graphs -
namely, the problem of deciding whether a given graph is a tolerance graph - as
well as the recognition of their main subclass of bounded tolerance graphs,
have been the most fundamental open problems on this class of graphs (cf. the
book on tolerance graphs \cite{GolTol04}) since their introduction in 1982
\cite{GoMo82}. In this article we prove that both recognition problems are
NP-complete, even in the case where the input graph is a trapezoid graph. The
presented results are surprising because, on the one hand, most subclasses of
perfect graphs admit polynomial recognition algorithms and, on the other hand,
bounded tolerance graphs were believed to be efficiently recognizable as they
are a natural special case of trapezoid graphs (which can be recognized in
polynomial time) and share a very similar structure with them. For our
reduction we extend the notion of an \emph{acyclic orientation} of permutation
and trapezoid graphs. Our main tool is a new algorithm that uses \emph{vertex
splitting} to transform a given trapezoid graph into a permutation graph, while
preserving this new acyclic orientation property. This method of vertex
splitting is of independent interest; very recently, it has been proved a
powerful tool also in the design of efficient recognition algorithms for other
classes of graphs \cite{MC-Trapezoid}.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.10680v4,"Sparse approximate matrix-matrix multiplication for density matrix
  purification with error control","We propose a method for strict error control in sparse approximate
matrix-matrix multiplication. The method combines an error bound and a
parameter sweep to select an appropriate threshold value. The scheme for error
control and the sparse approximate multiplication are implemented using the
Chunks and Tasks parallel programming model. We demonstrate the performance of
the method in parallel linear scaling electronic structure calculations using
density matrix purification with rigorous error control.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0009030v1,"From Syntactic Theories to Interpreters: A Specification Language and
  Its Compilation","Recent years have seen an increasing need of high-level specification
languages and tools generating code from specifications. In this paper, we
introduce a specification language, {\splname}, which is tailored to the
writing of syntactic theories of language semantics. More specifically, the
language supports specifying primitive notions such as dynamic constraints,
contexts, axioms, and inference rules. We also introduce a system which
generates interpreters from {\splname} specifications. A prototype system is
implemented and has been tested on a number of examples, including a syntactic
theory for Verilog.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1512.01370v1,Locally Adaptive Translation for Knowledge Graph Embedding,"Knowledge graph embedding aims to represent entities and relations in a
large-scale knowledge graph as elements in a continuous vector space. Existing
methods, e.g., TransE and TransH, learn embedding representation by defining a
global margin-based loss function over the data. However, the optimal loss
function is determined during experiments whose parameters are examined among a
closed set of candidates. Moreover, embeddings over two knowledge graphs with
different entities and relations share the same set of candidate loss
functions, ignoring the locality of both graphs. This leads to the limited
performance of embedding related applications. In this paper, we propose a
locally adaptive translation method for knowledge graph embedding, called
TransA, to find the optimal loss function by adaptively determining its margin
over different knowledge graphs. Experiments on two benchmark data sets
demonstrate the superiority of the proposed method, as compared to
the-state-of-the-art ones.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0311015v2,Make search become the internal function of Internet,"Domain Resource Integrated System (DRIS) is introduced in this paper. DRIS is
a distributed information retrieval system, which will solve problems like poor
coverage, long update interval in current web search system. The most distinct
character of DRIS is that it's a public opening system, and acts as an internal
component of Internet, but not the production of a company. The implementation
of DRIS is also represented.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1005.5181v1,"Compression Rate Method for Empirical Science and Application to
  Computer Vision","This philosophical paper proposes a modified version of the scientific
method, in which large databases are used instead of experimental observations
as the necessary empirical ingredient. This change in the source of the
empirical data allows the scientific method to be applied to several aspects of
physical reality that previously resisted systematic interrogation. Under the
new method, scientific theories are compared by instantiating them as
compression programs, and examining the codelengths they achieve on a database
of measurements related to a phenomenon of interest. Because of the
impossibility of compressing random data, ""real world"" data can only be
compressed by discovering and exploiting the empirical structure it exhibits.
The method also provides a new way of thinking about two longstanding issues in
the philosophy of science: the problem of induction and the problem of
demarcation.
  The second part of the paper proposes to reformulate computer vision as an
empirical science of visual reality, by applying the new method to large
databases of natural images. The immediate goal of the proposed reformulation
is to repair the chronic difficulties in evaluation experienced by the field of
computer vision. The reformulation should bring a wide range of benefits,
including a substantially increased degree of methodological rigor, the ability
to justify complex theories without overfitting, a scalable evaluation
paradigm, and the potential to make systematic progress. A crucial argument is
that the change is not especially drastic, because most computer vision tasks
can be reformulated as specialized image compression techniques. Finally, a
concrete proposal is discussed in which a database is produced by recording
from a roadside video camera, and compression is achieved by developing a
computational understanding of the appearance of moving cars.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.06984v1,"Constrained Synchronization and Subset Synchronization Problems for
  Weakly Acyclic Automata","We investigate the constrained synchronization problem for weakly acyclic, or
partially ordered, input automata. We show that, for input automata of this
type, the problem is always in NP. Furthermore, we give a full classification
of the realizable complexities for constraint automata with at most two states
and over a ternary alphabet. We find that most constrained problems that are
PSPACE-complete in general become NP-complete. However, there also exist
constrained problems that are PSPACE-complete in the general setting but become
polynomial time solvable when considered for weakly acyclic input automata. We
also investigate two problems related to subset synchronization, namely if
there exists a word mapping all states into a given target subset of states,
and if there exists a word mapping one subset into another. Both problems are
PSPACE-complete in general, but in our setting the former is polynomial time
solvable and the latter is NP-complete.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0105021v1,"Solving Composed First-Order Constraints from Discrete-Time Robust
  Control","This paper deals with a problem from discrete-time robust control which
requires the solution of constraints over the reals that contain both universal
and existential quantifiers. For solving this problem we formulate it as a
program in a (fictitious) constraint logic programming language with explicit
quantifier notation. This allows us to clarify the special structure of the
problem, and to extend an algorithm for computing approximate solution sets of
first-order constraints over the reals to exploit this structure. As a result
we can deal with inputs that are clearly out of reach for current symbolic
solvers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1207.4577v4,"Solving Stochastic B√ºchi Games on Infinite Arenas with a Finite
  Attractor","We consider games played on an infinite probabilistic arena where the first
player aims at satisfying generalized B\""uchi objectives almost surely, i.e.,
with probability one. We provide a fixpoint characterization of the winning
sets and associated winning strategies in the case where the arena satisfies
the finite-attractor property. From this we directly deduce the decidability of
these games on probabilistic lossy channel systems.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1909.10340v5,AHA! an 'Artificial Hippocampal Algorithm' for Episodic Machine Learning,"The majority of ML research concerns slow, statistical learning of i.i.d.
samples from large, labelled datasets. Animals do not learn this way. An
enviable characteristic of animal learning is `episodic' learning - the ability
to memorise a specific experience as a composition of existing concepts, after
just one experience, without provided labels. The new knowledge can then be
used to distinguish between similar experiences, to generalise between classes,
and to selectively consolidate to long-term memory. The Hippocampus is known to
be vital to these abilities. AHA is a biologically-plausible computational
model of the Hippocampus. Unlike most machine learning models, AHA is trained
without external labels and uses only local credit assignment. We demonstrate
AHA in a superset of the Omniglot one-shot classification benchmark. The
extended benchmark covers a wider range of known hippocampal functions by
testing pattern separation, completion, and recall of original input. These
functions are all performed within a single configuration of the computational
model. Despite these constraints, image classification results are comparable
to conventional deep convolutional ANNs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1109.3555v1,Using In-Memory Encrypted Databases on the Cloud,"Storing data in the cloud poses a number of privacy issues. A way to handle
them is supporting data replication and distribution on the cloud via a local,
centrally synchronized storage. In this paper we propose to use an in-memory
RDBMS with row-level data encryption for granting and revoking access rights to
distributed data. This type of solution is rarely adopted in conventional
RDBMSs because it requires several complex steps. In this paper we focus on
implementation and benchmarking of a test system, which shows that our simple
yet effective solution overcomes most of the problems.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1109.2075v1,"Evidence-Based Comparison of Modularity Support Between Java and Object
  Teams","Background: Aspect-oriented programming (AOP) is an emerging programming
paradigm whose focus is about improving modularity, with an emphasis on the
modularization of crosscutting concerns.
  Objective: The goal of this paper is to assess the extent to which an AOP
language -ObjectTeams/Java (OT/J) -improves the modularity of a software
system. This improvement has been claimed but, to the best of our knowledge,
this paper is the first attempting to present quantitative evidence of it.
  Method: We compare functionally-equivalent implementations of the
Gang-of-Four design patterns, developed in Java and OT/J, using software
metrics.
  Results: The results of our comparison support the modularity improvement
claims made in the literature. For six of the seven metrics used, the OT/J
versions of the patterns obtained significantly better results.
  Limitations: This work uses a set of metrics originally defined for
object-oriented (OO) systems. It may be the case that the metrics are biased,
in that they were created in the context of OO programming (OOP), before the
advent of AOP. We consider this comparison a stepping stone as, ultimately, we
plan to assess the modularity improvements with paradigm independent metrics,
which will conceivably eliminate the bias. Each individual example from the
sample used in this paper is small. In future, we plan to replicate this
experiment using larger systems, where the benefits of AOP may be more
noticeable.
  Conclusion: This work contributes with evidence to fill gaps in the body of
quantitative results supporting alleged benefits to software modularity brought
by AOP languages, namely OT/J.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0102023v1,Factored Notation for Interval I/O,"This note addresses the input and output of intervals in the sense of
interval arithmetic and interval constraints. The most obvious, and so far most
widely used notation, for intervals has drawbacks that we remedy with a new
notation that we propose to call factored notation. It is more compact and
allows one to find a good trade-off between interval width and ease of reading.
We describe how such a trade-off can be based on the information yield (in the
sense of information theory) of the last decimal shown.",0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.06872v1,Generalisation in Neural Networks Does not Require Feature Overlap,"That shared features between train and test data are required for
generalisation in artificial neural networks has been a common assumption of
both proponents and critics of these models. Here, we show that convolutional
architectures avoid this limitation by applying them to two well known
challenges, based on learning the identity function and learning rules
governing sequences of words. In each case, successful performance on the test
set requires generalising to features that were not present in the training
data, which is typically not feasible for standard connectionist models.
However, our experiments demonstrate that neural networks can succeed on such
problems when they incorporate the weight sharing employed by convolutional
architectures. In the image processing domain, such architectures are intended
to reflect the symmetry under spatial translations of the natural world that
such images depict. We discuss the role of symmetry in the two tasks and its
connection to generalisation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0003006v1,"Materialized View Selection and Maintenance Using Multi-Query
  Optimization","Because the presence of views enhances query performance, materialized views
are increasingly being supported by commercial database/data warehouse systems.
Whenever the data warehouse is updated, the materialized views must also be
updated. However, whereas the amount of data entering a warehouse, the query
loads, and the need to obtain up-to-date responses are all increasing, the time
window available for making the warehouse up-to-date is shrinking. These trends
necessitate efficient techniques for the maintenance of materialized views.
  In this paper, we show how to find an efficient plan for maintenance of a
{\em set} of views, by exploiting common subexpressions between different view
maintenance expressions. These common subexpressions may be materialized
temporarily during view maintenance. Our algorithms also choose
subexpressions/indices to be materialized permanently (and maintained along
with other materialized views), to speed up view maintenance. While there has
been much work on view maintenance in the past, our novel contributions lie in
exploiting a recently developed framework for multiquery optimization to
efficiently find good view maintenance plans as above. In addition to faster
view maintenance, our algorithms can also be used to efficiently select
materialized views to speed up workloads containing queries.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0612104v2,Sufficient Conditions for Coarse-Graining Evolutionary Dynamics,"It is commonly assumed that the ability to track the frequencies of a set of
schemata in the evolving population of an infinite population genetic algorithm
(IPGA) under different fitness functions will advance efforts to obtain a
theory of adaptation for the simple GA. Unfortunately, for IPGAs with long
genomes and non-trivial fitness functions there do not currently exist
theoretical results that allow such a study. We develop a simple framework for
analyzing the dynamics of an infinite population evolutionary algorithm (IPEA).
This framework derives its simplicity from its abstract nature. In particular
we make no commitment to the data-structure of the genomes, the kind of
variation performed, or the number of parents involved in a variation
operation. We use this framework to derive abstract conditions under which the
dynamics of an IPEA can be coarse-grained. We then use this result to derive
concrete conditions under which it becomes computationally feasible to closely
approximate the frequencies of a family of schemata of relatively low order
over multiple generations, even when the bitstsrings in the evolving population
of the IPGA are long.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.09003v1,Hopf bifurcation in addition-shattering kinetics,"In aggregation-fragmentation processes, a steady state is usually reached in
the long time limit. This indicates the existence of a fixed point in the
underlying system of ordinary differential equations. The next simplest
possibility is an asymptotically periodic motion. Never-ending oscillations
have not been rigorously established so far, although oscillations have been
recently numerically detected in a few systems. For a class of
addition-shattering processes, we provide convincing numerical evidence for
never-ending oscillations in a certain region $\mathcal{U}$ of the parameter
space. The processes which we investigate admit a fixed point that becomes
unstable when parameters belong to $\mathcal{U}$ and never-ending oscillations
effectively emerge through a Hopf bifurcation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1602.03593v1,Precise subtyping for synchronous multiparty sessions,"The notion of subtyping has gained an important role both in theoretical and
applicative domains: in lambda and concurrent calculi as well as in programming
languages. The soundness and the completeness, together referred to as the
preciseness of subtyping, can be considered from two different points of view:
operational and denotational. The former preciseness has been recently
developed with respect to type safety, i.e. the safe replacement of a term of a
smaller type when a term of a bigger type is expected. The latter preciseness
is based on the denotation of a type which is a mathematical object that
describes the meaning of the type in accordance with the denotations of other
expressions from the language. The result of this paper is the operational and
denotational preciseness of the subtyping for a synchronous multiparty session
calculus. The novelty of this paper is the introduction of characteristic
global types to prove the operational completeness.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9906008v2,A Lower Bound on the Average-Case Complexity of Shellsort,"We prove a general lower bound on the average-case complexity of Shellsort:
the average number of data-movements (and comparisons) made by a $p$-pass
Shellsort for any incremental sequence is $\Omega (pn^{1 + 1/p})$ for every
$p$. The proof method is an incompressibility argument based on Kolmogorov
complexity. Using similar techniques, the average-case complexity of several
other sorting algorithms is analyzed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.08349v1,Proof Search on Bilateralist Judgments over Non-deterministic Semantics,"The bilateralist approach to logical consequence maintains that judgments of
different qualities should be taken into account in determining
what-follows-from-what. We argue that such an approach may be actualized by a
two-dimensional notion of entailment induced by semantic structures that also
accommodate non-deterministic and partial interpretations, and propose a
proof-theoretical apparatus to reason over bilateralist judgments using
symmetrical two-dimensional analytical Hilbert-style calculi. We also provide a
proof-search algorithm for finite analytic calculi that runs in at most
exponential time, in general, and in polynomial time when only rules having at
most one formula in the succedent are present in the concerned calculus.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1302.7262v6,Towards a provably resilient scheme for graph-based watermarking,"Digital watermarks have been considered a promising way to fight software
piracy. Graph-based watermarking schemes encode authorship/ownership data as
control-flow graph of dummy code. In 2012, Chroni and Nikolopoulos developed an
ingenious such scheme which was claimed to withstand attacks in the form of a
single edge removal. We extend the work of those authors in various aspects.
First, we give a formal characterization of the class of graphs generated by
their encoding function. Then, we formulate a linear-time algorithm which
recovers from ill-intentioned removals of $k \leq 2$ edges, therefore proving
their claim. Furthermore, we provide a simpler decoding function and an
algorithm to restore watermarks with an arbitrary number of missing edges
whenever at all possible. By disclosing and improving upon the resilience of
Chroni and Nikolopoulos's watermark, our results reinforce the interest in
regarding it as a possible solution to numerous applications.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1810.04190v4,Uniform CSP Parameterized by Solution Size is in W[1],"We show that the uniform Constraint Satisfaction Problem (CSP) parameterized
by the size of the solution is in W[1] (the problem is W[1]-hard and it is easy
to place it in W[3]). Given a single ""free"" element of the domain, denoted by
$0$, we define the size of an assignment as the number of variables that are
mapped to a value other than $0$. Named by Kolaitis and Vardi (2000), uniform
CSP means that the input contains the domain and the list of tuples of each
relation in the instance. Uniform CSP is polynomial time equivalent to
homomorphism problem and also to evaluation of conjunctive queries on
relational databases. It also has applications in artificial intelligence.
  We do not restrict the problem to any (finite or infinite) family of
relations. Marx and Bulatov (2014) showed that Uniform CSP restricted to some
finite family of relations (thus with a bound on the arity of relations) and
over any finite domain is either W[1]-complete or fixed parameter tractable.
  We then prove that parameterized Subset Sum with weights bounded by $n^k$ is
in W[1]. Abboud et al. (2014) have already proved it, but our proof is much
shorter and arguably more intuitive.
  Lastly, we study the weighted CSP over the Boolean Domain, where each
variable is assigned a weight, and given a target value, it should be decided
if there is a satisfying assignment of size $k$ (the parameter) such that the
weight of its $1$-variables adds up to the target value. We prove that if the
weights are bounded by $n^k$, then the problem is in W[1]. Our proofs give a
nondeterministic RAM program with special properties deciding the problem.
First defined by Chen et al. (2005), such programs characterize W[1].",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.3729v1,Non-termination of Dalvik bytecode via compilation to CLP,"We present a set of rules for compiling a Dalvik bytecode program into a
logic program with array constraints. Non-termination of the resulting program
entails that of the original one, hence the techniques we have presented before
for proving non-termination of constraint logic programs can be used for
proving non-termination of Dalvik programs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1312.4422v1,"Which Memristor Theory is Best for Relating Devices Properties to
  Memristive Function?","There are three theoretical models which purport to relate
experimentally-measurable or fabrication-controllable device properties to the
memristor's operation: 1. Strukov et al's phenomenological model; 2. Georgiou
et al's Bernoulli rewrite of that phenomenological model; 3. Gale's
memory-conservation model. They differ in their prediction of the effect on
memristance of changing the electrode size and factors that affect the
hysteresis. Using a batch of TiO$_2$ sol-gel memristors fabricated with
different top electrode widths we test and compare these three theories. It was
found that, contrary to model 2's prediction, the `dimensionless lumped
parameter', $\beta$, did not correlate to any measure of the hysteresis.
Contrary to model 1, memristance was found to be dependent on the three spatial
dimensions of the TiO$_2$ layer, as was predicted by model 3. Model 3 was found
to fit the change in resistance value with electrode size. Simulations using
model 3 and experimentally derived values for contact resistance gave
hysteresis values that were linearly related to (and only one order of
magnitude out) from the experimentally-measured values. Memristor hysteresis
was found to be related to the ON state resistance and thus the electrode size
(as those two are related). These results offer a verification of the
memory-conservation theory of memristance and its association of the vacancy
magnetic flux with the missing magnetic flux in memristor theory. This is the
first paper to experimentally test various theories pertaining to the operation
of memristor devices.",0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0511012v1,"Parameters Affecting the Resilience of Scale-Free Networks to Random
  Failures","It is commonly believed that scale-free networks are robust to massive
numbers of random node deletions. For example, Cohen et al. study scale-free
networks including some which approximate the measured degree distribution of
the Internet. Their results suggest that if each node in this network failed
independently with probability 0.99, the remaining network would continue to
have a giant component. In this paper, we show that a large and important
subclass of scale-free networks are not robust to massive numbers of random
node deletions for practical purposes. In particular, we study finite
scale-free networks which have minimum node degree of 1 and a power-law degree
distribution beginning with nodes of degree 1 (power-law networks). We show
that, in a power-law network approximating the Internet's reported
distribution, when the probability of deletion of each node is 0.5 only about
25% of the surviving nodes in the network remain connected in a giant
component, and the giant component does not persist beyond a critical failure
rate of 0.9. The new result is partially due to improved analytical
accommodation of the large number of degree-0 nodes that result after node
deletions. Our results apply to finite power-law networks with a wide range of
power-law exponents, including Internet-like networks. We give both analytical
and empirical evidence that such networks are not generally robust to massive
random node deletions.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9902011v1,Content-Based Book Recommending Using Learning for Text Categorization,"Recommender systems improve access to relevant products and information by
making personalized suggestions based on previous examples of a user's likes
and dislikes. Most existing recommender systems use social filtering methods
that base recommendations on other users' preferences. By contrast,
content-based methods use information about an item itself to make suggestions.
This approach has the advantage of being able to recommended previously unrated
items to users with unique interests and to provide explanations for its
recommendations. We describe a content-based book recommending system that
utilizes information extraction and a machine-learning algorithm for text
categorization. Initial experimental results demonstrate that this approach can
produce accurate recommendations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.10692v1,"Parallel Statistical Model Checking for Safety Verification in Smart
  Grids","By using small computing devices deployed at user premises, Autonomous Demand
Response (ADR) adapts users electricity consumption to given time-dependent
electricity tariffs. This allows end-users to save on their electricity bill
and Distribution System Operators to optimise (through suitable time-dependent
tariffs) management of the electric grid by avoiding demand peaks.
Unfortunately, even with ADR, users power consumption may deviate from the
expected (minimum cost) one, e.g., because ADR devices fail to correctly
forecast energy needs at user premises. As a result, the aggregated power
demand may present undesirable peaks. In this paper we address such a problem
by presenting methods and a software tool (APD-Analyser) implementing them,
enabling Distribution System Operators to effectively verify that a given
time-dependent electricity tariff achieves the desired goals even when
end-users deviate from their expected behaviour. We show feasibility of the
proposed approach through a realistic scenario from a medium voltage Danish
distribution network.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1903.09755v3,Trifocal Relative Pose from Lines at Points and its Efficient Solution,"We present a new minimal problem for relative pose estimation mixing point
features with lines incident at points observed in three views and its
efficient homotopy continuation solver. We demonstrate the generality of the
approach by analyzing and solving an additional problem with mixed point and
line correspondences in three views. The minimal problems include
correspondences of (i) three points and one line and (ii) three points and two
lines through two of the points which is reported and analyzed here for the
first time. These are difficult to solve, as they have 216 and - as shown here
- 312 solutions, but cover important practical situations when line and point
features appear together, e.g., in urban scenes or when observing curves. We
demonstrate that even such difficult problems can be solved robustly using a
suitable homotopy continuation technique and we provide an implementation
optimized for minimal problems that can be integrated into engineering
applications. Our simulated and real experiments demonstrate our solvers in the
camera geometry computation task in structure from motion. We show that new
solvers allow for reconstructing challenging scenes where the standard two-view
initialization of structure from motion fails.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.07235v1,Evo* 2020 -- Late-Breaking Abstracts Volume,"This volume contains the Late-Breaking Abstracts submitted to the Evo* 2020
Conference, that took place online, from 15 to 17 of April 2020. These papers
where presented as short talks and also at the poster session of the conference
together with other regular submissions. All of them present ongoing research
and preliminary results investigating on the application of different
approaches of Bioinspired Methods (mainly Evolutionary Computation) to
different problems, most of them real world ones.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/cs/0208009v1,Offline Specialisation in Prolog Using a Hand-Written Compiler Generator,"The so called ``cogen approach'' to program specialisation, writing a
compiler generator instead of a specialiser, has been used with considerable
success in partial evaluation of both functional and imperative languages. This
paper demonstrates that the cogen approach is also applicable to the
specialisation of logic programs (also called partial deduction) and leads to
effective specialisers. Moreover, using good binding-time annotations, the
speed-ups of the specialised programs are comparable to the speed-ups obtained
with online specialisers. The paper first develops a generic approach to
offline partial deduction and then a specific offline partial deduction method,
leading to the offline system LIX for pure logic programs. While this is a
usable specialiser by itself, it is used to develop the cogen system LOGEN.
Given a program, a specification of what inputs will be static, and an
annotation specifying which calls should be unfolded, LOGEN generates a
specialised specialiser for the program at hand. Running this specialiser with
particular values for the static inputs results in the specialised program.
While this requires two steps instead of one, the efficiency of the
specialisation process is improved in situations where the same program is
specialised multiple times. The paper also presents and evaluates an automatic
binding-time analysis that is able to derive the annotations. While the derived
annotations are still suboptimal compared to hand-crafted ones, they enable
non-expert users to use the LOGEN system in a fully automated way. Finally,
LOGEN is extended so as to directly support a large part of Prolog's
declarative and non-declarative features and so as to be able to perform so
called mixline specialisations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1208.6342v1,"Is Wolfram and Cook's (2,5) Turing machine really universal?","Wolfram [2, p. 707] and Cook [1, p. 3] claim to prove that a (2,5) Turing
machine (2 states, 5 symbols) is universal, via a universal cellular automaton
known as Rule 110. The first part of this paper points out a critical gap in
their argument. The second part bridges the gap, thereby giving what appears to
be the first proof of universality.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.13643v1,On the Unreasonable Effectiveness of Centroids in Image Retrieval,"Image retrieval task consists of finding similar images to a query image from
a set of gallery (database) images. Such systems are used in various
applications e.g. person re-identification (ReID) or visual product search.
Despite active development of retrieval models it still remains a challenging
task mainly due to large intra-class variance caused by changes in view angle,
lighting, background clutter or occlusion, while inter-class variance may be
relatively low. A large portion of current research focuses on creating more
robust features and modifying objective functions, usually based on Triplet
Loss. Some works experiment with using centroid/proxy representation of a class
to alleviate problems with computing speed and hard samples mining used with
Triplet Loss. However, these approaches are used for training alone and
discarded during the retrieval stage. In this paper we propose to use the mean
centroid representation both during training and retrieval. Such an aggregated
representation is more robust to outliers and assures more stable features. As
each class is represented by a single embedding - the class centroid - both
retrieval time and storage requirements are reduced significantly. Aggregating
multiple embeddings results in a significant reduction of the search space due
to lowering the number of candidate target vectors, which makes the method
especially suitable for production deployments. Comprehensive experiments
conducted on two ReID and Fashion Retrieval datasets demonstrate effectiveness
of our method, which outperforms the current state-of-the-art. We propose
centroid training and retrieval as a viable method for both Fashion Retrieval
and ReID applications.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1711.03397v1,"SAIC: Identifying Configuration Files for System Configuration
  Management","Systems can become misconfigured for a variety of reasons such as operator
errors or buggy patches. When a misconfiguration is discovered, usually the
first order of business is to restore availability, often by undoing the
misconfiguration. To simplify this task, we propose the Statistical Analysis
for Identifying Configuration Files (SAIC), which analyzes how the contents of
a file changes over time to automatically determine which files contain
configuration state. In this way, SAIC reduces the number of files a user must
manually examine during recovery and allows versioning file systems to make
more efficient use of their versioning storage.
  The two key insights that enable SAIC to identify configuration files are
that configuration state must persist across executions of an application and
that configuration state changes at a slower rate than other types of
application state. SAIC applies these insights through a set of filters, which
eliminate non-persistent files from consideration, and a novel similarity
metric, which measures how similar a file's versions are to each other.
Together, these two mechanisms enable SAIC to identify all 72 configuration
files out of 2363 versioned files from 6 common applications in two user
traces, while mistaking only 33 non-configuration files as configuration files,
which allows a versioning file system to eliminate roughly 66% of
non-configuration file versions from its logs, thus reducing the number of file
versions that a user must try to recover from a misconfiguration.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.12425v1,Deep Job Understanding at LinkedIn,"As the world's largest professional network, LinkedIn wants to create
economic opportunity for everyone in the global workforce. One of its most
critical missions is matching jobs with processionals. Improving job targeting
accuracy and hire efficiency align with LinkedIn's Member First Motto. To
achieve those goals, we need to understand unstructured job postings with noisy
information. We applied deep transfer learning to create domain-specific job
understanding models. After this, jobs are represented by professional
entities, including titles, skills, companies, and assessment questions. To
continuously improve LinkedIn's job understanding ability, we designed an
expert feedback loop where we integrated job understanding models into
LinkedIn's products to collect job posters' feedback. In this demonstration, we
present LinkedIn's job posting flow and demonstrate how the integrated deep job
understanding work improves job posters' satisfaction and provides significant
metric lifts in LinkedIn's job recommendation system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1703.05953v1,Das Internet-Adressbuch bedroht unsere Privatsph√§re,"This paper summarizes selected results of the dissertation
""Beobachtungsm\""oglichkeiten im Domain Name System: Angriffe auf die
Privatsph\""are und Techniken zum Selbstdatenschutz"". The dissertation provides
new technical insights to answer the questions ""Who can monitor us on the
Internet?"" and ""How do we protect ourselves?"". It focuses on the Domain Name
System (DNS), the address book of the internet. It shows that recursive
nameservers have monitoring capabilities that have been neglected so far. In
particular, a behavior-based tracking method is introduced, which allows
operators to track the activities of users over an extended period of time. On
the one hand, this threatens the privacy of Internet users, on the other hand,
law enforcement could benefit from this research. Furthermore, new privacy
enhancing techniques are proposed, which are more effective and more
user-friendly than existing approaches.
  -----
  Dieser Beitrag fasst ausgew\""ahlte Ergebnisse der Dissertation
""Beobachtungsm\""oglichkeiten im Domain Name System: Angriffe auf die
Privatsph\""are und Techniken zum Selbstdatenschutz"" zusammen. Die Dissertation
liefert neue Antworten auf die Fragen ""Wer kann uns im Internet \""uberwachen?""
und ""Wie sch\""utzen wir uns davor?"". Die Arbeit befasst sich mit dem Domain
Name System (DNS), dem Adressbuch des Internets. Es wird gezeigt, dass es im
DNS bislang vernachl\""assigte \""Uberwachungsm\""oglichkeiten gibt. Insbesondere
wird ein Verfahren zum verhaltensbasierten Tracking vorgestellt, mit dem die
Aktivit\""aten von Internetnutzern unbemerkt \""uber l\""angere Zeitr\""aume
verfolgt werden k\""onnen. Einerseits wird dadurch die Privatsph\""are vieler
Internetnutzer bedroht, andererseits k\""onnten daraus neue Werkzeuge f\""ur die
Strafverfolgung entstehen. Weiterhin werden neue Datenschutz-Techniken
vorgeschlagen, die sicherer und benutzerfreundlicher sind als die bisherigen
Ans\""atze.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2108.12802v1,Interpretable Propaganda Detection in News Articles,"Online users today are exposed to misleading and propagandistic news articles
and media posts on a daily basis. To counter thus, a number of approaches have
been designed aiming to achieve a healthier and safer online news and media
consumption. Automatic systems are able to support humans in detecting such
content; yet, a major impediment to their broad adoption is that besides being
accurate, the decisions of such systems need also to be interpretable in order
to be trusted and widely adopted by users. Since misleading and propagandistic
content influences readers through the use of a number of deception techniques,
we propose to detect and to show the use of such techniques as a way to offer
interpretability. In particular, we define qualitatively descriptive features
and we analyze their suitability for detecting deception techniques. We further
show that our interpretable features can be easily combined with pre-trained
language models, yielding state-of-the-art results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0904.2340v2,Explicit fairness in testing semantics,"In this paper we investigate fair computations in the pi-calculus. Following
Costa and Stirling's approach for CCS-like languages, we consider a method to
label process actions in order to filter out unfair computations. We contrast
the existing fair-testing notion with those that naturally arise by imposing
weak and strong fairness. This comparison provides insight about the
expressiveness of the various `fair' testing semantics and about their
discriminating power.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1511.04197v1,Collaboration Framework in the EViE-m Platform,"Within the context of a 3D interactive strategy game, the EViE platform
allows participants to unlock game features using their knowledge and skills in
various thematic areas such as physics, mathematics, etc. By answering
questions organized by Educational Objective in stratified levels of
difficulty, users gather points which grant them access to desired world
elements. Richer world components become increasingly more difficult to access,
so that a players' individual (or cumulative if in a group) knowledge, ability
and / or dexterity is directly reflected by the level of complication of their
virtual world. In the present article we report on the communication
architecture of the platform and focus on framework components that allow group
activities such as cooperation (within the group to facilitate e.g.,
collaboration on more difficult problems), (inter-group) competition as well as
practice and skill honing activities (in single or in multi-player mode).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1904.09854v2,"A Triangle Algorithm for Semidefinite Version of Convex Hull Membership
  Problem","Given a subset $\mathbf{S}=\{A_1, \dots, A_m\}$ of $\mathbb{S}^n$, the set of
$n \times n$ real symmetric matrices, we define its {\it spectrahull} as the
set $SH(\mathbf{S}) = \{p(X) \equiv (Tr(A_1 X), \dots, Tr(A_m X))^T : X \in
\mathbf{\Delta}_n\}$, where ${\bf \Delta}_n$ is the {\it spectraplex}, $\{ X
\in \mathbb{S}^n : Tr(X)=1, X \succeq 0 \}$. We let {\it spectrahull
membership} (SHM) to be the problem of testing if a given $b \in \mathbb{R}^m$
lies in $SH(\mathbf{S})$. On the one hand when $A_i$'s are diagonal matrices,
SHM reduces to the {\it convex hull membership} (CHM), a fundamental problem in
LP. On the other hand, a bounded SDP feasibility is reducible to SHM. By
building on the {\it Triangle Algorithm} (TA) \cite{kalchar,kalsep}, developed
for CHM and its generalization, we design a TA for SHM, where given
$\varepsilon$, in $O(1/\varepsilon^2)$ iterations it either computes a
hyperplane separating $b$ from $SH(\mathbf{S})$, or $X_\varepsilon \in
\mathbf{\Delta}_n$ such that $\Vert p(X_\varepsilon) - b \Vert \leq \varepsilon
R$, $R$ maximum error over $\mathbf{\Delta}_n$. Under certain conditions
iteration complexity improves to $O(1/\varepsilon)$ or even $O(\ln
1/\varepsilon)$. The worst-case complexity of each iteration is $O(mn^2)$, plus
testing the existence of a pivot, shown to be equivalent to estimating the
least eigenvalue of a symmetric matrix. This together with a semidefinite
version of Carath\'eodory theorem allow implementing TA as if solving a CHM,
resorting to the {\it power method} only as needed, thereby improving the
complexity of iterations. The proposed Triangle Algorithm for SHM is simple,
practical and applicable to general SDP feasibility and optimization. Also, it
extends to a spectral analogue of SVM for separation of two spectrahulls.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1505.05187v1,Techniques for Deep Query Understanding,"Query Understanding concerns about inferring the precise intent of search by
the user with his formulated query, which is challenging because the queries
are often very short and ambiguous. The report discusses the various kind of
queries that can be put to a Search Engine and illustrates the Role of Query
Understanding for return of relevant results. With different advances in
techniques for deep understanding of queries as well as documents, the Search
Technology has witnessed three major era. A lot of interesting real world
examples have been used to illustrate the role of Query Understanding in each
of them. The Query Understanding Module is responsible to correct the mistakes
done by user in the query, to guide him in formulation of query with precise
intent, and to precisely infer the intent of the user query. The report
describes the complete architecture to handle aforementioned three tasks, and
then discusses basic as well as recent advanced techniques for each of the
component, through appropriate papers from reputed conferences and journals.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.00210v1,Scott Continuity in Generalized Probabilistic Theories,"Scott continuity is a concept from domain theory that had an unexpected
previous life in the theory of von Neumann algebras. Scott-continuous states
are known as normal states, and normal states are exactly the states coming
from density matrices. Given this, and the usefulness of Scott continuity in
domain theory, it is natural to ask whether this carries over to generalized
probabilistic theories. We show that the answer is no - there are
infinite-dimensional convex sets for which the set of Scott-continuous states
on the corresponding set of 2-valued POVMs does not recover the original convex
set, but is strictly larger. This shows the necessity of the use of topologies
for state-effect duality in the general case, rather than purely order
theoretic notions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0702121v5,"Induced Hilbert Space, Markov Chain, Diffusion Map and Fock Space in
  Thermophysics","In this article, we continue to explore Probability Bracket Notation (PBN),
proposed in our previous article. Using both Dirac vector bracket notation
(VBN) and PBN, we define induced Hilbert space and induced sample space, and
propose that there exists an equivalence relation between a Hilbert space and a
sample space constructed from the same base observable(s). Then we investigate
Markov transition matrices and their eigenvectors to make diffusion maps with
two examples: a simple graph theory example, to serve as a prototype of
bidirectional transition operator; a famous text document example in IR
literature, to serve as a tutorial of diffusion map in text document space. We
show that the sample space of the Markov chain and the Hilbert space spanned by
the eigenvectors of the transition matrix are not equivalent. At the end, we
apply our PBN and equivalence proposal to Thermophysics by associating sample
(phase) space with the Hilbert space of a single particle and the Fock space of
many-particle systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1004.3363v5,Faster Algorithms for Semi-Matching Problems,"We consider the problem of finding \textit{semi-matching} in bipartite graphs
which is also extensively studied under various names in the scheduling
literature. We give faster algorithms for both weighted and unweighted case.
  For the weighted case, we give an $O(nm\log n)$-time algorithm, where $n$ is
the number of vertices and $m$ is the number of edges, by exploiting the
geometric structure of the problem. This improves the classical $O(n^3)$
algorithms by Horn [Operations Research 1973] and Bruno, Coffman and Sethi
[Communications of the ACM 1974].
  For the unweighted case, the bound could be improved even further. We give a
simple divide-and-conquer algorithm which runs in $O(\sqrt{n}m\log n)$ time,
improving two previous $O(nm)$-time algorithms by Abraham [MSc thesis,
University of Glasgow 2003] and Harvey, Ladner, Lov\'asz and Tamir [WADS 2003
and Journal of Algorithms 2006]. We also extend this algorithm to solve the
\textit{Balance Edge Cover} problem in $O(\sqrt{n}m\log n)$ time, improving the
previous $O(nm)$-time algorithm by Harada, Ono, Sadakane and Yamashita [ISAAC
2008].",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0110058v1,"Teaching Parallel Programming Using Both High-Level and Low-Level
  Languages","We discuss the use of both MPI and OpenMP in the teaching of senior
undergraduate and junior graduate classes in parallel programming. We briefly
introduce the OpenMP standard and discuss why we have chosen to use it in
parallel programming classes. Advantages of using OpenMP over message passing
methods are discussed. We also include a brief enumeration of some of the
drawbacks of using OpenMP and how these drawbacks are being addressed by
supplementing OpenMP with additional MPI codes and projects. Several projects
given in my class are also described in this paper.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1509.04115v1,"Color-Phase Analysis for Sinusoidal Structured Light in Rapid Range
  Imaging","Active range sensing using structured-light is the most accurate and reliable
method for obtaining 3D information. However, most of the work has been limited
to range sensing of static objects, and range sensing of dynamic (moving or
deforming) objects has been investigated recently only by a few researchers.
Sinusoidal structured-light is one of the well-known optical methods for 3D
measurement. In this paper, we present a novel method for rapid high-resolution
range imaging using color sinusoidal pattern. We consider the real-world
problem of nonlinearity and color-band crosstalk in the color light projector
and color camera, and present methods for accurate recovery of color-phase. For
high-resolution ranging, we use high-frequency patterns and describe new
unwrapping algorithms for reliable range recovery. The experimental results
demonstrate the effectiveness of our methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1909.08784v3,"Characterizing Collective Attention via Descriptor Context: A Case Study
  of Public Discussions of Crisis Events","Social media datasets make it possible to rapidly quantify collective
attention to emerging topics and breaking news, such as crisis events.
Collective attention is typically measured by aggregate counts, such as the
number of posts that mention a name or hashtag. But according to rationalist
models of natural language communication, the collective salience of each
entity will be expressed not only in how often it is mentioned, but in the form
that those mentions take. This is because natural language communication is
premised on (and customized to) the expectations that speakers and writers have
about how their messages will be interpreted by the intended audience. We test
this idea by conducting a large-scale analysis of public online discussions of
breaking news events on Facebook and Twitter, focusing on five recent crisis
events. We examine how people refer to locations, focusing specifically on
contextual descriptors, such as ""San Juan"" versus ""San Juan, Puerto Rico.""
Rationalist accounts of natural language communication predict that such
descriptors will be unnecessary (and therefore omitted) when the named entity
is expected to have high prior salience to the reader. We find that the use of
contextual descriptors is indeed associated with proxies for social and
informational expectations, including macro-level factors like the location's
global salience and micro-level factors like audience engagement. We also find
a consistent decrease in descriptor context use over the lifespan of each
crisis event. These findings provide evidence about how social media users
communicate with their audiences, and point towards more fine-grained models of
collective attention that may help researchers and crisis response
organizations to better understand public perception of unfolding crisis
events.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1710.01294v1,"Multiple domination models for placement of electric vehicle charging
  stations in road networks","Electric and hybrid vehicles play an increasing role in the road transport
networks. Despite their advantages, they have a relatively limited cruising
range in comparison to traditional diesel/petrol vehicles, and require
significant battery charging time. We propose to model the facility location
problem of the placement of charging stations in road networks as a multiple
domination problem on reachability graphs. This model takes into consideration
natural assumptions such as a threshold for remaining battery load, and
provides some minimal choice for a travel direction to recharge the battery.
Experimental evaluation and simulations for the proposed facility location
model are presented in the case of real road networks corresponding to the
cities of Boston and Dublin.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.02804v3,"A Deep Learning Approach for the Computation of Curvature in the
  Level-Set Method","We propose a deep learning strategy to estimate the mean curvature of
two-dimensional implicit interfaces in the level-set method. Our approach is
based on fitting feed-forward neural networks to synthetic data sets
constructed from circular interfaces immersed in uniform grids of various
resolutions. These multilayer perceptrons process the level-set values from
mesh points next to the free boundary and output the dimensionless curvature at
their closest locations on the interface. Accuracy analyses involving irregular
interfaces, in both uniform and adaptive grids, show that our models are
competitive with traditional numerical schemes in the $L^1$ and $L^2$ norms. In
particular, our neural networks approximate curvature with comparable precision
in coarse resolutions, when the interface features steep curvature regions, and
when the number of iterations to reinitialize the level-set function is small.
Although the conventional numerical approach is more robust than our framework,
our results have unveiled the potential of machine learning for dealing with
computational tasks where the level-set method is known to experience
difficulties. We also establish that an application-dependent map of local
resolutions to neural models can be devised to estimate mean curvature more
effectively than a universal neural network.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.00679v1,Cooperative Control of Mobile Robots with Stackelberg Learning,"Multi-robot cooperation requires agents to make decisions that are consistent
with the shared goal without disregarding action-specific preferences that
might arise from asymmetry in capabilities and individual objectives. To
accomplish this goal, we propose a method named SLiCC: Stackelberg Learning in
Cooperative Control. SLiCC models the problem as a partially observable
stochastic game composed of Stackelberg bimatrix games, and uses deep
reinforcement learning to obtain the payoff matrices associated with these
games. Appropriate cooperative actions are then selected with the derived
Stackelberg equilibria. Using a bi-robot cooperative object transportation
problem, we validate the performance of SLiCC against centralized multi-agent
Q-learning and demonstrate that SLiCC achieves better combined utility.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0207027v6,"Permutation graphs, fast forward permutations, and sampling the cycle
  structure of a permutation","A permutation P on {1,..,N} is a_fast_forward_permutation_ if for each m the
computational complexity of evaluating P^m(x)$ is small independently of m and
x. Naor and Reingold constructed fast forward pseudorandom cycluses and
involutions. By studying the evolution of permutation graphs, we prove that the
number of queries needed to distinguish a random cyclus from a random
permutation on {1,..,N} is Theta(N) if one does not use queries of the form
P^m(x), but is only Theta(1) if one is allowed to make such queries.
  We construct fast forward permutations which are indistinguishable from
random permutations even when queries of the form P^m(x) are allowed. This is
done by introducing an efficient method to sample the cycle structure of a
random permutation, which in turn solves an open problem of Naor and Reingold.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0005033v1,"Multimethods and separate static typechecking in a language with
  C++-like object model","The goal of this paper is the description and analysis of multimethod
implementation in a new object-oriented, class-based programming language
called OOLANG. The implementation of the multimethod typecheck and selection,
deeply analyzed in the paper, is performed in two phases in order to allow
static typechecking and separate compilation of modules. The first phase is
performed at compile time, while the second is executed at link time and does
not require the modules' source code. OOLANG has syntax similar to C++; the
main differences are the absence of pointers and the realization of
polymorphism through subsumption. It adopts the C++ object model and supports
multiple inheritance as well as virtual base classes. For this reason, it has
been necessary to define techniques for realigning argument and return value
addresses when performing multimethod invocations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.11263v1,Editorial introduction: The power of words and networks,"According to Freud ""words were originally magic and to this day words have
retained much of their ancient magical power"". By words, behaviors are
transformed and problems are solved. The way we use words reveals our
intentions, goals and values. Novel tools for text analysis help understand the
magical power of words. This power is multiplied, if it is combined with the
study of social networks, i.e. with the analysis of relationships among social
units. This special issue of the International Journal of Information
Management, entitled ""Combining Social Network Analysis and Text Mining: from
Theory to Practice"", includes heterogeneous and innovative research at the
nexus of text mining and social network analysis. It aims to enrich work at the
intersection of these fields, which still lags behind in theoretical,
empirical, and methodological foundations. The nine articles accepted for
inclusion in this special issue all present methods and tools that have
business applications. They are summarized in this editorial introduction.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0912.0681v3,"A Local Spectral Method for Graphs: with Applications to Improving Graph
  Partitions and Exploring Data Graphs Locally","The second eigenvalue of the Laplacian matrix and its associated eigenvector
are fundamental features of an undirected graph, and as such they have found
widespread use in scientific computing, machine learning, and data analysis. In
many applications, however, graphs that arise have several \emph{local} regions
of interest, and the second eigenvector will typically fail to provide
information fine-tuned to each local region. In this paper, we introduce a
locally-biased analogue of the second eigenvector, and we demonstrate its
usefulness at highlighting local properties of data graphs in a semi-supervised
manner. To do so, we first view the second eigenvector as the solution to a
constrained optimization problem, and we incorporate the local information as
an additional constraint; we then characterize the optimal solution to this new
problem and show that it can be interpreted as a generalization of a
Personalized PageRank vector; and finally, as a consequence, we show that the
solution can be computed in nearly-linear time. In addition, we show that this
locally-biased vector can be used to compute an approximation to the best
partition \emph{near} an input seed set in a manner analogous to the way in
which the second eigenvector of the Laplacian can be used to obtain an
approximation to the best partition in the entire input graph. Such a primitive
is useful for identifying and refining clusters locally, as it allows us to
focus on a local region of interest in a semi-supervised manner. Finally, we
provide a detailed empirical evaluation of our method by showing how it can
applied to finding locally-biased sparse cuts around an input vertex seed set
in social and information networks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.09224v1,"Clandestino or Rifugiato? Anti-immigration Facebook Ad Targeting in
  Italy","Monitoring advertising around controversial issues is an important step in
ensuring accountability and transparency of political processes. To that end,
we use the Facebook Ads Library to collect 2312 migration-related advertising
campaigns in Italy over one year. Our pro- and anti-immigration classifier
(F1=0.85) reveals a partisan divide among the major Italian political parties,
with anti-immigration ads accounting for nearly 15M impressions. Although
composing 47.6% of all migration-related ads, anti-immigration ones receive
65.2% of impressions. We estimate that about two thirds of all captured
campaigns use some kind of demographic targeting by location, gender, or age.
We find sharp divides by age and gender: for instance, anti-immigration ads
from major parties are 17% more likely to be seen by a male user than a female.
Unlike pro-migration parties, we find that anti-immigration ones reach a
similar demographic to their own voters. However their audience change with
topic: an ad from anti-immigration parties is 24% more likely to be seen by a
male user when the ad speaks about migration, than if it does not. Furthermore,
the viewership of such campaigns tends to follow the volume of mainstream news
around immigration, supporting the theory that political advertisers try to
""ride the wave"" of current news. We conclude with policy implications for
political communication: since the Facebook Ads Library does not allow to
distinguish between advertisers intentions and algorithmic targeting, we argue
that more details should be shared by platforms regarding the targeting
configuration of socio-political campaigns.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1702.00829v1,Measuring Gender Inequalities of German Professions on Wikipedia,"Wikipedia is a community-created online encyclopedia; arguably, it is the
most popular and largest knowledge resource on the Internet. Thus, reliability
and neutrality are of high importance for Wikipedia. Previous research [3]
reveals gender bias in Google search results for many professions and
occupations. Also, Wikipedia was criticized for existing gender bias in
biographies [4] and gender gap in the editor community [5, 6]. Thus, one could
expect that gender bias related to professions and occupations may be present
in Wikipedia. The term gender bias is used here in the sense of conscious or
unconscious favoritism towards one gender over another [47] with respect to
professions and occupations. The objective of this work is to identify and
assess gender bias. To this end, the German Wikipedia articles about
professions and occupations were analyzed on three dimensions: redirections,
images, and people mentioned in the articles. This work provides evidence for
systematic overrepresentation of men in all three dimensions; female bias is
only present for a few professions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0612116v3,Adventures in time and space,"This paper investigates what is essentially a call-by-value version of PCF
under a complexity-theoretically motivated type system. The programming
formalism, ATR, has its first-order programs characterize the polynomial-time
computable functions, and its second-order programs characterize the type-2
basic feasible functionals of Mehlhorn and of Cook and Urquhart. (The ATR-types
are confined to levels 0, 1, and 2.) The type system comes in two parts, one
that primarily restricts the sizes of values of expressions and a second that
primarily restricts the time required to evaluate expressions. The
size-restricted part is motivated by Bellantoni and Cook's and Leivant's
implicit characterizations of polynomial-time. The time-restricting part is an
affine version of Barber and Plotkin's DILL. Two semantics are constructed for
ATR. The first is a pruning of the naive denotational semantics for ATR. This
pruning removes certain functions that cause otherwise feasible forms of
recursion to go wrong. The second semantics is a model for ATR's time
complexity relative to a certain abstract machine. This model provides a
setting for complexity recurrences arising from ATR recursions, the solutions
of which yield second-order polynomial time bounds. The time-complexity
semantics is also shown to be sound relative to the costs of interpretation on
the abstract machine.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1107.2482v2,Maximum Matchings via Glauber Dynamics,"In this paper we study the classic problem of computing a maximum cardinality
matching in general graphs $G = (V, E)$. The best known algorithm for this
problem till date runs in $O(m \sqrt{n})$ time due to Micali and Vazirani
\cite{MV80}. Even for general bipartite graphs this is the best known running
time (the algorithm of Karp and Hopcroft \cite{HK73} also achieves this bound).
For regular bipartite graphs one can achieve an $O(m)$ time algorithm which,
following a series of papers, has been recently improved to $O(n \log n)$ by
Goel, Kapralov and Khanna (STOC 2010) \cite{GKK10}. In this paper we present a
randomized algorithm based on the Markov Chain Monte Carlo paradigm which runs
in $O(m \log^2 n)$ time, thereby obtaining a significant improvement over
\cite{MV80}.
  We use a Markov chain similar to the \emph{hard-core model} for Glauber
Dynamics with \emph{fugacity} parameter $\lambda$, which is used to sample
independent sets in a graph from the Gibbs Distribution \cite{V99}, to design a
faster algorithm for finding maximum matchings in general graphs. Our result
crucially relies on the fact that the mixing time of our Markov Chain is
independent of $\lambda$, a significant deviation from the recent series of
works \cite{GGSVY11,MWW09, RSVVY10, S10, W06} which achieve computational
transition (for estimating the partition function) on a threshold value of
$\lambda$. As a result we are able to design a randomized algorithm which runs
in $O(m\log^2 n)$ time that provides a major improvement over the running time
of the algorithm due to Micali and Vazirani. Using the conductance bound, we
also prove that mixing takes $\Omega(\frac{m}{k})$ time where $k$ is the size
of the maximum matching.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0905.1300v1,Two-message quantum interactive proofs are in PSPACE,"We prove that QIP(2), the class of problems having two-message quantum
interactive proof systems, is a subset of PSPACE. This relationship is obtained
by means of an efficient parallel algorithm, based on the multiplicative
weights update method, for approximately solving a certain class of
semidefinite programs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1211.0886v1,Brain Computer Interface Technologies in the Coming Decades,"As the proliferation of technology dramatically infiltrates all aspects of
modern life, in many ways the world is becoming so dynamic and complex that
technological capabilities are overwhelming human capabilities to optimally
interact with and leverage those technologies. Fortunately, these technological
advancements have also driven an explosion of neuroscience research over the
past several decades, presenting engineers with a remarkable opportunity to
design and develop flexible and adaptive brain-based neurotechnologies that
integrate with and capitalize on human capabilities and limitations to improve
human-system interactions. Major forerunners of this conception are
brain-computer interfaces (BCIs), which to this point have been largely focused
on improving the quality of life for particular clinical populations and
include, for example, applications for advanced communications with paralyzed
or locked in patients as well as the direct control of prostheses and
wheelchairs. Near-term applications are envisioned that are primarily task
oriented and are targeted to avoid the most difficult obstacles to development.
In the farther term, a holistic approach to BCIs will enable a broad range of
task-oriented and opportunistic applications by leveraging pervasive
technologies and advanced analytical approaches to sense and merge critical
brain, behavioral, task, and environmental information. Communications and
other applications that are envisioned to be broadly impacted by BCIs are
highlighted; however, these represent just a small sample of the potential of
these technologies.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.12733v2,On the Expressive Power of Homomorphism Counts,"A classical result by Lov\'asz asserts that two graphs $G$ and $H$ are
isomorphic if and only if they have the same left profile, that is, for every
graph $F$, the number of homomorphisms from $F$ to $G$ coincides with the
number of homomorphisms from $F$ to $H$. Dvor{\'{a}}k and later on Dell, Grohe,
and Rattan showed that restrictions of the left profile to a class of graphs
can capture several different relaxations of isomorphism, including equivalence
in counting logics with a fixed number of variables (which contains fractional
isomorphism as a special case) and co-spectrality (i.e., two graphs having the
same characteristic polynomial). On the other side, a result by Chaudhuri and
Vardi asserts that isomorphism is also captured by the right profile, that is,
two graphs $G$ and $H$ are isomorphic if and only if for every graph $F$, the
number of homomorphisms from $G$ to $F$ coincides with the number of
homomorphisms from $H$ to $F$. In this paper, we embark on a study of the
restrictions of the right profile by investigating relaxations of isomorphism
that can or cannot be captured by restricting the right profile to a fixed
class of graphs. Our results unveil striking differences between the expressive
power of the left profile and the right profile. We show that fractional
isomorphism, equivalence in counting logics with a fixed number of variables,
and co-spectrality cannot be captured by restricting the right profile to a
class of graphs. In the opposite direction, we show that chromatic equivalence
cannot be captured by restricting the left profile to a class of graphs, while,
clearly, it can be captured by restricting the right profile to the class of
all cliques.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1906.00124v1,"Learning Patterns in Sample Distributions for Monte Carlo Variance
  Reduction","This paper investigates a novel a-posteriori variance reduction approach in
Monte Carlo image synthesis. Unlike most established methods based on lateral
filtering in the image space, our proposition is to produce the best possible
estimate for each pixel separately, from all the samples drawn for it. To
enable this, we systematically study the per-pixel sample distributions for
diverse scene configurations. Noting that these are too complex to be
characterized by standard statistical distributions (e.g. Gaussians), we
identify patterns recurring in them and exploit those for training a
variance-reduction model based on neural nets. In result, we obtain numerically
better estimates compared to simple averaging of samples. This method is
compatible with existing image-space denoising methods, as the improved
estimates of our model can be used for further processing. We conclude by
discussing how the proposed model could in future be extended for fully
progressive rendering with constant memory footprint and scene-sensitive
output.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1101.3594v2,"Classification under Data Contamination with Application to Remote
  Sensing Image Mis-registration","This work is motivated by the problem of image mis-registration in remote
sensing and we are interested in determining the resulting loss in the accuracy
of pattern classification. A statistical formulation is given where we propose
to use data contamination to model and understand the phenomenon of image
mis-registration. This model is widely applicable to many other types of errors
as well, for example, measurement errors and gross errors etc. The impact of
data contamination on classification is studied under a statistical learning
theoretical framework. A closed-form asymptotic bound is established for the
resulting loss in classification accuracy, which is less than
$\epsilon/(1-\epsilon)$ for data contamination of an amount of $\epsilon$. Our
bound is sharper than similar bounds in the domain adaptation literature and,
unlike such bounds, it applies to classifiers with an infinite
Vapnik-Chervonekis (VC) dimension. Extensive simulations have been conducted on
both synthetic and real datasets under various types of data contamination,
including label flipping, feature swapping and the replacement of feature
values with data generated from a random source such as a Gaussian or Cauchy
distribution. Our simulation results show that the bound we derive is fairly
tight.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0506057v2,About one 3-parameter Model of Testing,"This article offers a 3-parameter model of testing, with 1) the difference
between the ability level of the examinee and item difficulty; 2) the examinee
discrimination and 3) the item discrimination as model parameters.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/2001.02798v2,Self-guided Approximate Linear Programs,"Approximate linear programs (ALPs) are well-known models based on value
function approximations (VFAs) to obtain policies and lower bounds on the
optimal policy cost of discounted-cost Markov decision processes (MDPs).
Formulating an ALP requires (i) basis functions, the linear combination of
which defines the VFA, and (ii) a state-relevance distribution, which
determines the relative importance of different states in the ALP objective for
the purpose of minimizing VFA error. Both these choices are typically
heuristic: basis function selection relies on domain knowledge while the
state-relevance distribution is specified using the frequency of states visited
by a heuristic policy. We propose a self-guided sequence of ALPs that embeds
random basis functions obtained via inexpensive sampling and uses the known VFA
from the previous iteration to guide VFA computation in the current iteration.
Self-guided ALPs mitigate the need for domain knowledge during basis function
selection as well as the impact of the initial choice of the state-relevance
distribution, thus significantly reducing the ALP implementation burden. We
establish high probability error bounds on the VFAs from this sequence and show
that a worst-case measure of policy performance is improved. We find that these
favorable implementation and theoretical properties translate to encouraging
numerical results on perishable inventory control and options pricing
applications, where self-guided ALP policies improve upon policies from
problem-specific methods. More broadly, our research takes a meaningful step
toward application-agnostic policies and bounds for MDPs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1709.00194v5,"Improving Automated Symbolic Analysis for E-voting Protocols: A Method
  Based on Sufficient Conditions for Ballot Secrecy","We advance the state-of-the-art in automated symbolic analysis for e-voting
protocols by introducing three conditions that together are sufficient to
guarantee ballot secrecy. There are two main advantages to using our
conditions, compared to existing automated approaches. The first is a
substantial expansion of the class of protocols and threat models that can be
automatically analysed: we can systematically deal with (a) honest authorities
present in different phases, (b) threat models in which no dishonest voters
occur, and (c) protocols whose ballot secrecy depends on fresh data coming from
other phases. The second advantage is that it can significantly improve
verification efficiency, as the individual conditions are often simpler to
verify. E.g., for the LEE protocol, we obtain a speedup of over two orders of
magnitude. We show the scope and effectiveness of our approach using ProVerif
in several case studies, including FOO, LEE, JCJ, and Belenios. In these case
studies, our approach does not yield any false attacks, suggesting that our
conditions are tight.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.08588v1,"iWarded: A System for Benchmarking Datalog+/- Reasoning (technical
  report)","Recent years have seen increasing popularity of logic-based reasoning
systems, with research and industrial interest as well as many flourishing
applications in the area of Knowledge Graphs. Despite that, one can observe a
substantial lack of specific tools able to generate nontrivial reasoning
settings and benchmark scenarios. As a consequence, evaluating, analysing and
comparing reasoning systems is a complex task, especially when they embody
sophisticated optimizations and execution techniques that leverage the
theoretical underpinnings of the adopted logic fragment. In this paper, we aim
at filling this gap by introducing iWarded, a system that can generate very
large, complex, realistic reasoning settings to be used for the benchmarking of
logic-based reasoning systems adopting Datalog+/-, a family of extensions of
Datalog that has seen a resurgence in the last few years. In particular,
iWarded generates reasoning settings for Warded Datalog+/-, a language with a
very good tradeoff between computational complexity and expressive power. In
the paper, we present the iWarded system and a set of novel theoretical results
adopted to generate effective scenarios. As Datalog-based languages are of
general interest and see increasing adoption, we believe that iWarded is a step
forward in the empirical evaluation of current and future systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.12590v1,TraceSim: A Method for Calculating Stack Trace Similarity,"Many contemporary software products have subsystems for automatic crash
reporting. However, it is well-known that the same bug can produce slightly
different reports. To manage this problem, reports are usually grouped, often
manually by developers. Manual triaging, however, becomes infeasible for
products that have large userbases, which is the reason for many different
approaches to automating this task. Moreover, it is important to improve
quality of triaging due to the big volume of reports that needs to be processed
properly. Therefore, even a relatively small improvement could play a
significant role in overall accuracy of report bucketing. The majority of
existing studies use some kind of a stack trace similarity metric, either based
on information retrieval techniques or string matching methods. However, it
should be stressed that the quality of triaging is still insufficient. In this
paper, we describe TraceSim -- a novel approach to address this problem which
combines TF-IDF, Levenshtein distance, and machine learning to construct a
similarity metric. Our metric has been implemented inside an industrial-grade
report triaging system. The evaluation on a manually labeled dataset shows
significantly better results compared to baseline approaches.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.10529v1,White Paper Machine Learning in Certified Systems,"Machine Learning (ML) seems to be one of the most promising solution to
automate partially or completely some of the complex tasks currently realized
by humans, such as driving vehicles, recognizing voice, etc. It is also an
opportunity to implement and embed new capabilities out of the reach of
classical implementation techniques. However, ML techniques introduce new
potential risks. Therefore, they have only been applied in systems where their
benefits are considered worth the increase of risk. In practice, ML techniques
raise multiple challenges that could prevent their use in systems submitted to
certification constraints. But what are the actual challenges? Can they be
overcome by selecting appropriate ML techniques, or by adopting new engineering
or certification practices? These are some of the questions addressed by the ML
Certification 3 Workgroup (WG) set-up by the Institut de Recherche
Technologique Saint Exup\'ery de Toulouse (IRT), as part of the DEEL Project.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
http://arxiv.org/abs/1807.01966v2,The Cloud Technologies and Augmented Reality: the Prospects of Use,"The article discusses the prospects of the augmented reality using as a
component of a cloud-based environment. The research goals are the next: to
explore the possibility of the augmented reality using with the involvement of
the cloud-based environment components. The research objectives are the next:
to consider the notion of augmented reality; to analyze the experience the
augmented reality using within the cloud environment / system; to outline the
prospects of the augmented reality using in educational institutions; to
consider the technical conditions of the augmented reality use. The object of
research is: the educational process in educational institutions of Ukraine of
different levels of accreditation. The subject of research is: the educational
process in a cloud-based environment in educational institutions of Ukraine.
The research methods used are the next: analysis of scientific publications,
observations. The results of the research are the next: on the basis of the
analysis of scientific works, it has been established that the experience of
the augmented reality using in the systems based on cloud technologies already
exists. However, the success of such a combination has not yet been proven.
Currently, laboratory tests are known, while the experiment was not carried out
under natural conditions in control and experimental groups. It is revealed
that the attraction of the augmented reality for the educators requires the
development of new methodologies, didactic materials, updating and updating of
the curriculum. The main conclusions and recommendations: the main principles
of augmented reality use in the learning process are: designing of the
environment that is flexible enough, attention should be paid to the teaching
and didactic issues; adjusting the educational content for mastering the
material provided by the curriculum.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0
http://arxiv.org/abs/1701.03153v2,"Looking Beyond Appearances: Synthetic Training Data for Deep CNNs in
  Re-identification","Re-identification is generally carried out by encoding the appearance of a
subject in terms of outfit, suggesting scenarios where people do not change
their attire. In this paper we overcome this restriction, by proposing a
framework based on a deep convolutional neural network, SOMAnet, that
additionally models other discriminative aspects, namely, structural attributes
of the human figure (e.g. height, obesity, gender). Our method is unique in
many respects. First, SOMAnet is based on the Inception architecture, departing
from the usual siamese framework. This spares expensive data preparation
(pairing images across cameras) and allows the understanding of what the
network learned. Second, and most notably, the training data consists of a
synthetic 100K instance dataset, SOMAset, created by photorealistic human body
generation software. Synthetic data represents a good compromise between
realistic imagery, usually not required in re-identification since surveillance
cameras capture low-resolution silhouettes, and complete control of the
samples, which is useful in order to customize the data w.r.t. the surveillance
scenario at-hand, e.g. ethnicity. SOMAnet, trained on SOMAset and fine-tuned on
recent re-identification benchmarks, outperforms all competitors, matching
subjects even with different apparel. The combination of synthetic data with
Inception architectures opens up new research avenues in re-identification.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0503045v1,Contextual Constraint Modeling in Grid Application Workflows,"This paper introduces a new mechanism for specifying constraints in
distributed workflows. By introducing constraints in a contextual form, it is
shown how different people and groups within collaborative communities can
cooperatively constrain workflows. A comparison with existing state-of-the-art
workflow systems is made. These ideas are explored in practice with an
illustrative example from High Energy Physics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0201010v1,Bundling Equilibrium in Combinatorial auctions,"This paper analyzes individually-rational ex post equilibrium in the VC
(Vickrey-Clarke) combinatorial auctions. If $\Sigma$ is a family of bundles of
goods, the organizer may restrict the participants by requiring them to submit
their bids only for bundles in $\Sigma$. The $\Sigma$-VC combinatorial auctions
(multi-good auctions) obtained in this way are known to be
individually-rational truth-telling mechanisms. In contrast, this paper deals
with non-restricted VC auctions, in which the buyers restrict themselves to
bids on bundles in $\Sigma$, because it is rational for them to do so. That is,
it may be that when the buyers report their valuation of the bundles in
$\Sigma$, they are in an equilibrium. We fully characterize those $\Sigma$ that
induce individually rational equilibrium in every VC auction, and we refer to
the associated equilibrium as a bundling equilibrium. The number of bundles in
$\Sigma$ represents the communication complexity of the equilibrium. A special
case of bundling equilibrium is partition-based equilibrium, in which $\Sigma$
is a field, that is, it is generated by a partition. We analyze the tradeoff
between communication complexity and economic efficiency of bundling
equilibrium, focusing in particular on partition-based equilibrium.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1510.03023v1,Printed Perforated Lampshades for Continuous Projective Images,"We present a technique for designing 3D-printed perforated lampshades, which
project continuous grayscale images onto the surrounding walls. Given the
geometry of the lampshade and a target grayscale image, our method computes a
distribution of tiny holes over the shell, such that the combined footprints of
the light emanating through the holes form the target image on a nearby diffuse
surface. Our objective is to approximate the continuous tones and the spatial
detail of the target image, to the extent possible within the constraints of
the fabrication process.
  To ensure structural integrity, there are lower bounds on the thickness of
the shell, the radii of the holes, and the minimal distances between adjacent
holes. Thus, the holes are realized as thin tubes distributed over the
lampshade surface. The amount of light passing through a single tube may be
controlled by the tube's radius and by its direction (tilt angle). The core of
our technique thus consists of determining a suitable configuration of the
tubes: their distribution across the relevant portion of the lampshade, as well
as the parameters (radius, tilt angle) of each tube. This is achieved by
computing a capacity-constrained Voronoi tessellation over a suitably defined
density function, and embedding a tube inside the maximal inscribed circle of
each tessellation cell. The density function for a particular target image is
derived from a series of simulated images, each corresponding to a different
uniform density tube pattern on the lampshade.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1006.5440v1,Listing All Maximal Cliques in Sparse Graphs in Near-optimal Time,"The degeneracy of an $n$-vertex graph $G$ is the smallest number $d$ such
that every subgraph of $G$ contains a vertex of degree at most $d$. We show
that there exists a nearly-optimal fixed-parameter tractable algorithm for
enumerating all maximal cliques, parametrized by degeneracy. To achieve this
result, we modify the classic Bron--Kerbosch algorithm and show that it runs in
time $O(dn3^{d/3})$. We also provide matching upper and lower bounds showing
that the largest possible number of maximal cliques in an $n$-vertex graph with
degeneracy $d$ (when $d$ is a multiple of 3 and $n\ge d+3$) is $(n-d)3^{d/3}$.
Therefore, our algorithm matches the $\Theta(d(n-d)3^{d/3})$ worst-case output
size of the problem whenever $n-d=\Omega(n)$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1304.1876v3,"Proceedings of the 37th Annual Workshop of the Austrian Association for
  Pattern Recognition (√ñAGM/AAPR), 2013","This volume represents the proceedings of the 37th Annual Workshop of the
Austrian Association for Pattern Recognition (\""OAGM/AAPR), held May 23-24,
2013, in Innsbruck, Austria.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.08395v2,"Appraisal of data-driven and mechanistic emulators of nonlinear
  hydrodynamic urban drainage simulators","Many model based scientific and engineering methodologies, such as system
identification, sensitivity analysis, optimization and control, require a large
number of model evaluations. In particular, model based real-time control of
urban water infrastructures and online flood alarm systems require fast
prediction of the network response at different actuation and/or parameter
values. General purpose urban drainage simulators are too slow for this
application. Fast surrogate models, so-called emulators, provide a solution to
this efficiency demand. Emulators are attractive, because they sacrifice
unneeded accuracy in favor of speed. However, they have to be fine-tuned to
predict the system behavior satisfactorily. Also, some emulators fail to
extrapolate the system behavior beyond the training set. Although, there are
many strategies for developing emulators, up until now the selection of the
emulation strategy remains subjective. In this paper, we therefore compare the
performance of two families of emulators for open channel flows in the context
of urban drainage simulators. We compare emulators that explicitly use
knowledge of the simulator's equations, i.e. mechanistic emulators based on
Gaussian Processes, with purely data-driven emulators using matrix
factorization. Our results suggest that in many urban applications, naive
data-driven emulation outperforms mechanistic emulation. Nevertheless, we
discuss scenarios in which we think that mechanistic emulation might be
favorable for i) extrapolation in time and ii) dealing with sparse and unevenly
sampled data. We also provide many references to advances in the field of
Machine Learning that have not yet permeated into the Bayesian environmental
science community.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1805.02214v1,"Zero-shot Sequence Labeling: Transferring Knowledge from Sentences to
  Tokens","Can attention- or gradient-based visualization techniques be used to infer
token-level labels for binary sequence tagging problems, using networks trained
only on sentence-level labels? We construct a neural network architecture based
on soft attention, train it as a binary sentence classifier and evaluate
against token-level annotation on four different datasets. Inferring token
labels from a network provides a method for quantitatively evaluating what the
model is learning, along with generating useful feedback in assistance systems.
Our results indicate that attention-based methods are able to predict
token-level labels more accurately, compared to gradient-based methods,
sometimes even rivaling the supervised oracle network.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.05522v2,EduCOR: An Educational and Career-Oriented Recommendation Ontology,"With the increased dependence on online learning platforms and educational
resource repositories, a unified representation of digital learning resources
becomes essential to support a dynamic and multi-source learning experience. We
introduce the EduCOR ontology, an educational, career-oriented ontology that
provides a foundation for representing online learning resources for
personalised learning systems. The ontology is designed to enable learning
material repositories to offer learning path recommendations, which correspond
to the user's learning goals, academic and psychological parameters, and the
labour-market skills. We present the multiple patterns that compose the EduCOR
ontology, highlighting its cross-domain applicability and integrability with
other ontologies. A demonstration of the proposed ontology on the real-life
learning platform eDoer is discussed as a use-case. We evaluate the EduCOR
ontology using both gold standard and task-based approaches. The comparison of
EduCOR to three gold schemata, and its application in two use-cases, shows its
coverage and adaptability to multiple OER repositories, which allows generating
user-centric and labour-market oriented recommendations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1706.06087v1,"Aztec: A Platform to Render Biomedical Software Findable, Accessible,
  Interoperable, and Reusable","Precision medicine and health requires the characterization and phenotyping
of biological systems and patient datasets using a variety of data formats.
This scenario mandates the centralization of various tools and resources in a
unified platform to render them Findable, Accessible, Interoperable, and
Reusable (FAIR Principles). Leveraging these principles, Aztec provides the
scientific community with a new platform that promotes a long-term, sustainable
ecosystem of biomedical research software. Aztec is available at
https://aztec.bio and its source code is hosted at
https://github.com/BD2K-Aztec.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1408.5748v1,A Dual Model of Open Source License Growth,"Every open source project needs to decide on an open source license. This
decision is of high economic relevance: Just which license is the best one to
help the project grow and attract a community? The most common question is:
Should the project choose a restrictive (reciprocal) license or a more
permissive one? As an important step towards answering this question, this
paper analyses actual license choice and correlated project growth from ten
years of open source projects. It provides closed analytical models and finds
that around 2001 a reversal in license choice occurred from restrictive towards
permissive licenses.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1705.04709v1,Deep Learning Microscopy,"We demonstrate that a deep neural network can significantly improve optical
microscopy, enhancing its spatial resolution over a large field-of-view and
depth-of-field. After its training, the only input to this network is an image
acquired using a regular optical microscope, without any changes to its design.
We blindly tested this deep learning approach using various tissue samples that
are imaged with low-resolution and wide-field systems, where the network
rapidly outputs an image with remarkably better resolution, matching the
performance of higher numerical aperture lenses, also significantly surpassing
their limited field-of-view and depth-of-field. These results are
transformative for various fields that use microscopy tools, including e.g.,
life sciences, where optical microscopy is considered as one of the most widely
used and deployed techniques. Beyond such applications, our presented approach
is broadly applicable to other imaging modalities, also spanning different
parts of the electromagnetic spectrum, and can be used to design computational
imagers that get better and better as they continue to image specimen and
establish new transformations among different modes of imaging.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1210.0734v1,"Evaluation of linear classifiers on articles containing pharmacokinetic
  evidence of drug-drug interactions","Background. Drug-drug interaction (DDI) is a major cause of morbidity and
mortality. [...] Biomedical literature mining can aid DDI research by
extracting relevant DDI signals from either the published literature or large
clinical databases. However, though drug interaction is an ideal area for
translational research, the inclusion of literature mining methodologies in DDI
workflows is still very preliminary. One area that can benefit from literature
mining is the automatic identification of a large number of potential DDIs,
whose pharmacological mechanisms and clinical significance can then be studied
via in vitro pharmacology and in populo pharmaco-epidemiology. Experiments. We
implemented a set of classifiers for identifying published articles relevant to
experimental pharmacokinetic DDI evidence. These documents are important for
identifying causal mechanisms behind putative drug-drug interactions, an
important step in the extraction of large numbers of potential DDIs. We
evaluate performance of several linear classifiers on PubMed abstracts, under
different feature transformation and dimensionality reduction methods. In
addition, we investigate the performance benefits of including various
publicly-available named entity recognition features, as well as a set of
internally-developed pharmacokinetic dictionaries. Results. We found that
several classifiers performed well in distinguishing relevant and irrelevant
abstracts. We found that the combination of unigram and bigram textual features
gave better performance than unigram features alone, and also that
normalization transforms that adjusted for feature frequency and document
length improved classification. For some classifiers, such as linear
discriminant analysis (LDA), proper dimensionality reduction had a large impact
on performance. Finally, the inclusion of NER features and dictionaries was
found not to help classification.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1110.2400v1,"The CHRONIOUS Ontology-Driven Search Tool: Enabling Access to Focused
  and Up-to-Date Healthcare Literature","This paper presents an advanced search engine prototype for bibliography
retrieval developed within the CHRONIOUS European IP project of the seventh
Framework Program (FP7). This search engine is specifically targeted to
clinicians and healthcare practitioners searching for documents related to
Chronic Obstructive Pulmonary Disease (COPD) and Chronic Kidney Disease (CKD).
To this aim, the presented tool exploits two pathology-specific ontologies that
allow focused document indexing and retrieval. These ontologies have been
developed on the top of the Middle Layer Ontology for Clinical Care (MLOCC),
which provides a link with the Basic Formal Ontology, a foundational ontology
used in the Open Biological and Biomedical Ontologies (OBO) Foundry. In
addition link with the terms of the MeSH (Medical Subject Heading) thesaurus
has been provided to guarantee the coverage with the general certified medical
terms and multilingual capabilities.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.04684v3,"Trajectory-Constrained Deep Latent Visual Attention for Improved Local
  Planning in Presence of Heterogeneous Terrain","We present a reward-predictive, model-based deep learning method featuring
trajectory-constrained visual attention for local planning in visual navigation
tasks. Our method learns to place visual attention at locations in latent image
space which follow trajectories caused by vehicle control actions to enhance
predictive accuracy during planning. The attention model is jointly optimized
by the task-specific loss and an additional trajectory-constraint loss,
allowing adaptability yet encouraging a regularized structure for improved
generalization and reliability. Importantly, visual attention is applied in
latent feature map space instead of raw image space to promote efficient
planning. We validated our model in visual navigation tasks of planning low
turbulence, collision-free trajectories in off-road settings and hill climbing
with locking differentials in the presence of slippery terrain. Experiments
involved randomized procedural generated simulation and real-world
environments. We found our method improved generalization and learning
efficiency when compared to no-attention and self-attention alternatives.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1801.03162v2,"NP-Completeness and Inapproximability of the Virtual Network Embedding
  Problem and Its Variants","Many resource allocation problems in the cloud can be described as a basic
Virtual Network Embedding Problem (VNEP): the problem of finding a mapping of a
request graph (describing a workload) onto a substrate graph (describing the
physical infrastructure). Applications range from mapping testbeds (from where
the problem originated), over the embedding of batch-processing workloads
(virtual clusters) to the embedding of service function chains. The different
applications come with their own specific requirements and constraints,
including node mapping constraints, routing policies, and latency constraints.
While the VNEP has been studied intensively over the last years, complexity
results are only known for specific models and we lack a comprehensive
understanding of its hardness.
  This paper charts the complexity landscape of the VNEP by providing a
systematic analysis of the hardness of a wide range of VNEP variants, using a
unifying and rigorous proof framework. In particular, we show that the problem
of finding a feasible embedding is NP-complete in general, and, hence, the VNEP
cannot be approximated under any objective, unless P=NP holds. Importantly, we
derive NP-completeness results also for finding approximate embeddings, which
may violate, e.g., capacity constraints by certain factors. Lastly, we prove
that our results still pertain when restricting the request graphs to planar or
degree-bounded graphs.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0405043v2,"Prediction with Expert Advice by Following the Perturbed Leader for
  General Weights","When applying aggregating strategies to Prediction with Expert Advice, the
learning rate must be adaptively tuned. The natural choice of
sqrt(complexity/current loss) renders the analysis of Weighted Majority
derivatives quite complicated. In particular, for arbitrary weights there have
been no results proven so far. The analysis of the alternative ""Follow the
Perturbed Leader"" (FPL) algorithm from Kalai (2003} (based on Hannan's
algorithm) is easier. We derive loss bounds for adaptive learning rate and both
finite expert classes with uniform weights and countable expert classes with
arbitrary weights. For the former setup, our loss bounds match the best known
results so far, while for the latter our results are (to our knowledge) new.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0706.1642v1,On the growth of components with non fixed excesses,"Denote by an $l$-component a connected graph with $l$ edges more than
vertices. We prove that the expected number of creations of $(l+1)$-component,
by means of adding a new edge to an $l$-component in a randomly growing graph
with $n$ vertices, tends to 1 as $l,n$ tends to $\infty$ but with $l =
o(n^{1/4})$. We also show, under the same conditions on $l$ and $n$, that the
expected number of vertices that ever belong to an $l$-component is $\sim
(12l)^{1/3} n^{2/3}$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0401019v1,Using biased coins as oracles,"While it is well known that a Turing machine equipped with the ability to
flip a fair coin cannot compute more that a standard Turing machine, we show
that this is not true for a biased coin. Indeed, any oracle set $X$ may be
coded as a probability $p_{X}$ such that if a Turing machine is given a coin
which lands heads with probability $p_{X}$ it can compute any function
recursive in $X$ with arbitrarily high probability. We also show how the
assumption of a non-recursive bias can be weakened by using a sequence of
increasingly accurate recursive biases or by choosing the bias at random from a
distribution with a non-recursive mean. We conclude by briefly mentioning some
implications regarding the physical realisability of such methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1304.4471v4,"The Control Complexity of $r$-Approval: from the Single-Peaked Case to
  the General Case","We investigate the complexity of $r$-Approval control problems in $k$-peaked
elections, where at most $k$ peaks are allowed in each vote with respect to an
order of the candidates. We show that most NP-hardness results in general
elections also hold in k-peaked elections even for $k=2,3$. On the other hand,
we derive polynomial-time algorithms for some problems for $k=2$. All our
NP-hardness results apply to Approval and sincere-strategy preference-based
Approval as well. Our study leads to many dichotomy results for the problems
considered in this paper, with respect to the values of $k$ and $r$. In
addition, we study $r$-Approval control problems from the viewpoint of
parameterized complexity and achieve both fixed-parameter tractability results
and W-hardness results, with respect to the solution size. Along the way
exploring the complexity of control problems, we obtain two byproducts which
are of independent interest. First, we prove that every graph of maximum degree
3 admits a specific 2-interval representation where every 2-interval
corresponding to a vertex contains a trivial interval and, moreover,
2-intervals may only intersect at the endpoints of the intervals. Second, we
develop a fixed-parameter tractable algorithm for a generalized $r$-Set Packing
problem with respect to the solution size, where each element in the given
universal set is allowed to occur in more than one r-subset in the solution.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.05853v1,A Unified Analysis Approach for Hardware and Software Implementations,"Smart gadgets are being embedded almost in every aspect of our lives. From
smart cities to smart watches, modern industries are increasingly supporting
the Internet-of-Things (IoT). SysMART aims at making supermarkets smart,
productive, and with a touch of modern lifestyle. While similar implementations
to improve the shopping experience exists, they tend mainly to replace the
shopping activity at the store with online shopping. Although online shopping
reduces time and effort, it deprives customers from enjoying the experience.
SysMART relies on cutting-edge devices and technology to simplify and reduce
the time required during grocery shopping inside the supermarket. In addition,
the system monitors and maintains perishable products in good condition
suitable for human consumption. SysMART is built using state-of-the-art
technologies that support rapid prototyping and precision data acquisition. The
selected development environment is LabVIEW with its world-class interfacing
libraries. The paper comprises a detailed system description, development
strategy, interface design, software engineering, and a thorough analysis and
evaluation.",0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.06816v2,COVID-19 Smart Chatbot Prototype for Patient Monitoring,"Many COVID-19 patients developed prolonged symptoms after the infection,
including fatigue, delirium, and headache. The long-term health impact of these
conditions is still not clear. It is necessary to develop a way to follow up
with these patients for monitoring their health status to support timely
intervention and treatment. In the lack of sufficient human resources to follow
up with patients, we propose a novel smart chatbot solution backed with machine
learning to collect information (i.e., generating digital diary) in a
personalized manner. In this article, we describe the design framework and
components of our prototype.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.00139v1,A Swiss German Dictionary: Variation in Speech and Writing,"We introduce a dictionary containing forms of common words in various Swiss
German dialects normalized into High German. As Swiss German is, for now, a
predominantly spoken language, there is a significant variation in the written
forms, even between speakers of the same dialect. To alleviate the uncertainty
associated with this diversity, we complement the pairs of Swiss German - High
German words with the Swiss German phonetic transcriptions (SAMPA). This
dictionary becomes thus the first resource to combine large-scale spontaneous
translation with phonetic transcriptions. Moreover, we control for the regional
distribution and insure the equal representation of the major Swiss dialects.
The coupling of the phonetic and written Swiss German forms is powerful. We
show that they are sufficient to train a Transformer-based phoneme to grapheme
model that generates credible novel Swiss German writings. In addition, we show
that the inverse mapping - from graphemes to phonemes - can be modeled with a
transformer trained with the novel dictionary. This generation of
pronunciations for previously unknown words is key in training extensible
automated speech recognition (ASR) systems, which are key beneficiaries of this
dictionary.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1503.03148v1,A Neurodynamical System for finding a Minimal VC Dimension Classifier,"The recently proposed Minimal Complexity Machine (MCM) finds a hyperplane
classifier by minimizing an exact bound on the Vapnik-Chervonenkis (VC)
dimension. The VC dimension measures the capacity of a learning machine, and a
smaller VC dimension leads to improved generalization. On many benchmark
datasets, the MCM generalizes better than SVMs and uses far fewer support
vectors than the number used by SVMs. In this paper, we describe a neural
network based on a linear dynamical system, that converges to the MCM solution.
The proposed MCM dynamical system is conducive to an analogue circuit
implementation on a chip or simulation using Ordinary Differential Equation
(ODE) solvers. Numerical experiments on benchmark datasets from the UCI
repository show that the proposed approach is scalable and accurate, as we
obtain improved accuracies and fewer number of support vectors (upto 74.3%
reduction) with the MCM dynamical system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1904.09273v1,"""Why did you do that?"": Explaining black box models with Inductive
  Synthesis","By their nature, the composition of black box models is opaque. This makes
the ability to generate explanations for the response to stimuli challenging.
The importance of explaining black box models has become increasingly important
given the prevalence of AI and ML systems and the need to build legal and
regulatory frameworks around them. Such explanations can also increase trust in
these uncertain systems. In our paper we present RICE, a method for generating
explanations of the behaviour of black box models by (1) probing a model to
extract model output examples using sensitivity analysis; (2) applying
CNPInduce, a method for inductive logic program synthesis, to generate logic
programs based on critical input-output pairs; and (3) interpreting the target
program as a human-readable explanation. We demonstrate the application of our
method by generating explanations of an artificial neural network trained to
follow simple traffic rules in a hypothetical self-driving car simulation. We
conclude with a discussion on the scalability and usability of our approach and
its potential applications to explanation-critical scenarios.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1808.02686v1,An Improved Bound for Weak Epsilon-Nets in the Plane,"We show that for any finite set $P$ of points in the plane and $\epsilon>0$
there exist $\displaystyle O\left(\frac{1}{\epsilon^{3/2+\gamma}}\right)$
points in ${\mathbb{R}}^2$, for arbitrary small $\gamma>0$, that pierce every
convex set $K$ with $|K\cap P|\geq \epsilon |P|$. This is the first improvement
of the bound of $\displaystyle O\left(\frac{1}{\epsilon^2}\right)$ that was
obtained in 1992 by Alon, B\'{a}r\'{a}ny, F\""{u}redi and Kleitman for general
point sets in the plane.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0702053v1,The DFAs of Finitely Different Languages,"Two languages are ""finitely different"" if their symmetric difference is
finite. We consider the DFAs of finitely different regular languages and find
major structural similarities. We proceed to consider the smallest DFAs that
recognize a language finitely different from some given DFA. Such ""f-minimal""
DFAs are not unique, and this non-uniqueness is characterized. Finally, we
offer a solution to the minimization problem of finding such f-minimal DFAs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0306115v1,D0 Regional Analysis Center Concepts,"The D0 experiment is facing many exciting challenges providing a computing
environment for its worldwide collaboration. Transparent access to data for
processing and analysis has been enabled through deployment of its SAM system
to collaborating sites and additional functionality will be provided soon with
SAMGrid components. In order to maximize access to global storage,
computational and intellectual resources, and to enable the system to scale to
the large demands soon to be realized, several strategic sites have been
identified as Regional Analysis Centers (RAC's). These sites play an expanded
role within the system. The philosophy and function of these centers is
discussed and details of their composition and operation are outlined. The plan
for future additional centers is also addressed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.01626v1,"Performance Evaluation of a Substrate Integrated Waveguide Antenna for
  Vehicular Networks","This paper describes the design and evaluation of a Substrate Integrated
Waveguide (SIW) antenna operating at 2.4 GHz. This antenna was designed as a
possible solution for the implementation of the vehicular networks (VANETs).
The main advantages of the SIW antennas, such as their simplicity, small size
and low cost, make this kind of antennas particularly suitable for using in
wireless nodes where the size is a critical factor. For example, in Vehicular
Networks (VANETs) which is one of the most promising wireless communication
systems. We present in this paper the design of the SIW antenna using the
software ANSYS HFSS as well as the results of the assessment of some parameters
like the radiation pattern, operation frequency and bandwidth. Also, we present
the simulation of a VANET that include the designed parameters of the SIW
antenna in order to evaluate its integration into the vehicular nodes. This
simulation was performed in the NS2 simulator and some network performance
metrics, like packet delay and the packet loss rate, were evaluated. Other
additional software, for example MOVE and SUMO, were also used to generate the
routes and the movement of the vehicles",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9809024v2,A Lexicalized Tree Adjoining Grammar for English,"This document describes a sizable grammar of English written in the TAG
formalism and implemented for use with the XTAG system. This report and the
grammar described herein supersedes the TAG grammar described in an earlier
1995 XTAG technical report. The English grammar described in this report is
based on the TAG formalism which has been extended to include lexicalization,
and unification-based feature structures. The range of syntactic phenomena that
can be handled is large and includes auxiliaries (including inversion), copula,
raising and small clause constructions, topicalization, relative clauses,
infinitives, gerunds, passives, adjuncts, it-clefts, wh-clefts, PRO
constructions, noun-noun modifications, extraposition, determiner sequences,
genitives, negation, noun-verb contractions, sentential adjuncts and
imperatives. This technical report corresponds to the XTAG Release 8/31/98. The
XTAG grammar is continuously updated with the addition of new analyses and
modification of old ones, and an online version of this report can be found at
the XTAG web page at http://www.cis.upenn.edu/~xtag/",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.14740v1,"A Study On the Effects of Pre-processing On Spatio-temporal Action
  Recognition Using Spiking Neural Networks Trained with STDP","There has been an increasing interest in spiking neural networks in recent
years. SNNs are seen as hypothetical solutions for the bottlenecks of ANNs in
pattern recognition, such as energy efficiency. But current methods such as
ANN-to-SNN conversion and back-propagation do not take full advantage of these
networks, and unsupervised methods have not yet reached a success comparable to
advanced artificial neural networks. It is important to study the behavior of
SNNs trained with unsupervised learning methods such as spike-timing dependent
plasticity (STDP) on video classification tasks, including mechanisms to model
motion information using spikes, as this information is critical for video
understanding. This paper presents multiple methods of transposing temporal
information into a static format, and then transforming the visual information
into spikes using latency coding. These methods are paired with two types of
temporal fusion known as early and late fusion, and are used to help the
spiking neural network in capturing the spatio-temporal features from videos.
In this paper, we rely on the network architecture of a convolutional spiking
neural network trained with STDP, and we test the performance of this network
when challenged with action recognition tasks. Understanding how a spiking
neural network responds to different methods of movement extraction and
representation can help reduce the performance gap between SNNs and ANNs. In
this paper we show the effect of the similarity in the shape and speed of
certain actions on action recognition with spiking neural networks, we also
highlight the effectiveness of some methods compared to others.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.14244v1,Automated Database Indexing using Model-free Reinforcement Learning,"Configuring databases for efficient querying is a complex task, often carried
out by a database administrator. Solving the problem of building indexes that
truly optimize database access requires a substantial amount of database and
domain knowledge, the lack of which often results in wasted space and memory
for irrelevant indexes, possibly jeopardizing database performance for querying
and certainly degrading performance for updating. We develop an architecture to
solve the problem of automatically indexing a database by using reinforcement
learning to optimize queries by indexing data throughout the lifetime of a
database. In our experimental evaluation, our architecture shows superior
performance compared to related work on reinforcement learning and genetic
algorithms, maintaining near-optimal index configurations and efficiently
scaling to large databases.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1311.5799v2,Brzozowski type determinization for fuzzy automata,"In this paper we adapt the well-known Brzozowski determinization method to
fuzzy automata. This method gives better results than all previously known
methods for determinization of fuzzy automata developed by B\v{e}lohl\'avek
[Inform Sciences 143 (2002) 205--209], Li and Pedrycz [Fuzzy Set Syst 156
(2005) 68--92], Ignjatovi\'c et al. [Inform Sciences 178 (2008) 164--180], and
Jan\v{c}i\'c et al. [Inform Sciences 181 (2011) 1358--1368]. Namely, as in the
case of ordinary nondeterministic automata, Brzozowski type determinization of
a fuzzy automaton results in a minimal crisp-deterministic fuzzy automaton
equivalent to the starting fuzzy automaton, and we show that there are cases
when all previous methods result in infinite automata, while Brzozowski type
determinization results in a finite one. The paper deals with fuzzy automata
over complete residuated lattices, but identical results can also be obtained
in a more general context, for fuzzy automata over lattice-ordered monoids, and
even for weighted automata over commutative semirings.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0408038v1,The Dynamics of Group Codes: Dual Abelian Group Codes and Systems,"Fundamental results concerning the dynamics of abelian group codes
(behaviors) and their duals are developed. Duals of sequence spaces over
locally compact abelian groups may be defined via Pontryagin duality; dual
group codes are orthogonal subgroups of dual sequence spaces. The dual of a
complete code or system is finite, and the dual of a Laurent code or system is
(anti-)Laurent. If C and C^\perp are dual codes, then the state spaces of C act
as the character groups of the state spaces of C^\perp. The controllability
properties of C are the observability properties of C^\perp. In particular, C
is (strongly) controllable if and only if C^\perp is (strongly) observable, and
the controller memory of C is the observer memory of C^\perp. The controller
granules of C act as the character groups of the observer granules of C^\perp.
Examples of minimal observer-form encoder and syndrome-former constructions are
given. Finally, every observer granule of C is an ""end-around"" controller
granule of C.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0009019v1,Computing Presuppositions by Contextual Reasoning,"This paper describes how automated deduction methods for natural language
processing can be applied more efficiently by encoding context in a more
elaborate way. Our work is based on formal approaches to context, and we
provide a tableau calculus for contextual reasoning. This is explained by
considering an example from the problem area of presupposition projection.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.08621v1,A Knowledge Graph based Approach for Mobile Application Recommendation,"With the rapid prevalence of mobile devices and the dramatic proliferation of
mobile applications (apps), app recommendation becomes an emergent task that
would benefit both app users and stockholders. How to effectively organize and
make full use of rich side information of users and apps is a key challenge to
address the sparsity issue for traditional approaches. To meet this challenge,
we proposed a novel end-to-end Knowledge Graph Convolutional Embedding
Propagation Model (KGEP) for app recommendation. Specifically, we first
designed a knowledge graph construction method to model the user and app side
information, then adopted KG embedding techniques to capture the factual
triplet-focused semantics of the side information related to the first-order
structure of the KG, and finally proposed a relation-weighted convolutional
embedding propagation model to capture the recommendation-focused semantics
related to high-order structure of the KG. Extensive experiments conducted on a
real-world dataset validate the effectiveness of the proposed approach compared
to the state-of-the-art recommendation approaches.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1808.05676v3,"Why did the shape of your network change? (On detecting network
  anomalies via non-local curvatures)","$Anomaly$ $detection$ problems (also called $change$-$point$ $detection$
problems) have been studied in data mining, statistics and computer science
over the last several decades in applications such as medical condition
monitoring and weather change detection. In recent days, however, anomaly
detection problems have become increasing more relevant in the context of
$network$ $science$ since useful insights for many complex systems in biology,
finance and social science are often obtained by representing them via
networks. Notions of local and non-local curvatures of higher-dimensional
geometric shapes and topological spaces play a $fundamental$ role in physics
and mathematics in characterizing anomalous behaviours of these higher
dimensional entities. However, using curvature measures to detect anomalies in
networks is not yet very common. To this end, a main goal in this paper to
formulate and analyze curvature analysis methods to provide the foundations of
systematic approaches to find $critical$ $components$ and $detect$ $anomalies$
in networks. For this purpose, we use two measures of network curvatures which
depend on non-trivial global properties, such as distributions of geodesics and
higher-order correlations among nodes, of the given network. Based on these
measures, we precisely formulate several computational problems related to
anomaly detection in static or dynamic networks, and provide non-trivial
computational complexity results for these problems. This paper must $not$ be
viewed as delivering the final word on appropriateness and suitability of
specific curvature measures. Instead, it is our hope that this paper will
stimulate and motivate further theoretical or empirical research concerning the
exciting interplay between notions of curvatures from network and non-network
domains, a $much$ desired goal in our opinion.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0206017v1,The Prioritized Inductive Logic Programs,"The limit behavior of inductive logic programs has not been explored, but
when considering incremental or online inductive learning algorithms which
usually run ongoingly, such behavior of the programs should be taken into
account. An example is given to show that some inductive learning algorithm may
not be correct in the long run if the limit behavior is not considered. An
inductive logic program is convergent if given an increasing sequence of
example sets, the program produces a corresponding sequence of the Horn logic
programs which has the set-theoretic limit, and is limit-correct if the limit
of the produced sequence of the Horn logic programs is correct with respect to
the limit of the sequence of the example sets. It is shown that the GOLEM
system is not limit-correct. Finally, a limit-correct inductive logic system,
called the prioritized GOLEM system, is proposed as a solution.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0207024v1,On Concise Encodings of Preferred Extensions,"Much work on argument systems has focussed on preferred extensions which
define the maximal collectively defensible subsets. Identification and
enumeration of these subsets is (under the usual assumptions) computationally
demanding. We consider approaches to deciding if a subset S is a preferred
extension which query a representations encoding all such extensions, so that
the computational effort is invested once only (for the initial enumeration)
rather than for each separate query.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1607.04557v1,Local Search for Max-Sum Diversification,"We provide simple and fast polynomial time approximation schemes (PTASs) for
several variants of the max-sum diversification problem which, in its most
basic form, is as follows: Given n points p_1,...,p_n in R^d and an integer k,
select k points such that the average Euclidean distance between these points
is maximized. This problem commonly appears in information retrieval and
web-search in order to select a diverse set of points from the input. In this
context, it has recently received a lot of attention.
  We present new techniques to analyze natural local search algorithms. This
leads to a (1-O(1/k))-approximation for distances of negative type, even
subject to any matroid constraint of rank k, in time O(n k^2 log k), when
assuming that distance evaluations and calls to the independence oracle are
constant time. Negative type distances include as special cases Euclidean
distances and many further natural distances. Our result easily transforms into
a PTAS and improves on the only previously known PTAS for this setting, which
relies on convex optimization techniques in an n-dimensional space and is
impractical for large data sets. In contrast, our procedure has an (optimal)
linear dependence on n.
  Using generalized exchange properties of matroid intersection, we show that a
PTAS can be obtained for matroid intersection constraints as well. Moreover,
our techniques, being based on local search, are conceptually simple and allow
for various extensions. In particular, we get asymptotically optimal
O(1)-approximations when combining the classic dispersion function with a
monotone submodular objective, which is a very common class of functions to
measure diversity and relevance. This result leverages recent advances on local
search techniques based on proxy functions to obtain optimal approximations for
monotone submodular function maximization subject to a matroid constraint.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0905.1424v2,Concept Stability for Constructing Taxonomies of Web-site Users,"Owners of a web-site are often interested in analysis of groups of users of
their site. Information on these groups can help optimizing the structure and
contents of the site. In this paper we use an approach based on formal concepts
for constructing taxonomies of user groups. For decreasing the huge amount of
concepts that arise in applications, we employ stability index of a concept,
which describes how a group given by a concept extent differs from other such
groups. We analyze resulting taxonomies of user groups for three target
websites.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1707.03148v1,"An Opportunistic AODV Routing Scheme : A Cognitive Mobile Agents
  Approach","In Manets Dynamics and Robustness are the key features of the nodes and are
governed by several routing protocols such as AODV, DSR and so on. However in
the network the growing resource demand leads to resource scarcity. The Node
Mobility often leads to the link breakages and high routing overhead decreasing
the stability and reliability of the network connectivity. In this context, the
paper proposes a novel opportunistic AODV routing scheme which implements a
cognitive agent based intelligent technique to set up a stable connectivity
over the Manet. The Scheme computes the routing metric (rf) based on the
collaboration sensitivity levels of the nodes obtained based through the
knowledge based decision. This Routing Metric is subsequently used to set up
the stable path for network connectivity. Thus minimizes the route overhead and
increases the stability of the path. The Performance evaluation is conducted in
comparison with the AODV and sleep AODV routing protocol and validated.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.04013v5,"AI from concrete to abstract: demystifying artificial intelligence to
  the general public","Artificial Intelligence (AI) has been adopted in a wide range of domains.
This shows the imperative need to develop means to endow common people with a
minimum understanding of what AI means. Combining visual programming and WiSARD
weightless artificial neural networks, this article presents a new methodology,
AI from concrete to abstract (AIcon2abs), to enable general people (including
children) to achieve this goal. The main strategy adopted by is to promote a
demystification of artificial intelligence via practical activities related to
the development of learning machines, as well as through the observation of
their learning process. Thus, it is possible to provide subjects with skills
that contributes to making them insightful actors in debates and decisions
involving the adoption of artificial intelligence mechanisms. Currently,
existing approaches to the teaching of basic AI concepts through programming
treat machine intelligence as an external element/module. After being trained,
that external module is coupled to the main application being developed by the
learners. In the methodology herein presented, both training and classification
tasks are blocks that compose the main program, just as the other programming
constructs. As a beneficial side effect of AIcon2abs, the difference between a
program capable of learning from data and a conventional computer program
becomes more evident. In addition, the simplicity of the WiSARD weightless
artificial neural network model enables easy visualization and understanding of
training and classification tasks internal realization.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0
http://arxiv.org/abs/2003.11172v1,Holopix50k: A Large-Scale In-the-wild Stereo Image Dataset,"With the mass-market adoption of dual-camera mobile phones, leveraging stereo
information in computer vision has become increasingly important. Current
state-of-the-art methods utilize learning-based algorithms, where the amount
and quality of training samples heavily influence results. Existing stereo
image datasets are limited either in size or subject variety. Hence, algorithms
trained on such datasets do not generalize well to scenarios encountered in
mobile photography. We present Holopix50k, a novel in-the-wild stereo image
dataset, comprising 49,368 image pairs contributed by users of the Holopix
mobile social platform. In this work, we describe our data collection process
and statistically compare our dataset to other popular stereo datasets. We
experimentally show that using our dataset significantly improves results for
tasks such as stereo super-resolution and self-supervised monocular depth
estimation. Finally, we showcase practical applications of our dataset to
motivate novel works and use cases. The Holopix50k dataset is available at
http://github.com/leiainc/holopix50k",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.02727v1,Detecting Singleton Review Spammers Using Semantic Similarity,"Online reviews have increasingly become a very important resource for
consumers when making purchases. Though it is becoming more and more difficult
for people to make well-informed buying decisions without being deceived by
fake reviews. Prior works on the opinion spam problem mostly considered
classifying fake reviews using behavioral user patterns. They focused on
prolific users who write more than a couple of reviews, discarding one-time
reviewers. The number of singleton reviewers however is expected to be high for
many review websites. While behavioral patterns are effective when dealing with
elite users, for one-time reviewers, the review text needs to be exploited. In
this paper we tackle the problem of detecting fake reviews written by the same
person using multiple names, posting each review under a different name. We
propose two methods to detect similar reviews and show the results generally
outperform the vectorial similarity measures used in prior works. The first
method extends the semantic similarity between words to the reviews level. The
second method is based on topic modeling and exploits the similarity of the
reviews topic distributions using two models: bag-of-words and
bag-of-opinion-phrases. The experiments were conducted on reviews from three
different datasets: Yelp (57K reviews), Trustpilot (9K reviews) and Ott dataset
(800 reviews).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1302.2738v2,"RandFile package for Mathematica for accessing file-based sources of
  randomness","We present a package for Mathematica computer algebra system which allows the
exploitation of local files as sources of random data. We provide the
description of the package and illustrate its usage by showing some examples.
We also compare the provided functionality with alternative sources of
randomness, namely a built-in pseudo-random generator and the package for
accessing hardware true random number generators.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1202.4061v2,Implementation of a Unimodularity Test,"This paper describes implementation and computational results of a polynomial
test of total unimodularity. The test is a simplified version of a prior
method. The program also decides two related unimodularity properties. The
software is available free of charge in source code form under the Boost
Software License.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1607.05485v1,"Exact Maximum Entropy Inverse Optimal Control for Modelling Human
  Attention Switching and Control","Maximum Causal Entropy (MCE) Inverse Optimal Control (IOC) has become an
effective tool for modelling human behaviour in many control tasks. Its
advantage over classic techniques for estimating human policies is the
transferability of the inferred objectives: Behaviour can be predicted in
variations of the control task by policy computation using a relaxed optimality
criterion. However, exact policy inference is often computationally intractable
in control problems with imperfect state observation. In this work, we present
a model class that allows modelling human control of two tasks of which only
one be perfectly observed at a time requiring attention switching. We show how
efficient and exact objective and policy inference via MCE can be conducted for
these control problems. Both MCE-IOC and Maximum Causal Likelihood (MCL)-IOC, a
variant of the original MCE approach, as well as Direct Policy Estimation (DPE)
are evaluated using simulated and real behavioural data. Prediction error and
generalization over changes in the control process are both considered in the
evaluation. The results show a clear advantage of both IOC methods over DPE,
especially in the transfer over variation of the control process. MCE and MCL
performed similar when training on a large set of simulated data, but differed
significantly on small sets and real data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1602.06994v1,Spatio-Temporal Analysis of Team Sports -- A Survey,"Team-based invasion sports such as football, basketball and hockey are
similar in the sense that the players are able to move freely around the
playing area; and that player and team performance cannot be fully analysed
without considering the movements and interactions of all players as a group.
State of the art object tracking systems now produce spatio-temporal traces of
player trajectories with high definition and high frequency, and this, in turn,
has facilitated a variety of research efforts, across many disciplines, to
extract insight from the trajectories. We survey recent research efforts that
use spatio-temporal data from team sports as input, and involve non-trivial
computation. This article categorises the research efforts in a coherent
framework and identifies a number of open research questions.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1103.6067v2,Short proofs of the Quantum Substate Theorem,"The Quantum Substate Theorem due to Jain, Radhakrishnan, and Sen (2002) gives
us a powerful operational interpretation of relative entropy, in fact, of the
observational divergence of two quantum states, a quantity that is related to
their relative entropy. Informally, the theorem states that if the
observational divergence between two quantum states rho, sigma is small, then
there is a quantum state rho' close to rho in trace distance, such that rho'
when scaled down by a small factor becomes a substate of sigma. We present new
proofs of this theorem. The resulting statement is optimal up to a constant
factor in its dependence on observational divergence. In addition, the proofs
are both conceptually simpler and significantly shorter than the earlier proof.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1107.1201v1,Real-Reward Testing for Probabilistic Processes (Extended Abstract),"We introduce a notion of real-valued reward testing for probabilistic
processes by extending the traditional nonnegative-reward testing with negative
rewards. In this richer testing framework, the may and must preorders turn out
to be inverses. We show that for convergent processes with finitely many states
and transitions, but not in the presence of divergence, the real-reward
must-testing preorder coincides with the nonnegative-reward must-testing
preorder. To prove this coincidence we characterise the usual resolution-based
testing in terms of the weak transitions of processes, without having to
involve policies, adversaries, schedulers, resolutions, or similar structures
that are external to the process under investigation. This requires
establishing the continuity of our function for calculating testing outcomes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1407.1063v1,"CBM-Of-TRaCE: An Ontology-Driven Framework for the Improvement of
  Business Service Traceability, Consistency Management and Reusability","In this paper, we represent CBM-Of-TRaCE which is an ontological framework
that integrates two aspects of business components: conceptual and methodology.
In the development of our framework we have taken IBM Actionable Business
Approach (ABA) in to consideration. We evaluate our framework through some
aspects such as support and facilitation for a business from five different
aspects: service-orientation, business process, management integration,
reusability improvement, consistency rules, and traceability. As well, we
demonstrate the compatibility of our CBM-Of-TRaCE with ABA four phases.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1503.08476v1,Guided Grammar Convergence,"Relating formal grammars is a hard problem that balances between language
equivalence (which is known to be undecidable) and grammar identity (which is
trivial). In this paper, we investigate several milestones between those two
extremes and propose a methodology for inconsistency management in grammar
engineering. While conventional grammar convergence is a practical approach
relying on human experts to encode differences as transformation steps, guided
grammar convergence is a more narrowly applicable technique that infers such
transformation steps automatically by normalising the grammars and establishing
a structural equivalence relation between them. This allows us to perform a
case study with automatically inferring bidirectional transformations between
11 grammars (in a broad sense) of the same artificial functional language:
parser specifications with different combinator libraries, definite clause
grammars, concrete syntax definitions, algebraic data types, metamodels, XML
schemata, object models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1403.0745v1,"EnsembleSVM: A Library for Ensemble Learning Using Support Vector
  Machines","EnsembleSVM is a free software package containing efficient routines to
perform ensemble learning with support vector machine (SVM) base models. It
currently offers ensemble methods based on binary SVM models. Our
implementation avoids duplicate storage and evaluation of support vectors which
are shared between constituent models. Experimental results show that using
ensemble approaches can drastically reduce training complexity while
maintaining high predictive accuracy. The EnsembleSVM software package is
freely available online at http://esat.kuleuven.be/stadius/ensemblesvm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.09640v2,Mechanism Design for Facility Location Games with Candidate Locations,"We study the facility location games with candidate locations from a
mechanism design perspective. Suppose there are n agents located in a metric
space whose locations are their private information, and a group of candidate
locations for building facilities. The authority plans to build some
homogeneous facilities among these candidates to serve the agents, who bears a
cost equal to the distance to the closest facility. The goal is to design
mechanisms for minimizing the total/maximum cost among the agents. For the
single-facility problem under the maximum-cost objective, we give a
deterministic 3-approximation group strategy-proof mechanism, and prove that no
deterministic (or randomized) strategy-proof mechanism can have an
approximation ratio better than 3 (or 2). For the two-facility problem on a
line, we give an anonymous deterministic group strategy-proof mechanism that is
(2n-3)-approximation for the total-cost objective, and 3-approximation for the
maximum-cost objective. We also provide (asymptotically) tight lower bounds on
the approximation ratio.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.4361v2,You Tweet What You Eat: Studying Food Consumption Through Twitter,"Food is an integral part of our lives, cultures, and well-being, and is of
major interest to public health. The collection of daily nutritional data
involves keeping detailed diaries or periodic surveys and is limited in scope
and reach. Alternatively, social media is infamous for allowing its users to
update the world on the minutiae of their daily lives, including their eating
habits. In this work we examine the potential of Twitter to provide insight
into US-wide dietary choices by linking the tweeted dining experiences of 210K
users to their interests, demographics, and social networks. We validate our
approach by relating the caloric values of the foods mentioned in the tweets to
the state-wide obesity rates, achieving a Pearson correlation of 0.77 across
the 50 US states and the District of Columbia. We then build a model to predict
county-wide obesity and diabetes statistics based on a combination of
demographic variables and food names mentioned on Twitter. Our results show
significant improvement over previous CHI research (Culotta'14). We further
link this data to societal and economic factors, such as education and income,
illustrating that, for example, areas with higher education levels tweet about
food that is significantly less caloric. Finally, we address the somewhat
controversial issue of the social nature of obesity (first raised by Christakis
& Fowler in 2007) by inducing two social networks using mentions and reciprocal
following relationships.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1508.06171v1,"BREN: Body Reflection Essence-Neuter Model for Separation of Reflection
  Components","We propose a novel reflection color model consisting of body essence and
(mixed) neuter, and present an effective method for separating dichromatic
reflection components using a single image. Body essence is an entity invariant
to interface reflection, and has two degrees of freedom unlike hue and maximum
chromaticity. As a result, the proposed method is insensitive to noise and
proper for colors around CMY (cyan, magenta, and yellow) as well as RGB (red,
green, and blue), contrary to the maximum chromaticity-based methods. Interface
reflection is separated by using a Gaussian function, which removes a critical
thresholding problem. Furthermore, the method does not require any region
segmentation. Experimental results show the efficacy of the proposed model and
method.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.15037v2,"StreamTable: An Area Proportional Visualization for Tables with Flowing
  Streams","Let $M$ be a two-dimensional table with each cell weighted by a nonzero
positive number. A StreamTable visualization of $M$ represents the columns as
non-overlapping vertical streams and the rows as horizontal stripes such that
the intersection between a stream and a stripe is a rectangle with area equal
to the weight of the corresponding cell. To avoid large wiggle of the streams,
it is desirable to keep the consecutive cells in a stream to be adjacent. Let
$B$ be the smallest axis-aligned bounding box containing the StreamTable. Then
the difference between the area of $B$ and the sum of the weights is referred
to as the excess area.
  We attempt to optimize various StreamTable aesthetics (e.g., minimizing
excess area, or maximizing cell adjacencies in streams). (A) If the row
permutation is fixed and the row heights are given, then we give an
$O(rc)$-time algorithm to optimize these aesthetics, where $r$ and $c$ are the
number of rows and columns, respectively. (B) If the row permutation is fixed
but the row heights can be chosen, then we discuss a technique to compute an
aesthetic (but not necessarily optimal) StreamTable by solving a
quadratically-constrained quadratic program, followed by iterative
improvements. If the row heights are restricted to be integers, then we prove
the problem to be NP-hard. (C) If the row permutations can be chosen, then we
show that it is NP-hard to find a row permutation that optimizes the area or
adjacency aesthetics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0212023v1,How to Shift Bias: Lessons from the Baldwin Effect,"An inductive learning algorithm takes a set of data as input and generates a
hypothesis as output. A set of data is typically consistent with an infinite
number of hypotheses; therefore, there must be factors other than the data that
determine the output of the learning algorithm. In machine learning, these
other factors are called the bias of the learner. Classical learning algorithms
have a fixed bias, implicit in their design. Recently developed learning
algorithms dynamically adjust their bias as they search for a hypothesis.
Algorithms that shift bias in this manner are not as well understood as
classical algorithms. In this paper, we show that the Baldwin effect has
implications for the design and analysis of bias shifting algorithms. The
Baldwin effect was proposed in 1896, to explain how phenomena that might appear
to require Lamarckian evolution (inheritance of acquired characteristics) can
arise from purely Darwinian evolution. Hinton and Nowlan presented a
computational model of the Baldwin effect in 1987. We explore a variation on
their model, which we constructed explicitly to illustrate the lessons that the
Baldwin effect has for research in bias shifting algorithms. The main lesson is
that it appears that a good strategy for shift of bias in a learning algorithm
is to begin with a weak bias and gradually shift to a strong bias.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.01023v1,On the Combinatorial Power of the Weisfeiler-Lehman Algorithm,"The classical Weisfeiler-Lehman method WL[2] uses edge colors to produce a
powerful graph invariant. It is at least as powerful in its ability to
distinguish non-isomorphic graphs as the most prominent algebraic graph
invariants. It determines not only the spectrum of a graph, and the angles
between standard basis vectors and the eigenspaces, but even the angles between
projections of standard basis vectors into the eigenspaces. Here, we
investigate the combinatorial power of WL[2]. For sufficiently large k, WL[k]
determines all combinatorial properties of a graph. Many traditionally used
combinatorial invariants are determined by WL[k] for small k. We focus on two
fundamental invariants, the num- ber of cycles Cp of length p, and the number
of cliques Kp of size p. We show that WL[2] determines the number of cycles of
lengths up to 6, but not those of length 8. Also, WL[2] does not determine the
number of 4-cliques.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1408.5999v3,"A New Non-MDS Hash Function Resisting Birthday Attack and
  Meet-in-the-middle Attack","To examine the integrity and authenticity of an IP address efficiently and
economically, this paper proposes a new non-Merkle-Damgard structural (non-MDS)
hash function called JUNA that is based on a multivariate permutation problem
and an anomalous subset product problem to which no subexponential time
solutions are found so far. JUNA includes an initialization algorithm and a
compression algorithm, and converts a short message of n bits which is regarded
as only one block into a digest of m bits, where 80 <= m <= 232 and 80 <= m <=
n <= 4096. The analysis and proof show that the new hash is one-way, weakly
collision-free, and strongly collision-free, and its security against existent
attacks such as birthday attack and meet-in-the- middle attack is to O(2 ^ m).
Moreover, a detailed proof that the new hash function is resistant to the
birthday attack is given. Compared with the Chaum-Heijst-Pfitzmann hash based
on a discrete logarithm problem, the new hash is lightweight, and thus it opens
a door to convenience for utilization of lightweight digital signing schemes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.13265v1,"Robot Navigation in a Crowd by Integrating Deep Reinforcement Learning
  and Online Planning","It is still an open and challenging problem for mobile robots navigating
along time-efficient and collision-free paths in a crowd. The main challenge
comes from the complex and sophisticated interaction mechanism, which requires
the robot to understand the crowd and perform proactive and foresighted
behaviors. Deep reinforcement learning is a promising solution to this problem.
However, most previous learning methods incur a tremendous computational
burden. To address these problems, we propose a graph-based deep reinforcement
learning method, SG-DQN, that (i) introduces a social attention mechanism to
extract an efficient graph representation for the crowd-robot state; (ii)
directly evaluates the coarse q-values of the raw state with a learned dueling
deep Q network(DQN); and then (iii) refines the coarse q-values via online
planning on possible future trajectories. The experimental results indicate
that our model can help the robot better understand the crowd and achieve a
high success rate of more than 0.99 in the crowd navigation task. Compared
against previous state-of-the-art algorithms, our algorithm achieves an
equivalent, if not better, performance while requiring less than half of the
computational cost.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0701183v1,"Certification of the QR factor R, and of lattice basis reducedness","Given a lattice basis of n vectors in Z^n, we propose an algorithm using
12n^3+O(n^2) floating point operations for checking whether the basis is
LLL-reduced. If the basis is reduced then the algorithm will hopefully answer
''yes''. If the basis is not reduced, or if the precision used is not
sufficient with respect to n, and to the numerical properties of the basis, the
algorithm will answer ''failed''. Hence a positive answer is a rigorous
certificate. For implementing the certificate itself, we propose a floating
point algorithm for computing (certified) error bounds for the entries of the R
factor of the QR matrix factorization. This algorithm takes into account all
possible approximation and rounding errors. The cost 12n^3+O(n^2) of the
certificate is only six times more than the cost of numerical algorithms for
computing the QR factorization itself, and the certificate may be implemented
using matrix library routines only. We report experiments that show that for a
reduced basis of adequate dimension and quality the certificate succeeds, and
establish the effectiveness of the certificate. This effectiveness is applied
for certifying the output of fastest existing floating point heuristics of LLL
reduction, without slowing down the whole process.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.15968v1,The Impact of Disinformation on a Controversial Debate on Social Media,"In this work we study how pervasive is the presence of disinformation in the
Italian debate around immigration on Twitter and the role of automated accounts
in the diffusion of such content. By characterising the Twitter users with an
\textit{Untrustworthiness} score, that tells us how frequently they engage with
disinformation content, we are able to see that such bad information
consumption habits are not equally distributed across the users; adopting a
network analysis approach, we can identify communities characterised by a very
high presence of users that frequently share content from unreliable news
sources. Within this context, social bots tend to inject in the network more
malicious content, that often remains confined in a limited number of clusters;
instead, they target reliable content in order to diversify their reach. The
evidence we gather suggests that, at least in this particular case study, there
is a strong interplay between social bots and users engaging with unreliable
content, influencing the diffusion of the latter across the network.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/0905.1045v2,Descriptional complexity of bounded context-free languages,"Finite-turn pushdown automata (PDA) are investigated concerning their
descriptional complexity. It is known that they accept exactly the class of
ultralinear context-free languages. Furthermore, the increase in size when
converting arbitrary PDAs accepting ultralinear languages to finite-turn PDAs
cannot be bounded by any recursive function. The latter phenomenon is known as
non-recursive trade-off. In this paper, finite-turn PDAs accepting bounded
languages are considered. First, letter-bounded languages are studied. We prove
that in this case the non-recursive trade-off is reduced to a recursive
trade-off, more precisely, to an exponential trade-off. A conversion algorithm
is presented and the optimality of the construction is shown by proving tight
lower bounds. Furthermore, the question of reducing the number of turns of a
given finite-turn PDA is studied. Again, a conversion algorithm is provided
which shows that in this case the trade-off is at most polynomial. Finally, the
more general case of word-bounded languages is investigated. We show how the
results obtained for letter-bounded languages can be extended to word-bounded
languages.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1003.3880v2,"Lessons from the Failure and Subsequent Success of a Complex Healthcare
  Sector IT Project","This paper argues that IT failures diagnosed as errors at the technical or
project management level are often mistakenly pointing to symptoms of failure
rather than a project's underlying socio-complexity (complexity resulting from
the interactions of people and groups) which is usually the actual source of
failure. We propose a novel method, Stakeholder Impact Analysis, that can be
used to identify risks associated with socio-complexity as it is grounded in
insights from the social sciences, psychology and management science. This
paper demonstrates the effectiveness of Stakeholder Impact Analysis by using
the 1992 London Ambulance Service Computer Aided Dispatch project as a case
study, and shows that had our method been used to identify the risks and had
they been mitigated, it would have reduced the risk of project failure. This
paper's original contribution comprises expanding upon existing accounts of
failure by examining failures at a level of granularity not seen elsewhere that
enables the underlying socio-complexity sources of risk to be identified.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0
http://arxiv.org/abs/1409.5366v1,Quality of Service in Network Creation Games,"Network creation games model the creation and usage costs of networks formed
by n selfish nodes. Each node v can buy a set of edges, each for a fixed price
\alpha > 0. Its goal is to minimize its private costs, i.e., the sum (SUM-game,
Fabrikant et al., PODC 2003) or maximum (MAX-game, Demaine et al., PODC 2007)
of distances from $v$ to all other nodes plus the prices of the bought edges.
The above papers show the existence of Nash equilibria as well as upper and
lower bounds for the prices of anarchy and stability. In several subsequent
papers, these bounds were improved for a wide range of prices \alpha. In this
paper, we extend these models by incorporating quality-of-service aspects: Each
edge cannot only be bought at a fixed quality (edge length one) for a fixed
price \alpha. Instead, we assume that quality levels (i.e., edge lengths) are
varying in a fixed interval [\beta,B], 0 < \beta <= B. A node now cannot only
choose which edge to buy, but can also choose its quality x, for the price
p(x), for a given price function p. For both games and all price functions, we
show that Nash equilibria exist and that the price of stability is either
constant or depends only on the interval size of available edge lengths. Our
main results are bounds for the price of anarchy. In case of the SUM-game, we
show that they are tight if price functions decrease sufficiently fast.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0712.1205v2,Lambda-RBAC: Programming with Role-Based Access Control,"We study mechanisms that permit program components to express role
constraints on clients, focusing on programmatic security mechanisms, which
permit access controls to be expressed, in situ, as part of the code realizing
basic functionality. In this setting, two questions immediately arise: (1) The
user of a component faces the issue of safety: is a particular role sufficient
to use the component? (2) The component designer faces the dual issue of
protection: is a particular role demanded in all execution paths of the
component? We provide a formal calculus and static analysis to answer both
questions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1512.07783v1,"Hardware Architecture for Large Parallel Array of Random Feature
  Extractors applied to Image Recognition","We demonstrate a low-power and compact hardware implementation of Random
Feature Extractor (RFE) core. With complex tasks like Image Recognition
requiring a large set of features, we show how weight reuse technique can allow
to virtually expand the random features available from RFE core. Further, we
show how to avoid computation cost wasted for propagating ""incognizant"" or
redundant random features. For proof of concept, we validated our approach by
using our RFE core as the first stage of Extreme Learning Machine (ELM)--a two
layer neural network--and were able to achieve $>97\%$ accuracy on MNIST
database of handwritten digits. ELM's first stage of RFE is done on an analog
ASIC occupying $5$mm$\times5$mm area in $0.35\mu$m CMOS and consuming $5.95$
$\mu$J/classify while using $\approx 5000$ effective hidden neurons. The ELM
second stage consisting of just adders can be implemented as digital circuit
with estimated power consumption of $20.9$ nJ/classify. With a total energy
consumption of only $5.97$ $\mu$J/classify, this low-power mixed signal ASIC
can act as a co-processor in portable electronic gadgets with cameras.",0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0405036v1,Single-Strip Triangulation of Manifolds with Arbitrary Topology,"Triangle strips have been widely used for efficient rendering. It is
NP-complete to test whether a given triangulated model can be represented as a
single triangle strip, so many heuristics have been proposed to partition
models into few long strips. In this paper, we present a new algorithm for
creating a single triangle loop or strip from a triangulated model. Our method
applies a dual graph matching algorithm to partition the mesh into cycles, and
then merges pairs of cycles by splitting adjacent triangles when necessary. New
vertices are introduced at midpoints of edges and the new triangles thus formed
are coplanar with their parent triangles, hence the visual fidelity of the
geometry is not changed. We prove that the increase in the number of triangles
due to this splitting is 50% in the worst case, however for all models we
tested the increase was less than 2%. We also prove tight bounds on the number
of triangles needed for a single-strip representation of a model with holes on
its boundary. Our strips can be used not only for efficient rendering, but also
for other applications including the generation of space filling curves on a
manifold of any arbitrary topology.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1012.3656v1,"Adaptive Cluster Expansion (ACE): A Multilayer Network for Estimating
  Probability Density Functions","We derive an adaptive hierarchical method of estimating high dimensional
probability density functions. We call this method of density estimation the
""adaptive cluster expansion"" or ACE for short. We present an application of
this approach, based on a multilayer topographic mapping network, that
adaptively estimates the joint probability density function of the pixel values
of an image, and presents this result as a ""probability image"". We apply this
to the problem of identifying statistically anomalous regions in otherwise
statistically homogeneous images.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0412097v1,The Computational Power of Benenson Automata,"The development of autonomous molecular computers capable of making
independent decisions in vivo regarding local drug administration may
revolutionize medical science. Recently Benenson at el (2004) have envisioned
one form such a ``smart drug'' may take by implementing an in vitro scheme, in
which a long DNA state molecule is cut repeatedly by a restriction enzyme in a
manner dependent upon the presence of particular short DNA ``rule molecules.''
To analyze the potential of their scheme in terms of the kinds of computations
it can perform, we study an abstraction assuming that a certain class of
restriction enzymes is available and reactions occur without error. We also
discuss how our molecular algorithms could perform with known restriction
enzymes. By exhibiting a way to simulate arbitrary circuits, we show that these
``Benenson automata'' are capable of computing arbitrary Boolean functions.
Further, we show that they are able to compute efficiently exactly those
functions computable by log-depth circuits. Computationally, we formalize a new
variant of limited width branching programs with a molecular implementation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1205.3831v1,"Enabling Realistic Cross-Layer Analysis based on Satellite Physical
  Layer Traces","We present a solution to evaluate the performance of transport protocols as a
function of link layer reliability schemes (i.e. ARQ, FEC and Hybrid ARQ)
applied to satellite physical layer traces. As modelling such traces is complex
and may require approximations, the use of real traces will minimise the
potential for erroneous performance evaluations resulting from imperfect
models. Our Trace Manager Tool (TMT) produces the corresponding link layer
output, which is then used within the ns-2 network simulator via the
additionally developed ns-2 interface module. We first present the analytical
models for the link layer with bursty erasure packets and for the link layer
reliability mechanisms with bursty erasures. Then, we present details of the
TMT tool and our validation methodology, demonstrating that the selected
performance metrics (recovery delay and throughput efficiency) exhibit a good
match between the theoretical results and those obtained with TMT. Finally, we
present results showing the impact of different link layer reliability
mechanisms on the performance of TCP Cubic transport layer protocol.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0104010v1,Type Arithmetics: Computation based on the theory of types,"The present paper shows meta-programming turn programming, which is rich
enough to express arbitrary arithmetic computations. We demonstrate a type
system that implements Peano arithmetics, slightly generalized to negative
numbers. Certain types in this system denote numerals. Arithmetic operations on
such types-numerals - addition, subtraction, and even division - are expressed
as type reduction rules executed by a compiler. A remarkable trait is that
division by zero becomes a type error - and reported as such by a compiler.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1001.1610v9,Steps towards a theory and calculus of aliasing,"A theory, graphical notation, mathematical calculus and implementation for
finding whether two given expressions can, at execution time, denote references
attached to the same object. Intended as the basis for a comprehensive solution
to the ""frame problem"" and as a complement to, or even a replacement for,
separation logic, shape analysis, ownership types and dynamic frames.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0412087v1,Image Colour Segmentation by Genetic Algorithms,"Segmentation of a colour image composed of different kinds of texture regions
can be a hard problem, namely to compute for an exact texture fields and a
decision of the optimum number of segmentation areas in an image when it
contains similar and/or unstationary texture fields. In this work, a method is
described for evolving adaptive procedures for these problems. In many real
world applications data clustering constitutes a fundamental issue whenever
behavioural or feature domains can be mapped into topological domains. We
formulate the segmentation problem upon such images as an optimisation problem
and adopt evolutionary strategy of Genetic Algorithms for the clustering of
small regions in colour feature space. The present approach uses k-Means
unsupervised clustering methods into Genetic Algorithms, namely for guiding
this last Evolutionary Algorithm in his search for finding the optimal or
sub-optimal data partition, task that as we know, requires a non-trivial search
because of its intrinsic NP-complete nature. To solve this task, the
appropriate genetic coding is also discussed, since this is a key aspect in the
implementation. Our purpose is to demonstrate the efficiency of Genetic
Algorithms to automatic and unsupervised texture segmentation. Some examples in
Colour Maps, Ornamental Stones and in Human Skin Mark segmentation are
presented and overall results discussed. KEYWORDS: Genetic Algorithms, Colour
Image Segmentation, Classification, Clustering.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.13073v2,Fast and Accurate Light Field Saliency Detection through Deep Encoding,"Light field saliency detection -- important due to utility in many vision
tasks -- still lacks speed and can improve in accuracy. Due to the formulation
of the saliency detection problem in light fields as a segmentation task or a
memorizing task, existing approaches consume unnecessarily large amounts of
computational resources for training, and have longer execution times for
testing. We solve this by aggressively reducing the large light field images to
a much smaller three-channel feature map appropriate for saliency detection
using an RGB image saliency detector with attention mechanisms. We achieve this
by introducing a novel convolutional neural network based features extraction
and encoding module. Our saliency detector takes $0.4$ s to process a light
field of size $9\times9\times512\times375$ in a CPU and is significantly faster
than state-of-the-art light field saliency detectors, with better or comparable
accuracy. Furthermore, model size of our architecture is significantly lower
compared to state-of-the-art light field saliency detectors. Our work shows
that extracting features from light fields through aggressive size reduction
and the attention mechanism results in a faster and accurate light field
saliency detector leading to near real-time light field processing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0808.1364v1,On Bounded Integer Programming,"We present an efficient reduction from the Bounded integer programming (BIP)
to the Subspace avoiding problem (SAP) in lattice theory. The reduction has
some special properties with some interesting consequences. The first is the
new upper time bound for BIP, $poly(\varphi)\cdot n^{n+o(n)}$ (where $n$ and
$\varphi$ are the dimension and the input size of the problem, respectively).
This is the best bound up to now for BIP. The second consequence is the proof
that #SAP, for some norms, is #P-hard under semi-reductions. It follows that
the counting version of the Generalized closest vector problem is also #P-hard
under semi-reductions. Furthermore, we also show that under some reasonable
assumptions, BIP is solvable in probabilistic time $2^{O(n)}$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.02216v1,Automatic Embedding of Stories Into Collections of Independent Media,"We look at how machine learning techniques that derive properties of items in
a collection of independent media can be used to automatically embed stories
into such collections. To do so, we use models that extract the tempo of songs
to make a music playlist follow a narrative arc. Our work specifies an
open-source tool that uses pre-trained neural network models to extract the
global tempo of a set of raw audio files and applies these measures to create a
narrative-following playlist. This tool is available at
https://github.com/dylanashley/playlist-story-builder/releases/tag/v1.0.0",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0803.1875v1,"Breaking Out of the Cell: On The Benefits of a New Spreadsheet
  User-Interaction Paradigm","Contemporary spreadsheets are plagued by a profusion of errors, auditing
difficulties, lack of uniform development methodologies, and barriers to easy
comprehension of the underlying business models they represent. This paper
presents a case that most of these difficulties stem from the fact that the
standard spreadsheet user-interaction paradigm - the 'cell-matrix' approach -
is appropriate for spreadsheet data presentation but has significant drawbacks
with respect to spreadsheet creation, maintenance and comprehension when
workbooks pass a minimal threshold of complexity. An alternative paradigm for
the automated generation of spreadsheets directly from plain-language business
model descriptions is presented along with its potential benefits. Sunsight
Modeller (TM), a working software system implementing the suggested paradigm,
is briefly described.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1
http://arxiv.org/abs/2010.16013v1,Teaching Interactive Proofs to Mathematicians,"This work discusses an approach to teach to mathematicians the importance and
effectiveness of the application of Interactive Theorem Proving tools in their
specific fields of interest. The approach aims to motivate the use of such
tools through short courses. In particular, it is discussed how, using as
case-of-study algebraic notions and properties, the use of the proof assistant
Prototype Verification System PVS is promoted to interest mathematicians in the
development of their mechanized proofs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1612.06671v1,Inferring the location of authors from words in their texts,"For the purposes of computational dialectology or other geographically bound
text analysis tasks, texts must be annotated with their or their authors'
location. Many texts are locatable through explicit labels but most have no
explicit annotation of place. This paper describes a series of experiments to
determine how positionally annotated microblog posts can be used to learn
location-indicating words which then can be used to locate blog texts and their
authors. A Gaussian distribution is used to model the locational qualities of
words. We introduce the notion of placeness to describe how locational words
are.
  We find that modelling word distributions to account for several locations
and thus several Gaussian distributions per word, defining a filter which picks
out words with high placeness based on their local distributional context, and
aggregating locational information in a centroid for each text gives the most
useful results. The results are applied to data in the Swedish language.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1511.02650v1,"Two Flow-Based Approaches for the Static Relocation Problem in
  Carsharing Systems","In a carsharing system, a fleet of cars is distributed at stations in an
urban area, customers can take and return cars at any time and station. For
operating such a system in a satisfactory way, the stations have to keep a good
ratio between the numbers of free places and cars in each station. This leads
to the problem of relocating cars between stations, which can be modeled within
the framework of a metric task system. In this paper, we focus on the Static
Relocation Problem, where the system has to be set into a certain state,
outgoing from the current state. We present two approaches to solve this
problem, a fast heuristic approach and an exact integer programming based
method using flows in time-expanded networks, and provide some computational
results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0309053v1,A Hierarchical Situation Calculus,"A situation calculus is presented that provides a solution to the frame
problem for hierarchical situations, that is, situations that have a modular
structure in which parts of the situation behave in a relatively independent
manner. This situation calculus is given in a relational, functional, and modal
logic form. Each form permits both a single level hierarchy or a multiple level
hierarchy, giving six versions of the formalism in all, and a number of
sub-versions of these. For multiple level hierarchies, it is possible to give
equations between parts of the situation to impose additional structure on the
problem. This approach is compared to others in the literature.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.13294v1,Modeling of crisis periods in stock markets,"We exploit a recent computational framework to model and detect financial
crises in stock markets, as well as shock events in cryptocurrency markets,
which are characterized by a sudden or severe drop in prices. Our method
manages to detect all past crises in the French industrial stock market
starting with the crash of 1929, including financial crises after 1990 (e.g.
dot-com bubble burst of 2000, stock market downturn of 2002), and all past
crashes in the cryptocurrency market, namely in 2018, and also in 2020 due to
covid-19. We leverage copulae clustering, based on the distance between
probability distributions, in order to validate the reliability of the
framework; we show that clusters contain copulae from similar market states
such as normal states, or crises. Moreover, we propose a novel regression model
that can detect successfully all past events using less than 10% of the
information that the previous framework requires. We train our model by
historical data on the industry assets, and we are able to detect all past
shock events in the cryptocurrency market. Our tools provide the essential
components of our software framework that offers fast and reliable detection,
or even prediction, of shock events in stock and cryptocurrency markets of
hundreds of assets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1410.6960v1,"Parameterizing the semantics of fuzzy attribute implications by systems
  of isotone Galois connections","We study the semantics of fuzzy if-then rules called fuzzy attribute
implications parameterized by systems of isotone Galois connections. The rules
express dependencies between fuzzy attributes in object-attribute incidence
data. The proposed parameterizations are general and include as special cases
the parameterizations by linguistic hedges used in earlier approaches. We
formalize the general parameterizations, propose bivalent and graded notions of
semantic entailment of fuzzy attribute implications, show their
characterization in terms of least models and complete axiomatization, and
provide characterization of bases of fuzzy attribute implications derived from
data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0811.4672v1,"Fast and Quality-Guaranteed Data Streaming in Resource-Constrained
  Sensor Networks","In many emerging applications, data streams are monitored in a network
environment. Due to limited communication bandwidth and other resource
constraints, a critical and practical demand is to online compress data streams
continuously with quality guarantee. Although many data compression and digital
signal processing methods have been developed to reduce data volume, their
super-linear time and more-than-constant space complexity prevents them from
being applied directly on data streams, particularly over resource-constrained
sensor networks. In this paper, we tackle the problem of online quality
guaranteed compression of data streams using fast linear approximation (i.e.,
using line segments to approximate a time series). Technically, we address two
versions of the problem which explore quality guarantees in different forms. We
develop online algorithms with linear time complexity and constant cost in
space. Our algorithms are optimal in the sense they generate the minimum number
of segments that approximate a time series with the required quality guarantee.
To meet the resource constraints in sensor networks, we also develop a fast
algorithm which creates connecting segments with very simple computation. The
low cost nature of our methods leads to a unique edge on the applications of
massive and fast streaming environment, low bandwidth networks, and heavily
constrained nodes in computational power. We implement and evaluate our methods
in the application of an acoustic wireless sensor network.",0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0205018v2,Typed Generic Traversal With Term Rewriting Strategies,"A typed model of strategic term rewriting is developed. The key innovation is
that generic traversal is covered. To this end, we define a typed rewriting
calculus S'_{gamma}. The calculus employs a many-sorted type system extended by
designated generic strategy types gamma. We consider two generic strategy
types, namely the types of type-preserving and type-unifying strategies.
S'_{gamma} offers traversal combinators to construct traversals or schemes
thereof from many-sorted and generic strategies. The traversal combinators
model different forms of one-step traversal, that is, they process the
immediate subterms of a given term without anticipating any scheme of recursion
into terms. To inhabit generic types, we need to add a fundamental combinator
to lift a many-sorted strategy $s$ to a generic type gamma. This step is called
strategy extension. The semantics of the corresponding combinator states that s
is only applied if the type of the term at hand fits, otherwise the extended
strategy fails. This approach dictates that the semantics of strategy
application must be type-dependent to a certain extent. Typed strategic term
rewriting with coverage of generic term traversal is a simple but expressive
model of generic programming. It has applications in program transformation and
program analysis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0501042v3,Maintaining Consistency of Data on the Web,"Increasingly more data is becoming available on the Web, estimates speaking
of 1 billion documents in 2002. Most of the documents are Web pages whose data
is considered to be in XML format, expecting it to eventually replace HTML.
  A common problem in designing and maintaining a Web site is that data on a
Web page often replicates or derives from other data, the so-called base data,
that is usually not contained in the deriving or replicating page.
Consequently, replicas and derivations become inconsistent upon modifying base
data in a Web page or a relational database. For example, after assigning a
thesis to a student and modifying the Web page that describes it in detail, the
thesis is still incorrectly contained in the list of offered thesis, missing in
the list of ongoing thesis, and missing in the advisor's teaching record.
  The thesis presents a solution by proposing a combined approach that provides
for maintaining consistency of data in Web pages that (i) replicate data in
relational databases, or (ii) replicate or derive from data in Web pages. Upon
modifying base data, the modification is immediately pushed to affected Web
pages. There, maintenance is performed incrementally by only modifying the
affected part of the page instead of re-generating the whole page from scratch.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1508.03601v1,Is Stack Overflow Overflowing With Questions and Tags,"Programming question and answer (Q & A) websites, such as Quora, Stack
Overflow, and Yahoo! Answer etc. helps us to understand the programming
concepts easily and quickly in a way that has been tested and applied by many
software developers. Stack Overflow is one of the most frequently used
programming Q\&A website where the questions and answers posted are presently
analyzed manually, which requires a huge amount of time and resource. To save
the effort, we present a topic modeling based technique to analyze the words of
the original texts to discover the themes that run through them. We also
propose a method to automate the process of reviewing the quality of questions
on Stack Overflow dataset in order to avoid ballooning the stack overflow with
insignificant questions. The proposed method also recommends the appropriate
tags for the new post, which averts the creation of unnecessary tags on Stack
Overflow.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.01636v1,Analyses of Multi-collection Corpora via Compound Topic Modeling,"As electronically stored data grow in daily life, obtaining novel and
relevant information becomes challenging in text mining. Thus people have
sought statistical methods based on term frequency, matrix algebra, or topic
modeling for text mining. Popular topic models have centered on one single text
collection, which is deficient for comparative text analyses. We consider a
setting where one can partition the corpus into subcollections. Each
subcollection shares a common set of topics, but there exists relative
variation in topic proportions among collections. Including any prior knowledge
about the corpus (e.g. organization structure), we propose the compound latent
Dirichlet allocation (cLDA) model, improving on previous work, encouraging
generalizability, and depending less on user-input parameters. To identify the
parameters of interest in cLDA, we study Markov chain Monte Carlo (MCMC) and
variational inference approaches extensively, and suggest an efficient MCMC
method. We evaluate cLDA qualitatively and quantitatively using both synthetic
and real-world corpora. The usability study on some real-world corpora
illustrates the superiority of cLDA to explore the underlying topics
automatically but also model their connections and variations across multiple
collections.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.05648v3,"Cut Sparsification of the Clique Beyond the Ramanujan Bound: A
  Separation of Cut Versus Spectral Sparsification","We prove that a random $d$-regular graph, with high probability, is a cut
sparsifier of the clique with approximation error at most $\left(2\sqrt{\frac 2
\pi} + o_{n,d}(1)\right)/\sqrt d$, where $2\sqrt{\frac 2 \pi} = 1.595\ldots$
and $o_{n,d}(1)$ denotes an error term that depends on $n$ and $d$ and goes to
zero if we first take the limit $n\rightarrow \infty$ and then the limit $d
\rightarrow \infty$.
  This is established by analyzing linear-size cuts using techniques of
Jagannath and Sen derived from ideas in statistical physics, and analyzing
small cuts via martingale inequalities.
  We also prove new lower bounds on spectral sparsification of the clique. If
$G$ is a spectral sparsifier of the clique and $G$ has average degree $d$, we
prove that the approximation error is at least the ""Ramanujan bound''
$(2-o_{n,d}(1))/\sqrt d$, which is met by $d$-regular Ramanujan graphs,
provided that either the weighted adjacency matrix of $G$ is a (multiple of) a
doubly stochastic matrix, or that $G$ satisfies a certain high ""odd
pseudo-girth"" property. The first case can be seen as an ""Alon-Boppana theorem
for symmetric doubly stochastic matrices,"" showing that a symmetric doubly
stochastic matrix with $dn$ non-zero entries has a non-trivial eigenvalue of
magnitude at least $(2-o_{n,d}(1))/\sqrt d$; the second case generalizes a
lower bound of Srivastava and Trevisan, which requires a large girth
assumption.
  Together, these results imply a separation between spectral sparsification
and cut sparsification. If $G$ is a random $\log n$-regular graph on $n$
vertices, we show that, with high probability, $G$ admits a (weighted subgraph)
cut sparsifier of average degree $d$ and approximation error at most
$(1.595\ldots + o_{n,d}(1))/\sqrt d$, while every (weighted subgraph) spectral
sparsifier of $G$ having average degree $d$ has approximation error at least
$(2-o_{n,d}(1))/\sqrt d$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.00740v1,The Meaning of Null in Databases and Programming Languages,"The meaning of null in relational databases is a major source of confusion
not only among database users but also among database textbook writers. The
purpose of this article is to examine what database nulls could mean and to
make some modest suggestions about how to reduce the confusion.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1906.05945v5,"A Tight and Unified Analysis of Gradient-Based Methods for a Whole
  Spectrum of Games","We consider differentiable games where the goal is to find a Nash
equilibrium. The machine learning community has recently started using variants
of the gradient method (GD). Prime examples are extragradient (EG), the
optimistic gradient method (OG) and consensus optimization (CO), which enjoy
linear convergence in cases like bilinear games, where the standard GD fails.
The full benefits of theses relatively new methods are not known as there is no
unified analysis for both strongly monotone and bilinear games. We provide new
analyses of the EG's local and global convergence properties and use is to get
a tighter global convergence rate for OG and CO. Our analysis covers the whole
range of settings between bilinear and strongly monotone games. It reveals that
these methods converge via different mechanisms at these extremes; in between,
it exploits the most favorable mechanism for the given problem. We then prove
that EG achieves the optimal rate for a wide class of algorithms with any
number of extrapolations. Our tight analysis of EG's convergence rate in games
shows that, unlike in convex minimization, EG may be much faster than GD.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.01534v2,"Learning to automate cryo-electron microscopy data collection with
  Ptolemy","Over the past decade, cryogenic electron microscopy (cryo-EM) has emerged as
a primary method for determining near-native, near-atomic resolution 3D
structures of biological macromolecules. In order to meet increasing demand for
cryo-EM, automated methods to improve throughput and efficiency while lowering
costs are needed. Currently, all high-magnification cryo-EM data collection
softwares require human input and manual tuning of parameters. Expert operators
must navigate low- and medium-magnification images to find good
high-magnification collection locations. Automating this is non-trivial: the
images suffer from low signal-to-noise ratio and are affected by a range of
experimental parameters that can differ for each collection session. Here, we
use various computer vision algorithms, including mixture models, convolutional
neural networks, and U-Nets to develop the first pipeline to automate low- and
medium-magnification targeting. Learned models in this pipeline are trained on
a large internal dataset of images from real world cryo-EM data collection
sessions, labeled with locations that were selected by operators. Using these
models, we show that we can effectively detect and classify regions of interest
in low- and medium-magnification images, and can generalize to unseen sessions,
as well as to images captured using different microscopes from external
facilities. We expect our open-source pipeline, Ptolemy, will be both
immediately useful as a tool for automation of cryo-EM data collection, and
serve as a foundation for future advanced methods for efficient and automated
cryo-EM microscopy.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1604.05959v1,FRAPPE: Fast Replication Platform for Elastic Services,"Elasticity is critical for today's cloud services, which must be able to
quickly adapt to dynamically changing load conditions and resource
availability. We introduce FRAPPE, a new consistent replication platform aiming
at improving elasticity of the replicated services hosted in clouds or large
data centers. In the core of FRAPPE is a novel replicated state machine
protocol, which employs speculative executions to ensure continuous operation
during the reconfiguration periods as well as in situations where failures
prevent the agreement on the next stable configuration from being reached in a
timely fashion. We present the FRAPPE's architecture and describe the basic
techniques underlying the implementation of our speculative state machine
protocol.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0309016v1,"Using Simulated Annealing to Calculate the Trembles of Trembling Hand
  Perfection","Within the literature on non-cooperative game theory, there have been a
number of attempts to propose logorithms which will compute Nash equilibria.
Rather than derive a new algorithm, this paper shows that the family of
algorithms known as Markov chain Monte Carlo (MCMC) can be used to calculate
Nash equilibria. MCMC is a type of Monte Carlo simulation that relies on Markov
chains to ensure its regularity conditions. MCMC has been widely used
throughout the statistics and optimization literature, where variants of this
algorithm are known as simulated annealing. This paper shows that there is
interesting connection between the trembles that underlie the functioning of
this algorithm and the type of Nash refinement known as trembling hand
perfection.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1808.03053v1,Optimal conditions for connectedness of discretized sets,"Constructing a discretization of a given set is a major problem in various
theoretical and applied disciplines. An offset discretization of a set $X$ is
obtained by taking the integer points inside a closed neighborhood of $X$ of a
certain radius. In this note we determine a minimum threshold for the offset
radius, beyond which the discretization of a disconnected set is always
connected. The results hold for a broad class of disconnected and unbounded
subsets of $R^n$, and generalize several previous results. Algorithmic aspects
and possible applications are briefly discussed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1712.07029v1,"Sonification of Network Traffic Flow for Monitoring and Situational
  Awareness","Maintaining situational awareness of what is happening within a network is
challenging, not least because the behaviour happens within computers and
communications networks, but also because data traffic speeds and volumes are
beyond human ability to process. Visualisation is widely used to present
information about the dynamics of network traffic dynamics. Although it
provides operators with an overall view and specific information about
particular traffic or attacks on the network, it often fails to represent the
events in an understandable way. Visualisations require visual attention and so
are not well suited to continuous monitoring scenarios in which network
administrators must carry out other tasks. Situational awareness is critical
and essential for decision-making in the domain of computer network monitoring
where it is vital to be able to identify and recognize network environment
behaviours.Here we present SoNSTAR (Sonification of Networks for SiTuational
AwaReness), a real-time sonification system to be used in the monitoring of
computer networks to support the situational awareness of network
administrators. SoNSTAR provides an auditory representation of all the TCP/IP
protocol traffic within a network based on the different traffic flows between
between network hosts. SoNSTAR raises situational awareness levels for computer
network defence by allowing operators to achieve better understanding and
performance while imposing less workload compared to visual techniques. SoNSTAR
identifies the features of network traffic flows by inspecting the status flags
of TCP/IP packet headers and mapping traffic events to recorded sounds to
generate a soundscape representing the real-time status of the network traffic
environment. Listening to the soundscape allows the administrator to recognise
anomalous behaviour quickly and without having to continuously watch a computer
screen.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0412072v1,Swarms on Continuous Data,"While being it extremely important, many Exploratory Data Analysis (EDA)
systems have the inhability to perform classification and visualization in a
continuous basis or to self-organize new data-items into the older ones
(evenmore into new labels if necessary), which can be crucial in KDD -
Knowledge Discovery, Retrieval and Data Mining Systems (interactive and online
forms of Web Applications are just one example). This disadvantge is also
present in more recent approaches using Self-Organizing Maps. On the present
work, and exploiting past sucesses in recently proposed Stigmergic Ant Systems
a robust online classifier is presented, which produces class decisions on a
continuous stream data, allowing for continuous mappings. Results show that
increasingly better results are achieved, as demonstraded by other authors in
different areas. KEYWORDS: Swarm Intelligence, Ant Systems, Stigmergy,
Data-Mining, Exploratory Data Analysis, Image Retrieval, Continuous
Classification.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0810.5725v2,"A triangle-based logic for affine-invariant querying of spatial and
  spatio-temporal data","In spatial databases, incompatibilities often arise due to different choices
of origin or unit of measurement (e.g., centimeters versus inches). By
representing and querying the data in an affine-invariant manner, we can avoid
these incompatibilities.
  In practice, spatial (resp., spatio-temporal) data is often represented as a
finite union of triangles (resp., moving triangles). As two arbitrary triangles
are equal up to a unique affinity of the plane, they seem perfect candidates as
basic units for an affine-invariant query language.
  We propose a so-called ""triangle logic"", a query language that is
affine-generic and has triangles as basic elements. We show that this language
has the same expressive power as the affine-generic fragment of first-order
logic over the reals on triangle databases. We illustrate that the proposed
language is simple and intuitive. It can also serve as a first step towards a
""moving-triangle logic"" for spatio-temporal data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.07223v1,"GANDALF: A fine-grained hardware-software co-design for preventing
  memory attacks","Reading or writing outside the bounds of a buffer is a serious security
vulnerability that has been exploited in numerous occasions. These attacks can
be prevented by ensuring that every buffer is only accessed within its
specified bounds. In this paper we present Gandalf, a compiler-assisted
hardware extension for the OpenRISC processor that thwarts all forms of memory
based attacks including buffer overflows and over-reads.The feature associates
lightweight base and bound capabilities to all pointer variables, which are
checked at run time by the hardware. Gandalf is transparent to the user and
does not require significant OS modifications. Moreover, it achieves locality,
thus resulting in small performance penalties.",0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1509.02218v1,Wikipedia Page View Reflects Web Search Trend,"The frequency of a web search keyword generally reflects the degree of public
interest in a particular subject matter. Search logs are therefore useful
resources for trend analysis. However, access to search logs is typically
restricted to search engine providers. In this paper, we investigate whether
search frequency can be estimated from a different resource such as Wikipedia
page views of open data. We found frequently searched keywords to have
remarkably high correlations with Wikipedia page views. This suggests that
Wikipedia page views can be an effective tool for determining popular global
web search trends.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1610.03052v3,"Verification of the Tree-Based Hierarchical Read-Copy Update in the
  Linux Kernel","Read-Copy Update (RCU) is a scalable, high-performance Linux-kernel
synchronization mechanism that runs low-overhead readers concurrently with
updaters. Production-quality RCU implementations for multi-core systems are
decidedly non-trivial. Giving the ubiquity of Linux, a rare ""million-year"" bug
can occur several times per day across the installed base. Stringent validation
of RCU's complex behaviors is thus critically important. Exhaustive testing is
infeasible due to the exponential number of possible executions, which suggests
use of formal verification.
  Previous verification efforts on RCU either focus on simple implementations
or use modeling languages, the latter requiring error-prone manual translation
that must be repeated frequently due to regular changes in the Linux kernel's
RCU implementation. In this paper, we first describe the implementation of Tree
RCU in the Linux kernel. We then discuss how to construct a model directly from
Tree RCU's source code in C, and use the CBMC model checker to verify its
safety and liveness properties. To our best knowledge, this is the first
verification of a significant part of RCU's source code, and is an important
step towards integration of formal verification into the Linux kernel's
regression test suite.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9809029v1,Incremental Parser Generation for Tree Adjoining Grammars,"This paper describes the incremental generation of parse tables for the
LR-type parsing of Tree Adjoining Languages (TALs). The algorithm presented
handles modifications to the input grammar by updating the parser generated so
far. In this paper, a lazy generation of LR-type parsers for TALs is defined in
which parse tables are created by need while parsing. We then describe an
incremental parser generator for TALs which responds to modification of the
input grammar by updating parse tables built so far.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.6787v5,Instruction sequence size complexity of parity,"Each Boolean function can be computed by a single-pass instruction sequence
that contains only instructions to set and get the content of Boolean
registers, forward jump instructions, and a termination instruction. Auxiliary
Boolean registers are not necessary for this. In the current paper, we show
that, in the case of the parity functions, shorter instruction sequences are
possible with the use of an auxiliary Boolean register in the presence of
instructions to complement the content of auxiliary Boolean registers. This
result supports, in a setting where programs are instruction sequences acting
on Boolean registers, a basic intuition behind the storage of auxiliary data,
namely the intuition that this makes possible a reduction of the size of a
program.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1307.4164v2,Approximating Minimum Cost Connectivity Orientation and Augmentation,"We investigate problems addressing combined connectivity augmentation and
orientations settings. We give a polynomial-time 6-approximation algorithm for
finding a minimum cost subgraph of an undirected graph $G$ that admits an
orientation covering a nonnegative crossing $G$-supermodular demand function,
as defined by Frank. An important example is $(k,\ell)$-edge-connectivity, a
common generalization of global and rooted edge-connectivity.
  Our algorithm is based on a non-standard application of the iterative
rounding method. We observe that the standard linear program with cut
constraints is not amenable and use an alternative linear program with
partition and co-partition constraints instead. The proof requires a new type
of uncrossing technique on partitions and co-partitions.
  We also consider the problem setting when the cost of an edge can be
different for the two possible orientations. The problem becomes substantially
more difficult already for the simpler requirement of $k$-edge-connectivity.
Khanna, Naor, and Shepherd showed that the integrality gap of the natural
linear program is at most $4$ when $k=1$ and conjectured that it is constant
for all fixed $k$. We disprove this conjecture by showing an $\Omega(|V|)$
integrality gap even when $k=2$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.07414v1,"Combinatorial Algorithms for Rooted Prize-Collecting Walks and
  Applications to Orienteering and Minimum-Latency Problems","We consider the rooted prize-collecting walks (PCW) problem, wherein we seek
a collection $C$ of rooted walks having minimum prize-collecting cost, which is
the (total cost of walks in $C$) + (total node-reward of nodes not visited by
any walk in $C$). This problem arises naturally as the Lagrangian relaxation of
both orienteering, where we seek a length-bounded walk of maximum reward, and
the $\ell$-stroll problem, where we seek a minimum-length walk covering at
least $\ell$ nodes. Our main contribution is to devise a simple, combinatorial
algorithm for the PCW problem in directed graphs that returns a rooted tree
whose prize-collecting cost is at most the optimum value of the
prize-collecting walks problem.
  We utilize our algorithm to develop combinatorial approximation algorithms
for two fundamental vehicle-routing problems (VRPs): (1) orienteering; and (2)
$k$-minimum-latency problem ($k$-MLP), wherein we seek to cover all nodes using
$k$ paths starting at a prescribed root node, so as to minimize the sum of the
node visiting times. Our combinatorial algorithm allows us to sidestep the part
where we solve a preflow-based LP in the LP-rounding algorithms of Friggstand
and Swamy (2017) for orienteering, and in the state-of-the-art
$7.183$-approximation algorithm for $k$-MP in Post and Swamy (2015).
Consequently, we obtain combinatorial implementations of these algorithms with
substantially improved running times compared with the current-best
approximation factors.
  We report computational results for our resulting (combinatorial
implementations of) orienteering algorithms, which show that the algorithms
perform quite well in practice, both in terms of the quality of the solution
they return, as also the upper bound they yield on the orienteering optimum
(which is obtained by leveraging the workings of our PCW algorithm).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1204.4765v1,String Trees,"A string-like compact data structure for unlabelled rooted trees is given
using 2n bits.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.09574v1,Machine Learning Evaluation of the Echo-Chamber Effect in Medical Forums,"We propose the Echo-Chamber Effect assessment of an online forum. Sentiments
perceived by the forum readers are at the core of the analysis; a complete
message is the unit of the study. We build 14 models and apply those to
represent discussions gathered from an online medical forum. We use four
multi-class sentiment classification applications and two Machine Learning
algorithms to evaluate prowess of the assessment models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.11251v1,"A machine learning based software pipeline to pick the variable ordering
  for algorithms with polynomial inputs","We are interested in the application of Machine Learning (ML) technology to
improve mathematical software. It may seem that the probabilistic nature of ML
tools would invalidate the exact results prized by such software, however, the
algorithms which underpin the software often come with a range of choices which
are good candidates for ML application. We refer to choices which have no
effect on the mathematical correctness of the software, but do impact its
performance.
  In the past we experimented with one such choice: the variable ordering to
use when building a Cylindrical Algebraic Decomposition (CAD). We used the
Python library Scikit-Learn (sklearn) to experiment with different ML models,
and developed new techniques for feature generation and hyper-parameter
selection.
  These techniques could easily be adapted for making decisions other than our
immediate application of CAD variable ordering. Hence in this paper we present
a software pipeline to use sklearn to pick the variable ordering for an
algorithm that acts on a polynomial system. The code described is freely
available online.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1807.07485v3,"Enhanced adaptive surrogate models with applications in uncertainty
  quantification for nanoplasmonics","We propose an efficient surrogate modeling technique for uncertainty
quantification. The method is based on a well-known dimension-adaptive
collocation scheme. We improve the scheme by enhancing sparse polynomial
surrogates with conformal maps and adjoint error correction. The methodology is
applied to Maxwell's source problem with random input data. This setting
comprises many applications of current interest from computational
nanoplasmonics, such as grating couplers or optical waveguides. Using a
non-trivial benchmark model we show the benefits and drawbacks of using
enhanced surrogate models through various numerical studies. The proposed
strategy allows us to conduct a thorough uncertainty analysis, taking into
account a moderately large number of random parameters.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.07604v1,"Kategorisasi dokumen web secara otomatis berdasarkan folksonomy
  menggunakan multinomial naive Bayes classifier","Folksonomy is a non-hierarchical document categorizing system, that treats
every category in a flat manner, dan every category is entered freely by anyone
who submitted a document in these categories. Categorization is done
automatically at the time a document is submitted, by entering the list of
categories that best fit the document. del.icio.us (http://del.icio.us) site is
one of the most popular social bookmarking sites that uses folksonomy.
  Usage of folksonomy, although very easy, also has its weaknesses, such as use
of different tags for the same concept, use of the same tag for different
concepts, no quality control, etc. We try to provide a solution for some of
these problems by analyzing Web documents' contents and categorizing them
automatically using multinomial naive Bayes algorithm.
  Bayes classifier works by using a set of evidences and a set of classes. By
training the system using sample data, we can determine the probability of an
evidence given a particular class. Bayes classifier also uses prior probability
of a class, which can be calculated from sample data. From these analysis, when
given a new document which is formed by a set of evidences (words), the
probabilities of each class given that document (posterior probabilities) can
be determined.
  This system is implemented using PHP 5, Apache, and MySQL. The conclusion
from building this system is that the Bayes method can be used to automatically
categorize documents and also as an assistive tool for manual categorization.
  -----
  Folksonomy merupakan metode kategorisasi dokumen yang tidak hierarkis,
menyamaratakan kedudukan setiap kategori, dan judul kategori ditentukan secara
bebas oleh siapa saja yang memasukkan sebuah dokumen di dalam kategori-kategori
tersebut.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0307054v1,"Contributions to the Development and Improvement of a Regulatory and
  Pre-Regulatory Digitally System for the Tools within Flexible Fabrication
  Systems","The paper reports the obtained results for the projection and realization of
a digitally system aiming to assist the equipment for a regulatory and
pre-regulatory tools and holding tools within the flexible fabrication systems
(FFS). Moreover, based on the present results, the same methodology can be
applied for assisting tools from the point of view of their integrity and to
wear compensation in the FFS framework.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1603.02057v3,Bootstrap percolation on geometric inhomogeneous random graphs,"Geometric inhomogeneous random graphs (GIRGs) are a model for scale-free
networks with underlying geometry. We study bootstrap percolation on these
graphs, which is a process modelling the spread of an infection of vertices
starting within a (small) local region. We show that the process exhibits a
phase transition in terms of the initial infection rate in this region. We
determine the speed of the process in the supercritical case, up to lower order
terms, and show that its evolution is fundamentally influenced by the
underlying geometry. For vertices with given position and expected degree, we
determine the infection time up to lower order terms. Finally, we show how this
knowledge can be used to contain the infection locally by removing relatively
few edges from the graph. This is the first time that the role of geometry on
bootstrap percolation is analysed mathematically for geometric scale-free
networks.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0408009v1,"Performance Analysis of Multicast Mobility in a Hierarchical Mobile IP
  Proxy Environment","Mobility support in IPv6 networks is ready for release as an RFC, stimulating
major discussions on improvements to meet real-time communication requirements.
Sprawling hot spots of IP-only wireless networks at the same time await voice
and videoconferencing as standard mobile Internet services, thereby adding the
request for multicast support to real-time mobility. This paper briefly
introduces current approaches for seamless multicast extensions to Mobile IPv6.
Key issues of multicast mobility are discussed. Both analytically and in
simulations comparisons are drawn between handover performance characteristics,
dedicating special focus on the M-HMIPv6 approach.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.03399v1,"Deep Learning on Chest X-ray Images to Detect and Evaluate Pneumonia
  Cases at the Era of COVID-19","Coronavirus disease 2019 (COVID-19) is an infectious disease with first
symptoms similar to the flu. COVID-19 appeared first in China and very quickly
spreads to the rest of the world, causing then the 2019-20 coronavirus
pandemic. In many cases, this disease causes pneumonia. Since pulmonary
infections can be observed through radiography images, this paper investigates
deep learning methods for automatically analyzing query chest X-ray images with
the hope to bring precision tools to health professionals towards screening the
COVID-19 and diagnosing confirmed patients. In this context, training datasets,
deep learning architectures and analysis strategies have been experimented from
publicly open sets of chest X-ray images. Tailored deep learning models are
proposed to detect pneumonia infection cases, notably viral cases. It is
assumed that viral pneumonia cases detected during an epidemic COVID-19 context
have a high probability to presume COVID-19 infections. Moreover, easy-to-apply
health indicators are proposed for estimating infection status and predicting
patient status from the detected pneumonia cases. Experimental results show
possibilities of training deep learning models over publicly open sets of chest
X-ray images towards screening viral pneumonia. Chest X-ray test images of
COVID-19 infected patients are successfully diagnosed through detection models
retained for their performances. The efficiency of proposed health indicators
is highlighted through simulated scenarios of patients presenting infections
and health problems by combining real and synthetic health data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1007.2364v1,"A Note on Semantic Web Services Specification and Composition in
  Constructive Description Logics","The idea of the Semantic Web is to annotate Web content and services with
computer interpretable descriptions with the aim to automatize many tasks
currently performed by human users. In the context of Web services, one of the
most interesting tasks is their composition. In this paper we formalize this
problem in the framework of a constructive description logic. In particular we
propose a declarative service specification language and a calculus for service
composition. We show by means of an example how this calculus can be used to
define composed Web services and we discuss the problem of automatic service
synthesis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1904.10605v1,A Linear-logical Reconstruction of Intuitionistic Modal Logic S4,"We propose a ""modal linear logic"" to reformulate intuitionistic modal logic
S4 (IS4) in terms of linear logic, establishing an S4-version of Girard
translation from IS4 to it. While the Girard translation from intuitionistic
logic to linear logic is well-known, its extension to modal logic is
non-trivial since a naive combination of the S4 modality and the exponential
modality causes an undesirable interaction between the two modalities. To solve
the problem, we introduce an extension of intuitionistic multiplicative
exponential linear logic with a modality combining the S4 modality and the
exponential modality, and show that it admits a sound translation from IS4.
Through the Curry-Howard correspondence we further obtain a Geometry of
Interaction Machine semantics of the modal lambda-calculus by Pfenning and
Davies for staged computation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1207.2341v1,I/O-Efficient Dynamic Planar Range Skyline Queries,"We present the first fully dynamic worst case I/O-efficient data structures
that support planar orthogonal \textit{3-sided range skyline reporting queries}
in $\bigO (\log_{2B^\epsilon} n + \frac{t}{B^{1-\epsilon}})$ I/Os and updates
in $\bigO (\log_{2B^\epsilon} n)$ I/Os, using $\bigO
(\frac{n}{B^{1-\epsilon}})$ blocks of space, for $n$ input planar points, $t$
reported points, and parameter $0 \leq \epsilon \leq 1$. We obtain the result
by extending Sundar's priority queues with attrition to support the operations
\textsc{DeleteMin} and \textsc{CatenateAndAttrite} in $\bigO (1)$ worst case
I/Os, and in $\bigO(1/B)$ amortized I/Os given that a constant number of blocks
is already loaded in main memory. Finally, we show that any pointer-based
static data structure that supports \textit{dominated maxima reporting
queries}, namely the difficult special case of 4-sided skyline queries, in
$\bigO(\log^{\bigO(1)}n +t)$ worst case time must occupy $\Omega(n \frac{\log
n}{\log \log n})$ space, by adapting a similar lower bounding argument for
planar 4-sided range reporting queries.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.07423v2,The Role of Context in Detecting Previously Fact-Checked Claims,"Recent years have seen the proliferation of disinformation and fake news
online. Traditional approaches to mitigate these issues is to use manual or
automatic fact-checking. Recently, another approach has emerged: checking
whether the input claim has previously been fact-checked, which can be done
automatically, and thus fast, while also offering credibility and
explainability, thanks to the human fact-checking and explanations in the
associated fact-checking article. Here, we focus on claims made in a political
debate and we study the impact of modeling the context of the claim: both on
the source side, i.e., in the debate, as well as on the target side, i.e., in
the fact-checking explanation document. We do this by modeling the local
context, the global context, as well as by means of co-reference resolution,
and multi-hop reasoning over the sentences of the document describing the
fact-checked claim. The experimental results show that each of these represents
a valuable information source, but that modeling the source-side context is
most important, and can yield 10+ points of absolute improvement over a
state-of-the-art model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1911.09065v3,The complexity of promise SAT on non-Boolean domains,"While 3-SAT is NP-hard, 2-SAT is solvable in polynomial time. Austrin,
Guruswami, and H\r{a}stad roved a result known as ""$(2+\varepsilon)$-SAT is
NP-hard"" [FOCS'14/SICOMP'17]. They showed that the problem of distinguishing
k-CNF formulas that are g-satisfiable (i.e. some assignment satisfies at least
g literals in every clause) from those that are not even 1-satisfiable is
NP-hard if $\frac{g}{k} < \frac{1}{2}$ and is in P otherwise. We study a
generalisation of SAT on arbitrary finite domains, with clauses that are
disjunctions of unary constraints, and establish analogous behaviour. Thus we
give a dichotomy for a natural fragment of promise constraint satisfaction
problems (PCSPs) on arbitrary finite domains.
  The hardness side is proved using the algebraic approach, via a new general
NP-hardness criterion on polymorphisms of the problem, based on a gap version
of the Layered Label Cover problem. We show that previously used criteria are
insufficient -- the problem hence gives an interesting benchmark of algebraic
techniques for proving hardness of approximation problems such as PCSPs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0312028v1,"Minimal founded semantics for disjunctive logic programs and deductive
  databases","In this paper, we propose a variant of stable model semantics for disjunctive
logic programming and deductive databases. The semantics, called minimal
founded, generalizes stable model semantics for normal (i.e. non disjunctive)
programs but differs from disjunctive stable model semantics (the extension of
stable model semantics for disjunctive programs). Compared with disjunctive
stable model semantics, minimal founded semantics seems to be more intuitive,
it gives meaning to programs which are meaningless under stable model semantics
and is no harder to compute. More specifically, minimal founded semantics
differs from stable model semantics only for disjunctive programs having
constraint rules or rules working as constraints. We study the expressive power
of the semantics and show that for general disjunctive datalog programs it has
the same power as disjunctive stable model semantics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1403.4749v3,Parameterized Complexity of Synchronization and Road Coloring,"First, we close the multivariate analysis of a canonical problem concerning
short reset words (SYN), as it was started by Fernau et al. (2013). Namely, we
prove that the problem, parameterized by the number of states, does not admit a
polynomial kernel unless the polynomial hierarchy collapses. Second, we
consider a related canonical problem concerning synchronizing road colorings
(SRCP). Here we give a similar complete multivariate analysis. Namely, we show
that the problem, parameterized by the number of states, admits a polynomial
kernel and we close the previous research of restrictions to particular values
of both the alphabet size and the maximum word length.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1201.5853v1,Descriptive complexity for pictures languages (extended abstract),"This paper deals with descriptive complexity of picture languages of any
dimension by syntactical fragments of existential second-order logic.
  - We uniformly generalize to any dimension the characterization by
Giammarresi et al. \cite{GRST96} of the class of \emph{recognizable} picture
languages in existential monadic second-order logic. - We state several logical
characterizations of the class of picture languages recognized in linear time
on nondeterministic cellular automata of any dimension. They are the first
machine-independent characterizations of complexity classes of cellular
automata.
  Our characterizations are essentially deduced from normalization results we
prove for first-order and existential second-order logics over pictures. They
are obtained in a general and uniform framework that allows to extend them to
other ""regular"" structures. Finally, we describe some hierarchy results that
show the optimality of our logical characterizations and delineate their
limits.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.08367v2,"Stream Differential Equations: Specification Formats and Solution
  Methods","Streams, or infinite sequences, are infinite objects of a very simple type,
yet they have a rich theory partly due to their ubiquity in mathematics and
computer science. Stream differential equations are a coinductive method for
specifying streams and stream operations, and their theory has been developed
in many papers over the past two decades. In this paper we present a survey of
the many results in this area. Our focus is on the classification of different
formats of stream differential equations, their solution methods, and the
classes of streams they can define. Moreover, we describe in detail the
connection between the so-called syntactic solution method and abstract GSOS.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0401016v3,Generalized Strong Preservation by Abstract Interpretation,"Standard abstract model checking relies on abstract Kripke structures which
approximate concrete models by gluing together indistinguishable states, namely
by a partition of the concrete state space. Strong preservation for a
specification language L encodes the equivalence of concrete and abstract model
checking of formulas in L. We show how abstract interpretation can be used to
design abstract models that are more general than abstract Kripke structures.
Accordingly, strong preservation is generalized to abstract
interpretation-based models and precisely related to the concept of
completeness in abstract interpretation. The problem of minimally refining an
abstract model in order to make it strongly preserving for some language L can
be formulated as a minimal domain refinement in abstract interpretation in
order to get completeness w.r.t. the logical/temporal operators of L. It turns
out that this refined strongly preserving abstract model always exists and can
be characterized as a greatest fixed point. As a consequence, some well-known
behavioural equivalences, like bisimulation, simulation and stuttering, and
their corresponding partition refinement algorithms can be elegantly
characterized in abstract interpretation as completeness properties and
refinements.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1610.08419v3,Tracing where IoT data are collected and aggregated,"The Internet of Things (IoT) offers the infrastructure of the information
society. It hosts smart objects that automatically collect and exchange data of
various kinds, directly gathered from sensors or generated by aggregations.
Suitable coordination primitives and analysis mechanisms are in order to design
and reason about IoT systems, and to intercept the implied technological
shifts. We address these issues from a foundational point of view. To study
them, we define IoT-LySa, a process calculus endowed with a static analysis
that tracks the provenance and the manipulation of IoT data, and how they flow
in the system. The results of the analysis can be used by a designer to check
the behaviour of smart objects, in particular to verify non-functional
properties, among which security.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0306040v1,"The Open Language Archives Community: An infrastructure for distributed
  archiving of language resources","New ways of documenting and describing language via electronic media coupled
with new ways of distributing the results via the World-Wide Web offer a degree
of access to language resources that is unparalleled in history. At the same
time, the proliferation of approaches to using these new technologies is
causing serious problems relating to resource discovery and resource creation.
This article describes the infrastructure that the Open Language Archives
Community (OLAC) has built in order to address these problems. Its technical
and usage infrastructures address problems of resource discovery by
constructing a single virtual library of distributed resources. Its governance
infrastructure addresses problems of resource creation by providing a mechanism
through which the language-resource community can express its consensus on
recommended best practices.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.04668v2,The distance backbone of complex networks,"Redundancy needs more precise characterization as it is a major factor in the
evolution and robustness of networks of multivariate interactions. We
investigate the complexity of such interactions by inferring a connection
transitivity that includes all possible measures of path length for weighted
graphs. The result, without breaking the graph into smaller components, is a
distance backbone subgraph sufficient to compute all shortest paths. This is
important for understanding the dynamics of spread and communication phenomena
in real-world networks. The general methodology we formally derive yields a
principled graph reduction technique and provides a finer characterization of
the triangular geometry of all edges -- those that contribute to shortest paths
and those that do not but are involved in other network phenomena. We
demonstrate that the distance backbone is very small in large networks across
domains ranging from air traffic to the human brain connectome, revealing that
network robustness to attacks and failures seems to stem from surprisingly vast
amounts of redundancy.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0611047v1,"The Reaction RuleML Classification of the Event / Action / State
  Processing and Reasoning Space","Reaction RuleML is a general, practical, compact and user-friendly
XML-serialized language for the family of reaction rules. In this white paper
we give a review of the history of event / action /state processing and
reaction rule approaches and systems in different domains, define basic
concepts and give a classification of the event, action, state processing and
reasoning space as well as a discussion of relevant / related work",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.05027v1,"Boosted EfficientNet: Detection of Lymph Node Metastases in Breast
  Cancer Using Convolutional Neural Network","In recent years, advances in the development of whole-slide images have laid
a foundation for the utilization of digital images in pathology. With the
assistance of computer images analysis that automatically identifies tissue or
cell types, they have greatly improved the histopathologic interpretation and
diagnosis accuracy. In this paper, the Convolutional Neutral Network (CNN) has
been adapted to predict and classify lymph node metastasis in breast cancer.
Unlike traditional image cropping methods that are only suitable for large
resolution images, we propose a novel data augmentation method named Random
Center Cropping (RCC) to facilitate small resolution images. RCC enriches the
datasets while retaining the image resolution and the center area of images. In
addition, we reduce the downsampling scale of the network to further facilitate
small resolution images better. Moreover, Attention and Feature Fusion (FF)
mechanisms are employed to improve the semantic information of images.
Experiments demonstrate that our methods boost performances of basic CNN
architectures. And the best-performed method achieves an accuracy of 97.96% and
an AUC of 99.68% on RPCam datasets, respectively.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1003.4679v1,Bounds for Bilinear Complexity of Noncommutative Group Algebras,"We study the complexity of multiplication in noncommutative group algebras
which is closely related to the complexity of matrix multiplication. We
characterize such semisimple group algebras of the minimal bilinear complexity
and show nontrivial lower bounds for the rest of the group algebras. These
lower bounds are built on the top of Bl\""aser's results for semisimple algebras
and algebras with large radical and the lower bound for arbitrary associative
algebras due to Alder and Strassen. We also show subquadratic upper bounds for
all group algebras turning into ""almost linear"" provided the exponent of matrix
multiplication equals 2.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0010006v1,Applications of Data Mining to Electronic Commerce,"Electronic commerce is emerging as the killer domain for data mining
technology.
  The following are five desiderata for success. Seldom are they they all
present in one data mining application.
  1. Data with rich descriptions. For example, wide customer records with many
potentially useful fields allow data mining algorithms to search beyond obvious
correlations.
  2. A large volume of data. The large model spaces corresponding to rich data
demand many training instances to build reliable models.
  3. Controlled and reliable data collection. Manual data entry and integration
from legacy systems both are notoriously problematic; fully automated
collection is considerably better.
  4. The ability to evaluate results. Substantial, demonstrable return on
investment can be very convincing.
  5. Ease of integration with existing processes. Even if pilot studies show
potential benefit, deploying automated solutions to previously manual processes
is rife with pitfalls. Building a system to take advantage of the mined
knowledge can be a substantial undertaking. Furthermore, one often must deal
with social and political issues involved in the automation of a previously
manual business process.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.15001v1,"Circuit-Free General-Purpose Multi-Party Computation via Co-Utile
  Unlinkable Outsourcing","Multiparty computation (MPC) consists in several parties engaging in joint
computation in such a way that each party's input and output remain private to
that party. Whereas MPC protocols for specific computations have existed since
the 1980s, only recently general-purpose compilers have been developed to allow
MPC on arbitrary functions. Yet, using today's MPC compilers requires
substantial programming effort and skill on the user's side, among other things
because nearly all compilers translate the code of the computation into a
Boolean or arithmetic circuit. In particular, the circuit representation
requires unrolling loops and recursive calls, which forces programmers to
(often manually) define loop bounds and hardly use recursion. We present an
approach allowing MPC on an arbitrary computation expressed as ordinary code
with all functionalities that does not need to be translated into a circuit.
Our notion of input and output privacy is predicated on unlinkability. Our
method leverages co-utile computation outsourcing using anonymous channels via
decentralized reputation, makes a minimalistic use of cryptography and does not
require participants to be honest-but-curious: it works as long as participants
are rational (self-interested), which may include rationally malicious peers
(who become attackers if this is advantageous to them). We present example
applications, including e-voting. Our empirical work shows that reputation
captures well the behavior of peers and ensures that parties with high
reputation obtain correct results.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2006.04544v3,Optimal Work Extraction and the Minimum Description Length Principle,"We discuss work extraction from classical information engines (e.g.,
Szil\'ard) with $N$-particles, $q$ partitions, and initial arbitrary
non-equilibrium states. In particular, we focus on their {\em optimal}
behaviour, which includes the measurement of a set of quantities $\Phi$ with a
feedback protocol that extracts the maximal average amount of work. We show
that the optimal non-equilibrium state to which the engine should be driven
before the measurement is given by the normalised maximum-likelihood
probability distribution of a statistical model that admits $\Phi$ as
sufficient statistics. Furthermore, we show that the minimax universal code
redundancy $\mathcal{R}^*$ associated to this model, provides an upper bound to
the work that the demon can extract on average from the cycle, in units of
$k_{\rm B}T$. We also find that, in the limit of $N$ large, the maximum average
extracted work cannot exceed $H[\Phi]/2$, i.e. one half times the Shannon
entropy of the measurement. Our results establish a connection between optimal
work extraction in stochastic thermodynamics and optimal universal data
compression, providing design principles for optimal information engines. In
particular, they suggest that: (i) optimal coding is thermodynamically
efficient, and (ii) it is essential to drive the system into a critical state
in order to achieve optimal performance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1404.0062v1,On redundancy of memoryless sources over countable alphabets,"The minimum average number of bits need to describe a random variable is its
entropy, assuming knowledge of the underlying statistics On the other hand,
universal compression supposes that the distribution of the random variable,
while unknown, belongs to a known set $\cal P$ of distributions. Such universal
descriptions for the random variable are agnostic to the identity of the
distribution in $\cal P$. But because they are not matched exactly to the
underlying distribution of the random variable, the average number of bits they
use is higher, and the excess over the entropy used is the ""redundancy"". This
formulation is fundamental to problems not just in compression, but also
estimation and prediction and has a wide variety of applications from language
modeling to insurance.
  In this paper, we study the redundancy of universal encodings of strings
generated by independent identically distributed (iid) sampling from a set
$\cal P$ of distributions over a countable support. We first show that if
describing a single sample from $\cal P$ incurs finite redundancy, then $\cal
P$ is tight but that the converse does not always hold. If a single sample can
be described with finite worst-case-regret (a more stringent formulation than
redundancy above), then it is known that length-$n$ iid samples only incurs a
diminishing (in $n$) redundancy per symbol as $n$ increases. However, we show
it is possible that a collection $\cal P$ incurs finite redundancy, yet
description of length-$n$ iid samples incurs a constant redundancy per symbol
encoded. We then show a sufficient condition on $\cal P$ such that length-$n$
iid samples will incur diminishing redundancy per symbol encoded.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1708.09662v1,Quality Enhancement by Weighted Rank Aggregation of Crowd Opinion,"Expertise of annotators has a major role in crowdsourcing based opinion
aggregation models. In such frameworks, accuracy and biasness of annotators are
occasionally taken as important features and based on them priority of the
annotators are assigned. But instead of relying on a single feature, multiple
features can be considered and separate rankings can be produced to judge the
annotators properly. Finally, the aggregation of those rankings with perfect
weightage can be done with an aim to produce better ground truth prediction.
Here, we propose a novel weighted rank aggregation method and its efficacy with
respect to other existing approaches is shown on artificial dataset. The
effectiveness of weighted rank aggregation to enhance quality prediction is
also shown by applying it on an Amazon Mechanical Turk (AMT) dataset.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2004.01431v1,"Modeling Rare Interactions in Time Series Data Through Qualitative
  Change: Application to Outcome Prediction in Intensive Care Units","Many areas of research are characterised by the deluge of large-scale
highly-dimensional time-series data. However, using the data available for
prediction and decision making is hampered by the current lag in our ability to
uncover and quantify true interactions that explain the outcomes.We are
interested in areas such as intensive care medicine, which are characterised by
i) continuous monitoring of multivariate variables and non-uniform sampling of
data streams, ii) the outcomes are generally governed by interactions between a
small set of rare events, iii) these interactions are not necessarily definable
by specific values (or value ranges) of a given group of variables, but rather,
by the deviations of these values from the normal state recorded over time, iv)
the need to explain the predictions made by the model. Here, while numerous
data mining models have been formulated for outcome prediction, they are unable
to explain their predictions.
  We present a model for uncovering interactions with the highest likelihood of
generating the outcomes seen from highly-dimensional time series data.
Interactions among variables are represented by a relational graph structure,
which relies on qualitative abstractions to overcome non-uniform sampling and
to capture the semantics of the interactions corresponding to the changes and
deviations from normality of variables of interest over time. Using the
assumption that similar templates of small interactions are responsible for the
outcomes (as prevalent in the medical domains), we reformulate the discovery
task to retrieve the most-likely templates from the data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1610.00082v2,"PTAS for Ordered Instances of Resource Allocation Problems with
  Restrictions on Inclusions","We consider the problem of allocating a set $I$ of $m$ indivisible resources
(items) to a set $P$ of $n$ customers (players) competing for the resources.
Each resource $j \in I$ has a same value $v_j > 0$ for a subset of customers
interested in $j$, and zero value for the remaining customers. The utility
received by each customer is the sum of the values of the resources allocated
to her. The goal is to find a feasible allocation of the resources to the
interested customers such that for the Max-Min allocation problem (Min-Max
allocation problem) the minimum of the utilities (maximum of the utilities)
received by the customers is maximized (minimized). The Max-Min allocation
problem is also known as the \textit{Fair Allocation problem}, or the
\textit{Santa Claus problem}. The Min-Max allocation problem is the problem of
Scheduling on Unrelated Parallel Machines, and is also known as the $R \, | \,
| C_{\max}$ problem.
  In this paper, we are interested in instances of the problem that admit a
Polynomial Time Approximation Scheme (PTAS). We show that an ordering property
on the resources and the customers is important and paves the way for a PTAS.
For the Max-Min allocation problem, we start with instances of the problem that
can be viewed as a \textit{convex bipartite graph}; a bipartite graph for which
there exists an ordering of the resources such that each customer is interested
in (has a positive evaluation for) a set of \textit{consecutive} resources. We
demonstrate a PTAS for the inclusion-free cases. This class of instances is
equivalent to the class of bipartite permutation graphs. For the Min-Max
allocation problem, we also obtain a PTAS for inclusion-free instances. These
instances are not only of theoretical interest but also have practical
applications.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0309044v1,The combinatorics of resource sharing,"We discuss general models of resource-sharing computations, with emphasis on
the combinatorial structures and concepts that underlie the various deadlock
models that have been proposed, the design of algorithms and deadlock-handling
policies, and concurrency issues. These structures are mostly graph-theoretic
in nature, or partially ordered sets for the establishment of priorities among
processes and acquisition orders on resources. We also discuss graph-coloring
concepts as they relate to resource sharing.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1208.3124v6,On the computation of zone and double zone diagrams,"Classical objects in computational geometry are defined by explicit
relations. Several years ago the pioneering works of T. Asano, J. Matousek and
T. Tokuyama introduced ""implicit computational geometry"", in which the
geometric objects are defined by implicit relations involving sets. An
important member in this family is called ""a zone diagram"". The implicit nature
of zone diagrams implies, as already observed in the original works, that their
computation is a challenging task. In a continuous setting this task has been
addressed (briefly) only by these authors in the Euclidean plane with point
sites. We discuss the possibility to compute zone diagrams in a wide class of
spaces and also shed new light on their computation in the original setting.
The class of spaces, which is introduced here, includes, in particular,
Euclidean spheres and finite dimensional strictly convex normed spaces. Sites
of a general form are allowed and it is shown that a generalization of the
iterative method suggested by Asano, Matousek and Tokuyama converges to a
double zone diagram, another implicit geometric object whose existence is known
in general. Occasionally a zone diagram can be obtained from this procedure.
The actual (approximate) computation of the iterations is based on a simple
algorithm which enables the approximate computation of Voronoi diagrams in a
general setting. Our analysis also yields a few byproducts of independent
interest, such as certain topological properties of Voronoi cells (e.g., that
in the considered setting their boundaries cannot be ""fat"").",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1910.05014v1,"Automatic segmentation of texts into units of meaning for reading
  assistance","The emergence of the digital book is a major step forward in providing access
to reading, and therefore often to the common culture and the labour market. By
allowing the enrichment of texts with cognitive crutches, EPub 3 compatible
accessibility formats such as FROG have proven their effectiveness in
alleviating but also reducing dyslexic disorders. In this paper, we show how
Artificial Intelligence and particularly Transfer Learning with Google BERT can
automate the division into units of meaning, and thus facilitate the creation
of enriched digital books at a moderate cost.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/2005.03011v1,Overview of Surgical Simulation,"Motivated by the current demand of clinical governance, surgical simulation
is now a well-established modality for basic skills training and assessment.
The practical deployment of the technique is a multi-disciplinary venture
encompassing areas in engineering, medicine and psychology. This paper provides
an overview of the key topics involved in surgical simulation and associated
technical challenges. The paper discusses the clinical motivation for surgical
simulation, the use of virtual environments for surgical training, model
acquisition and simplification, deformable models, collision detection, tissue
property measurement, haptic rendering and image synthesis. Additional topics
include surgical skill training and assessment metrics as well as challenges
facing the incorporation of surgical simulation into medical education
curricula.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1403.3594v3,"Sparse Polynomial Interpolation Codes and their decoding beyond half the
  minimal distance","We present algorithms performing sparse univariate polynomial interpolation
with errors in the evaluations of the polynomial. Based on the initial work by
Comer, Kaltofen and Pernet [Proc. ISSAC 2012], we define the sparse polynomial
interpolation codes and state that their minimal distance is precisely the
length divided by twice the sparsity. At ISSAC 2012, we have given a decoding
algorithm for as much as half the minimal distance and a list decoding
algorithm up to the minimal distance. Our new polynomial-time list decoding
algorithm uses sub-sequences of the received evaluations indexed by a linear
progression, allowing the decoding for a larger radius, that is, more errors in
the evaluations while returning a list of candidate sparse polynomials. We
quantify this improvement for all typically small values of number of terms and
number of errors, and provide a worst case asymptotic analysis of this
improvement. For instance, for sparsity T = 5 with up to 10 errors we can list
decode in polynomial-time from 74 values of the polynomial with unknown terms,
whereas our earlier algorithm required 2T (E + 1) = 110 evaluations. We then
propose two variations of these codes in characteristic zero, where appropriate
choices of values for the variable yield a much larger minimal distance: the
length minus twice the sparsity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1709.01670v3,Parameterized complexity of machine scheduling: 15 open problems,"Machine scheduling problems are a long-time key domain of algorithms and
complexity research. A novel approach to machine scheduling problems are
fixed-parameter algorithms. To stimulate this thriving research direction, we
propose 15 open questions in this area whose resolution we expect to lead to
the discovery of new approaches and techniques both in scheduling and
parameterized complexity theory.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1905.06118v2,Learning to Groove with Inverse Sequence Transformations,"We explore models for translating abstract musical ideas (scores, rhythms)
into expressive performances using Seq2Seq and recurrent Variational
Information Bottleneck (VIB) models. Though Seq2Seq models usually require
painstakingly aligned corpora, we show that it is possible to adapt an approach
from the Generative Adversarial Network (GAN) literature (e.g. Pix2Pix (Isola
et al., 2017) and Vid2Vid (Wang et al. 2018a)) to sequences, creating large
volumes of paired data by performing simple transformations and training
generative models to plausibly invert these transformations. Music, and
drumming in particular, provides a strong test case for this approach because
many common transformations (quantization, removing voices) have clear
semantics, and models for learning to invert them have real-world applications.
Focusing on the case of drum set players, we create and release a new dataset
for this purpose, containing over 13 hours of recordings by professional
drummers aligned with fine-grained timing and dynamics information. We also
explore some of the creative potential of these models, including demonstrating
improvements on state-of-the-art methods for Humanization (instantiating a
performance from a musical score).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1503.07073v3,Overhauling SC Atomics in C11 and OpenCL,"Despite the conceptual simplicity of sequential consistency (SC), the
semantics of SC atomic operations and fences in the C11 and OpenCL memory
models is subtle, leading to convoluted prose descriptions that translate to
complex axiomatic formalisations. We conduct an overhaul of SC atomics in C11,
reducing the associated axioms in both number and complexity. A consequence of
our simplification is that the SC operations in an execution no longer need to
be totally ordered. This relaxation enables, for the first time, efficient and
exhaustive simulation of litmus tests that use SC atomics. We extend our
improved C11 model to obtain the first rigorous memory model formalisation for
OpenCL (which extends C11 with support for heterogeneous many-core
programming). In the OpenCL setting, we refine the SC axioms still further to
give a sensible semantics to SC operations that employ a 'memory scope' to
restrict their visibility to specific threads. Our overhaul requires slight
strengthenings of both the C11 and the OpenCL memory models, causing some
behaviours to become disallowed. We argue that these strengthenings are
natural, and that all of the formalised C11 and OpenCL compilation schemes of
which we are aware (Power and x86 CPUs for C11, AMD GPUs for OpenCL) remain
valid in our revised models. Using the Herd memory model simulator, we show
that our overhaul leads to an exponential improvement in simulation time for
C11 litmus tests compared with the original model, making exhaustive simulation
competitive, time-wise, with the non-exhaustive CDSChecker tool.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9905011v1,"Ensembles of Radial Basis Function Networks for Spectroscopic Detection
  of Cervical Pre-Cancer","The mortality related to cervical cancer can be substantially reduced through
early detection and treatment. However, current detection techniques, such as
Pap smear and colposcopy, fail to achieve a concurrently high sensitivity and
specificity.
  In vivo fluorescence spectroscopy is a technique which quickly,
non-invasively and quantitatively probes the biochemical and morphological
changes that occur in pre-cancerous tissue.
  A multivariate statistical algorithm was used to extract clinically useful
information from tissue spectra acquired from 361 cervical sites from 95
patients at 337, 380 and 460 nm excitation wavelengths. The multivariate
statistical analysis was also employed to reduce the number of fluorescence
excitation-emission wavelength pairs required to discriminate healthy tissue
samples from pre-cancerous tissue samples. The use of connectionist methods
such as multi layered perceptrons, radial basis function networks, and
ensembles of such networks was investigated. RBF ensemble algorithms based on
fluorescence spectra potentially provide automated, and near real-time
implementation of pre-cancer detection in the hands of non-experts. The results
are more reliable, direct and accurate than those achieved by either human
experts or multivariate statistical algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1512.05814v1,General Framework for Evaluating Password Complexity and Strength,"Although it is common for users to select bad passwords that can be easily
cracked by attackers, password-based authentication remains the most
widely-used method. To encourage users to select good passwords, enterprises
often enforce policies. Such policies have been proven to be ineffectual in
practice. Accurate assessment of a password's resistance to cracking attacks is
still an unsolved problem, and our work addresses this challenge. Although the
best way to determine how difficult it may be to crack a user-selected password
is to check its resistance to cracking attacks employed by attackers in the
wild, implementing such a strategy at an enterprise would be infeasible in
practice. We first formalize the concepts of password complexity and strength
with concrete definitions emphasizing their differences. Our framework captures
human biases and many known techniques attackers use to recover stolen
credentials in real life, such as brute-force attacks. Building on our
definitions, we develop a general framework for calculating password complexity
and strength that could be used in practice. Our approach is based on the key
insight that an attacker's success at cracking a password must be defined by
its available computational resources, time, function used to store that
password, as well as the topology that bounds that attacker's search space
based on that attacker's available inputs, transformations it can use to tweak
and explore its inputs, and the path of exploration which can be based on the
attacker's perceived probability of success. We also provide a general
framework for assessing the accuracy of password complexity and strength
estimators that can be used to compare other tools available in the wild.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/cs/0203012v1,Interface agents: A review of the field,"This paper reviews the origins of interface agents, discusses challenges that
exist within the interface agent field and presents a survey of current
attempts to find solutions to these challenges. A history of agent systems from
their birth in the 1960's to the current day is described, along with the
issues they try to address. A taxonomy of interface agent systems is presented,
and today's agent systems categorized accordingly. Lastly, an analysis of the
machine learning and user modelling techniques used by today's agents is
presented.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0608036v1,Reversal Complexity Revisited,"We study a generalized version of reversal bounded Turing machines where,
apart from several tapes on which the number of head reversals is bounded by
r(n), there are several further tapes on which head reversals remain
unrestricted, but size is bounded by s(n). Recently, such machines were
introduced as a formalization of a computation model that restricts random
access to external memory and internal memory space. Here, each of the tapes
with a restriction on the head reversals corresponds to an external memory
device, and the tapes of restricted size model internal memory. We use
ST(r(n),s(n),O(1)) to denote the class of all problems that can be solved by
deterministic Turing machines that comply to the above resource bounds.
Similarly, NST and RST, respectively, are used for the corresponding
nondeterministic and randomized classes.
  While previous papers focused on lower bounds for particular problems,
including sorting, the set equality problem, and several query evaluation
problems, the present paper addresses the relations between the (R,N)ST-classes
and classical complexity classes and investigates the structural complexity of
the (R,N)ST-classes. Our main results are (1) a trade-off between internal
memory space and external memory head reversals, (2) correspondences between
the (R,N)ST-classes and ``classical'' time-bounded, space-bounded,
reversal-bounded, and circuit complexity classes, and (3) hierarchies of
(R)ST-classes in terms of increasing numbers of head reversals on external
memory tapes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1304.4974v1,Fast exact digital differential analyzer for circle generation,"In the first part of the paper we present a short review of applications of
digital differential analyzers (DDA) to generation of circles showing that they
can be treated as one-step numerical schemes. In the second part we present and
discuss a novel fast algorithm based on a two-step numerical scheme (explicit
midpoint rule). Although our algorithm is as cheap as the simplest one-step DDA
algoritm (and can be represented in terms of shifts and additions), it
generates circles with maximal accuracy, i.e., it is exact up to round-off
errors.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1410.8233v1,Do Artificial Reinforcement-Learning Agents Matter Morally?,"Artificial reinforcement learning (RL) is a widely used technique in
artificial intelligence that provides a general method for training agents to
perform a wide variety of behaviours. RL as used in computer science has
striking parallels to reward and punishment learning in animal and human
brains. I argue that present-day artificial RL agents have a very small but
nonzero degree of ethical importance. This is particularly plausible for views
according to which sentience comes in degrees based on the abilities and
complexities of minds, but even binary views on consciousness should assign
nonzero probability to RL programs having morally relevant experiences. While
RL programs are not a top ethical priority today, they may become more
significant in the coming decades as RL is increasingly applied to industry,
robotics, video games, and other areas. I encourage scientists, philosophers,
and citizens to begin a conversation about our ethical duties to reduce the
harm that we inflict on powerless, voiceless RL agents.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/cs/0506004v4,Non-asymptotic calibration and resolution,"We analyze a new algorithm for probability forecasting of binary observations
on the basis of the available data, without making any assumptions about the
way the observations are generated. The algorithm is shown to be well
calibrated and to have good resolution for long enough sequences of
observations and for a suitable choice of its parameter, a kernel on the
Cartesian product of the forecast space $[0,1]$ and the data space. Our main
results are non-asymptotic: we establish explicit inequalities, shown to be
tight, for the performance of the algorithm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1508.07859v1,Multi-Projector Color Structured-Light Vision,"Research interest in rapid structured-light imaging has grown increasingly
for the modeling of moving objects, and a number of methods have been suggested
for the range capture in a single video frame. The imaging area of a 3D object
using a single projector is restricted since the structured light is projected
only onto a limited area of the object surface. Employing additional projectors
to broaden the imaging area is a challenging problem since simultaneous
projection of multiple patterns results in their superposition in the
light-intersected areas and the recognition of original patterns is by no means
trivial. This paper presents a novel method of multi-projector color
structured-light vision based on projector-camera triangulation. By analyzing
the behavior of superposed-light colors in a chromaticity domain, we show that
the original light colors cannot be properly extracted by the conventional
direct estimation. We disambiguate multiple projectors by multiplexing the
orientations of projector patterns so that the superposed patterns can be
separated by explicit derivative computations. Experimental studies are carried
out to demonstrate the validity of the presented method. The proposed method
increases the efficiency of range acquisition compared to conventional active
stereo using multiple projectors.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0701032v4,Polygraphic programs and polynomial-time functions,"We study the computational model of polygraphs. For that, we consider
polygraphic programs, a subclass of these objects, as a formal description of
first-order functional programs. We explain their semantics and prove that they
form a Turing-complete computational model. Their algebraic structure is used
by analysis tools, called polygraphic interpretations, for complexity analysis.
In particular, we delineate a subclass of polygraphic programs that compute
exactly the functions that are Turing-computable in polynomial time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.13359v1,"Correctly Implementing Synchronous Message Passing in the Pi-Calculus By
  Concurrent Haskell's MVars","Comparison of concurrent programming languages and correctness of program
transformations in concurrency are the focus of this research. As criterion we
use contextual semantics adapted to concurrency, where may -- as well as should
-- convergence are observed. We investigate the relation between the
synchronous pi-calculus and a core language of Concurrent Haskell (CH). The
contextual semantics is on the one hand forgiving with respect to the details
of the operational semantics, and on the other hand implies strong requirements
for the interplay between the processes after translation. Our result is that
CH embraces the synchronous pi-calculus. Our main task is to find and prove
correctness of encodings of pi-calculus channels by CH's concurrency
primitives, which are MVars. They behave like (blocking) 1-place buffers
modelling the shared-memory. The first developed translation uses an extra
private MVar for every communication.We also automatically generate and check
potentially correct translations that reuse the MVars where one MVar contains
the message and two additional MVars for synchronization are used to model the
synchronized communication of a single channel in the pi-calculus.Our automated
experimental results lead to the conjecture that one additional MVar is
insufficient.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0502058v2,The Complexity of Computing the Size of an Interval,"Given a p-order A over a universe of strings (i.e., a transitive, reflexive,
antisymmetric relation such that if (x, y) is an element of A then |x| is
polynomially bounded by |y|), an interval size function of A returns, for each
string x in the universe, the number of strings in the interval between strings
b(x) and t(x) (with respect to A), where b(x) and t(x) are functions that are
polynomial-time computable in the length of x.
  By choosing sets of interval size functions based on feasibility requirements
for their underlying p-orders, we obtain new characterizations of complexity
classes. We prove that the set of all interval size functions whose underlying
p-orders are polynomial-time decidable is exactly #P. We show that the interval
size functions for orders with polynomial-time adjacency checks are closely
related to the class FPSPACE(poly). Indeed, FPSPACE(poly) is exactly the class
of all nonnegative functions that are an interval size function minus a
polynomial-time computable function.
  We study two important functions in relation to interval size functions. The
function #DIV maps each natural number n to the number of nontrivial divisors
of n. We show that #DIV is an interval size function of a polynomial-time
decidable partial p-order with polynomial-time adjacency checks. The function
#MONSAT maps each monotone boolean formula F to the number of satisfying
assignments of F. We show that #MONSAT is an interval size function of a
polynomial-time decidable total p-order with polynomial-time adjacency checks.
  Finally, we explore the related notion of cluster computation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.10256v1,Measuring the impact of spammers on e-mail and Twitter networks,"This paper investigates the research question if senders of large amounts of
irrelevant or unsolicited information - commonly called ""spammers"" - distort
the network structure of social networks. Two large social networks are
analyzed, the first extracted from the Twitter discourse about a big
telecommunication company, and the second obtained from three years of email
communication of 200 managers working for a large multinational company. This
work compares network robustness and the stability of centrality and
interaction metrics, as well as the use of language, after removing spammers
and the most and least connected nodes. The results show that spammers do not
significantly alter the structure of the information-carrying network, for most
of the social indicators. The authors additionally investigate the correlation
between e-mail subject line and content by tracking language sentiment,
emotionality, and complexity, addressing the cases where collecting email
bodies is not permitted for privacy reasons. The findings extend the research
about robustness and stability of social networks metrics, after the
application of graph simplification strategies. The results have practical
implication for network analysts and for those company managers who rely on
network analytics (applied to company emails and social media data) to support
their decision-making processes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.01136v2,Improved Guarantees for Vertex Sparsification in Planar Graphs,"Graph Sparsification aims at compressing large graphs into smaller ones while
preserving important characteristics of the input graph. In this work we study
Vertex Sparsifiers, i.e., sparsifiers whose goal is to reduce the number of
vertices. We focus on the following notions:
  (1) Given a digraph $G=(V,E)$ and terminal vertices $K \subset V$ with $|K| =
k$, a (vertex) reachability sparsifier of $G$ is a digraph $H=(V_H,E_H)$, $K
\subset V_H$ that preserves all reachability information among terminal pairs.
In this work we introduce the notion of reachability-preserving minors (RPMs) ,
i.e., we require $H$ to be a minor of $G$. We show any directed graph $G$
admits a RPM $H$ of size $O(k^3)$, and if $G$ is planar, then the size of $H$
improves to $O(k^{2} \log k)$. We complement our upper-bound by showing that
there exists an infinite family of grids such that any RPM must have
$\Omega(k^{2})$ vertices.
  (2) Given a weighted undirected graph $G=(V,E)$ and terminal vertices $K$
with $|K|=k$, an exact (vertex) cut sparsifier of $G$ is a graph $H$ with $K
\subset V_H$ that preserves the value of minimum-cuts separating any
bipartition of $K$. We show that planar graphs with all the $k$ terminals lying
on the same face admit exact cut sparsifiers of size $O(k^{2})$ that are also
planar. Our result extends to flow and distance sparsifiers. It improves the
previous best-known bound of $O(k^22^{2k})$ for cut and flow sparsifiers by an
exponential factor, and matches an $\Omega(k^2)$ lower-bound for this class of
graphs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1607.04763v1,"Design and implementation of computational platform for social-humanoid
  robot Lumen as an exhibition guide in Electrical Engineering Days 2015","Social Robot Lumen is an Artificial Intelligence development project that
aims to create an Artificial Intelligence (AI) which allows a humanoid robot to
communicate with human being naturally. In this study, Lumen will be developed
to be a tour guide in Electrical Engineering Days 2015 exhibition. In
developing an AI, there are a lot of modules that need to be developed
separately. To make the development easier, we need a computational platform
which becomes basis for all developers to give easiness in developing the
modules in parallel way. That computational platform that developed by the
writer is called Lumen Server. Lumen Server has two main function, which are to
be a bridge between all Lumen intelligence modules with NAO robot, and to be
the communication bridge between those Lumen intelligence modules. For the
second function, Lumen Server implements the AMQP protocol using RabbitMQ.
Besides that, writer also developed a control system for robot movement called
Lumen Motion. Lumen motion is implemented by modelling the movement of NAO
robot and also by creating a control system using fuzzy logic controller.
Writer also developed a program that connects all Lumen intelligence modules so
that Lumen can act like a tour guide. The implementation of this program uses
FSM and event-driven program. From implementation result, all the features
which were designed are successfully implemented. By the developing of this
computational platform, it can ease the development of Lumen in the future. For
next development, it must be focused on creating integration system so that
Lumen can be more responsive to the environment.
  -----
  Sosial Robot Lumen adalah proyek pengembangan kecerdasan buatan yang
bertujuan untuk menciptakan kecerdasan buatan atau artificial intelligence (AI)
yang memungkinkan robot untuk dapat berkomunikasi dengan manusia secara alami.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1602.04552v1,"Extending the Nested Parallel Model to the Nested Dataflow Model with
  Provably Efficient Schedulers","The nested parallel (a.k.a. fork-join) model is widely used for writing
parallel programs. However, the two composition constructs, i.e. ""$\parallel$""
(parallel) and ""$;$"" (serial), are insufficient in expressing ""partial
dependencies"" or ""partial parallelism"" in a program. We propose a new dataflow
composition construct ""$\leadsto$"" to express partial dependencies in
algorithms in a processor- and cache-oblivious way, thus extending the Nested
Parallel (NP) model to the \emph{Nested Dataflow} (ND) model. We redesign
several divide-and-conquer algorithms ranging from dense linear algebra to
dynamic-programming in the ND model and prove that they all have optimal span
while retaining optimal cache complexity. We propose the design of runtime
schedulers that map ND programs to multicore processors with multiple levels of
possibly shared caches (i.e, Parallel Memory Hierarchies) and provide
theoretical guarantees on their ability to preserve locality and load balance.
For this, we adapt space-bounded (SB) schedulers for the ND model. We show that
our algorithms have increased ""parallelizability"" in the ND model, and that SB
schedulers can use the extra parallelizability to achieve asymptotically
optimal bounds on cache misses and running time on a greater number of
processors than in the NP model. The running time for the algorithms in this
paper is $O\left(\frac{\sum_{i=0}^{h-1} Q^{*}({\mathsf t};\sigma\cdot M_i)\cdot
C_i}{p}\right)$, where $Q^{*}$ is the cache complexity of task ${\mathsf t}$,
$C_i$ is the cost of cache miss at level-$i$ cache which is of size $M_i$,
$\sigma\in(0,1)$ is a constant, and $p$ is the number of processors in an
$h$-level cache hierarchy.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.03997v2,MeshSDF: Differentiable Iso-Surface Extraction,"Geometric Deep Learning has recently made striking progress with the advent
of continuous Deep Implicit Fields. They allow for detailed modeling of
watertight surfaces of arbitrary topology while not relying on a 3D Euclidean
grid, resulting in a learnable parameterization that is not limited in
resolution.
  Unfortunately, these methods are often not suitable for applications that
require an explicit mesh-based surface representation because converting an
implicit field to such a representation relies on the Marching Cubes algorithm,
which cannot be differentiated with respect to the underlying implicit field.
  In this work, we remove this limitation and introduce a differentiable way to
produce explicit surface mesh representations from Deep Signed Distance
Functions. Our key insight is that by reasoning on how implicit field
perturbations impact local surface geometry, one can ultimately differentiate
the 3D location of surface samples with respect to the underlying deep implicit
field. We exploit this to define MeshSDF, an end-to-end differentiable mesh
representation which can vary its topology.
  We use two different applications to validate our theoretical insight:
Single-View Reconstruction via Differentiable Rendering and Physically-Driven
Shape Optimization. In both cases our differentiable parameterization gives us
an edge over state-of-the-art algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0312020v1,Modeling Object Oriented Constraint Programs in Z,"Object oriented constraint programs (OOCPs) emerge as a leading evolution of
constraint programming and artificial intelligence, first applied to a range of
industrial applications called configuration problems. The rich variety of
technical approaches to solving configuration problems (CLP(FD), CC(FD), DCSP,
Terminological systems, constraint programs with set variables ...) is a source
of difficulty. No universally accepted formal language exists for communicating
about OOCPs, which makes the comparison of systems difficult. We present here a
Z based specification of OOCPs which avoids the falltrap of hidden object
semantics. The object system is part of the specification, and captures all of
the most advanced notions from the object oriented modeling standard UML. The
paper illustrates these issues and the conciseness and precision of Z by the
specification of a working OOCP that solves an historical AI problem : parsing
a context free grammar. Being written in Z, an OOCP specification also supports
formal proofs. The whole builds the foundation of an adaptative and evolving
framework for communicating about constrained object models and programs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1106.4333v1,Residual Component Analysis,"Probabilistic principal component analysis (PPCA) seeks a low dimensional
representation of a data set in the presence of independent spherical Gaussian
noise, Sigma = (sigma^2)*I. The maximum likelihood solution for the model is an
eigenvalue problem on the sample covariance matrix. In this paper we consider
the situation where the data variance is already partially explained by other
factors, e.g. covariates of interest, or temporal correlations leaving some
residual variance. We decompose the residual variance into its components
through a generalized eigenvalue problem, which we call residual component
analysis (RCA). We show that canonical covariates analysis (CCA) is a special
case of our algorithm and explore a range of new algorithms that arise from the
framework. We illustrate the ideas on a gene expression time series data set
and the recovery of human pose from silhouette.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0406042v1,Business Process Measures,"The paper proposes a new methodology for defining business process measures
and their computation. The approach is based on metamodeling according to MOF.
Especially, a metamodel providing precise definitions of typical process
measures for UML activity diagram-like notation is proposed, including precise
definitions how measures should be aggregated for composite process elements.
The proposed approach allows defining values in a natural way, and measurement
of data, which are of interest to business, without deep investigation into
specific technical solutions. This provides new possibilities for business
process measurement, decreasing the gap between technical solutions and asset
management methodologies.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.06051v2,The Power of Two Choices in Graphical Allocation,"The graphical balls-into-bins process is a generalization of the classical
2-choice balls-into-bins process, where the bins correspond to vertices of an
arbitrary underlying graph $G$. At each time step an edge of $G$ is chosen
uniformly at random, and a ball must be assigned to either of the two endpoints
of this edge. The standard 2-choice process corresponds to the case of $G=K_n$.
  For any $k(n)$-edge-connected, $d(n)$-regular graph on $n$ vertices, and any
number of balls, we give an allocation strategy that, with high probability,
ensures a gap of $O((d/k) \log^4\hspace{-1pt}n \log \log n)$, between the load
of any two bins. In particular, this implies polylogarithmic bounds for natural
graphs such as cycles and tori, for which the classical greedy allocation
strategy is conjectured to have a polynomial gap between the bin loads. For
every graph $G$, we also show an $\Omega((d/k) + \log n)$ lower bound on the
gap achievable by any allocation strategy. This implies that our strategy
achieves the optimal gap, up to polylogarithmic factors, for every graph $G$.
  Our allocation algorithm is simple to implement and requires only
$O(\log(n))$ time per allocation. It can be viewed as a more global version of
the greedy strategy that compares average load on certain fixed sets of
vertices, rather than on individual vertices. A key idea is to relate the
problem of designing a good allocation strategy to that of finding suitable
multi-commodity flows. To this end, we consider R\""{a}cke's cut-based
decomposition tree and define certain orthogonal flows on it.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1105.1424v1,"DNA Circuits Based on Isothermal Constrained Loop Extension DNA
  Amplification","In this paper, we first describe the isothermal constrained loop extension
DNA amplification (ICLEDA), which is a new variant of amplification combining
the advantages of rolling circle amplification (RCA) and of strand displacement
amplification (SDA). Then, we formalize this process in terms of the theory of
formal languages and show, on the basis of this formulation, how to manage OR
and AND gates. We then explain how to introduce negation, which allows us to
prove that, in principle, it is possible to implement the computation of any
boolean function on DNA strands using ICLEDA.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0511035v2,Decoding the structure of the WWW: facts versus sampling biases,"The understanding of the immense and intricate topological structure of the
World Wide Web (WWW) is a major scientific and technological challenge. This
has been tackled recently by characterizing the properties of its
representative graphs in which vertices and directed edges are identified with
web-pages and hyperlinks, respectively. Data gathered in large scale crawls
have been analyzed by several groups resulting in a general picture of the WWW
that encompasses many of the complex properties typical of rapidly evolving
networks. In this paper, we report a detailed statistical analysis of the
topological properties of four different WWW graphs obtained with different
crawlers. We find that, despite the very large size of the samples, the
statistical measures characterizing these graphs differ quantitatively, and in
some cases qualitatively, depending on the domain analyzed and the crawl used
for gathering the data. This spurs the issue of the presence of sampling biases
and structural differences of Web crawls that might induce properties not
representative of the actual global underlying graph. In order to provide a
more accurate characterization of the Web graph and identify observables which
are clearly discriminating with respect to the sampling process, we study the
behavior of degree-degree correlation functions and the statistics of
reciprocal connections. The latter appears to enclose the relevant correlations
of the WWW graph and carry most of the topological information of theWeb. The
analysis of this quantity is also of major interest in relation to the
navigability and searchability of the Web.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1208.5933v1,TLA+ Proofs,"TLA+ is a specification language based on standard set theory and temporal
logic that has constructs for hierarchical proofs. We describe how to write
TLA+ proofs and check them with TLAPS, the TLA+ Proof System. We use Peterson's
mutual exclusion algorithm as a simple example to describe the features of
TLAPS and show how it and the Toolbox (an IDE for TLA+) help users to manage
large, complex proofs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.01219v1,Learning Stylometric Representations for Authorship Analysis,"Authorship analysis (AA) is the study of unveiling the hidden properties of
authors from a body of exponentially exploding textual data. It extracts an
author's identity and sociolinguistic characteristics based on the reflected
writing styles in the text. It is an essential process for various areas, such
as cybercrime investigation, psycholinguistics, political socialization, etc.
However, most of the previous techniques critically depend on the manual
feature engineering process. Consequently, the choice of feature set has been
shown to be scenario- or dataset-dependent. In this paper, to mimic the human
sentence composition process using a neural network approach, we propose to
incorporate different categories of linguistic features into distributed
representation of words in order to learn simultaneously the writing style
representations based on unlabeled texts for authorship analysis. In
particular, the proposed models allow topical, lexical, syntactical, and
character-level feature vectors of each document to be extracted as
stylometrics. We evaluate the performance of our approach on the problems of
authorship characterization and authorship verification with the Twitter,
novel, and essay datasets. The experiments suggest that our proposed text
representation outperforms the bag-of-lexical-n-grams, Latent Dirichlet
Allocation, Latent Semantic Analysis, PVDM, PVDBOW, and word2vec
representations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1501.07250v1,FMAP: Distributed Cooperative Multi-Agent Planning,"This paper proposes FMAP (Forward Multi-Agent Planning), a fully-distributed
multi-agent planning method that integrates planning and coordination. Although
FMAP is specifically aimed at solving problems that require cooperation among
agents, the flexibility of the domain-independent planning model allows FMAP to
tackle multi-agent planning tasks of any type. In FMAP, agents jointly explore
the plan space by building up refinement plans through a complete and flexible
forward-chaining partial-order planner. The search is guided by $h_{DTG}$, a
novel heuristic function that is based on the concepts of Domain Transition
Graph and frontier state and is optimized to evaluate plans in distributed
environments. Agents in FMAP apply an advanced privacy model that allows them
to adequately keep private information while communicating only the data of the
refinement plans that is relevant to each of the participating agents.
Experimental results show that FMAP is a general-purpose approach that
efficiently solves tightly-coupled domains that have specialized agents and
cooperative goals as well as loosely-coupled problems. Specifically, the
empirical evaluation shows that FMAP outperforms current MAP systems at solving
complex planning tasks that are adapted from the International Planning
Competition benchmarks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.05653v1,"An Ontology-driven Treatment Article Retrieval System for Precision
  Oncology","This paper presents an ontology-driven treatment article retrieval system
developed and experimented using the data and ground truths provided by the
TREC 2017 precision medicine track. The key aspects of our system include:
meaningful integration of various disease, gene, and drug name ontologies,
training of a novel perceptron model for article relevance labeling, a ranking
module that considers additional factors such as journal impact and article
publication year, and comprehensive query matching rules. Experimental results
demonstrate that our proposed system considerably outperforms the results of
the best participating system of the TREC 2017 precision medicine challenge.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1902.01219v3,Local minimax rates for closeness testing of discrete distributions,"We consider the closeness testing problem for discrete distributions. The
goal is to distinguish whether two samples are drawn from the same unspecified
distribution, or whether their respective distributions are separated in
$L_1$-norm. In this paper, we focus on adapting the rate to the shape of the
underlying distributions, i.e. we consider \textit{a local minimax setting}. We
provide, to the best of our knowledge, the first local minimax rate for the
separation distance up to logarithmic factors, together with a test that
achieves it. In view of the rate, closeness testing turns out to be
substantially harder than the related one-sample testing problem over a wide
range of cases.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1001.0441v1,Semantic Modeling and Retrieval of Dance Video Annotations,"Dance video is one of the important types of narrative videos with semantic
rich content. This paper proposes a new meta model, Dance Video Content Model
(DVCM) to represent the expressive semantics of the dance videos at multiple
granularity levels. The DVCM is designed based on the concepts such as video,
shot, segment, event and object, which are the components of MPEG-7 MDS. This
paper introduces a new relationship type called Temporal Semantic Relationship
to infer the semantic relationships between the dance video objects. Inverted
file based index is created to reduce the search time of the dance queries. The
effectiveness of containment queries using precision and recall is depicted.
Keywords: Dance Video Annotations, Effectiveness Metrics, Metamodeling,
Temporal Semantic Relationships.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1303.1152v2,An Equivalence between the Lasso and Support Vector Machines,"We investigate the relation of two fundamental tools in machine learning and
signal processing, that is the support vector machine (SVM) for classification,
and the Lasso technique used in regression. We show that the resulting
optimization problems are equivalent, in the following sense. Given any
instance of an $\ell_2$-loss soft-margin (or hard-margin) SVM, we construct a
Lasso instance having the same optimal solutions, and vice versa.
  As a consequence, many existing optimization algorithms for both SVMs and
Lasso can also be applied to the respective other problem instances. Also, the
equivalence allows for many known theoretical insights for SVM and Lasso to be
translated between the two settings. One such implication gives a simple
kernelized version of the Lasso, analogous to the kernels used in the SVM
setting. Another consequence is that the sparsity of a Lasso solution is equal
to the number of support vectors for the corresponding SVM instance, and that
one can use screening rules to prune the set of support vectors. Furthermore,
we can relate sublinear time algorithms for the two problems, and give a new
such algorithm variant for the Lasso. We also study the regularization paths
for both methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.10401v1,"Multi-Attention-Based Soft Partition Network for Vehicle
  Re-Identification","Vehicle re-identification (Re-ID) distinguishes between the same vehicle and
other vehicles in images. It is challenging due to significant intra-instance
differences between identical vehicles from different views and subtle
inter-instance differences of similar vehicles. Researchers have tried to
address this problem by extracting features robust to variations of viewpoints
and environments. More recently, they tried to improve performance by using
additional metadata such as key points, orientation, and temporal information.
Although these attempts have been relatively successful, they all require
expensive annotations. Therefore, this paper proposes a novel deep neural
network called a multi-attention-based soft partition (MUSP) network to solve
this problem. This network does not use metadata and only uses multiple soft
attentions to identify a specific vehicle area. This function was performed by
metadata in previous studies. Experiments verified that MUSP achieved
state-of-the-art (SOTA) performance for the VehicleID dataset without any
additional annotations and was comparable to VeRi-776 and VERI-Wild.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1110.4034v1,Topological Logics with Connectedness over Euclidean Spaces,"We consider the quantifier-free languages, Bc and Bc0, obtained by augmenting
the signature of Boolean algebras with a unary predicate representing,
respectively, the property of being connected, and the property of having a
connected interior. These languages are interpreted over the regular closed
sets of n-dimensional Euclidean space (n greater than 1) and, additionally,
over the regular closed polyhedral sets of n-dimensional Euclidean space. The
resulting logics are examples of formalisms that have recently been proposed in
the Artificial Intelligence literature under the rubric ""Qualitative Spatial
Reasoning."" We prove that the satisfiability problem for Bc is undecidable over
the regular closed polyhedra in all dimensions greater than 1, and that the
satisfiability problem for both languages is undecidable over both the regular
closed sets and the regular closed polyhedra in the Euclidean plane. However,
we also prove that the satisfiability problem for Bc0 is NP-complete over the
regular closed sets in all dimensions greater than 2, while the corresponding
problem for the regular closed polyhedra is ExpTime-complete. Our results show,
in particular, that spatial reasoning over Euclidean spaces is much harder than
reasoning over arbitrary topological spaces.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0212036v1,Myths and Legends of the Baldwin Effect,"This position paper argues that the Baldwin effect is widely misunderstood by
the evolutionary computation community. The misunderstandings appear to fall
into two general categories. Firstly, it is commonly believed that the Baldwin
effect is concerned with the synergy that results when there is an evolving
population of learning individuals. This is only half of the story. The full
story is more complicated and more interesting. The Baldwin effect is concerned
with the costs and benefits of lifetime learning by individuals in an evolving
population. Several researchers have focussed exclusively on the benefits, but
there is much to be gained from attention to the costs. This paper explains the
two sides of the story and enumerates ten of the costs and benefits of lifetime
learning by individuals in an evolving population. Secondly, there is a cluster
of misunderstandings about the relationship between the Baldwin effect and
Lamarckian inheritance of acquired characteristics. The Baldwin effect is not
Lamarckian. A Lamarckian algorithm is not better for most evolutionary
computing problems than a Baldwinian algorithm. Finally, Lamarckian inheritance
is not a better model of memetic (cultural) evolution than the Baldwin effect.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1308.3174v1,"Communication Network Design: Balancing Modularity and Mixing via
  Optimal Graph Spectra","By leveraging information technologies, organizations now have the ability to
design their communication networks and crowdsourcing platforms to pursue
various performance goals, but existing research on network design does not
account for the specific features of social networks, such as the notion of
teams. We fill this gap by demonstrating how desirable aspects of
organizational structure can be mapped parsimoniously onto the spectrum of the
graph Laplacian allowing the specification of structural objectives and build
on recent advances in non-convex programming to optimize them. This design
framework is general, but we focus here on the problem of creating graphs that
balance high modularity and low mixing time, and show how ""liaisons"" rather
than brokers maximize this objective.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1309.1257v1,Continuation calculus,"Programs with control are usually modeled using lambda calculus extended with
control operators. Instead of modifying lambda calculus, we consider a
different model of computation. We introduce continuation calculus, or CC, a
deterministic model of computation that is evaluated using only head reduction,
and argue that it is suitable for modeling programs with control. It is
demonstrated how to define programs, specify them, and prove them correct. This
is shown in detail by presenting in CC a list multiplication program that
prematurely returns when it encounters a zero. The correctness proof includes
termination of the program. In continuation calculus we can model both
call-by-name and call-by-value. In addition, call-by-name functions can be
applied to call-by-value results, and conversely.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1209.4379v1,Defining Quantum Control Flow,"A remarkable difference between quantum and classical programs is that the
control flow of the former can be either classical or quantum. One of the key
issues in the theory of quantum programming languages is defining and
understanding quantum control flow. A functional language with quantum control
flow was defined by Altenkirch and Grattage [\textit{Proc. LICS'05}, pp.
249-258]. This paper extends their work, and we introduce a general quantum
control structure by defining three new quantum program constructs, namely
quantum guarded command, quantum choice and quantum recursion. We clarify the
relation between quantum choices and probabilistic choices. An interesting
difference between quantum recursions with classical control flows and with
quantum control flows is revealed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.04966v1,"Towards Explainable Abnormal Infant Movements Identification: A
  Body-part Based Prediction and Visualisation Framework","Providing early diagnosis of cerebral palsy (CP) is key to enhancing the
developmental outcomes for those affected. Diagnostic tools such as the General
Movements Assessment (GMA), have produced promising results in early diagnosis,
however these manual methods can be laborious.
  In this paper, we propose a new framework for the automated classification of
infant body movements, based upon the GMA, which unlike previous methods, also
incorporates a visualization framework to aid with interpretability. Our
proposed framework segments extracted features to detect the presence of
Fidgety Movements (FMs) associated with the GMA spatiotemporally. These
features are then used to identify the body-parts with the greatest
contribution towards a classification decision and highlight the related
body-part segment providing visual feedback to the user.
  We quantitatively compare the proposed framework's classification performance
with several other methods from the literature and qualitatively evaluate the
visualization's veracity. Our experimental results show that the proposed
method performs more robustly than comparable techniques in this setting whilst
simultaneously providing relevant visual interpretability.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0505083v1,Defensive forecasting,"We consider how to make probability forecasts of binary labels. Our main
mathematical result is that for any continuous gambling strategy used for
detecting disagreement between the forecasts and the actual labels, there
exists a forecasting strategy whose forecasts are ideal as far as this gambling
strategy is concerned. A forecasting strategy obtained in this way from a
gambling strategy demonstrating a strong law of large numbers is simplified and
studied empirically.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1710.03666v3,"A categorical foundation for structured reversible flowchart languages:
  Soundness and adequacy","Structured reversible flowchart languages is a class of imperative reversible
programming languages allowing for a simple diagrammatic representation of
control flow built from a limited set of control flow structures. This class
includes the reversible programming language Janus (without recursion), as well
as more recently developed reversible programming languages such as R-CORE and
R-WHILE.
  In the present paper, we develop a categorical foundation for this class of
languages based on inverse categories with joins. We generalize the notion of
extensivity of restriction categories to one that may be accommodated by
inverse categories, and use the resulting decisions to give a reversible
representation of predicates and assertions. This leads to a categorical
semantics for structured reversible flowcharts, which we show to be
computationally sound and adequate, as well as equationally fully abstract with
respect to the operational semantics under certain conditions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.04047v3,Computing Lengths of Non-Crossing Shortest Paths in Planar Graphs,"Given a plane undirected graph $G$ with non-negative edge weights and a set
of $k$ terminal pairs on the external face, it is shown in Takahashi et al.
(Algorithmica, 16, 1996, pp. 339-357) that the union $U$ of $k$ non-crossing
shortest paths joining the $k$ terminal pairs (if they exist) can be computed
in $O(n\log n)$ time, where $n$ is the number of vertices of $G$. In the
restricted case in which the union $U$ of the shortest paths is a forest, it is
also shown that their lengths can be computed in the same time bound. We show
in this paper that it is always possible to compute the lengths of $k$
non-crossing shortest paths joining the $k$ terminal pairs in linear time, once
the shortest paths union $U$ has been computed, also in the case $U$ contains
cycles.
  Moreover, each shortest path $\pi$ can be listed in $O(\max\{\ell, \ell
\log\frac{k}{\ell} \})$, where $\ell$ is the number of edges in $\pi$.
  As a consequence, the problem of computing non-crossing shortest paths and
their lengths in a plane undirected weighted graph can be solved in $O(n\log
k)$ time in the general case.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1512.02737v1,Using Symmetry to Schedule Classical Matrix Multiplication,"Presented with a new machine with a specific interconnect topology, algorithm
designers use intuition about the symmetry of the algorithm to design time and
communication-efficient schedules that map the algorithm to the machine. Is
there a systematic procedure for designing schedules? We present a new
technique to design schedules for algorithms with no non-trivial dependencies,
focusing on the classical matrix multiplication algorithm.
  We model the symmetry of algorithm with the set of instructions $X$ as the
action of the group formed by the compositions of bijections from the set $X$
to itself. We model the machine as the action of the group $N\times \Delta$,
where $N$ and $\Delta$ represent the interconnect topology and time increments
respectively, on the set $P\times T$ of processors iterated over time steps. We
model schedules as symmetry-preserving equivariant maps between the set $X$ and
a subgroup of its symmetry and the set $P\times T$ with the symmetry
$N\times\Delta$. Such equivariant maps are the solutions of a set of algebraic
equations involving group homomorphisms. We associate time and communication
costs with the solutions to these equations.
  We solve these equations for the classical matrix multiplication algorithm
and show that equivariant maps correspond to time- and communication-efficient
schedules for many topologies. We recover well known variants including the
Cannon's algorithm and the communication-avoiding ""2.5D"" algorithm for toroidal
interconnects, systolic computation for planar hexagonal VLSI arrays, recursive
algorithms for fat-trees, the cache-oblivious algorithm for the ideal cache
model, and the space-bounded schedule for the parallel memory hierarchy model.
This suggests that the design of a schedule for a new class of machines can be
motivated by solutions to algebraic equations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0801.4802v1,"Investigating the Potential of Test-Driven Development for Spreadsheet
  Engineering","It is widely documented that the absence of a structured approach to
spreadsheet engineering is a key factor in the high level of spreadsheet
errors. In this paper we propose and investigate the application of Test-Driven
Development to the creation of spreadsheets. Test-Driven Development is an
emerging development technique in software engineering that has been shown to
result in better quality software code. It has also been shown that this code
requires less testing and is easier to maintain. Through a pair of case studies
we demonstrate that Test-Driven Development can be applied to the development
of spreadsheets. We present the detail of these studies preceded by a clear
explanation of the technique and its application to spreadsheet engineering. A
supporting tool under development by the authors is also documented along with
proposed research to determine the effectiveness of the methodology and the
associated tool.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1
http://arxiv.org/abs/2105.09508v2,Fast Nonblocking Persistence for Concurrent Data Structures,"We present a fully lock-free variant of the recent Montage system for
persistent data structures. Our variant, nbMontage, adds persistence to almost
any nonblocking concurrent structure without introducing significant overhead
or blocking of any kind. Like its predecessor, nbMontage is buffered durably
linearizable: it guarantees that the state recovered in the wake of a crash
will represent a consistent prefix of pre-crash execution. Unlike its
predecessor, nbMontage ensures wait-free progress of the persistence frontier,
thereby bounding the number of recent updates that may be lost on a crash, and
allowing a thread to force an update of the frontier (i.e., to perform a sync
operation) without the risk of blocking. As an extra benefit, the helping
mechanism employed by our wait-free sync significantly reduces its latency.
  Performance results for nonblocking queues, skip lists, trees, and hash
tables rival custom data structures in the literature -- dramatically faster
than achieved with prior general-purpose systems, and generally within 50% of
equivalent non-persistent structures placed in DRAM.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.07982v1,Exact Algorithms via Multivariate Subroutines,"We consider the family of $\Phi$-Subset problems, where the input consists of
an instance $I$ of size $N$ over a universe $U_I$ of size $n$ and the task is
to check whether the universe contains a subset with property $\Phi$ (e.g.,
$\Phi$ could be the property of being a feedback vertex set for the input graph
of size at most $k$). Our main tool is a simple randomized algorithm which
solves $\Phi$-Subset in time $(1+b-\frac{1}{c})^n N^{O(1)}$, provided that
there is an algorithm for the $\Phi$-Extension problem with running time
$b^{n-|X|} c^k N^{O(1)}$. Here, the input for $\Phi$-Extension is an instance
$I$ of size $N$ over a universe $U_I$ of size $n$, a subset $X\subseteq U_I$,
and an integer $k$, and the task is to check whether there is a set $Y$ with
$X\subseteq Y \subseteq U_I$ and $|Y\setminus X|\le k$ with property $\Phi$. We
derandomize this algorithm at the cost of increasing the running time by a
subexponential factor in $n$, and we adapt it to the enumeration setting where
we need to enumerate all subsets of the universe with property $\Phi$. This
generalizes the results of Fomin et al. [STOC 2016] who proved the case where
$b=1$. As case studies, we use these results to design faster deterministic
algorithms for: - checking whether a graph has a feedback vertex set of size at
most $k$ - enumerating all minimal feedback vertex sets - enumerating all
minimal vertex covers of size at most $k$, and - enumerating all minimal
3-hitting sets. We obtain these results by deriving new $b^{n-|X|} c^k
N^{O(1)}$-time algorithms for the corresponding $\Phi$-Extension problems (or
enumeration variant). In some cases, this is done by adapting the analysis of
an existing algorithm, or in other cases by designing a new algorithm. Our
analyses are based on Measure and Conquer, but the value to minimize,
$1+b-\frac{1}{c}$, is unconventional and requires non-convex optimization.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1701.00877v2,On the Usability of Probably Approximately Correct Implication Bases,"We revisit the notion of probably approximately correct implication bases
from the literature and present a first formulation in the language of formal
concept analysis, with the goal to investigate whether such bases represent a
suitable substitute for exact implication bases in practical use-cases. To this
end, we quantitatively examine the behavior of probably approximately correct
implication bases on artificial and real-world data sets and compare their
precision and recall with respect to their corresponding exact implication
bases. Using a small example, we also provide qualitative insight that
implications from probably approximately correct bases can still represent
meaningful knowledge from a given data set.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1306.5308v1,"Cognitive Interpretation of Everyday Activities: Toward Perceptual
  Narrative Based Visuo-Spatial Scene Interpretation","We position a narrative-centred computational model for high-level knowledge
representation and reasoning in the context of a range of assistive
technologies concerned with ""visuo-spatial perception and cognition"" tasks. Our
proposed narrative model encompasses aspects such as \emph{space, events,
actions, change, and interaction} from the viewpoint of commonsense reasoning
and learning in large-scale cognitive systems. The broad focus of this paper is
on the domain of ""human-activity interpretation"" in smart environments, ambient
intelligence etc. In the backdrop of a ""smart meeting cinematography"" domain,
we position the proposed narrative model, preliminary work on perceptual
narrativisation, and the immediate outlook on constructing general-purpose
open-source tools for perceptual narrativisation.
  ACM Classification: I.2 Artificial Intelligence: I.2.0 General -- Cognitive
Simulation, I.2.4 Knowledge Representation Formalisms and Methods, I.2.10
Vision and Scene Understanding: Architecture and control structures, Motion,
Perceptual reasoning, Shape, Video analysis
  General keywords: cognitive systems; human-computer interaction; spatial
cognition and computation; commonsense reasoning; spatial and temporal
reasoning; assistive technologies",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1810.03241v1,Diagnosing Convolutional Neural Networks using their Spectral Response,"Convolutional Neural Networks (CNNs) are a class of artificial neural
networks whose computational blocks use convolution, together with other linear
and non-linear operations, to perform classification or regression. This paper
explores the spectral response of CNNs and its potential use in diagnosing
problems with their training. We measure the gain of CNNs trained for image
classification on ImageNet and observe that the best models are also the most
sensitive to perturbations of their input. Further, we perform experiments on
MNIST and CIFAR-10 to find that the gain rises as the network learns and then
saturates as the network converges. Moreover, we find that strong gain
fluctuations can point to overfitting and learning problems caused by a poor
choice of learning rate. We argue that the gain of CNNs can act as a diagnostic
tool and potential replacement for the validation loss when hold-out validation
data are not available.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0504076v1,An Improved Remote User Authentication Scheme Using Smart Cards,"In 2000, Hwang and Li proposed a new remote user authentication scheme using
smart cards. In the same year, Chan and Cheng pointed out that Hwang and
Li&#8217;s scheme is not secure against the masquerade attack. Further, in
2003, Shen, Lin and Hwang pointed out a different type of attack on Hwang and
Li&#8217;s scheme and presented a modified scheme to remove its security
pitfalls. This paper presents an improved scheme which is secure against
Chan-Cheng and all the extended attacks.",0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1606.06811v1,"Where to Focus: Query Adaptive Matching for Instance Retrieval Using
  Convolutional Feature Maps","Instance retrieval requires one to search for images that contain a
particular object within a large corpus. Recent studies show that using image
features generated by pooling convolutional layer feature maps (CFMs) of a
pretrained convolutional neural network (CNN) leads to promising performance
for this task. However, due to the global pooling strategy adopted in those
works, the generated image feature is less robust to image clutter and tends to
be contaminated by the irrelevant image patterns. In this article, we alleviate
this drawback by proposing a novel reranking algorithm using CFMs to refine the
retrieval result obtained by existing methods. Our key idea, called query
adaptive matching (QAM), is to first represent the CFMs of each image by a set
of base regions which can be freely combined into larger regions-of-interest.
Then the similarity between the query and a candidate image is measured by the
best similarity score that can be attained by comparing the query feature and
the feature pooled from a combined region. We show that the above procedure can
be cast as an optimization problem and it can be solved efficiently with an
off-the-shelf solver. Besides this general framework, we also propose two
practical ways to create the base regions. One is based on the property of the
CFM and the other one is based on a multi-scale spatial pyramid scheme. Through
extensive experiments, we show that our reranking approaches bring substantial
performance improvement and by applying them we can outperform the state of the
art on several instance retrieval benchmarks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0601087v1,Processing of Test Matrices with Guessing Correction,"It is suggested to insert into test matrix 1s for correct responses, 0s for
response refusals, and negative corrective elements for incorrect responses.
With the classical test theory approach test scores of examinees and items are
calculated traditionally as sums of matrix elements, organized in rows and
columns. Correlation coefficients are estimated using correction coefficients.
In item response theory approach examinee and item logits are estimated using
maximum likelihood method and probabilities of all matrix elements.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1703.03262v1,Does Nash Envy Immunity,"The most popular stability notion in games should be Nash equilibrium under
the rationality of players who maximize their own payoff individually. In
contrast, in many scenarios, players can be (partly) irrational with some
unpredictable factors. Hence a strategy profile can be more robust if it is
resilient against certain irrational behaviors. In this paper, we propose a
stability notion that is resilient against envy. A strategy profile is said to
be envy-proof if each player cannot gain a competitive edge with respect to the
change in utility over the other players by deviation. Together with Nash
equilibrium and another stability notion called immunity, we show how these
separate notions are related to each other, whether they exist in games, and
whether and when a strategy profile satisfying these notions can be efficiently
found. We answer these questions by starting with the general two player game
and extend the discussion for the approximate stability and for the
corresponding fault-tolerance notions in multi-player games.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1406.5282v2,"STAIR Codes: A General Family of Erasure Codes for Tolerating Device and
  Sector Failures","Practical storage systems often adopt erasure codes to tolerate device
failures and sector failures, both of which are prevalent in the field.
However, traditional erasure codes employ device-level redundancy to protect
against sector failures, and hence incur significant space overhead. Recent
sector-disk (SD) codes are available only for limited configurations. By making
a relaxed but practical assumption, we construct a general family of erasure
codes called \emph{STAIR codes}, which efficiently and provably tolerate both
device and sector failures without any restriction on the size of a storage
array and the numbers of tolerable device failures and sector failures. We
propose the \emph{upstairs encoding} and \emph{downstairs encoding} methods,
which provide complementary performance advantages for different
configurations. We conduct extensive experiments on STAIR codes in terms of
space saving, encoding/decoding speed, and update cost. We demonstrate that
STAIR codes not only improve space efficiency over traditional erasure codes,
but also provide better computational efficiency than SD codes based on our
special code construction. Finally, we present analytical models that
characterize the reliability of STAIR codes, and show that the support of a
wider range of configurations by STAIR codes is critical for tolerating sector
failure bursts discovered in the field.",0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.01024v3,Efficient Data Management with a Flexible Address Space,"Data management applications store their data using structured files in which
data are usually sorted to serve indexing and queries. However, in-place
insertions and removals of data are not naturally supported in a file's address
space. To avoid repeatedly rewriting existing data in a sorted file to admit
changes in place, applications usually employ extra layers of indirections,
such as mapping tables and logs, to admit changes out of place. However, this
approach leads to increased access cost and excessive complexity.
  This paper presents a novel storage engine that provides a flexible address
space, where in-place updates of arbitrary-sized data, such as insertions and
removals, can be performed efficiently. With this mechanism, applications can
manage sorted data in a linear address space with minimal complexity. Extensive
evaluations show that a key-value store built on top of it can achieve high
performance and efficiency with a simple implementation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1911.08600v2,Steepest ascent can be exponential in bounded treewidth problems,"We investigate the complexity of local search based on steepest ascent. We
show that even when all variables have domains of size two and the underlying
constraint graph of variable interactions has bounded treewidth (in our
construction, treewidth 7), there are fitness landscapes for which an
exponential number of steps may be required to reach a local optimum. This is
an improvement on prior recursive constructions of long steepest ascents, which
we prove to need constraint graphs of unbounded treewidth.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0310051v10,"Nearly-Linear Time Algorithms for Graph Partitioning, Graph
  Sparsification, and Solving Linear Systems","This paper has been divided into three papers. arXiv:0809.3232,
arXiv:0808.4134, arXiv:cs/0607105",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0708.3226v7,A Dichotomy Theorem for General Minimum Cost Homomorphism Problem,"In the constraint satisfaction problem ($CSP$), the aim is to find an
assignment of values to a set of variables subject to specified constraints. In
the minimum cost homomorphism problem ($MinHom$), one is additionally given
weights $c_{va}$ for every variable $v$ and value $a$, and the aim is to find
an assignment $f$ to the variables that minimizes $\sum_{v} c_{vf(v)}$. Let
$MinHom(\Gamma)$ denote the $MinHom$ problem parameterized by the set of
predicates allowed for constraints. $MinHom(\Gamma)$ is related to many
well-studied combinatorial optimization problems, and concrete applications can
be found in, for instance, defence logistics and machine learning. We show that
$MinHom(\Gamma)$ can be studied by using algebraic methods similar to those
used for CSPs. With the aid of algebraic techniques, we classify the
computational complexity of $MinHom(\Gamma)$ for all choices of $\Gamma$. Our
result settles a general dichotomy conjecture previously resolved only for
certain classes of directed graphs, [Gutin, Hell, Rafiey, Yeo, European J. of
Combinatorics, 2008].",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1404.1129v2,An Efficient Two-Stage Sparse Representation Method,"There are a large number of methods for solving under-determined linear
inverse problem. Many of them have very high time complexity for large
datasets. We propose a new method called Two-Stage Sparse Representation (TSSR)
to tackle this problem. We decompose the representing space of signals into two
parts, the measurement dictionary and the sparsifying basis. The dictionary is
designed to approximate a sub-Gaussian distribution to exploit its
concentration property. We apply sparse coding to the signals on the dictionary
in the first stage, and obtain the training and testing coefficients
respectively. Then we design the basis to approach an identity matrix in the
second stage, to acquire the Restricted Isometry Property (RIP) and
universality property. The testing coefficients are encoded over the basis and
the final representing coefficients are obtained. We verify that the projection
of testing coefficients onto the basis is a good approximation of the signal
onto the representing space. Since the projection is conducted on a much
sparser space, the runtime is greatly reduced. For concrete realization, we
provide an instance for the proposed TSSR. Experiments on four biometrics
databases show that TSSR is effective and efficient, comparing with several
classical methods for solving linear inverse problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0811.4713v2,Compact Labelings For Efficient First-Order Model-Checking,"We consider graph properties that can be checked from labels, i.e., bit
sequences, of logarithmic length attached to vertices. We prove that there
exists such a labeling for checking a first-order formula with free set
variables in the graphs of every class that is \emph{nicely locally
cwd-decomposable}. This notion generalizes that of a \emph{nicely locally
tree-decomposable} class. The graphs of such classes can be covered by graphs
of bounded \emph{clique-width} with limited overlaps. We also consider such
labelings for \emph{bounded} first-order formulas on graph classes of
\emph{bounded expansion}. Some of these results are extended to counting
queries.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1406.6772v2,MSPlayer: Multi-Source and multi-Path LeverAged YoutubER,"Online video streaming through mobile devices has become extremely popular
nowadays. YouTube, for example, reported that the percentage of its traffic
streaming to mobile devices has soared from 6% to more than 40% over the past
two years. Moreover, people are constantly seeking to stream high quality video
for better experience while often suffering from limited bandwidth. Thanks to
the rapid deployment of content delivery networks (CDNs), popular videos are
now replicated at different sites, and users can stream videos from close-by
locations with low latencies. As mobile devices nowadays are equipped with
multiple wireless interfaces (e.g., WiFi and 3G/4G), aggregating bandwidth for
high definition video streaming has become possible.
  We propose a client-based video streaming solution, MSPlayer, that takes
advantage of multiple video sources as well as multiple network paths through
different interfaces. MSPlayer reduces start-up latency and provides high
quality video streaming and robust data transport in mobile scenarios. We
experimentally demonstrate our solution on a testbed and through the YouTube
video service.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0703153v1,The periodic domino problem is undecidable in the hyperbolic plane,"In this paper, we consider the periodic tiling problem which was proved
undecidable in the Euclidean plane by Yu. Gurevich and I. Koriakov in 1972.
Here, we prove that the same problem for the hyperbolic plane is also
undecidable.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.02102v3,Teaching Autoregressive Language Models Complex Tasks By Demonstration,"This paper demonstrates that by fine-tuning an autoregressive language model
(GPT-Neo) on appropriately structured step-by-step demonstrations, it is
possible to teach it to execute a mathematical task that has previously proved
difficult for Transformers - longhand modulo operations - with a relatively
small number of examples. Specifically, we fine-tune GPT-Neo to solve the
numbers__div_remainder task from the DeepMind Mathematics Dataset; Saxton et
al. (arXiv:1904.01557) reported below 40% accuracy on this task with 2 million
training examples. We show that after fine-tuning on 200 appropriately
structured demonstrations of solving long division problems and reporting the
remainders, the smallest available GPT-Neo model achieves over 80% accuracy.
This is achieved by constructing an appropriate dataset for fine-tuning, with
no changes to the learning algorithm. These results suggest that fine-tuning
autoregressive language models on small sets of well-crafted demonstrations may
be a useful paradigm for enabling individuals without training in machine
learning to coax such models to perform some kinds of complex multi-step tasks.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1001.2735v4,Stochastic Budget Optimization in Internet Advertising,"Internet advertising is a sophisticated game in which the many advertisers
""play"" to optimize their return on investment. There are many ""targets"" for the
advertisements, and each ""target"" has a collection of games with a potentially
different set of players involved. In this paper, we study the problem of how
advertisers allocate their budget across these ""targets"". In particular, we
focus on formulating their best response strategy as an optimization problem.
Advertisers have a set of keywords (""targets"") and some stochastic information
about the future, namely a probability distribution over scenarios of cost vs
click combinations. This summarizes the potential states of the world assuming
that the strategies of other players are fixed. Then, the best response can be
abstracted as stochastic budget optimization problems to figure out how to
spread a given budget across these keywords to maximize the expected number of
clicks.
  We present the first known non-trivial poly-logarithmic approximation for
these problems as well as the first known hardness results of getting better
than logarithmic approximation ratios in the various parameters involved. We
also identify several special cases of these problems of practical interest,
such as with fixed number of scenarios or with polynomial-sized parameters
related to cost, which are solvable either in polynomial time or with improved
approximation ratios. Stochastic budget optimization with scenarios has
sophisticated technical structure. Our approximation and hardness results come
from relating these problems to a special type of (0/1, bipartite) quadratic
programs inherent in them. Our research answers some open problems raised by
the authors in (Stochastic Models for Budget Optimization in Search-Based
Advertising, Algorithmica, 58 (4), 1022-1044, 2010).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.8712v1,"Detecting Malicious Code by Exploiting Dependencies of System-call
  Groups","In this paper we present an elaborated graph-based algorithmic technique for
efficient malware detection. More precisely, we utilize the system-call
dependency graphs (or, for short ScD graphs), obtained by capturing taint
analysis traces and a set of various similarity metrics in order to detect
whether an unknown test sample is a malicious or a benign one. For the sake of
generalization, we decide to empower our model against strong mutations by
applying our detection technique on a weighted directed graph resulting from
ScD graph after grouping disjoint subsets of its vertices. Additionally, we
have developed a similarity metric, which we call NP-similarity, that combines
qualitative, quantitative, and relational characteristics that are spread among
the members of known malware families to archives a clear distinction between
graph-representations of malware and the ones of benign software. Finally, we
evaluate our detection model and compare our results against the results
achieved by a variety of techniques proving the potentials of our model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2007.15338v1,New approach to MPI program execution time prediction,"The problem of MPI programs execution time prediction on a certain set of
computer installations is considered. This problem emerges with orchestration
and provisioning a virtual infrastructure in a cloud computing environment over
a heterogeneous network of computer installations: supercomputers or clusters
of servers (e.g. mini data centers). One of the key criteria for the
effectiveness of the cloud computing environment is the time staying by the
program inside the environment. This time consists of the waiting time in the
queue and the execution time on the selected physical computer installation, to
which the computational resource of the virtual infrastructure is dynamically
mapped. One of the components of this problem is the estimation of the MPI
programs execution time on a certain set of computer installations. This is
necessary to determine a proper choice of order and place for program
execution. The article proposes two new approaches to the program execution
time prediction problem. The first one is based on computer installations
grouping based on the Pearson correlation coefficient. The second one is based
on vector representations of computer installations and MPI programs, so-called
embeddings. The embedding technique is actively used in recommendation systems,
such as for goods (Amazon), for articles (Arxiv.org), for videos (YouTube,
Netflix). The article shows how the embeddings technique helps to predict the
execution time of a MPI program on a certain set of computer installations.",0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1302.1726v2,"Uncovering the Wider Structure of Extreme Right Communities Spanning
  Popular Online Networks","Recent years have seen increased interest in the online presence of extreme
right groups. Although originally composed of dedicated websites, the online
extreme right milieu now spans multiple networks, including popular social
media platforms such as Twitter, Facebook and YouTube. Ideally therefore, any
contemporary analysis of online extreme right activity requires the
consideration of multiple data sources, rather than being restricted to a
single platform. We investigate the potential for Twitter to act as a gateway
to communities within the wider online network of the extreme right, given its
facility for the dissemination of content. A strategy for representing
heterogeneous network data with a single homogeneous network for the purpose of
community detection is presented, where these inherently dynamic communities
are tracked over time. We use this strategy to discover and analyze persistent
English and German language extreme right communities.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.03186v1,"An Optimal Control Theory for the Traveling Salesman Problem and Its
  Variants","We show that the traveling salesman problem (TSP) and its many variants may
be modeled as functional optimization problems over a graph. In this
formulation, all vertices and arcs of the graph are functionals; i.e., a
mapping from a space of measurable functions to the field of real numbers. Many
variants of the TSP, such as those with neighborhoods, with forbidden
neighborhoods, with time-windows and with profits, can all be framed under this
construct. In sharp contrast to their discrete-optimization counterparts, the
modeling constructs presented in this paper represent a fundamentally new
domain of analysis and computation for TSPs and their variants. Beyond its
apparent mathematical unification of a class of problems in graph theory, the
main advantage of the new approach is that it facilitates the modeling of
certain application-specific problems in their home space of measurable
functions. Consequently, certain elements of economic system theory such as
dynamical models and continuous-time cost/profit functionals can be directly
incorporated in the new optimization problem formulation. Furthermore, subtour
elimination constraints, prevalent in discrete optimization formulations, are
naturally enforced through continuity requirements. The price for the new
modeling framework is nonsmooth functionals. Although a number of theoretical
issues remain open in the proposed mathematical framework, we demonstrate the
computational viability of the new modeling constructs over a sample set of
problems to illustrate the rapid production of end-to-end TSP solutions to
extensively-constrained practical problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0611048v2,"Dense-Timed Petri Nets: Checking Zenoness, Token liveness and
  Boundedness","We consider Dense-Timed Petri Nets (TPN), an extension of Petri nets in which
each token is equipped with a real-valued clock and where the semantics is lazy
(i.e., enabled transitions need not fire; time can pass and disable
transitions). We consider the following verification problems for TPNs. (i)
Zenoness: whether there exists a zeno-computation from a given marking, i.e.,
an infinite computation which takes only a finite amount of time. We show
decidability of zenoness for TPNs, thus solving an open problem from [Escrig et
al.]. Furthermore, the related question if there exist arbitrarily fast
computations from a given marking is also decidable. On the other hand,
universal zenoness, i.e., the question if all infinite computations from a
given marking are zeno, is undecidable. (ii) Token liveness: whether a token is
alive in a marking, i.e., whether there is a computation from the marking which
eventually consumes the token. We show decidability of the problem by reducing
it to the coverability problem, which is decidable for TPNs. (iii) Boundedness:
whether the size of the reachable markings is bounded. We consider two versions
of the problem; namely semantic boundedness where only live tokens are taken
into consideration in the markings, and syntactic boundedness where also dead
tokens are considered. We show undecidability of semantic boundedness, while we
prove that syntactic boundedness is decidable through an extension of the
Karp-Miller algorithm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1801.05974v1,Privacy-preserving Data Splitting: A Combinatorial Approach,"Privacy-preserving data splitting is a technique that aims to protect data
privacy by storing different fragments of data in different locations. In this
work we give a new combinatorial formulation to the data splitting problem. We
see the data splitting problem as a purely combinatorial problem, in which we
have to split data attributes into different fragments in a way that satisfies
certain combinatorial properties derived from processing and privacy
constraints. Using this formulation, we develop new combinatorial and algebraic
techniques to obtain solutions to the data splitting problem. We present an
algebraic method which builds an optimal data splitting solution by using
Gr\""{o}bner bases. Since this method is not efficient in general, we also
develop a greedy algorithm for finding solutions that are not necessarily
minimal sized.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1906.00091v1,"Deep Learning Recommendation Model for Personalization and
  Recommendation Systems","With the advent of deep learning, neural network-based recommendation models
have emerged as an important tool for tackling personalization and
recommendation tasks. These networks differ significantly from other deep
learning networks due to their need to handle categorical features and are not
well studied or understood. In this paper, we develop a state-of-the-art deep
learning recommendation model (DLRM) and provide its implementation in both
PyTorch and Caffe2 frameworks. In addition, we design a specialized
parallelization scheme utilizing model parallelism on the embedding tables to
mitigate memory constraints while exploiting data parallelism to scale-out
compute from the fully-connected layers. We compare DLRM against existing
recommendation models and characterize its performance on the Big Basin AI
platform, demonstrating its usefulness as a benchmark for future algorithmic
experimentation and system co-design.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1604.07478v3,Collection and Dissemination of Data on Time-Varying Digraphs,"Given a network of fixed size $n$ and an initial distribution of data, we
derive sufficient connectivity conditions on a sequence of time-varying
digraphs for (a) data collection and (b) data dissemination, within at most
$(n-1)$ iterations. The former is shown to enable distributed computation of
the network size $n$, while the latter does not. Knowledge of $n$ subsequently
enables each node to acknowledge the earliest time point at which they can
cease communication, specifically we find the number of redundant signals can
be truncated at the finite time $n$. Using a probabilistic approach, we obtain
tight upper and lower bounds for the expected time until the $\textit{last}$
node obtains the entire collection of data, in other words complete data
dissemination. Similarly tight upper and lower bounds are also found for the
expected time until the $\textit{first}$ node obtains the entire collection of
data. Interestingly, these bounds are both $\Theta (\text{log}_2(n))$ and in
fact differ by only two iterations. Numerical results are explored and verify
each result.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1402.1726v2,For-all Sparse Recovery in Near-Optimal Time,"An approximate sparse recovery system in $\ell_1$ norm consists of parameters
$k$, $\epsilon$, $N$, an $m$-by-$N$ measurement $\Phi$, and a recovery
algorithm, $\mathcal{R}$. Given a vector, $\mathbf{x}$, the system approximates
$x$ by $\widehat{\mathbf{x}} = \mathcal{R}(\Phi\mathbf{x})$, which must satisfy
$\|\widehat{\mathbf{x}}-\mathbf{x}\|_1 \leq
(1+\epsilon)\|\mathbf{x}-\mathbf{x}_k\|_1$. We consider the 'for all' model, in
which a single matrix $\Phi$, possibly 'constructed' non-explicitly using the
probabilistic method, is used for all signals $\mathbf{x}$. The best existing
sublinear algorithm by Porat and Strauss (SODA'12) uses $O(\epsilon^{-3}
k\log(N/k))$ measurements and runs in time $O(k^{1-\alpha}N^\alpha)$ for any
constant $\alpha > 0$.
  In this paper, we improve the number of measurements to $O(\epsilon^{-2} k
\log(N/k))$, matching the best existing upper bound (attained by super-linear
algorithms), and the runtime to $O(k^{1+\beta}\textrm{poly}(\log
N,1/\epsilon))$, with a modest restriction that $\epsilon \leq (\log k/\log
N)^{\gamma}$, for any constants $\beta,\gamma > 0$. When $k\leq \log^c N$ for
some $c>0$, the runtime is reduced to $O(k\textrm{poly}(N,1/\epsilon))$. With
no restrictions on $\epsilon$, we have an approximation recovery system with $m
= O(k/\epsilon \log(N/k)((\log N/\log k)^\gamma + 1/\epsilon))$ measurements.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1204.2131v1,On Thresholds for the Appearance of 2-cores in Mixed Hypergraphs,"We study thresholds for the appearance of a 2-core in random hypergraphs that
are a mixture of a constant number of random uniform hypergraphs each with a
linear number of edges but with different edge sizes. For the case of two
overlapping hypergraphs we give a solution for the optimal (expected) number of
edges of each size such that the 2-core threshold for the resulting mixed
hypergraph is maximized. We show that for adequate edge sizes this threshold
exceeds the maximum 2-core threshold for any random uniform hypergraph, which
can be used to improve the space utilization of several data structures that
rely on this parameter.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1808.04744v1,Spanning Properties of Theta-Theta-6,"We show that, unlike the Yao-Yao graph $YY_6$, the Theta-Theta graph
$\Theta\Theta_6$ defined by six cones is a spanner for sets of points in convex
position. We also show that, for sets of points in non-convex position, the
spanning ratio of $\Theta\Theta_6$ is unbounded.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.01496v1,"Analysis of an adaptive lead weighted ResNet for multiclass
  classification of 12-lead ECGs","Background: Twelve lead ECGs are a core diagnostic tool for cardiovascular
diseases. Here, we describe and analyse an ensemble deep neural network
architecture to classify 24 cardiac abnormalities from 12-lead ECGs.
  Method: We proposed a squeeze and excite ResNet to automatically learn deep
features from 12-lead ECGs, in order to identify 24 cardiac conditions. The
deep features were augmented with age and gender features in the final fully
connected layers. Output thresholds for each class were set using a constrained
grid search. To determine why the model made incorrect predictions, two expert
clinicians independently interpreted a random set of 100 misclassified ECGs
concerning Left Axis Deviation.
  Results: Using the bespoke weighted accuracy metric, we achieved a 5-fold
cross validation score of 0.684, and sensitivity and specificity of 0.758 and
0.969, respectively. We scored 0.520 on the full test data, and ranked 2nd out
of 41 in the official challenge rankings. On a random set of misclassified
ECGs, agreement between two clinicians and training labels was poor (clinician
1: kappa = -0.057, clinician 2: kappa = -0.159). In contrast, agreement between
the clinicians was very high (kappa = 0.92).
  Discussion: The proposed prediction model performed well on the validation
and hidden test data in comparison to models trained on the same data. We also
discovered considerable inconsistency in training labels, which is likely to
hinder development of more accurate models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0805.4134v1,Design and Implementation Aspects of a novel Java P2P Simulator with GUI,"Peer-to-peer networks consist of thousands or millions of nodes that might
join and leave arbitrarily. The evaluation of new protocols in real
environments is many times practically impossible, especially at design and
testing stages. The purpose of this paper is to describe the implementation
aspects of a new Java based P2P simulator that has been developed to support
scalability in the evaluation of such P2P dynamic environments. Evolving the
functionality presented by previous solutions, we provide a friendly graphical
user interface through which the high-level theoretic researcher/designer of a
P2P system can easily construct an overlay with the desirable number of nodes
and evaluate its operations using a number of key distributions. Furthermore,
the simulator has built-in ability to produce statistics about the distributed
structure. Emphasis was given to the parametrical configuration of the
simulator. As a result the developed tool can be utilized in the simulation and
evaluation procedures of a variety of different protocols, with only few
changes in the Java code.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1809.01170v2,"""This is why we play"": Characterizing Online Fan Communities of the NBA
  Teams","Professional sports constitute an important part of people's modern life.
People spend substantial amounts of time and money supporting their favorite
players and teams, and sometimes even riot after games. However, how team
performance affects fan behavior remains understudied at a large scale. As
almost every notable professional team has its own online fan community, these
communities provide great opportunities for investigating this research
question. In this work, we provide the first large-scale characterization of
online fan communities of professional sports teams.
  Since user behavior in these online fan communities is inherently connected
to game events and team performance, we construct a unique dataset that
combines 1.5M posts and 43M comments in NBA-related communities on Reddit with
statistics that document team performance in the NBA. We analyze the impact of
team performance on fan behavior both at the game level and the season level.
First, we study how team performance in a game relates to user activity during
that game. We find that surprise plays an important role: the fans of the top
teams are more active when their teams lose and so are the fans of the bottom
teams in an unexpected win. Second, we study fan behavior over consecutive
seasons and show that strong team performance is associated with fans of low
loyalty, likely due to ""bandwagon fans."" Fans of the bottom teams tend to
discuss their team's future such as young talents in the roster, which may help
them stay optimistic during adversity. Our results not only contribute to
understanding the interplay between online sports communities and offline
context but also provide significant insights into sports management.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.13084v1,"Best Practices for Implementing FAIR Vocabularies and Ontologies on the
  Web","With the adoption of Semantic Web technologies, an increasing number of
vocabularies and ontologies have been developed in different domains, ranging
from Biology to Agronomy or Geosciences. However, many of these ontologies are
still difficult to find, access and understand by researchers due to a lack of
documentation, URI resolving issues, versioning problems, etc. In this chapter
we describe guidelines and best practices for creating accessible,
understandable and reusable ontologies on the Web, using standard practices and
pointing to existing tools and frameworks developed by the Semantic Web
community. We illustrate our guidelines with concrete examples, in order to
help researchers implement these practices in their future vocabularies.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0602024v4,"Algorithmic correspondence and completeness in modal logic. I. The core
  algorithm SQEMA","Modal formulae express monadic second-order properties on Kripke frames, but
in many important cases these have first-order equivalents. Computing such
equivalents is important for both logical and computational reasons. On the
other hand, canonicity of modal formulae is important, too, because it implies
frame-completeness of logics axiomatized with canonical formulae.
  Computing a first-order equivalent of a modal formula amounts to elimination
of second-order quantifiers. Two algorithms have been developed for
second-order quantifier elimination: SCAN, based on constraint resolution, and
DLS, based on a logical equivalence established by Ackermann.
  In this paper we introduce a new algorithm, SQEMA, for computing first-order
equivalents (using a modal version of Ackermann's lemma) and, moreover, for
proving canonicity of modal formulae. Unlike SCAN and DLS, it works directly on
modal formulae, thus avoiding Skolemization and the subsequent problem of
unskolemization. We present the core algorithm and illustrate it with some
examples. We then prove its correctness and the canonicity of all formulae on
which the algorithm succeeds. We show that it succeeds not only on all
Sahlqvist formulae, but also on the larger class of inductive formulae,
introduced in our earlier papers. Thus, we develop a purely algorithmic
approach to proving canonical completeness in modal logic and, in particular,
establish one of the most general completeness results in modal logic so far.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1504.06355v2,On Freeze LTL with Ordered Attributes,"This paper is concerned with Freeze LTL, a temporal logic on data words with
registers. In a (multi-attributed) data word each position carries a letter
from a finite alphabet and assigns a data value to a fixed, finite set of
attributes. The satisfiability problem of Freeze LTL is undecidable if more
than one register is available or tuples of data values can be stored and
compared arbitrarily. Starting from the decidable one-register fragment we
propose an extension that allows for specifying a dependency relation on
attributes. This restricts in a flexible way how collections of attribute
values can be stored and compared. This conceptual dimension is orthogonal to
the number of registers or the available temporal operators. The extension is
strict. Admitting arbitrary dependency relations satisfiability becomes
undecidable. Tree-like relations, however, induce a family of decidable
fragments escalating the ordinal-indexed hierarchy of fast-growing complexity
classes, a recently introduced framework for non-primitive recursive
complexities. This results in completeness for the class ${\bf
F}_{\epsilon_0}$. We employ nested counter systems and show that they relate to
the hierarchy in terms of the nesting depth.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1504.03995v3,"Undecidability of Equality in the Free Locally Cartesian Closed Category
  (Extended version)","We show that a version of Martin-L\""of type theory with an extensional
identity type former I, a unit type N1 , Sigma-types, Pi-types, and a base type
is a free category with families (supporting these type formers) both in a 1-
and a 2-categorical sense. It follows that the underlying category of contexts
is a free locally cartesian closed category in a 2-categorical sense because of
a previously proved biequivalence. We show that equality in this category is
undecidable by reducing it to the undecidability of convertibility in
combinatory logic. Essentially the same construction also shows a slightly
strengthened form of the result that equality in extensional Martin-L\""of type
theory with one universe is undecidable.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.04590v2,"A Catalogue of Locus Algorithm Pointings for Optimal Differential
  Photometry for 23,779 Quasars","This paper presents a catalogue of optimised pointings for differential
photometry of 23,779 quasars extracted from the Sloan Digital Sky Survey (SDSS)
Catalogue and a score for each indicating the quality of the Field of View
(FoV) associated with that pointing. Observation of millimagnitude variability
on a timescale of minutes typically requires differential observations with
reference to an ensemble of reference stars. For optimal performance, these
reference stars should have similar colour and magnitude to the target quasar.
In addition, the greatest quantity and quality of suitable reference stars may
be found by using a telescope pointing which offsets the target object from the
centre of the field of view. By comparing each quasar with the stars which
appear close to it on the sky in the SDSS Catalogue, an optimum pointing can be
calculated, and a figure of merit, referred to as the ""score"" calculated for
that pointing. Highly flexible software has been developed to enable this
process to be automated and implemented in a distributed computing paradigm,
which enables the creation of catalogues of pointings given a set of input
targets. Applying this technique to a sample of 40,000 targets from the 4th
SDSS quasar catalogue resulted in the production of pointings and scores for
23,779 quasars. This catalogue is a useful resource for observers planning
differential photometry studies and surveys of quasars to select those which
have many suitable celestial neighbours for differential photometry",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.04372v2,"Freeform shape optimization of a compact DC photo-electron gun using
  isogeometric analysis","Compact DC high-voltage photo-electron guns are able to meet the
sophisticated demands of high-current applications such as energy recovery
linacs. A main design parameter for such sources is the electric field
strength, which depends on the electrode geometry and is limited by the field
emission threshold of the electrode material. In order to minimize the maximum
field strength for optimal gun operation, isogeometric analysis (IGA) can be
used to exploit the axisymmetric geometry and describe its cross section by
non-uniform rational B-splines, the control points of which are the parameters
to be optimized. This computationally efficient method is capable of describing
CAD-generated geometries using open source software (GeoPDEs, NLopt, Octave)
and it can simplify the step from design to simulation. We will present the
mathematical formulation, the software workflow, and the results of an
IGA-based shape optimization for a planned high-voltage upgrade of the DC
photogun teststand Photo-CATCH at TU Darmstadt. The software builds on a
general framework for isogeometric analysis and allows for easy adaptations to
other geometries or quantities of interest. Simulations assuming a bias voltage
of -300 kV yielded maximum field gradients of 9.06 MV/m on the surface of an
inverted insulator electrode and below 3 MV/m on the surface of the
photocathode.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1208.2763v1,Intrinsic Simulations between Stochastic Cellular Automata,"The paper proposes a simple formalism for dealing with deterministic,
non-deterministic and stochastic cellular automata in a unifying and composable
manner. Armed with this formalism, we extend the notion of intrinsic simulation
between deterministic cellular automata, to the non-deterministic and
stochastic settings. We then provide explicit tools to prove or disprove the
existence of such a simulation between two stochastic cellular automata, even
though the intrinsic simulation relation is shown to be undecidable in
dimension two and higher. The key result behind this is the caracterization of
equality of stochastic global maps by the existence of a coupling between the
random sources. We then prove that there is a universal non-deterministic
cellular automaton, but no universal stochastic cellular automaton. Yet we
provide stochastic cellular automata achieving optimal partial universality.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1202.5797v2,Stochastic Vehicle Routing with Recourse,"We study the classic Vehicle Routing Problem in the setting of stochastic
optimization with recourse. StochVRP is a two-stage optimization problem, where
demand is satisfied using two routes: fixed and recourse. The fixed route is
computed using only a demand distribution. Then after observing the demand
instantiations, a recourse route is computed -- but costs here become more
expensive by a factor lambda.
  We present an O(log^2 n log(n lambda))-approximation algorithm for this
stochastic routing problem, under arbitrary distributions. The main idea in
this result is relating StochVRP to a special case of submodular orienteering,
called knapsack rank-function orienteering. We also give a better approximation
ratio for knapsack rank-function orienteering than what follows from prior
work. Finally, we provide a Unique Games Conjecture based omega(1) hardness of
approximation for StochVRP, even on star-like metrics on which our algorithm
achieves a logarithmic approximation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1103.2903v1,"A new ANEW: Evaluation of a word list for sentiment analysis in
  microblogs","Sentiment analysis of microblogs such as Twitter has recently gained a fair
amount of attention. One of the simplest sentiment analysis approaches compares
the words of a posting against a labeled word list, where each word has been
scored for valence, -- a 'sentiment lexicon' or 'affective word lists'. There
exist several affective word lists, e.g., ANEW (Affective Norms for English
Words) developed before the advent of microblogging and sentiment analysis. I
wanted to examine how well ANEW and other word lists performs for the detection
of sentiment strength in microblog posts in comparison with a new word list
specifically constructed for microblogs. I used manually labeled postings from
Twitter scored for sentiment. Using a simple word matching I show that the new
word list may perform better than ANEW, though not as good as the more
elaborate approach found in SentiStrength.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1904.07916v1,"IAN: Combining Generative Adversarial Networks for Imaginative Face
  Generation","Generative Adversarial Networks (GANs) have gained momentum for their ability
to model image distributions. They learn to emulate the training set and that
enables sampling from that domain and using the knowledge learned for useful
applications. Several methods proposed enhancing GANs, including regularizing
the loss with some feature matching. We seek to push GANs beyond the data in
the training and try to explore unseen territory in the image manifold. We
first propose a new regularizer for GAN based on K-nearest neighbor (K-NN)
selective feature matching to a target set Y in high-level feature space,
during the adversarial training of GAN on the base set X, and we call this
novel model K-GAN. We show that minimizing the added term follows from
cross-entropy minimization between the distributions of GAN and the set Y.
Then, We introduce a cascaded framework for GANs that try to address the task
of imagining a new distribution that combines the base set X and target set Y
by cascading sampling GANs with translation GANs, and we dub the cascade of
such GANs as the Imaginative Adversarial Network (IAN). We conduct an objective
and subjective evaluation for different IAN setups in the addressed task and
show some useful applications for these IANs, like manifold traversing and
creative face generation for characters' design in movies or video games.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.04740v1,River: machine learning for streaming data in Python,"River is a machine learning library for dynamic data streams and continual
learning. It provides multiple state-of-the-art learning methods, data
generators/transformers, performance metrics and evaluators for different
stream learning problems. It is the result from the merger of the two most
popular packages for stream learning in Python: Creme and scikit-multiflow.
River introduces a revamped architecture based on the lessons learnt from the
seminal packages. River's ambition is to be the go-to library for doing machine
learning on streaming data. Additionally, this open source package brings under
the same umbrella a large community of practitioners and researchers. The
source code is available at https://github.com/online-ml/river.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1810.04722v2,A van Benthem Theorem for Quantitative Probabilistic Modal Logic,"In probabilistic transition systems, behavioural metrics provide a more
fine-grained and stable measure of system equivalence than crisp notions of
bisimilarity. They correlate strongly to quantitative probabilistic logics, and
in fact the distance induced by a probabilistic modal logic taking values in
the real unit interval has been shown to coincide with behavioural distance.
For probabilistic systems, probabilistic modal logic thus plays an analogous
role to that of Hennessy-Milner logic on classical labelled transition systems.
In the quantitative setting, invariance of modal logic under bisimilarity
becomes non-expansivity of formula evaluation w.r.t. behavioural distance. In
the present paper, we provide a characterization of the expressive power of
probabilistic modal logic based on this observation: We prove a probabilistic
analogue of the classical van Benthem theorem, which states that modal logic is
precisely the bisimulation-invariant fragment of first-order logic.
Specifically, we show that quantitative probabilistic modal logic lies dense in
the bisimulation-invariant fragment, in the indicated sense of non-expansive
formula evaluation, of quantitative probabilistic first-order logic; more
precisely, bisimulation-invariant first-order formulas are approximable by
modal formulas of bounded rank.
  For a description logic perspective on the same result, see arXiv:1906.00784.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0012022v1,Performance and Scalability Models for a Hypergrowth e-Commerce Web Site,"The performance of successful Web-based e-commerce services has all the
allure of a roller-coaster ride: accelerated fiscal growth combined with the
ever-present danger of running out of server capacity. This chapter presents a
case study based on the author's own capacity planning engagement with one of
the hottest e-commerce Web sites in the world. Several spreadsheet techniques
are presented for forecasting both short-term and long-term trends in the
consumption of server capacity. Two new performance metrics are introduced for
site planning and procurement: the effective demand, and the doubling period.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1602.05555v2,Repetition-Free Derivability from a Regular Grammar is NP-Hard,"We prove the NP-hardness of the problem whether a given word can be derived
from a given regular grammar without repeated occurrence of any nonterminal.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1105.2392v1,"Workflows for the Management of Change in Science, Technologies,
  Engineering and Mathematics","Mathematical knowledge is a central component in science, engineering, and
technology (documentation). Most of it is represented informally, and -- in
contrast to published research mathematics -- subject to continual change.
Unfortunately, machine support for change management has either been very
coarse grained and thus barely useful, or restricted to formal languages, where
automation is possible. In this paper, we report on an effort to extend change
management to collections of semi-formal documents which flexibly intermix
mathematical formulas and natural language and to integrate it into a semantic
publishing system for mathematical knowledge. We validate the long-standing
assumption that the semantic annotations in these flexiformal documents that
drive the machine-supported interaction with documents can support semantic
impact analyses at the same time. But in contrast to the fully formal setting,
where adaptations of impacted documents can be automated to some degree, the
flexiformal setting requires much more user interaction and thus a much tighter
integration into document management workflows.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1204.0479v1,"A collaborative ant colony metaheuristic for distributed multi-level
  lot-sizing","The paper presents an ant colony optimization metaheuristic for collaborative
planning. Collaborative planning is used to coordinate individual plans of
self-interested decision makers with private information in order to increase
the overall benefit of the coalition. The method consists of a new search graph
based on encoded solutions. Distributed and private information is integrated
via voting mechanisms and via a simple but effective collaborative local search
procedure. The approach is applied to a distributed variant of the multi-level
lot-sizing problem and evaluated by means of 352 benchmark instances from the
literature. The proposed approach clearly outperforms existing approaches on
the sets of medium and large sized instances. While the best method in the
literature so far achieves an average deviation from the best known
non-distributed solutions of 46 percent for the set of the largest instances,
for example, the presented approach reduces the average deviation to only 5
percent.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0005021v1,Modeling the Uncertainty in Complex Engineering Systems,"Existing procedures for model validation have been deemed inadequate for many
engineering systems. The reason of this inadequacy is due to the high degree of
complexity of the mechanisms that govern these systems. It is proposed in this
paper to shift the attention from modeling the engineering system itself to
modeling the uncertainty that underlies its behavior. A mathematical framework
for modeling the uncertainty in complex engineering systems is developed. This
framework uses the results of computational learning theory. It is based on the
premise that a system model is a learning machine.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.00139v1,SHARI -- An Integration of Tools to Visualize the Story of the Day,"Tools such as Google News and Flipboard exist to convey daily news, but what
about the past? In this paper, we describe how to combine several existing
tools with web archive holdings to perform news analysis and visualization of
the ""biggest story"" for a given date. StoryGraph clusters news articles
together to identify a common news story. Hypercane leverages ArchiveNow to
store URLs produced by StoryGraph in web archives. Hypercane analyzes these
URLs to identify the most common terms, entities, and highest quality images
for social media storytelling. Raintale then uses the output of these tools to
produce a visualization of the news story for a given day. We name this process
SHARI (StoryGraph Hypercane ArchiveNow Raintale Integration).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1111.0368v1,"Platform Dependent Verification: On Engineering Verification Tools for
  21st Century","The paper overviews recent developments in platform-dependent explicit-state
LTL model checking.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1411.2940v3,"Automated generation and symbolic manipulation of tensor product finite
  elements","We describe and implement a symbolic algebra for scalar and vector-valued
finite elements, enabling the computer generation of elements with tensor
product structure on quadrilateral, hexahedral and triangular prismatic cells.
The algebra is implemented as an extension to the domain-specific language UFL,
the Unified Form Language. This allows users to construct many finite element
spaces beyond those supported by existing software packages. We have made
corresponding extensions to FIAT, the FInite element Automatic Tabulator, to
enable numerical tabulation of such spaces. This tabulation is consequently
used during the automatic generation of low-level code that carries out local
assembly operations, within the wider context of solving finite element
problems posed over such function spaces. We have done this work within the
code-generation pipeline of the software package Firedrake; we make use of the
full Firedrake package to present numerical examples.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0008034v1,"Lexicalized Stochastic Modeling of Constraint-Based Grammars using
  Log-Linear Measures and EM Training","We present a new approach to stochastic modeling of constraint-based grammars
that is based on log-linear models and uses EM for estimation from unannotated
data. The techniques are applied to an LFG grammar for German. Evaluation on an
exact match task yields 86% precision for an ambiguity rate of 5.4, and 90%
precision on a subcat frame match for an ambiguity rate of 25. Experimental
comparison to training from a parsebank shows a 10% gain from EM training.
Also, a new class-based grammar lexicalization is presented, showing a 10% gain
over unlexicalized models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1707.08901v1,Providing Self-Aware Systems with Reflexivity,"We propose a new type of self-aware systems inspired by ideas from
higher-order theories of consciousness. First, we discussed the crucial
distinction between introspection and reflexion. Then, we focus on
computational reflexion as a mechanism by which a computer program can inspect
its own code at every stage of the computation. Finally, we provide a formal
definition and a proof-of-concept implementation of computational reflexion,
viewed as an enriched form of program interpretation and a way to dynamically
""augment"" a computational process.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1105.4468v1,"Standardization of information systems development processes and banking
  industry adaptations","This paper examines the current system development processes of three major
Turkish banks in terms of compliance to internationally accepted system
development and software engineering standards to determine the common process
problems of banks. After an in-depth investigation into system development and
software engineering standards, related process-based standards were selected.
Questions were then prepared covering the whole system development process by
applying the classical Waterfall life cycle model. Each question is made up of
guidance and suggestions from the international system development standards.
To collect data, people from the information technology departments of three
major banks in Turkey were interviewed. Results have been aggregated by
examining the current process status of the three banks together. Problematic
issues were identified using the international system development standards.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/cs/0504004v1,Statistical analysis of quality measures for mobile ad hoc networks,"How can the quality of a mobile ad hoc network (MANET) be quantified? This
work aims at an answer based on the lower network layers, i.e. on connectivity
between the wireless nodes, using statistical methods. A number of different
quality measures are introduced and classified according to their scaling
behaviour. They are analysed in a statistical model of a 1-dimensional MANET
system (corresponding e.g. to cars on a road). Neglecting boundary effects, the
model turns out to be exactly solvable, so that explicit analytical results for
the quality levels can be obtained both at fixed system size and in the limit
of large systems. In particular, this improves estimates known in the
literature for the probability of connectedness of 1-dimensional MANETs.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0204025v1,Phonology,"Phonology is the systematic study of the sounds used in language, their
internal structure, and their composition into syllables, words and phrases.
Computational phonology is the application of formal and computational
techniques to the representation and processing of phonological information.
This chapter will present the fundamentals of descriptive phonology along with
a brief overview of computational phonology.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1910.06826v1,"Statically Detecting Vulnerabilities by Processing Programming Languages
  as Natural Languages","Web applications continue to be a favorite target for hackers due to a
combination of wide adoption and rapid deployment cycles, which often lead to
the introduction of high impact vulnerabilities. Static analysis tools are
important to search for bugs automatically in the program source code,
supporting developers on their removal. However, building these tools requires
programming the knowledge on how to discover the vulnerabilities. This paper
presents an alternative approach in which tools learn to detect flaws
automatically by resorting to artificial intelligence concepts, more concretely
to natural language processing. The approach employs a sequence model to learn
to characterize vulnerabilities based on an annotated corpus. Afterwards, the
model is utilized to discover and identify vulnerabilities in the source code.
It was implemented in the DEKANT tool and evaluated experimentally with a large
set of PHP applications and WordPress plugins. Overall, we found several
hundred vulnerabilities belonging to 12 classes of input validation
vulnerabilities, where 62 of them were zero-day.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0010015v1,"On Exponential-Time Completeness of the Circularity Problem for
  Attribute Grammars","Attribute grammars (AGs) are a formal technique for defining semantics of
programming languages. Existing complexity proofs on the circularity problem of
AGs are based on automata theory, such as writing pushdown acceptor and
alternating Turing machines. They reduced the acceptance problems of above
automata, which are exponential-time (EXPTIME) complete, to the AG circularity
problem. These proofs thus show that the circularity problem is EXPTIME-hard,
at least as hard as the most difficult problems in EXPTIME. However, none has
given a proof for the EXPTIME-completeness of the problem. This paper first
presents an alternating Turing machine for the circularity problem. The
alternating Turing machine requires polynomial space. Thus, the circularity
problem is in EXPTIME and is then EXPTIME-complete.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1602.01385v1,The Minimum Shared Edges Problem on Planar Graphs,"We study the Minimum Shared Edges problem introduced by Omran et al. [Journal
of Combinatorial Optimization, 2015] on planar graphs: Planar MSE asks, given a
planar graph G = (V,E), two distinct vertices s,t in V , and two integers p, k,
whether there are p s-t paths in G that share at most k edges, where an edges
is called shared if it appears in at least two of the p s-t paths. We show that
Planar MSE is NP-hard by reduction from Vertex Cover. We make use of a
grid-like structure, where the alignment (horizontal/vertical) of the edges in
the grid correspond to selection and validation gadgets respectively.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/quant-ph/0402095v5,Limitations of Quantum Advice and One-Way Communication,"Although a quantum state requires exponentially many classical bits to
describe, the laws of quantum mechanics impose severe restrictions on how that
state can be accessed. This paper shows in three settings that quantum messages
have only limited advantages over classical ones.
  First, we show that $\mathsf{BQP/qpoly}\subseteq\mathsf{PP/poly}$, where
$\mathsf{BQP/qpoly}$ is the class of problems solvable in quantum polynomial
time, given a polynomial-size ""quantum advice state"" that depends only on the
input length. This resolves a question of Buhrman, and means that we should not
hope for an unrelativized separation between quantum and classical advice.
Underlying our complexity result is a general new relation between
deterministic and quantum one-way communication complexities, which applies to
partial as well as total functions.
  Second, we construct an oracle relative to which $\mathsf{NP}\not \subset
\mathsf{BQP/qpoly}$. To do so, we use the polynomial method to give the first
correct proof of a direct product theorem for quantum search. This theorem has
other applications; for example, it can be used to fix a result of Klauck about
quantum time-space tradeoffs for sorting.
  Third, we introduce a new trace distance method for proving lower bounds on
quantum one-way communication complexity. Using this method, we obtain optimal
quantum lower bounds for two problems of Ambainis, for which no nontrivial
lower bounds were previously known even for classical randomized protocols.
  A preliminary version of this paper appeared in the 2004 Conference on
Computational Complexity (CCC).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0201012v1,Efficient Groundness Analysis in Prolog,"Boolean functions can be used to express the groundness of, and trace
grounding dependencies between, program variables in (constraint) logic
programs. In this paper, a variety of issues pertaining to the efficient Prolog
implementation of groundness analysis are investigated, focusing on the domain
of definite Boolean functions, Def. The systematic design of the representation
of an abstract domain is discussed in relation to its impact on the algorithmic
complexity of the domain operations; the most frequently called operations
should be the most lightweight. This methodology is applied to Def, resulting
in a new representation, together with new algorithms for its domain operations
utilising previously unexploited properties of Def -- for instance,
quadratic-time entailment checking. The iteration strategy driving the analysis
is also discussed and a simple, but very effective, optimisation of induced
magic is described. The analysis can be implemented straightforwardly in Prolog
and the use of a non-ground representation results in an efficient, scalable
tool which does not require widening to be invoked, even on the largest
benchmarks. An extensive experimental evaluation is given",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0806.1796v1,Evaluation for Uncertain Image Classification and Segmentation,"Each year, numerous segmentation and classification algorithms are invented
or reused to solve problems where machine vision is needed. Generally, the
efficiency of these algorithms is compared against the results given by one or
many human experts. However, in many situations, the location of the real
boundaries of the objects as well as their classes are not known with certainty
by the human experts. Furthermore, only one aspect of the segmentation and
classification problem is generally evaluated. In this paper we present a new
evaluation method for classification and segmentation of image, where we take
into account both the classification and segmentation results as well as the
level of certainty given by the experts. As a concrete example of our method,
we evaluate an automatic seabed characterization algorithm based on sonar
images.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1212.3817v1,"Probability Bracket Notation: Markov State Chain Projector, Hidden
  Markov Models and Dynamic Bayesian Networks","After a brief discussion of Markov Evolution Formula (MEF) expressed in
Probability Bracket Notation (PBN), its close relation with the joint
probability distribution (JPD) of Visible Markov Models (VMM) is demonstrated
by introducing Markov State Chain Projector (MSCP). The state basis and the
observed basis are defined in the Sequential Event Space (SES) of Hidden Markov
Models (HMM). The JPD of HMM is derived by using basis transformation in SES.
The Viterbi algorithm is revisited and applied to the famous Weather HMM
example, whose node graph and inference results are displayed by using software
package Elvira. In the end, the formulas of VMM, HMM and some factorial HMM
(FHMM) are expressed in PBN as instances of dynamic Bayesian Networks (DBN).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1908.00559v2,"Design by Immersion: A Transdisciplinary Approach to Problem-Driven
  Visualizations","While previous work exists on how to conduct and disseminate insights from
problem-driven visualization projects and design studies, the literature does
not address how to accomplish these goals in transdisciplinary teams in ways
that advance all disciplines involved. In this paper we introduce and define a
new methodological paradigm we call design by immersion, which provides an
alternative perspective on problem-driven visualization work. Design by
immersion embeds transdisciplinary experiences at the center of the
visualization process by having visualization researchers participate in the
work of the target domain (or domain experts participate in visualization
research). Based on our own combined experiences of working on
cross-disciplinary, problem-driven visualization projects, we present six case
studies that expose the opportunities that design by immersion enables,
including (1) exploring new domain-inspired visualization design spaces, (2)
enriching domain understanding through personal experiences, and (3) building
strong transdisciplinary relationships. Furthermore, we illustrate how the
process of design by immersion opens up a diverse set of design activities that
can be combined in different ways depending on the type of collaboration,
project, and goals. Finally, we discuss the challenges and potential pitfalls
of design by immersion.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1410.8774v1,Combinatorics and algorithms for augmenting graphs,"The notion of augmenting graphs generalizes Berge's idea of augmenting
chains, which was used by Edmonds in his celebrated solution of the maximum
matching problem. This problem is a special case of the more general maximum
independent set (MIS) problem. Recently, the augmenting graph approach has been
successfully applied to solve MIS in various other special cases. However, our
knowledge of augmenting graphs is still very limited, and we do not even know
what the minimal infinite classes of augmenting graphs are. In the present
paper, we find an answer to this question and apply it to extend the area of
polynomial-time solvability of the maximum independent set problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.04868v1,"Equivalence Checking and Intersection of Deterministic Timed Finite
  State Machines","There has been a growing interest in defining models of automata enriched
with time, such as finite automata extended with clocks (timed automata). In
this paper, we study deterministic timed finite state machines (TFSMs), i.e.,
finite state machines with a single clock, timed guards and timeouts which
transduce timed input words into timed output words. We solve the problem of
equivalence checking by defining a bisimulation from timed FSMs to untimed ones
and viceversa. Moreover, we apply these bisimulation relations to build the
intersection of two timed finite state machines by untiming them, intersecting
them and transforming back to the timed intersection.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0005023v1,"C++ programming language for an abstract massively parallel SIMD
  architecture","The aim of this work is to define and implement an extended C++ language to
support the SIMD programming paradigm. The C++ programming language has been
extended to express all the potentiality of an abstract SIMD machine consisting
of a central Control Processor and a N-dimensional toroidal array of Numeric
Processors. Very few extensions have been added to the standard C++ with the
goal of minimising the effort for the programmer in learning a new language and
to keep very high the performance of the compiled code. The proposed language
has been implemented as a porting of the GNU C++ Compiler on a SIMD
supercomputer.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1612.06476v2,"Computational Complexity of Testing Proportional Justified
  Representation","We consider a committee voting setting in which each voter approves of a
subset of candidates and based on the approvals, a target number of candidates
are selected. Aziz et al. (2015) proposed two representation axioms called
justified representation and extended justified representation. Whereas the
former can be tested as well as achieved in polynomial time, the latter
property is coNP-complete to test and no polynomial-time algorithm is known to
achieve it. Interestingly, S{\'a}nchez-Fern{\'a}ndez et~al. (2016) proposed an
intermediate property called proportional justified representation that admits
a polynomial-time algorithm to achieve. The complexity of testing proportional
justified representation has remained an open problem. In this paper, we settle
the complexity by proving that testing proportional justified representation is
coNP-complete. We complement the complexity result by showing that the problem
admits efficient algorithms if any of the following parameters are bounded: (1)
number of voters (2) number of candidates (3) maximum number of candidates
approved by a voter (4) maximum number of voters approving a given candidate.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.01731v1,"Strong Consistency and Thomas Decomposition of Finite Difference
  Approximations to Systems of Partial Differential Equations","For a wide class of polynomially nonlinear systems of partial differential
equations we suggest an algorithmic approach that combines differential and
difference algebra to analyze s(trong)-consistency of finite difference
approximations. Our approach is applicable to regular solution grids. For the
grids of this type we give a new definition of s-consistency for finite
difference approximations which generalizes our definition given earlier for
Cartesian grids. The algorithmic verification of s-consistency presented in the
paper is based on the use of both differential and difference Thomas
decomposition. First, we apply the differential decomposition to the input
system, resulting in a partition of its solution space. Then, to the output
subsystem that contains a solution of interest we apply a difference analogue
of the differential Thomas decomposition which allows to check the
s-consistency. For linear and some quasi-linear differential systems one can
also apply difference \Gr bases for the s-consistency analysis. We illustrate
our methods and algorithms by a number of examples, which include Navier-Stokes
equations for viscous incompressible flow.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.00259v2,How much pre-training is enough to discover a good subnetwork?,"Neural network pruning is useful for discovering efficient, high-performing
subnetworks within pre-trained, dense network architectures. However, more
often than not, it involves a three-step process--pre-training, pruning, and
re-training--that is computationally expensive, as the dense model must be
fully pre-trained. Luckily, several works have empirically shown that
high-performing subnetworks can be discovered via pruning without fully
pre-training the dense network. Aiming to theoretically analyze the amount of
dense network pre-training needed for a pruned network to perform well, we
discover a theoretical bound in the number of SGD pre-training iterations on a
two-layer, fully-connected network, beyond which pruning via greedy forward
selection yields a subnetwork that achieves good training error. This threshold
is shown to be logarithmically dependent upon the size of the dataset, meaning
that experiments with larger datasets require more pre-training for subnetworks
obtained via pruning to perform well. We empirically demonstrate the validity
of our theoretical results across a variety of architectures and datasets,
including fully-connected networks trained on MNIST and several deep
convolutional neural network (CNN) architectures trained on CIFAR10 and
ImageNet.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1003.2547v1,The C Object System: Using C as a High-Level Object-Oriented Language,"The C Object System (Cos) is a small C library which implements high-level
concepts available in Clos, Objc and other object-oriented programming
languages: uniform object model (class, meta-class and property-metaclass),
generic functions, multi-methods, delegation, properties, exceptions, contracts
and closures. Cos relies on the programmable capabilities of the C programming
language to extend its syntax and to implement the aforementioned concepts as
first-class objects. Cos aims at satisfying several general principles like
simplicity, extensibility, reusability, efficiency and portability which are
rarely met in a single programming language. Its design is tuned to provide
efficient and portable implementation of message multi-dispatch and message
multi-forwarding which are the heart of code extensibility and reusability.
With COS features in hand, software should become as flexible and extensible as
with scripting languages and as efficient and portable as expected with C
programming. Likewise, Cos concepts should significantly simplify adaptive and
aspect-oriented programming as well as distributed and service-oriented
computing",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1209.5036v7,Proof of Church's Thesis,"We prove that if our calculating capability is that of a universal Turing
machine with a finite tape, then Church's thesis is true. This way we
accomplish Post (1936) program.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1212.1095v4,"The projector algorithm: a simple parallel algorithm for computing
  Voronoi diagrams and Delaunay graphs","The Voronoi diagram is a certain geometric data structure which has numerous
applications in various scientific and technological fields. The theory of
algorithms for computing 2D Euclidean Voronoi diagrams of point sites is rich
and useful, with several different and important algorithms. However, this
theory has been quite steady during the last few decades in the sense that no
essentially new algorithms have entered the game. In addition, most of the
known algorithms are serial in nature and hence cast inherent difficulties on
the possibility to compute the diagram in parallel. In this paper we present
the projector algorithm: a new and simple algorithm which enables the
(combinatorial) computation of 2D Voronoi diagrams. The algorithm is
significantly different from previous ones and some of the involved concepts in
it are in the spirit of linear programming and optics. Parallel implementation
is naturally supported since each Voronoi cell can be computed independently of
the other cells. A new combinatorial structure for representing the cells (and
any convex polytope) is described along the way and the computation of the
induced Delaunay graph is obtained almost automatically.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.06130v2,"On the Trend-corrected Variant of Adaptive Stochastic Optimization
  Methods","Adam-type optimizers, as a class of adaptive moment estimation methods with
the exponential moving average scheme, have been successfully used in many
applications of deep learning. Such methods are appealing due to the capability
on large-scale sparse datasets with high computational efficiency. In this
paper, we present a new framework for Adam-type methods with the trend
information when updating the parameters with the adaptive step size and
gradients. The additional terms in the algorithm promise an efficient movement
on the complex cost surface, and thus the loss would converge more rapidly. We
show empirically the importance of adding the trend component, where our
framework outperforms the conventional Adam and AMSGrad methods constantly on
the classical models with several real-world datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1605.08714v1,I Accidentally the Whole Internet,"Whether as telecommunications or power systems, networks are very important
in everyday life. Maintaining these networks properly functional and connected,
even under attacks or failures, is of special concern. This topic has been
previously studied with a whole network robustness perspective,modeling
networks as undirected graphs (such as roads or simply cables). This
perspective measures the average behavior of the network after its last node
has failed. In this article we propose two alternatives to well-known studies
about the robustness of the backbone Internet: to use a supply network model
and metrics for its representation (we called it the Go-Index), and to use
robustness metrics that can be calculated while disconnections appear. Our
research question is: if a smart adversary has a limited number of strikes to
attack the Internet, how much will the damage be after each one in terms of
network disconnection? Our findings suggest that in order to design robust
networks it might be better to have a complete view of the robustness evolution
of the network, from both the infrastructure and the users perspective.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0005027v1,A Bayesian Reflection on Surfaces,"The topic of this paper is a novel Bayesian continuous-basis field
representation and inference framework. Within this paper several problems are
solved: The maximally informative inference of continuous-basis fields, that is
where the basis for the field is itself a continuous object and not
representable in a finite manner; the tradeoff between accuracy of
representation in terms of information learned, and memory or storage capacity
in bits; the approximation of probability distributions so that a maximal
amount of information about the object being inferred is preserved; an
information theoretic justification for multigrid methodology. The maximally
informative field inference framework is described in full generality and
denoted the Generalized Kalman Filter. The Generalized Kalman Filter allows the
update of field knowledge from previous knowledge at any scale, and new data,
to new knowledge at any other scale. An application example instance, the
inference of continuous surfaces from measurements (for example, camera image
data), is presented.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1109.0069v2,Inter-rater Agreement on Sentence Formality,"Formality is one of the most important dimensions of writing style variation.
In this study we conducted an inter-rater reliability experiment for assessing
sentence formality on a five-point Likert scale, and obtained good agreement
results as well as different rating distributions for different sentence
categories. We also performed a difficulty analysis to identify the bottlenecks
of our rating procedure. Our main objective is to design an automatic scoring
mechanism for sentence-level formality, and this study is important for that
purpose.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1307.7261v1,The Power of Spreadsheet Computations,"We investigate the expressive power of spreadsheets. We consider spreadsheets
which contain only formulas, and assume that they are small templates, which
can be filled to a larger area of the grid to process input data of variable
size. Therefore we can compare them to well-known machine models of
computation. We consider a number of classes of spreadsheets defined by
restrictions on their reference structure. Two of the classes correspond
closely to parallel complexity classes: we prove a direct correspondence
between the dimensions of the spreadsheet and amount of hardware and time used
by a parallel computer to compute the same function. As a tool, we produce
spreadsheets which are universal in these classes, i.e. can emulate any other
spreadsheet from them. In other cases we implement in the spreadsheets in
question instances of a polynomial-time complete problem, which indicates that
the the spreadsheets are unlikely to have efficient parallel evaluation
algorithms. Thus we get a picture how the computational power of spreadsheets
depends on their dimensions and structure of references.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
http://arxiv.org/abs/1302.6224v5,Distributed Computability in Byzantine Asynchronous Systems,"In this work, we extend the topology-based approach for characterizing
computability in asynchronous crash-failure distributed systems to asynchronous
Byzantine systems. We give the first theorem with necessary and sufficient
conditions to solve arbitrary tasks in asynchronous Byzantine systems where an
adversary chooses faulty processes. In our adversarial formulation, outputs of
non-faulty processes are constrained in terms of inputs of non-faulty processes
only. For colorless tasks, an important subclass of distributed problems, the
general result reduces to an elegant model that effectively captures the
relation between the number of processes, the number of failures, as well as
the topological structure of the task's simplicial complexes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1809.00821v1,"The MISRA C Coding Standard and its Role in the Development and Analysis
  of Safety- and Security-Critical Embedded Software","The MISRA project started in 1990 with the mission of providing world-leading
best practice guidelines for the safe and secure application of both embedded
control systems and standalone software. MISRA C is a coding standard defining
a subset of the C language, initially targeted at the automotive sector, but
now adopted across all industry sectors that develop C software in safety-
and/or security-critical contexts. In this paper, we introduce MISRA C, its
role in the development of critical software, especially in embedded systems,
its relevance to industry safety standards, as well as the challenges of
working with a general-purpose programming language standard that is written in
natural language with a slow evolution over the last 40+ years. We also outline
the role of static analysis in the automatic checking of compliance with
respect to MISRA C, and the role of the MISRA C language subset in enabling a
wider application of formal methods to industrial software written in C.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.14110v1,Mining frequency-based sequential trajectory co-clusters,"Co-clustering is a specific type of clustering that addresses the problem of
finding groups of objects without necessarily considering all attributes. This
technique has shown to have more consistent results in high-dimensional sparse
data than traditional clustering. In trajectory co-clustering, the methods
found in the literature have two main limitations: first, the space and time
dimensions have to be constrained by user-defined thresholds; second, elements
(trajectory points) are clustered ignoring the trajectory sequence, assuming
that the points are independent among them. To address the limitations above,
we propose a new trajectory co-clustering method for mining semantic trajectory
co-clusters. It simultaneously clusters the trajectories and their elements
taking into account the order in which they appear. This new method uses the
element frequency to identify candidate co-clusters. Besides, it uses an
objective cost function that automatically drives the co-clustering process,
avoiding the need for constraining dimensions. We evaluate the proposed
approach using real-world a publicly available dataset. The experimental
results show that our proposal finds frequent and meaningful contiguous
sequences revealing mobility patterns, thereby the most relevant elements.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1210.0167v1,Exhaustive Search-based Model for Hybrid Sensor Network,"A new model for a cluster of hybrid sensors network with multi sub-clusters
is proposed. The model is in particular relevant to the early warning system in
a large scale monitoring system in, for example, a nuclear power plant. It
mainly addresses to a safety critical system which requires real-time processes
with high accuracy. The mathematical model is based on the extended
conventional search algorithm with certain interactions among the nearest
neighborhood of sensors. It is argued that the model could realize a highly
accurate decision support system with less number of parameters. A case of one
dimensional interaction function is discussed, and a simple algorithm for the
model is also given.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1309.1290v3,Logspace computations in graph products,"We consider three important and well-studied algorithmic problems in group
theory: the word, geodesic, and conjugacy problem. We show transfer results
from individual groups to graph products. We concentrate on logspace complexity
because the challenge is actually in small complexity classes, only. The most
difficult transfer result is for the conjugacy problem. We have a general
result for graph products, but even in the special case of a graph group the
result is new. Graph groups are closely linked to the theory of Mazurkiewicz
traces which form an algebraic model for concurrent processes. Our proofs are
combinatorial and based on well-known concepts in trace theory. We also use
rewriting techniques over traces. For the group-theoretical part we apply
Bass-Serre theory. But as we need explicit formulae and as we design concrete
algorithms all our group-theoretical calculations are completely explicit and
accessible to non-specialists.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1303.7329v1,Minimal lambda-theories by ultraproducts,"A longstanding open problem in lambda calculus is whether there exist
continuous models of the untyped lambda calculus whose theory is exactly the
least lambda-theory lambda-beta or the least sensible lambda-theory H
(generated by equating all the unsolvable terms). A related question is
whether, given a class of lambda models, there is a minimal lambda-theory
represented by it. In this paper, we give a general tool to answer positively
to this question and we apply it to a wide class of webbed models: the
i-models. The method then applies also to graph models, Krivine models,
coherent models and filter models. In particular, we build an i-model whose
theory is the set of equations satisfied in all i-models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9909019v1,Knowledge in Multi-Agent Systems: Initial Configurations and Broadcast,"The semantic framework for the modal logic of knowledge due to Halpern and
Moses provides a way to ascribe knowledge to agents in distributed and
multi-agent systems. In this paper we study two special cases of this
framework: full systems and hypercubes. Both model static situations in which
no agent has any information about another agent's state. Full systems and
hypercubes are an appropriate model for the initial configurations of many
systems of interest. We establish a correspondence between full systems and
hypercube systems and certain classes of Kripke frames. We show that these
classes of systems correspond to the same logic. Moreover, this logic is also
the same as that generated by the larger class of weakly directed frames. We
provide a sound and complete axiomatization, S5WDn, of this logic. Finally, we
show that under certain natural assumptions, in a model where knowledge evolves
over time, S5WDn characterizes the properties of knowledge not just at the
initial configuration, but also at all later configurations. In particular,
this holds for homogeneous broadcast systems, which capture settings in which
agents are initially ignorant of each others local states, operate
synchronously, have perfect recall and can communicate only by broadcasting.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0202020v3,"The Mysterious Optimality of Naive Bayes: Estimation of the Probability
  in the System of ""Classifiers""","Bayes Classifiers are widely used currently for recognition, identification
and knowledge discovery. The fields of application are, for example, image
processing, medicine, chemistry (QSAR). But by mysterious way the Naive Bayes
Classifier usually gives a very nice and good presentation of a recognition. It
can not be improved considerably by more complex models of Bayes Classifier. We
demonstrate here a very nice and simple proof of the Naive Bayes Classifier
optimality, that can explain this interesting fact.The derivation in the
current paper is based on arXiv:cs/0202020v1",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.09442v1,TADOC: Text Analytics Directly on Compression,"This article provides a comprehensive description of Text Analytics Directly
on Compression (TADOC), which enables direct document analytics on compressed
textual data. The article explains the concept of TADOC and the challenges to
its effective realizations. Additionally, a series of guidelines and technical
solutions that effectively address those challenges, including the adoption of
a hierarchical compression method and a set of novel algorithms and data
structure designs, are presented. Experiments on six data analytics tasks of
various complexities show that TADOC can save 90.8% storage space and 87.9%
memory usage, while halving data processing times.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1508.04854v1,On the Expressiveness of Joining,"The expressiveness of communication primitives has been explored in a common
framework based on the pi-calculus by considering four features: synchronism
(asynchronous vs synchronous), arity (monadic vs polyadic data), communication
medium (shared dataspaces vs channel-based), and pattern-matching (binding to a
name vs testing name equality vs intensionality). Here another dimension
coordination is considered that accounts for the number of processes required
for an interaction to occur. Coordination generalises binary languages such as
pi-calculus to joining languages that combine inputs such as the Join Calculus
and general rendezvous calculus. By means of possibility/impossibility of
encodings, this paper shows coordination is unrelated to the other features.
That is, joining languages are more expressive than binary languages, and no
combination of the other features can encode a joining language into a binary
language. Further, joining is not able to encode any of the other features
unless they could be encoded otherwise.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.01759v1,Tailoring for Every Body: Reshaping Convex Polyhedra,"Given any two convex polyhedra P and Q, we prove as one of our main results
that the surface of P can be reshaped to a homothet of Q by a finite sequence
of ""tailoring"" steps. Each tailoring excises a digon surrounding a single
vertex and sutures the digon closed. One phrasing of this result is that, if Q
can be ""sculpted"" from P by a series of slices with planes, then Q can be
tailored from P. And there is a sense in which tailoring is finer than
sculpting in that P may be tailored to polyhedra that are not achievable by
sculpting P. It is an easy corollary that, if S is the surface of any convex
body, then any convex polyhedron P may be tailored to approximate a homothet of
S as closely as desired. So P can be ""whittled"" to e.g., a sphere S.
  Another main result achieves the same reshaping, but by excising more
complicated shapes we call ""crests,"" still each enclosing one vertex. Reversing
either digon-tailoring or crest-tailoring leads to proofs that any Q inside P
can be enlarged to P by cutting Q and inserting and sealing surface patches.
  One surprising corollary of these results is that, for Q a subset of P, we
can cut-up Q into pieces and paste them non-overlapping onto an isometric
subset of P. This can be viewed as a form of ""unfolding"" Q onto P.
  All our proofs are constructive, and lead to polynomial-time algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.07847v2,"Automated Verification and Synthesis of Embedded Systems using Machine
  Learning","The dependency on the correct functioning of embedded systems is rapidly
growing, mainly due to their wide range of applications, such as micro-grids,
automotive device control, health care, surveillance, mobile devices, and
consumer electronics. Their structures are becoming more and more complex and
now require multi-core processors with scalable shared memory, in order to meet
increasing computational power demands. As a consequence, reliability of
embedded (distributed) software becomes a key issue during system development,
which must be carefully addressed and assured. The present research discusses
challenges, problems, and recent advances to ensure correctness and timeliness
regarding embedded systems. Reliability issues, in the development of
micro-grids and cyber-physical systems, are then considered, as a prominent
verification and synthesis application. In particular, machine learning
techniques emerge as one of the main approaches to learn reliable
implementations of embedded software for achieving a correct-by-construction
design.",0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0504075v1,Dichotomy for Voting Systems,"Scoring protocols are a broad class of voting systems. Each is defined by a
vector $(\alpha_1,\alpha_2,...,\alpha_m)$, $\alpha_1 \geq \alpha_2 \geq >...
\geq \alpha_m$, of integers such that each voter contributes $\alpha_1$ points
to his/her first choice, $\alpha_2$ points to his/her second choice, and so on,
and any candidate receiving the most points is a winner.
  What is it about scoring-protocol election systems that makes some have the
desirable property of being NP-complete to manipulate, while others can be
manipulated in polynomial time? We find the complete, dichotomizing answer:
Diversity of dislike. Every scoring-protocol election system having two or more
point values assigned to candidates other than the favorite--i.e., having
$||\{\alpha_i \condition 2 \leq i \leq m\}||\geq 2$--is NP-complete to
manipulate. Every other scoring-protocol election system can be manipulated in
polynomial time. In effect, we show that--other than trivial systems (where all
candidates alway tie), plurality voting, and plurality voting's transparently
disguised translations--\emph{every} scoring-protocol election system is
NP-complete to manipulate.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1808.00046v1,Lip-Reading Driven Deep Learning Approach for Speech Enhancement,"This paper proposes a novel lip-reading driven deep learning framework for
speech enhancement. The proposed approach leverages the complementary strengths
of both deep learning and analytical acoustic modelling (filtering based
approach) as compared to recently published, comparatively simpler benchmark
approaches that rely only on deep learning. The proposed audio-visual (AV)
speech enhancement framework operates at two levels. In the first level, a
novel deep learning-based lip-reading regression model is employed. In the
second level, lip-reading approximated clean-audio features are exploited,
using an enhanced, visually-derived Wiener filter (EVWF), for the clean audio
power spectrum estimation. Specifically, a stacked long-short-term memory
(LSTM) based lip-reading regression model is designed for clean audio features
estimation using only temporal visual features considering different number of
prior visual frames. For clean speech spectrum estimation, a new
filterbank-domain EVWF is formulated, which exploits estimated speech features.
The proposed EVWF is compared with conventional Spectral Subtraction and
Log-Minimum Mean-Square Error methods using both ideal AV mapping and LSTM
driven AV mapping. The potential of the proposed speech enhancement framework
is evaluated under different dynamic real-world commercially-motivated
scenarios (e.g. cafe, public transport, pedestrian area) at different SNR
levels (ranging from low to high SNRs) using benchmark Grid and ChiME3 corpora.
For objective testing, perceptual evaluation of speech quality is used to
evaluate the quality of restored speech. For subjective testing, the standard
mean-opinion-score method is used with inferential statistics. Comparative
simulation results demonstrate significant lip-reading and speech enhancement
improvement in terms of both speech quality and speech intelligibility.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1408.0629v5,Bounds for variables with few occurrences in conjunctive normal forms,"We investigate connections between SAT (the propositional satisfiability
problem) and combinatorics, around the minimum degree (number of occurrences)
of variables in various forms of redundancy-free boolean conjunctive normal
forms (clause-sets).
  Lean clause-sets do not have non-trivial autarkies, that is, it is not
possible to satisfy some clauses and leave the other clauses untouched. The
deficiency of a clause-set is the difference of the number of clauses and the
number of variables. We prove a precise upper bound on the minimum variable
degree of lean clause-sets in dependency on the deficiency. If a clause-set
does not fulfil this upper bound, then it must have a non-trivial autarky; we
show that the autarky-reduction (elimination of affected clauses) can be done
in polynomial time, while it is open to find the autarky itself in polynomial
time.
  Then we investigate this upper bound for the special case of minimally
unsatisfiable clause-sets. We show that the bound can be improved here,
introducing a general method to improve the underlying recurrence.
  We consider precise relations, and thus the investigations have a
number-theoretical flavour. We try to build a bridge from logic to
combinatorics (especially to hypergraph colouring), and we discuss thoroughly
the background and open problems, and provide many examples and explanations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1910.14594v2,Deep Learning for 2D and 3D Rotatable Data: An Overview of Methods,"Convolutional networks are successful due to their equivariance/invariance
under translations. However, rotatable data such as images, volumes, shapes, or
point clouds require processing with equivariance/invariance under rotations in
cases where the rotational orientation of the coordinate system does not affect
the meaning of the data (e.g. object classification). On the other hand,
estimation/processing of rotations is necessary in cases where rotations are
important (e.g. motion estimation). There has been recent progress in methods
and theory in all these regards. Here we provide an overview of existing
methods, both for 2D and 3D rotations (and translations), and identify
commonalities and links between them.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.06825v2,A Generalized Lottery Ticket Hypothesis,"We introduce a generalization to the lottery ticket hypothesis in which the
notion of ""sparsity"" is relaxed by choosing an arbitrary basis in the space of
parameters. We present evidence that the original results reported for the
canonical basis continue to hold in this broader setting. We describe how
structured pruning methods, including pruning units or factorizing
fully-connected layers into products of low-rank matrices, can be cast as
particular instances of this ""generalized"" lottery ticket hypothesis. The
investigations reported here are preliminary and are provided to encourage
further research along this direction.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1604.08066v3,"The Manne et al. self-stabilizing 2/3-approximation matching algorithm
  is sub-exponential","Manne et al. designed the first algorithm computing a maximal matching that
is a 2/3 -approximation of the maximum matching in $O(^2n)$ moves. However, the
complexity tightness was not proved. In this paper, we exhibit a
sub-exponential execution of this matching algorithm.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0707.3979v1,"Clifford Algebra of the Vector Space of Conics for decision boundary
  Hyperplanes in m-Euclidean Space","In this paper we embed $m$-dimensional Euclidean space in the geometric
algebra $Cl_m $ to extend the operators of incidence in ${R^m}$ to operators of
incidence in the geometric algebra to generalize the notion of separator to a
decision boundary hyperconic in the Clifford algebra of hyperconic sections
denoted as ${Cl}({Co}_{2})$. This allows us to extend the concept of a linear
perceptron or the spherical perceptron in conformal geometry and introduce the
more general conic perceptron, namely the {elliptical perceptron}. Using
Clifford duality a vector orthogonal to the decision boundary hyperplane is
determined. Experimental results are shown in 2-dimensional Euclidean space
where we separate data that are naturally separated by some typical plane conic
separators by this procedure. This procedure is more general in the sense that
it is independent of the dimension of the input data and hence we can speak of
the hyperconic elliptic perceptron.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.01864v1,"From Human Mesenchymal Stromal Cells to Osteosarcoma Cells
  Classification by Deep Learning","Early diagnosis of cancer often allows for a more vast choice of therapy
opportunities. After a cancer diagnosis, staging provides essential information
about the extent of disease in the body and the expected response to a
particular treatment. The leading importance of classifying cancer patients at
the early stage into high or low-risk groups has led many research teams, both
from the biomedical and bioinformatics field, to study the application of Deep
Learning (DL) methods. The ability of DL to detect critical features from
complex datasets is a significant achievement in early diagnosis and cell
cancer progression. In this paper, we focus the attention on osteosarcoma.
Osteosarcoma is one of the primary malignant bone tumors which usually afflicts
people in adolescence. Our contribution to the classification of osteosarcoma
cells is made as follows: a DL approach is applied to discriminate human
Mesenchymal Stromal Cells (MSCs) from osteosarcoma cells and to classify the
different cell populations under investigation. Glass slides of differ-ent cell
populations were cultured including MSCs, differentiated in healthy bone cells
(osteoblasts) and osteosarcoma cells, both single cell populations or mixed.
Images of such samples of isolated cells (single-type of mixed) are recorded
with traditional optical microscopy. DL is then applied to identify and
classify single cells. Proper data augmentation techniques and cross-fold
validation are used to appreciate the capabilities of a convolutional neural
network to address the cell detection and classification problem. Based on the
results obtained on individual cells, and to the versatility and scalability of
our DL approach, the next step will be its application to discriminate and
classify healthy or cancer tissues to advance digital pathology.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1809.05381v1,Tools of computer simulation in learning physics,"The article deals with the problem of intellectual development of students in
learning of physics by means of computer simulation. The main objectives of
teaching computer simulation in learning of physics is the general outlook
development, mastery of modelling as a method of knowledge, the development of
practical skills of computer simulation, the implementation of inter-subject
relationship, development and professionalization of computer skills, building
skills of project activities. It is shown that the means of computer simulation
of physical processes is one of the components of intellectual learning
environment. It is spoken in detail about classification of simulation software
such as software of demonstration and modelling and educational software tools
which means environment of activity. Using software of demonstration and
modelling allows for establish interdisciplinary communication, improve the
quality of knowledge, create a positive motivation, enhance student interest in
the subject. Educational software tools which means environment of activity is
an intelligent educational systems, the use of which contributes to the
understanding of the essence of the logical relationship between the original
and models, especially the construction of models, form the students an idea of
modelling as a method of learning about the world. Accordingly the adequate use
of educational software tools which means environment of activity in learning
of physics is contribute to the development of intelligence of students.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1508.06918v1,"Classification of the Extremely Low Frequency Magnetic Field Radiation
  Measurement from the Laptop Computers","The paper considers the level of the extremely low-frequency magnetic field,
which is produced by laptop computers. The magnetic field, which is
characterized by extremely low frequencies up to 300 Hz is measured due to its
hazardous effects to the laptop user's health. The experiment consists of
testing 13 different laptop computers in normal operation conditions. The
measuring of the magnetic field is performed in the adjacent neighborhood of
the laptop computers. The measured data are presented and then classified. The
classification is performed by the K-Medians method in order to determine the
critical positions of the laptop. At the end, the measured magnetic field
values are compared with the critical values suggested by different safety
standards. It is shown that some of the laptop computers emit a very strong
magnetic field. Hence, they must be used with extreme caution.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.14711v2,Explanation-Aware Experience Replay in Rule-Dense Environments,"Human environments are often regulated by explicit and complex rulesets.
Integrating Reinforcement Learning (RL) agents into such environments motivates
the development of learning mechanisms that perform well in rule-dense and
exception-ridden environments such as autonomous driving on regulated roads. In
this paper, we propose a method for organising experience by means of
partitioning the experience buffer into clusters labelled on a per-explanation
basis. We present discrete and continuous navigation environments compatible
with modular rulesets and 9 learning tasks. For environments with explainable
rulesets, we convert rule-based explanations into case-based explanations by
allocating state-transitions into clusters labelled with explanations. This
allows us to sample experiences in a curricular and task-oriented manner,
focusing on the rarity, importance, and meaning of events. We label this
concept Explanation-Awareness (XA). We perform XA experience replay (XAER) with
intra and inter-cluster prioritisation, and introduce XA-compatible versions of
DQN, TD3, and SAC. Performance is consistently superior with XA versions of
those algorithms, compared to traditional Prioritised Experience Replay
baselines, indicating that explanation engineering can be used in lieu of
reward engineering for environments with explainable features.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9301104v1,Natural Deduction as Higher-Order Resolution,"An interactive theorem prover, Isabelle, is under development. In LCF, each
inference rule is represented by one function for forwards proof and another (a
tactic) for backwards proof. In Isabelle, each inference rule is represented by
a Horn clause. Resolution gives both forwards and backwards proof, supporting a
large class of logics. Isabelle has been used to prove theorems in
Martin-L\""of's Constructive Type Theory. Quantifiers pose several difficulties:
substitution, bound variables, Skolemization. Isabelle's representation of
logical syntax is the typed lambda-calculus, requiring higher- order
unification. It may have potential for logic programming. Depth-first
subgoaling along inference rules constitutes a higher-order Prolog.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0912.2269v4,Computing a Discrete Logarithm in O(n^3),"This paper presents a means with time complexity of at worst O(n^3) to
compute the discrete logarithm on cyclic finite groups of integers modulo p.
The algorithm makes use of reduction of the problem to that of finding the
concurrent zeros of two periodic functions in the real numbers. The problem is
treated as an analog to a form of analog rotor-code computed cipher.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.04938v3,Dune-CurvedGrid -- A Dune module for surface parametrization,"In this paper we introduce and describe an implementation of curved (surface)
geometries within the Dune framework for grid-based discretizations. Therefore,
we employ the abstraction of geometries as local-functions bound to a grid
element, and the abstraction of a grid as connectivity of elements together
with a grid-function that can be localized to the elements to provide element
local parametrizations of the curved surface.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.08540v5,A Reduced Semantics for Deciding Trace Equivalence,"Many privacy-type properties of security protocols can be modelled using
trace equivalence properties in suitable process algebras. It has been shown
that such properties can be decided for interesting classes of finite processes
(i.e., without replication) by means of symbolic execution and constraint
solving. However, this does not suffice to obtain practical tools. Current
prototypes suffer from a classical combinatorial explosion problem caused by
the exploration of many interleavings in the behaviour of processes.
M\""odersheim et al. have tackled this problem for reachability properties using
partial order reduction techniques. We revisit their work, generalize it and
adapt it for equivalence checking. We obtain an optimisation in the form of a
reduced symbolic semantics that eliminates redundant interleavings on the fly.
The obtained partial order reduction technique has been integrated in a tool
called APTE. We conducted complete benchmarks showing dramatic improvements.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1510.00738v1,Rank Aggregation: New Bounds for MCx,"The rank aggregation problem has received significant recent attention within
the computer science community. Its applications today range far beyond the
original aim of building metasearch engines to problems in machine learning,
recommendation systems and more. Several algorithms have been proposed for
these problems, and in many cases approximation guarantees have been proven for
them. However, it is also known that some Markov chain based algorithms (MC1,
MC2, MC3, MC4) perform extremely well in practice, yet had no known performance
guarantees. We prove supra-constant lower bounds on approximation guarantees
for all of them. We also raise the lower bound for sorting by Copeland score
from 3/2 to 2 and prove an upper bound of 11, before showing that in particular
ways, MC4 can nevertheless be seen as a generalization of Copeland score.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.3097v2,Computer-Aided Proof of Erdos Discrepancy Properties,"In 1930s Paul Erdos conjectured that for any positive integer $C$ in any
infinite $\pm 1$ sequence $(x_n)$ there exists a subsequence $x_d, x_{2d},
x_{3d},\dots, x_{kd}$, for some positive integers $k$ and $d$, such that $\mid
\sum_{i=1}^k x_{i\cdot d} \mid >C$. The conjecture has been referred to as one
of the major open problems in combinatorial number theory and discrepancy
theory. For the particular case of $C=1$ a human proof of the conjecture
exists; for $C=2$ a bespoke computer program had generated sequences of length
$1124$ of discrepancy $2$, but the status of the conjecture remained open even
for such a small bound. We show that by encoding the problem into Boolean
satisfiability and applying the state of the art SAT solvers, one can obtain a
discrepancy $2$ sequence of length $1160$ and a proof of the Erd\H{o}s
discrepancy conjecture for $C=2$, claiming that no discrepancy 2 sequence of
length $1161$, or more, exists. In the similar way, we obtain a precise bound
of $127\,645$ on the maximal lengths of both multiplicative and completely
multiplicative sequences of discrepancy $3$. We also demonstrate that
unrestricted discrepancy 3 sequences can be longer than $130\,000$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1701.06431v1,Feasibility Study of Social Media for Public Health Behaviour Changes,"Social networking sites such as Twitter and Facebook have been shown to
function as effective social sensors that can ""feel the pulse"" of a community.
The aim of the current study is to test the feasibility of designing,
implementing and evaluating a bespoke social media-enabled intervention that
can be effective for sharing and changing knowledge, attitudes and behaviours
in meaningful ways to promote public health, specifically with regards to
prevention of skin cancer. We present the design and implementation details of
the campaign followed by summary findings and analysis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1311.2362v1,"Efficient Runtime Monitoring with Metric Temporal Logic: A Case Study in
  the Android Operating System","We present a design and an implementation of a security policy specification
language based on metric linear-time temporal logic (MTL). MTL features
temporal operators that are indexed by time intervals, allowing one to specify
timing-dependent security policies. The design of the language is driven by the
problem of runtime monitoring of applications in mobile devices. A main case
the study is the privilege escalation attack in the Android operating system,
where an app gains access to certain resource or functionalities that are not
explicitly granted to it by the user, through indirect control flow. To capture
these attacks, we extend MTL with recursive definitions, that are used to
express call chains betwen apps. We then show how the metric operators of MTL,
in combination with recursive definitions, can be used to specify policies to
detect privilege escalation, under various fine grained constraints. We present
a new algorithm, extending that of linear time temporal logic, for monitoring
safety policies written in our specification language. The monitor does not
need to store the entire history of events generated by the apps, something
that is crucial for practical implementations. We modified the Android OS
kernel to allow us to insert our generated monitors modularly. We have tested
the modified OS on an actual device, and show that it is effective in detecting
policy violations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.05229v1,Machine Translation of Mathematical Text,"We have implemented a machine translation system, the PolyMath Translator,
for LaTeX documents containing mathematical text. The current implementation
translates English LaTeX to French LaTeX, attaining a BLEU score of 53.5 on a
held-out test corpus of mathematical sentences. It produces LaTeX documents
that can be compiled to PDF without further editing. The system first converts
the body of an input LaTeX document into English sentences containing math
tokens, using the pandoc universal document converter to parse LaTeX input. We
have trained a Transformer-based translator model, using OpenNMT, on a combined
corpus containing a small proportion of domain-specific sentences. Our full
system uses both this Transformer model and Google Translate, the latter being
used as a backup to better handle linguistic features that do not appear in our
training dataset. If the Transformer model does not have confidence in its
translation, as determined by a high perplexity score, then we use Google
Translate with a custom glossary. This backup was used 26% of the time on our
test corpus of mathematical sentences. The PolyMath Translator is available as
a web service at www.polymathtrans.ai.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.02124v1,Specification of the Giskard Consensus Protocol,"The Giskard consensus protocol is used to validate transactions and
computations in the PlatON network. In this paper, we provide a rigorous
specification of Giskard, suitable to serve as a reference in protocol
implementation and in formal verification. Using our specification, we prove
that the protocol guarantees several notable safety properties.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0306059v1,The Use of HepRep in GLAST,"HepRep is a generic, hierarchical format for description of graphics
representables that can be augmented by physics information and relational
properties. It was developed for high energy physics event display applications
and is especially suited to client/server or component frameworks. The GLAST
experiment, an international effort led by NASA for a gamma-ray telescope to
launch in 2006, chose HepRep to provide a flexible, extensible and maintainable
framework for their event display without tying their users to any one graphics
application. To support HepRep in their GUADI infrastructure, GLAST developed a
HepRep filler and builder architecture. The architecture hides the details of
XML and CORBA in a set of base and helper classes allowing physics experts to
focus on what data they want to represent. GLAST has two GAUDI services:
HepRepSvc, which registers HepRep fillers in a global registry and allows the
HepRep to be exported to XML, and CorbaSvc, which allows the HepRep to be
published through a CORBA interface and which allows the client application to
feed commands back to GAUDI (such as start next event, or run some GAUDI
algorithm). GLAST's HepRep solution gives users a choice of client
applications, WIRED (written in Java) or FRED (written in C++ and Ruby), and
leaves them free to move to any future HepRep-compliant event display.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0912.4164v1,Distance k-Sectors Exist,"The bisector of two nonempty sets P and Q in a metric space is the set of all
points with equal distance to P and to Q. A distance k-sector of P and Q, where
k is an integer, is a (k-1)-tuple (C_1, C_2, ..., C_{k-1}) such that C_i is the
bisector of C_{i-1} and C_{i+1} for every i = 1, 2, ..., k-1, where C_0 = P and
C_k = Q. This notion, for the case where P and Q are points in Euclidean plane,
was introduced by Asano, Matousek, and Tokuyama, motivated by a question of
Murata in VLSI design. They established the existence and uniqueness of the
distance trisector in this special case. We prove the existence of a distance
k-sector for all k and for every two disjoint, nonempty, closed sets P and Q in
Euclidean spaces of any (finite) dimension, or more generally, in proper
geodesic spaces (uniqueness remains open). The core of the proof is a new
notion of k-gradation for P and Q, whose existence (even in an arbitrary metric
space) is proved using the Knaster-Tarski fixed point theorem, by a method
introduced by Reem and Reich for a slightly different purpose.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.09529v1,Term Interrelations and Trends in Software Engineering,"The Software Engineering (SE) community is prolific, making it challenging
for experts to keep up with the flood of new papers and for neophytes to enter
the field. Therefore, we posit that the community may benefit from a tool
extracting terms and their interrelations from the SE community's text corpus
and showing terms' trends. In this paper, we build a prototyping tool using the
word embedding technique. We train the embeddings on the SE Body of Knowledge
handbook and 15,233 research papers' titles and abstracts. We also create test
cases necessary for validation of the training of the embeddings. We provide
representative examples showing that the embeddings may aid in summarizing
terms and uncovering trends in the knowledge base.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
http://arxiv.org/abs/2012.15351v1,A Decentralized Dynamic PKI based on Blockchain,"The central role of the certificate authority (CA) in traditional public key
infrastructure (PKI) makes it fragile and prone to compromises and operational
failures. Maintaining CAs and revocation lists is demanding especially in
loosely-connected and large systems. Log-based PKIs have been proposed as a
remedy but they do not solve the problem effectively. We provide a general
model and a solution for decentralized and dynamic PKI based on a blockchain
and web of trust model where the traditional CA and digital certificates are
removed and instead, everything is registered on the blockchain. Registration,
revocation, and update of public keys are based on a consensus mechanism
between a certain number of entities that are already part of the system. Any
node which is part of the system can be an auditor and initiate the revocation
procedure once it finds out malicious activities. Revocation lists are no
longer required as any node can efficiently verify the public keys through
witnesses.",0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1005.0675v1,Empowered by Wireless Communication: Self-Organizing Traffic Collectives,"In recent years, tremendous progress has been made in understanding the
dynamics of vehicle traffic flow and traffic congestion by interpreting traffic
as a multi-particle system. This helps to explain the onset and persistence of
many undesired phenomena, e.g., traffic jams. It also reflects the apparent
helplessness of drivers in traffic, who feel like passive particles that are
pushed around by exterior forces; one of the crucial aspects is the inability
to communicate and coordinate with other traffic participants. We present
distributed methods for solving these fundamental problems, employing modern
wireless, ad-hoc, multi-hop networks. The underlying idea is to use these
capabilities as the basis for self-organizing methods for coordinating data
collection and processing, recognizing traffic phenomena, and changing their
structure by coordinated behavior. The overall objective is a multi-level
approach that reaches from protocols for local wireless communication, data
dissemination, pattern recognition, over hierarchical structuring and
coordinated behavior, all the way to large-scale traffic regulation. In this
article we describe three types of results: (i) self-organizing and distributed
methods for maintaining and collecting data (using our concept of Hovering Data
Clouds); (ii) adaptive data dissemination for traffic information systems;
(iii) methods for self-recognition of traffic jams. We conclude by describing
higher-level aspects of our work.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1402.4950v3,"On algorithmic equivalence of instruction sequences for computing bit
  string functions","Every partial function from bit strings of a given length to bit strings of a
possibly different given length can be computed by a finite instruction
sequence that contains only instructions to set and get the content of Boolean
registers, forward jump instructions, and a termination instruction. We look
for an equivalence relation on instruction sequences of this kind that captures
to a reasonable degree the intuitive notion that two instruction sequences
express the same algorithm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1905.05434v1,"Joint Power Control and Rate Allocation enabling Ultra-Reliability and
  Energy Efficiency in SIMO Wireless Networks","Coming cellular systems are envisioned to open up to new services with
stringent reliability and energy efficiency requirements. In this paper we
focus on the joint power control and rate allocation problem in Single-Input
Multiple-Output (SIMO) wireless systems with Rayleigh fading and stringent
reliability constraints. We propose an allocation scheme that maximizes the
energy efficiency of the system while making use only of average statistics of
the signal and interference, and the number of antennas $M$ that are available
at the receiver side. We show the superiority of the Maximum Ratio Combining
(MRC) scheme over Selection Combining (SC) in terms of energy efficiency, and
prove that the gap between the optimum allocated resources converges to
$(M!)^(1/(2M))$ as the reliability constraint becomes more stringent.
Meanwhile, in most cases MRC was also shown to be more energy efficient than
Switch and Stay Combining (SSC) scheme, although this does not hold only when
operating with extremely large $M$, extremely high/small average
signal/interference power and/or highly power consuming receiving circuitry.
Numerical results show the feasibility of the ultra-reliable operation when $M$
increases, while greater the fixed power consumption and/or drain efficiency of
the transmit amplifier is, the greater the optimum transmit power and rate.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0506041v3,Competitive on-line learning with a convex loss function,"We consider the problem of sequential decision making under uncertainty in
which the loss caused by a decision depends on the following binary
observation. In competitive on-line learning, the goal is to design decision
algorithms that are almost as good as the best decision rules in a wide
benchmark class, without making any assumptions about the way the observations
are generated. However, standard algorithms in this area can only deal with
finite-dimensional (often countable) benchmark classes. In this paper we give
similar results for decision rules ranging over an arbitrary reproducing kernel
Hilbert space. For example, it is shown that for a wide class of loss functions
(including the standard square, absolute, and log loss functions) the average
loss of the master algorithm, over the first $N$ observations, does not exceed
the average loss of the best decision rule with a bounded norm plus
$O(N^{-1/2})$. Our proof technique is very different from the standard ones and
is based on recent results about defensive forecasting. Given the probabilities
produced by a defensive forecasting algorithm, which are known to be well
calibrated and to have good resolution in the long run, we use the expected
loss minimization principle to find a suitable decision.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1502.07659v2,"On the complexity of computing the $k$-restricted edge-connectivity of a
  graph","The \emph{$k$-restricted edge-connectivity} of a graph $G$, denoted by
$\lambda_k(G)$, is defined as the minimum size of an edge set whose removal
leaves exactly two connected components each containing at least $k$ vertices.
This graph invariant, which can be seen as a generalization of a minimum
edge-cut, has been extensively studied from a combinatorial point of view.
However, very little is known about the complexity of computing $\lambda_k(G)$.
Very recently, in the parameterized complexity community the notion of
\emph{good edge separation} of a graph has been defined, which happens to be
essentially the same as the $k$-restricted edge-connectivity. Motivated by the
relevance of this invariant from both combinatorial and algorithmic points of
view, in this article we initiate a systematic study of its computational
complexity, with special emphasis on its parameterized complexity for several
choices of the parameters. We provide a number of NP-hardness and W[1]-hardness
results, as well as FPT-algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0802.3480v1,Why Task-Based Training is Superior to Traditional Training Methods,"The risks of spreadsheet use do not just come from the misuse of formulae. As
such, training needs to go beyond this technical aspect of spreadsheet use and
look at the spreadsheet in its full business context. While standard training
is by and large unable to do this, task-based training is perfectly suited to a
contextual approach to training.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1
http://arxiv.org/abs/cs/0405109v1,Conversation Analysis and the User Experience,"We provide two case studies in the application of ideas drawn from
conversation analysis to the design of technologies that enhance the experience
of human conversation. We first present a case study of the design of an
electronic guidebook, focusing on how conversation analytic principles played a
role in the design process. We then discuss how the guidebook project has
inspired our continuing work in social, mobile audio spaces. In particular, we
describe some as yet unrealized concepts for adaptive audio spaces.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.13035v1,Enactive Mandala: Audio-visualizing Brain Waves,"We are exploring the design and implementation of artificial expressions,
kinetic audio-visual representations of real-time physiological data that
reflect emotional and cognitive state. In this work, we demonstrate a
prototype, the Enactive Mandala, which maps real-time EEG signals to modulate
ambient music and animated visual music. Transparent real-time audio-visual
feedback of brainwave qualities supports intuitive insight into the connection
between thoughts and physiological states.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0506008v1,Bounds on the Automata Size for Presburger Arithmetic,"Automata provide a decision procedure for Presburger arithmetic. However,
until now only crude lower and upper bounds were known on the sizes of the
automata produced by this approach. In this paper, we prove an upper bound on
the the number of states of the minimal deterministic automaton for a
Presburger arithmetic formula. This bound depends on the length of the formula
and the quantifiers occurring in the formula. The upper bound is established by
comparing the automata for Presburger arithmetic formulas with the formulas
produced by a quantifier elimination method. We also show that our bound is
tight, even for nondeterministic automata. Moreover, we provide optimal
automata constructions for linear equations and inequations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.07802v1,Hybrid computer approach to train a machine learning system,"This book chapter describes a novel approach to training machine learning
systems by means of a hybrid computer setup i.e. a digital computer tightly
coupled with an analog computer. As an example a reinforcement learning system
is trained to balance an inverted pendulum which is simulated on an analog
computer, thus demonstrating a solution to the major challenge of adequately
simulating the environment for reinforcement learning.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1902.01461v2,(Near) Optimal Adaptivity Gaps for Stochastic Multi-Value Probing,"Consider a kidney-exchange application where we want to find a max-matching
in a random graph. To find whether an edge $e$ exists, we need to perform an
expensive test, in which case the edge $e$ appears independently with a
\emph{known} probability $p_e$. Given a budget on the total cost of the tests,
our goal is to find a testing strategy that maximizes the expected maximum
matching size.
  The above application is an example of the stochastic probing problem. In
general the optimal stochastic probing strategy is difficult to find because it
is \emph{adaptive}---decides on the next edge to probe based on the outcomes of
the probed edges. An alternate approach is to show the \emph{adaptivity gap} is
small, i.e., the best \emph{non-adaptive} strategy always has a value close to
the best adaptive strategy. This allows us to focus on designing non-adaptive
strategies that are much simpler. Previous works, however, have focused on
Bernoulli random variables that can only capture whether an edge appears or
not. In this work we introduce a multi-value stochastic probing problem, which
can also model situations where the weight of an edge has a probability
distribution over multiple values.
  Our main technical contribution is to obtain (near) optimal bounds for the
(worst-case) adaptivity gaps for multi-value stochastic probing over
prefix-closed constraints. For a monotone submodular function, we show the
adaptivity gap is at most $2$ and provide a matching lower bound. For a
weighted rank function of a $k$-extendible system (a generalization of
intersection of $k$ matroids), we show the adaptivity gap is between $O(k\log
k)$ and $k$. None of these results were known even in the Bernoulli case where
both our upper and lower bounds also apply, thereby resolving an open question
of Gupta et al.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0701088v2,A Theory and Calculus for Reasoning about Sequential Behavior,"Basic results in combinatorial mathematics provide the foundation for a
theory and calculus for reasoning about sequential behavior. A key concept of
the theory is a generalization of Boolean implicant which deals with statements
of the form:
  A sequence of Boolean expressions alpha is an implicant of a set of sequences
of Boolean expressions A
  This notion of a generalized implicant takes on special significance when
each of the sequences in the set A describes a disallowed pattern of behavior.
That is because a disallowed sequence of Boolean expressions represents a
logical/temporal dependency, and because the implicants of a set of disallowed
Boolean sequences A are themselves disallowed and represent precisely those
dependencies that follow as a logical consequence from the dependencies
represented by A. The main result of the theory is a necessary and sufficient
condition for a sequence of Boolean expressions to be an implicant of a regular
set of sequences of Boolean expressions. This result is the foundation for two
new proof methods. Sequential resolution is a generalization of Boolean
resolution which allows new logical/temporal dependencies to be inferred from
existing dependencies. Normalization starts with a model (system) and a set of
logical/temporal dependencies and determines which of those dependencies are
satisfied by the model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1503.00448v2,The Routing of Complex Contagion in Kleinberg's Small-World Networks,"In Kleinberg's small-world network model, strong ties are modeled as
deterministic edges in the underlying base grid and weak ties are modeled as
random edges connecting remote nodes. The probability of connecting a node $u$
with node $v$ through a weak tie is proportional to $1/|uv|^\alpha$, where
$|uv|$ is the grid distance between $u$ and $v$ and $\alpha\ge 0$ is the
parameter of the model. Complex contagion refers to the propagation mechanism
in a network where each node is activated only after $k \ge 2$ neighbors of the
node are activated.
  In this paper, we propose the concept of routing of complex contagion (or
complex routing), where we can activate one node at one time step with the goal
of activating the targeted node in the end. We consider decentralized routing
scheme where only the weak ties from the activated nodes are revealed. We study
the routing time of complex contagion and compare the result with simple
routing and complex diffusion (the diffusion of complex contagion, where all
nodes that could be activated are activated immediately in the same step with
the goal of activating all nodes in the end).
  We show that for decentralized complex routing, the routing time is lower
bounded by a polynomial in $n$ (the number of nodes in the network) for all
range of $\alpha$ both in expectation and with high probability (in particular,
$\Omega(n^{\frac{1}{\alpha+2}})$ for $\alpha \le 2$ and
$\Omega(n^{\frac{\alpha}{2(\alpha+2)}})$ for $\alpha > 2$ in expectation),
while the routing time of simple contagion has polylogarithmic upper bound when
$\alpha = 2$. Our results indicate that complex routing is harder than complex
diffusion and the routing time of complex contagion differs exponentially
compared to simple contagion at sweetspot.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.03721v3,Demystifying the Draft EU Artificial Intelligence Act,"In April 2021, the European Commission proposed a Regulation on Artificial
Intelligence, known as the AI Act. We present an overview of the Act and
analyse its implications, drawing on scholarship ranging from the study of
contemporary AI practices to the structure of EU product safety regimes over
the last four decades. Aspects of the AI Act, such as different rules for
different risk-levels of AI, make sense. But we also find that some provisions
of the Draft AI Act have surprising legal implications, whilst others may be
largely ineffective at achieving their stated goals. Several overarching
aspects, including the enforcement regime and the risks of maximum
harmonisation pre-empting legitimate national AI policy, engender significant
concern. These issues should be addressed as a priority in the legislative
process.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
http://arxiv.org/abs/2003.07596v1,"Construe: a software solution for the explanation-based interpretation
  of time series","This paper presents a software implementation of a general framework for time
series interpretation based on abductive reasoning. The software provides a
data model and a set of algorithms to make inference to the best explanation of
a time series, resulting in a description in multiple abstraction levels of the
processes underlying the time series. As a proof of concept, a comprehensive
knowledge base for the electrocardiogram (ECG) domain is provided, so it can be
used directly as a tool for ECG analysis. This tool has been successfully
validated in several noteworthy problems, such as heartbeat classification or
atrial fibrillation detection.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.1547v1,Efficient algorithms to decide tightness,"Tightness is a generalisation of the notion of convexity: a space is tight if
and only if it is ""as convex as possible"", given its topological constraints.
For a simplicial complex, deciding tightness has a straightforward exponential
time algorithm, but efficient methods to decide tightness are only known in the
trivial setting of triangulated surfaces.
  In this article, we present a new polynomial time procedure to decide
tightness for triangulations of $3$-manifolds -- a problem which previously was
thought to be hard. Furthermore, we describe an algorithm to decide general
tightness in the case of $4$-dimensional combinatorial manifolds which is fixed
parameter tractable in the treewidth of the $1$-skeletons of their vertex
links, and we present an algorithm to decide $\mathbb{F}_2$-tightness for weak
pseudomanifolds $M$ of arbitrary but fixed dimension which is fixed parameter
tractable in the treewidth of the dual graph of $M$.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1503.03378v1,Browserbite: Cross-Browser Testing via Image Processing,"Cross-browser compatibility testing is concerned with identifying perceptible
differences in the way a Web page is rendered across different browsers or
configurations thereof. Existing automated cross-browser compatibility testing
methods are generally based on Document Object Model (DOM) analysis, or in some
cases, a combination of DOM analysis with screenshot capture and image
processing. DOM analysis however may miss incompatibilities that arise not
during DOM construction, but rather during rendering. Conversely, DOM analysis
produces false alarms because different DOMs may lead to identical or
sufficiently similar renderings. This paper presents a novel method for
cross-browser testing based purely on image processing. The method relies on
image segmentation to extract regions from a Web page and computer vision
techniques to extract a set of characteristic features from each region.
Regions extracted from a screenshot taken on a baseline browser are compared
against regions extracted from the browser under test based on characteristic
features. A machine learning classifier is used to determine if differences
between two matched regions should be classified as an incompatibility. An
evaluation involving 140 pages shows that the proposed method achieves an
F-score exceeding 0.9, outperforming a state-of-the-art cross-browser testing
tool based on DOM analysis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1007.3926v1,"Ear Identification by Fusion of Segmented Slice Regions using Invariant
  Features: An Experimental Manifold with Dual Fusion Approach","This paper proposes a robust ear identification system which is developed by
fusing SIFT features of color segmented slice regions of an ear. The proposed
ear identification method makes use of Gaussian mixture model (GMM) to build
ear model with mixture of Gaussian using vector quantization algorithm and K-L
divergence is applied to the GMM framework for recording the color similarity
in the specified ranges by comparing color similarity between a pair of
reference ear and probe ear. SIFT features are then detected and extracted from
each color slice region as a part of invariant feature extraction. The
extracted keypoints are then fused separately by the two fusion approaches,
namely concatenation and the Dempster-Shafer theory. Finally, the fusion
approaches generate two independent augmented feature vectors which are used
for identification of individuals separately. The proposed identification
technique is tested on IIT Kanpur ear database of 400 individuals and is found
to achieve 98.25% accuracy for identification while top 5 matched criteria is
set for each subject.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0812.1061v6,Decidability of the Equivalence of Multi-Letter Quantum Finite Automata,"Multi-letter {\it quantum finite automata} (QFAs) were a quantum variant of
classical {\it one-way multi-head finite automata} (J. Hromkovi\v{c}, Acta
Informatica 19 (1983) 377-384), and it has been shown that this new one-way
QFAs (multi-letter QFAs) can accept with no error some regular languages
$(a+b)^{*}b$ that are unacceptable by the previous one-way QFAs. In this paper,
we study the decidability of the equivalence of multi-letter QFAs, and the main
technical contributions are as follows: (1) We show that any two automata, a
$k_{1}$-letter QFA ${\cal A}_1$ and a $k_{2}$-letter QFA ${\cal A}_2$, over the
same input alphabet $\Sigma$ are equivalent if and only if they are
$(n^2m^{k-1}-m^{k-1}+k)$-equivalent, where $m=|\Sigma|$ is the cardinality of
$\Sigma$, $k=\max(k_{1},k_{2})$, and $n=n_{1}+n_{2}$, with $n_{1}$ and $n_{2}$
being the numbers of states of ${\cal A}_{1}$ and ${\cal A}_{2}$, respectively.
When $k=1$, we obtain the decidability of equivalence of measure-once QFAs in
the literature. It is worth mentioning that our technical method is essentially
different from that for the decidability of the case of single input alphabet
(i.e., $m=1$). (2) However, if we determine the equivalence of multi-letter
QFAs by checking all strings of length not more than $ n^2m^{k-1}-m^{k-1}+k$,
then the worst time complexity is exponential, i.e.,
$O(n^6m^{n^2m^{k-1}-m^{k-1}+2k-1})$. Therefore, we design a polynomial-time
$O(m^{2k-1}n^{8}+km^kn^{6})$ algorithm for determining the equivalence of any
two multi-letter QFAs. Here, the time complexity is concerning the number of
states in the multi-letter QFAs, and $k$ is thought of as a constant.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.07354v2,Scheduling Under Power and Energy Constraints,"Given a system model where machines have distinct speeds and power ratings
but are otherwise compatible, we consider various problems of scheduling under
resource constraints on the system which place the restriction that not all
machines can be run at once. These can be power, energy, or makespan
constraints on the system. Given such constraints, there are problems with
divisible as well as non-divisible jobs. In the setting where there is a
constraint on power, we show that the problem of minimizing makespan for a set
of divisible jobs is NP-hard by reduction to the knapsack problem. We then show
that scheduling to minimize energy with power constraints is also NP-hard. We
then consider scheduling with energy and makespan constraints with divisible
jobs and show that these can be solved in polynomial time, and the problems
with non-divisible jobs are NP-hard. We give exact and approximation algorithms
for these problems as required.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1707.08209v1,On the letter frequencies and entropy of written Marathi,"We carry out a comprehensive analysis of letter frequencies in contemporary
written Marathi. We determine sets of letters which statistically predominate
any large generic Marathi text, and use these sets to estimate the entropy of
Marathi.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.07077v5,"Yet Another Representation of Binary Decision Trees: A Mathematical
  Demonstration","A decision tree looks like a simple computational graph without cycles, where
only the leaf nodes specify the output values and the non-terminals specify
their tests or split conditions. From the numerical perspective, we express
decision trees in the language of computational graph. We explicitly
parameterize the test phase, traversal phase and prediction phase of decision
trees based on the bitvectors of non-terminal nodes. As shown later, the
decision tree is a shallow binary network in some sense. Especially, we
introduce the bitvector matrix to implement the tree traversal in numerical
approach, where the core is to convert the logical `AND' operation to
arithmetic operations. And we apply this numerical representation to extend and
unify diverse decision trees in concept.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.13114v1,"Deep CHORES: Estimating Hallmark Measures of Physical Activity Using
  Deep Learning","Wrist accelerometers for assessing hallmark measures of physical activity
(PA) are rapidly growing with the advent of smartwatch technology. Given the
growing popularity of wrist-worn accelerometers, there needs to be a rigorous
evaluation for recognizing (PA) type and estimating energy expenditure (EE)
across the lifespan. Participants (66% women, aged 20-89 yrs) performed a
battery of 33 daily activities in a standardized laboratory setting while a
tri-axial accelerometer collected data from the right wrist. A portable
metabolic unit was worn to measure metabolic intensity. We built deep learning
networks to extract spatial and temporal representations from the time-series
data, and used them to recognize PA type and estimate EE. The deep learning
models resulted in high performance; the F1 score was: 0.82, 0.81, and 95 for
recognizing sedentary, locomotor, and lifestyle activities, respectively. The
root mean square error was 1.1 (+/-0.13) for the estimation of EE.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1204.1082v2,Set It and Forget It: Approximating the Set Once Strip Cover Problem,"We consider the Set Once Strip Cover problem, in which n wireless sensors are
deployed over a one-dimensional region. Each sensor has a fixed battery that
drains in inverse proportion to a radius that can be set just once, but
activated at any time. The problem is to find an assignment of radii and
activation times that maximizes the length of time during which the entire
region is covered. We show that this problem is NP-hard. Second, we show that
RoundRobin, the algorithm in which the sensors simply take turns covering the
entire region, has a tight approximation guarantee of 3/2 in both Set Once
Strip Cover and the more general Strip Cover problem, in which each radius may
be set finitely-many times. Moreover, we show that the more general class of
duty cycle algorithms, in which groups of sensors take turns covering the
entire region, can do no better. Finally, we give an optimal O(n^2 log n)-time
algorithm for the related Set Radius Strip Cover problem, in which all sensors
must be activated immediately.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.05724v3,A Compact Index for Order-Preserving Pattern Matching,"Order-preserving pattern matching was introduced recently but it has already
attracted much attention. Given a reference sequence and a pattern, we want to
locate all substrings of the reference sequence whose elements have the same
relative order as the pattern elements. For this problem we consider the
offline version in which we build an index for the reference sequence so that
subsequent searches can be completed very efficiently. We propose a
space-efficient index that works well in practice despite its lack of good
worst-case time bounds. Our solution is based on the new approach of
decomposing the indexed sequence into an order component, containing ordering
information, and a delta component, containing information on the absolute
values. Experiments show that this approach is viable, faster than the
available alternatives, and it is the first one offering simultaneously small
space usage and fast retrieval.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0205077v1,Designing Multi-Commodity Flow Trees,"The traditional multi-commodity flow problem assumes a given flow network in
which multiple commodities are to be maximally routed in response to given
demands. This paper considers the multi-commodity flow network-design problem:
given a set of multi-commodity flow demands, find a network subject to certain
constraints such that the commodities can be maximally routed.
  This paper focuses on the case when the network is required to be a tree. The
main result is an approximation algorithm for the case when the tree is
required to be of constant degree. The algorithm reduces the problem to the
minimum-weight balanced-separator problem; the performance guarantee of the
algorithm is within a factor of 4 of the performance guarantee of the
balanced-separator procedure. If Leighton and Rao's balanced-separator
procedure is used, the performance guarantee is O(log n). This improves the
O(log^2 n) approximation factor that is trivial to obtain by a direct
application of the balanced-separator method.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.11714v1,Self-sovereign identity as a tool for digital democracy,"The importance of digital identity as a foundation for digital public
services is considered. As the classical, centralised model digital identity
has proven to be subject to several limitations, self-sovereign identities are
proposed as replacement, especially in the context of e-government platforms
and direct participation to policymaking (e.g. through e-voting tools).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0
http://arxiv.org/abs/1709.00001v1,The Painter's Problem: covering a grid with colored connected polygons,"Motivated by a new way of visualizing hypergraphs, we study the following
problem. Consider a rectangular grid and a set of colors $\chi$. Each cell $s$
in the grid is assigned a subset of colors $\chi_s \subseteq \chi$ and should
be partitioned such that for each color $c\in \chi_s$ at least one piece in the
cell is identified with $c$. Cells assigned the empty color set remain white.
We focus on the case where $\chi = \{\text{red},\text{blue}\}$. Is it possible
to partition each cell in the grid such that the unions of the resulting red
and blue pieces form two connected polygons? We analyze the combinatorial
properties and derive a necessary and sufficient condition for such a painting.
We show that if a painting exists, there exists a painting with bounded
complexity per cell. This painting has at most five colored pieces per cell if
the grid contains white cells, and at most two colored pieces per cell if it
does not.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.13974v3,Competence-Aware Path Planning via Introspective Perception,"Robots deployed in the real world over extended periods of time need to
reason about unexpected failures, learn to predict them, and to proactively
take actions to avoid future failures. Existing approaches for competence-aware
planning are either model-based, requiring explicit enumeration of known
failure modes, or purely statistical, using state- and location-specific
failure statistics to infer competence. We instead propose a structured
model-free approach to competence-aware planning by reasoning about plan
execution failures due to errors in perception, without requiring a priori
enumeration of failure sources or requiring location-specific failure
statistics. We introduce competence-aware path planning via introspective
perception (CPIP), a Bayesian framework to iteratively learn and exploit
task-level competence in novel deployment environments. CPIP factorizes the
competence-aware planning problem into two components. First, perception errors
are learned in a model-free and location-agnostic setting via introspective
perception prior to deployment in novel environments. Second, during actual
deployments, the prediction of task-level failures is learned in a
context-aware setting. Experiments in a simulation show that the proposed CPIP
approach outperforms the frequentist baseline in multiple mobile robot tasks,
and is further validated via real robot experiments in an environment with
perceptually challenging obstacles and terrain.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.03702v3,"Media Cloud: Massive Open Source Collection of Global News on the Open
  Web","We present the first full description of Media Cloud, an open source platform
based on crawling hyperlink structure in operation for over 10 years, that for
many uses will be the best way to collect data for studying the media ecosystem
on the open web. We document the key choices behind what data Media Cloud
collects and stores, how it processes and organizes these data, and its open
API access as well as user-facing tools. We also highlight the strengths and
limitations of the Media Cloud collection strategy compared to relevant
alternatives. We give an overview two sample datasets generated using Media
Cloud and discuss how researchers can use the platform to create their own
datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,1,0,0,0,0
http://arxiv.org/abs/1805.05515v1,Parameterized Model Checking Modulo Explicit Weak Memory Models,"We present a modular framework for model checking parameterized array-based
transition systems with explicit access operations on weak memory. Our approach
extends the MCMT (Model Checking Modulo Theories) framework of Ghilardi and
Ranise with explicit weak memory models. We have implemented this new framework
in Cubicle-W, an extension of the Cubicle model checker. The modular
architecture of our tool allows us to change the underlying memory model
seamlessly (TSO, PSO...). Our first experiments with a TSO-like memory model
look promising.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1602.01659v1,Accelerating Local Search for the Maximum Independent Set Problem,"Computing high-quality independent sets quickly is an important problem in
combinatorial optimization. Several recent algorithms have shown that
kernelization techniques can be used to find exact maximum independent sets in
medium-sized sparse graphs, as well as high-quality independent sets in huge
sparse graphs that are intractable for exact (exponential-time) algorithms.
However, a major drawback of these algorithms is that they require significant
preprocessing overhead, and therefore cannot be used to find a high-quality
independent set quickly.
  In this paper, we show that performing simple kernelization techniques in an
online fashion significantly boosts the performance of local search, and is
much faster than pre-computing a kernel using advanced techniques. In addition,
we show that cutting high-degree vertices can boost local search performance
even further, especially on huge (sparse) complex networks. Our experiments
show that we can drastically speed up the computation of large independent sets
compared to other state-of-the-art algorithms, while also producing results
that are very close to the best known solutions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1203.2041v1,Towards MAC/Anycast Diversity in IEEE 802.11n MIMO Networks,"Opportunistic Routing (OR) is a novel routing technique for wireless mesh
networks that exploits the broadcast nature of the wireless medium. OR combines
frames from multiple receivers and therefore creates a form of Spatial
Diversity, called MAC Diversity. The gain from OR is especially high in
networks where the majority of links has a high packet loss probability. The
updated IEEE 802.11n standard improves the physical layer with the ability to
use multiple transmit and receive antennas, i.e. Multiple-Input and
Multiple-Output (MIMO), and therefore already offers spatial diversity on the
physical layer, i.e. called Physical Diversity, which improves the reliability
of a wireless link by reducing its error rate. In this paper we quantify the
gain from MAC diversity as utilized by OR in the presence of PHY diversity as
provided by a MIMO system like 802.11n. We experimented with an IEEE 802.11n
indoor testbed and analyzed the nature of packet losses. Our experiment results
show negligible MAC diversity gains for both interference-prone 2.4 GHz and
interference-free 5 GHz channels when using 802.11n. This is different to the
observations made with single antenna systems based on 802.11b/g, as well as in
initial studies with 802.11n.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.09986v2,"A Survey on Human and Personality Vulnerability Assessment in
  Cyber-security: Challenges, Approaches, and Open Issues","These days, cyber-criminals target humans rather than machines since they try
to accomplish their malicious intentions by exploiting the weaknesses of end
users. Thus, human vulnerabilities pose a serious threat to the security and
integrity of computer systems and data. The human tendency to trust and help
others, as well as personal, social, and cultural characteristics, are
indicative of the level of susceptibility that one may exhibit towards certain
attack types and deception strategies. This work aims to investigate the
factors that affect human susceptibility by studying the existing literature
related to this subject. The objective is also to explore and describe state of
the art human vulnerability assessment models, current prevention, and
mitigation approaches regarding user susceptibility, as well as educational and
awareness raising training strategies. Following the review of the literature,
several conclusions are reached. Among them, Human Vulnerability Assessment has
been included in various frameworks aiming to assess the cyber security
capacity of organizations, but it concerns a one time assessment rather than a
continuous practice. Moreover, human maliciousness is still neglected from
current Human Vulnerability Assessment frameworks; thus, insider threat actors
evade identification, which may lead to an increased cyber security risk.
Finally, this work proposes a user susceptibility profile according to the
factors stemming from our research.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0
http://arxiv.org/abs/0905.4717v3,"Reengineering PDF-Based Documents Targeting Complex Software
  Specifications","This article aims at reengineering of PDF-based complex documents, where
specifications of the Object Management Group (OMG) are our initial targets.
Our motivation is that such specifications are dense and intricate to use, and
tend to have complicated structures. Our objective is therefore to create an
approach that allows us to reengineer PDF-based documents, and to illustrate
how to make more usable versions of electronic documents (such as
specifications, technical books, etc) so that end users to have a better
experience with them. The first step was to extract the logical structure of
the document in a meaningful XML format for subsequent processing. Our initial
assumption was that, many key concepts of a document are expressed in this
structure. In the next phase, we created a multilayer hypertext version of the
document to facilitate browsing and navigating. Although we initially focused
on OMG software specifications, we chose a general approach for different
phases of our work including format conversions, logical structure extraction,
text extraction, multilayer hypertext generation, and concept exploration. As a
consequence, we can process other complex documents to achieve our goals.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1707.03903v2,"Negative Sampling Improves Hypernymy Extraction Based on Projection
  Learning","We present a new approach to extraction of hypernyms based on projection
learning and word embeddings. In contrast to classification-based approaches,
projection-based methods require no candidate hyponym-hypernym pairs. While it
is natural to use both positive and negative training examples in supervised
relation extraction, the impact of negative examples on hypernym prediction was
not studied so far. In this paper, we show that explicit negative examples used
for regularization of the model significantly improve performance compared to
the state-of-the-art approach of Fu et al. (2014) on three datasets from
different languages.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1504.04498v1,"Simpler, faster and shorter labels for distances in graphs","We consider how to assign labels to any undirected graph with n nodes such
that, given the labels of two nodes and no other information regarding the
graph, it is possible to determine the distance between the two nodes. The
challenge in such a distance labeling scheme is primarily to minimize the
maximum label lenght and secondarily to minimize the time needed to answer
distance queries (decoding). Previous schemes have offered different trade-offs
between label lengths and query time. This paper presents a simple algorithm
with shorter labels and shorter query time than any previous solution, thereby
improving the state-of-the-art with respect to both label length and query time
in one single algorithm. Our solution addresses several open problems
concerning label length and decoding time and is the first improvement of label
length for more than three decades.
  More specifically, we present a distance labeling scheme with label size (log
3)/2 + o(n) (logarithms are in base 2) and O(1) decoding time. This outperforms
all existing results with respect to both size and decoding time, including
Winkler's (Combinatorica 1983) decade-old result, which uses labels of size
(log 3)n and O(n/log n) decoding time, and Gavoille et al. (SODA'01), which
uses labels of size 11n + o(n) and O(loglog n) decoding time. In addition, our
algorithm is simpler than the previous ones. In the case of integral edge
weights of size at most W, we present almost matching upper and lower bounds
for label sizes. For r-additive approximation schemes, where distances can be
off by an additive constant r, we give both upper and lower bounds. In
particular, we present an upper bound for 1-additive approximation schemes
which, in the unweighted case, has the same size (ignoring second order terms)
as an adjacency scheme: n/2. We also give results for bipartite graphs and for
exact and 1-additive distance oracles.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1204.2903v2,Disconnectivity and Relative Positions in Simultaneous Embeddings,"The problem Simultaneous Embedding with Fixed Edges (SEFE) asks for two
planar graph $G^1 = (V^1, E^1)$ and $G^2 = (V^2, E^2)$ sharing a common
subgraph $G = G^1 \cap G^2$ whether they admit planar drawings such that the
common graph is drawn the same in both. Previous results on this problem
require $G$, $G^1$ and $G^2$ to be connected. This paper is a first step
towards solving instances where these graphs are disconnected.
  First, we show that an instance of the general SEFE-problem can be reduced in
linear time to an equivalent instance where $V^1 = V^2$ and $G^1$ and $G^2$ are
connected. This shows that it can be assumed without loss of generality that
both input graphs are connected. Second, we consider instances where $G$ is
disconnected. We show that SEFE can be solved in linear time if $G$ is a family
of disjoint cycles by introducing the CC-tree, which represents all
simultaneous embeddings. We extend these results (including the CC-tree) to the
case where $G$ consists of arbitrary connected components, each with a fixed
embedding.
  Note that previous results require $G$ to be connected and thus do not need
to care about relative positions of connected components. By contrast, we
assume the embedding of each connected component to be fixed and thus focus on
these relative positions. As SEFE requires to deal with both, embeddings of
connected components and their relative positions, this complements previous
work.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.07861v1,"Symmetry Breaking in the Congest Model: Time- and Message-Efficient
  Algorithms for Ruling Sets","We study local symmetry breaking problems in the CONGEST model, focusing on
ruling set problems, which generalize the fundamental Maximal Independent Set
(MIS) problem. A $\beta$-ruling set is an independent set such that every node
in the graph is at most $\beta$ hops from a node in the independent set. Our
work is motivated by the following central question: can we break the
$\Theta(\log n)$ time complexity barrier and the $\Theta(m)$ message complexity
barrier in the CONGEST model for MIS or closely-related symmetry breaking
problems? We present the following results:
  - Time Complexity: We show that we can break the $O(\log n)$ ""barrier"" for 2-
and 3-ruling sets. We compute 3-ruling sets in $O\left(\frac{\log n}{\log \log
n}\right)$ rounds with high probability (whp). More generally we show that
2-ruling sets can be computed in $O\left(\log \Delta \cdot (\log n)^{1/2 +
\varepsilon} + \frac{\log n}{\log\log n}\right)$ rounds for any $\varepsilon >
0$, which is $o(\log n)$ for a wide range of $\Delta$ values (e.g., $\Delta =
2^{(\log n)^{1/2-\varepsilon}}$). These are the first 2- and 3-ruling set
algorithms to improve over the $O(\log n)$-round complexity of Luby's algorithm
in the CONGEST model.
  - Message Complexity: We show an $\Omega(n^2)$ lower bound on the message
complexity of computing an MIS (i.e., 1-ruling set) which holds also for
randomized algorithms and present a contrast to this by showing a randomized
algorithm for 2-ruling sets that, whp, uses only $O(n \log^2 n)$ messages and
runs in $O(\Delta \log n)$ rounds. This is the first message-efficient
algorithm known for ruling sets, which has message complexity nearly linear in
$n$ (which is optimal up to a polylogarithmic factor).",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1209.3516v3,An FMM Based on Dual Tree Traversal for Many-core Architectures,"The present work attempts to integrate the independent efforts in the fast
N-body community to create the fastest N-body library for many-core and
heterogenous architectures. Focus is placed on low accuracy optimizations, in
response to the recent interest to use FMM as a preconditioner for sparse
linear solvers. A direct comparison with other state-of-the-art fast N-body
codes demonstrates that orders of magnitude increase in performance can be
achieved by careful selection of the optimal algorithm and low-level
optimization of the code. The current N-body solver uses a fast multipole
method with an efficient strategy for finding the list of cell-cell
interactions by a dual tree traversal. A task-based threading model is used to
maximize thread-level parallelism and intra-node load-balancing. In order to
extract the full potential of the SIMD units on the latest CPUs, the inner
kernels are optimized using AVX instructions. Our code -- exaFMM -- is an order
of magnitude faster than the current state-of-the-art FMM codes, which are
themselves an order of magnitude faster than the average FMM code.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1906.10877v1,Security Rating Metrics for Distributed Wireless Systems,"The paper examines quantitative assessment of wireless distribution system
security, as well as an assessment of risks from attacks and security
violations. Furthermore, it describes typical security breach and formal attack
models and five methods for assessing security. The proposed normalized method
for assessing the degree of security assurance operates with at least three
characteristics, which allows comparatively analyze heterogeneous information
systems. The improved calculating formulas have been proposed for two security
assessment methods, and the elements of functional-cost analysis have been
applied to calculate the degree of security. To check the results of the
analysis, the coefficient of concordance was calculated, which gives
opportunity to determine the quality of expert assessment. The simultaneous use
of several models to describe attacks and the effectiveness of countering them
allows us to create a comprehensive approach to countering modern security
threats to information networks at the commercial enterprises and critical
infrastructure facilities.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1012.2599v1,"A Tutorial on Bayesian Optimization of Expensive Cost Functions, with
  Application to Active User Modeling and Hierarchical Reinforcement Learning","We present a tutorial on Bayesian optimization, a method of finding the
maximum of expensive cost functions. Bayesian optimization employs the Bayesian
technique of setting a prior over the objective function and combining it with
evidence to get a posterior function. This permits a utility-based selection of
the next observation to make on the objective function, which must take into
account both exploration (sampling from areas of high uncertainty) and
exploitation (sampling areas likely to offer improvement over the current best
observation). We also present two detailed extensions of Bayesian optimization,
with experiments---active user modelling with preferences, and hierarchical
reinforcement learning---and a discussion of the pros and cons of Bayesian
optimization based on our experiences.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0108004v1,Links tell us about lexical and semantic Web content,"The latest generation of Web search tools is beginning to exploit hypertext
link information to improve ranking\cite{Brin98,Kleinberg98} and
crawling\cite{Menczer00,Ben-Shaul99etal,Chakrabarti99} algorithms. The hidden
assumption behind such approaches, a correlation between the graph structure of
the Web and its content, has not been tested explicitly despite increasing
research on Web topology\cite{Lawrence98,Albert99,Adamic99,Butler00}. Here I
formalize and quantitatively validate two conjectures drawing connections from
link information to lexical and semantic Web content. The clink-content
conjecture states that a page is similar to the pages that link to it, i.e.,
one can infer the lexical content of a page by looking at the pages that link
to it. I also show that lexical inferences based on link cues are quite
heterogeneous across Web communities. The link-cluster conjecture states that
pages about the same topic are clustered together, i.e., one can infer the
meaning of a page by looking at its neighbours. These results explain the
success of the newest search technologies and open the way for more dynamic and
scalable methods to locate information in a topic or user driven way.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0605064v2,Modal Logics of Topological Relations,"Logical formalisms for reasoning about relations between spatial regions play
a fundamental role in geographical information systems, spatial and constraint
databases, and spatial reasoning in AI. In analogy with Halpern and Shoham's
modal logic of time intervals based on the Allen relations, we introduce a
family of modal logics equipped with eight modal operators that are interpreted
by the Egenhofer-Franzosa (or RCC8) relations between regions in topological
spaces such as the real plane. We investigate the expressive power and
computational complexity of logics obtained in this way. It turns out that our
modal logics have the same expressive power as the two-variable fragment of
first-order logic, but are exponentially less succinct. The complexity ranges
from (undecidable and) recursively enumerable to highly undecidable, where the
recursively enumerable logics are obtained by considering substructures of
structures induced by topological spaces. As our undecidability results also
capture logics based on the real line, they improve upon undecidability results
for interval temporal logics by Halpern and Shoham. We also analyze modal
logics based on the five RCC5 relations, with similar results regarding the
expressive power, but weaker results regarding the complexity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.12967v1,"Modelling Art Interpretation and Meaning. A Data Model for Describing
  Iconology and Iconography","Iconology is a branch of art history that investigates the meaning of
artworks in relation to their social and cultural background. Nowadays, several
interdisciplinary research fields leverage theoretical frameworks close to
iconology to pursue quantitative Art History with data science methods and
Semantic Web technologies. However, while Iconographic studies have been
recently addressed in ontologies, a complete description of aspects relevant to
iconological studies is still missing. In this article, we present a
preliminary study on eleven case studies selected from the literature and we
envision new terms for extending existing ontologies. We validate new terms
according to a common evaluation method and we discuss our results in the light
of the opportunities that such an extended ontology would arise in the
community of Digital Art History.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0509063v1,Order Independence and Rationalizability,"Two natural strategy elimination procedures have been studied for strategic
games. The first one involves the notion of (strict, weak, etc) dominance and
the second the notion of rationalizability. In the case of dominance the
criterion of order independence allowed us to clarify which notions and under
what circumstances are robust. In the case of rationalizability this criterion
has not been considered. In this paper we investigate the problem of order
independence for rationalizability by focusing on three naturally entailed
reduction relations on games. These reduction relations are distinguished by
the adopted reference point for the notion of a better response. Additionally,
they are parametrized by the adopted system of beliefs. We show that for one
reduction relation the outcome of its (possibly transfinite) iterations does
not depend on the order of elimination of the strategies. This result does not
hold for the other two reduction relations. However, under a natural assumption
the iterations of all three reduction relations yield the same outcome. The
obtained order independence results apply to the frameworks considered in
Bernheim 84 and Pearce 84. For finite games the iterations of all three
reduction relations coincide and the order independence holds for three natural
systems of beliefs considered in the literature.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1112.3783v1,L-FLAT: Logtalk Toolkit for Formal Languages and Automata Theory,"We describe L-FLAT, a Logtalk Toolkit for teaching Formal Languages and
Automata Theory. L-FLAT supports the definition of \textsl{alphabets}, the
definition of \textsl{orders} over alphabet symbols, the partial definition of
\textsl{languages} using unit tests, and the definition of \textsl{mechanisms},
which implement language generators or language recognizers. Supported
mechanisms include \textsl{predicates}, \textsl{regular expressions},
\textsl{finite automata}, \textsl{context-free grammars}, \textsl{Turing
machines}, and \textsl{push-down automata}. L-FLAT entities are implemented
using the object-oriented features of Logtalk, providing a highly portable and
easily extendable framework. The use of L-FLAT in educational environments is
enhanced by supporting Mooshak, a web application that features automatic
grading of submitted programs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1001.3259v1,Research Agenda in Cloud Technologies,"Cloud computing is the latest effort in delivering computing resources as a
service. It represents a shift away from computing as a product that is
purchased, to computing as a service that is delivered to consumers over the
internet from large-scale data centres - or ""clouds"". Whilst cloud computing is
gaining growing popularity in the IT industry, academia appeared to be lagging
behind the rapid developments in this field. This paper is the first systematic
review of peer-reviewed academic research published in this field, and aims to
provide an overview of the swiftly developing advances in the technical
foundations of cloud computing and their research efforts. Structured along the
technical aspects on the cloud agenda, we discuss lessons from related
technologies; advances in the introduction of protocols, interfaces, and
standards; techniques for modelling and building clouds; and new use-cases
arising through cloud computing.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.12682v4,Pantomorphic Perspective for Immersive Imagery,"A wide choice of cinematic lenses enables motion-picture creators to adapt
image visual-appearance to their creative vision. Such choice does not exist in
the realm of real-time computer graphics, where only one type of perspective
projection is widely used. This work provides a perspective imaging model that
in an artistically convincing manner resembles anamorphic photography lens
variety and more. It presents an asymmetrical-anamorphic azimuthal projection
map with natural vignetting and realistic chromatic aberration. The
mathematical model for this projection has been chosen such that its parameters
reflect the psycho-physiological aspects of visual perception. That enables its
use in artistic and professional environments, where specific aspects of the
photographed space are to be presented.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1511.07865v2,"Structural Resolution: a Framework for Coinductive Proof Search and
  Proof Construction in Horn Clause Logic","Logic programming (LP) is a programming language based on first-order Horn
clause logic that uses SLD-resolution as a semi-decision procedure. Finite
SLD-computations are inductively sound and complete with respect to least
Herbrand models of logic programs. Dually, the corecursive approach to
SLD-resolution views infinite SLD-computations as successively approximating
infinite terms contained in programs' greatest complete Herbrand models.
State-of-the-art algorithms implementing corecursion in LP are based on loop
detection. However, such algorithms support inference of logical entailment
only for rational terms, and they do not account for the important property of
productivity in infinite SLD-computations. Loop detection thus lags behind
coinductive methods in interactive theorem proving (ITP) and term-rewriting
systems (TRS).
  Structural resolution is a newly proposed alternative to SLD-resolution that
makes it possible to define and semi-decide a notion of productivity
appropriate to LP. In this paper, we prove soundness of structural resolution
relative to Herbrand model semantics for productive inductive, coinductive, and
mixed inductive-coinductive logic programs.
  We introduce two algorithms that support coinductive proof search for
infinite productive terms. One algorithm combines the method of loop detection
with productive structural resolution, thus guaranteeing productivity of
coinductive proofs for infinite rational terms. The other allows to make lazy
sound observations of fragments of infinite irrational productive terms. This
puts coinductive methods in LP on par with productivity-based observational
approaches to coinduction in ITP and TRS.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0808.4133v1,"Tableau-based decision procedure for the multi-agent epistemic logic
  with operators of common and distributed knowledge","We develop an incremental-tableau-based decision procedure for the
multi-agent epistemic logic MAEL(CD) (aka S5_n (CD)), whose language contains
operators of individual knowledge for a finite set Ag of agents, as well as
operators of distributed and common knowledge among all agents in Ag. Our
tableau procedure works in (deterministic) exponential time, thus establishing
an upper bound for MAEL(CD)-satisfiability that matches the (implicit)
lower-bound known from earlier results, which implies ExpTime-completeness of
MAEL(CD)-satisfiability. Therefore, our procedure provides a complexity-optimal
algorithm for checking MAEL(CD)-satisfiability, which, however, in most cases
is much more efficient. We prove soundness and completeness of the procedure,
and illustrate it with an example.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0506033v4,"An Event-driven Operator Model for Dynamic Simulation of Construction
  Machinery","Prediction and optimisation of a wheel loader's dynamic behaviour is a
challenge due to tightly coupled, non-linear subsystems of different technical
domains. Furthermore, a simulation regarding performance, efficiency, and
operability cannot be limited to the machine itself, but has to include
operator, environment, and work task. This paper presents some results of our
approach to an event-driven simulation model of a human operator. Describing
the task and the operator model independently of the machine's technical
parameters, gives the possibility to change whole sub-system characteristics
without compromising the relevance and validity of the simulation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1001.4119v2,The tropical double description method,"We develop a tropical analogue of the classical double description method
allowing one to compute an internal representation (in terms of vertices) of a
polyhedron defined externally (by inequalities). The heart of the tropical
algorithm is a characterization of the extreme points of a polyhedron in terms
of a system of constraints which define it. We show that checking the
extremality of a point reduces to checking whether there is only one minimal
strongly connected component in an hypergraph. The latter problem can be solved
in almost linear time, which allows us to eliminate quickly redundant
generators. We report extensive tests (including benchmarks from an application
to static analysis) showing that the method outperforms experimentally the
previous ones by orders of magnitude. The present tools also lead to worst case
bounds which improve the ones provided by previous methods.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1607.03432v3,Tight lower bounds for the complexity of multicoloring,"In the multicoloring problem, also known as ($a$:$b$)-coloring or $b$-fold
coloring, we are given a graph G and a set of $a$ colors, and the task is to
assign a subset of $b$ colors to each vertex of G so that adjacent vertices
receive disjoint color subsets. This natural generalization of the classic
coloring problem (the $b=1$ case) is equivalent to finding a homomorphism to
the Kneser graph $KG_{a,b}$, and gives relaxations approaching the fractional
chromatic number.
  We study the complexity of determining whether a graph has an
($a$:$b$)-coloring. Our main result is that this problem does not admit an
algorithm with running time $f(b)\cdot 2^{o(\log b)\cdot n}$, for any
computable $f(b)$, unless the Exponential Time Hypothesis (ETH) fails. A
$(b+1)^n\cdot \text{poly}(n)$-time algorithm due to Nederlof [2008] shows that
this is tight. A direct corollary of our result is that the graph homomorphism
problem does not admit a $2^{O(n+h)}$ algorithm unless ETH fails, even if the
target graph is required to be a Kneser graph. This refines the understanding
given by the recent lower bound of Cygan et al. [SODA 2016].
  The crucial ingredient in our hardness reduction is the usage of detecting
matrices of Lindstr\""om [Canad. Math. Bull., 1965], which is a combinatorial
tool that, to the best of our knowledge, has not yet been used for proving
complexity lower bounds. As a side result, we prove that the running time of
the algorithms of Abasi et al. [MFCS 2014] and of Gabizon et al. [ESA 2015] for
the r-monomial detection problem are optimal under ETH.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1004.5439v1,On the periods of generalized Fibonacci recurrences,"We give a simple condition for a linear recurrence (mod 2^w) of degree r to
have the maximal possible period 2^(w-1).(2^r-1). It follows that the period is
maximal in the cases of interest for pseudo-random number generation, i.e. for
3-term linear recurrences defined by trinomials which are primitive (mod 2) and
of degree r > 2. We consider the enumeration of certain exceptional polynomials
which do not give maximal period, and list all such polynomials of degree less
than 15.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1805.06238v1,Distributed Automata and Logic,"Distributed automata are finite-state machines that operate on finite
directed graphs. Acting as synchronous distributed algorithms, they use their
input graph as a network in which identical processors communicate for a
possibly infinite number of synchronous rounds. For the local variant of those
automata, where the number of rounds is bounded by a constant, Hella et al.
(2012, 2015) have established a logical characterization in terms of basic
modal logic. In this thesis, we provide similar logical characterizations for
two more expressive classes of distributed automata.
  The first class extends local automata with a global acceptance condition and
the ability to alternate between nondeterministic and parallel computations. We
show that it is equivalent to monadic second-order logic on graphs. By
restricting transitions to be nondeterministic or deterministic, we also obtain
two strictly weaker variants for which the emptiness problem is decidable.
  Our second class transfers the standard notion of asynchronous algorithm to
the setting of nonlocal distributed automata. The resulting machines are shown
to be equivalent to a small fragment of least fixpoint logic, and more
specifically, to a restricted variant of the modal {\mu}-calculus that allows
least fixpoints but forbids greatest fixpoints. Exploiting the connection with
logic, we additionally prove that the expressive power of those asynchronous
automata is independent of whether or not messages can be lost.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1706.08675v1,"Proceedings of the First International Workshop on Deep Learning and
  Music","Proceedings of the First International Workshop on Deep Learning and Music,
joint with IJCNN, Anchorage, US, May 17-18, 2017",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1009.2452v1,"Facility Location with Client Latencies: Linear-Programming based
  Techniques for Minimum-Latency Problems","We introduce a problem that is a common generalization of the uncapacitated
facility location and minimum latency (ML) problems, where facilities need to
be opened to serve clients and also need to be sequentially activated before
they can provide service. Formally, we are given a set \F of n facilities with
facility-opening costs {f_i}, a set of m clients, and connection costs {c_{ij}}
specifying the cost of assigning a client j to a facility i, a root node r
denoting the depot, and a time metric d on \F\cup{r}. Our goal is to open a
subset F of facilities, find a path P starting at r and spanning F to activate
the open facilities, and connect each client j to a facility \phi(j)\in F, so
as to minimize \sum_{i\in F}f_i +\sum_{clients j}(c_{\phi(j),j}+t_j), where t_j
is the time taken to reach \phi(j) along path P. We call this the minimum
latency uncapacitated facility location (MLUFL) problem.
  Our main result is an O(\log n\max{\log n,\log m})-approximation for MLUFL.
We also show that any improvement in this approximation guarantee, implies an
improvement in the (current-best) approximation factor for group Steiner tree.
We obtain constant approximations for two natural special cases of the problem:
(a) related MLUFL (metric connection costs that are a scalar multiple of the
time metric); (b) metric uniform MLUFL (metric connection costs, unform
time-metric). Our LP-based methods are versatile and easily adapted to yield
approximation guarantees for MLUFL in various more general settings, such as
(i) when the latency-cost of a client is a function of the delay faced by the
facility to which it is connected; and (ii) the k-route version, where k
vehicles are routed in parallel to activate the open facilities. Our LP-based
understanding of MLUFL also offers some LP-based insights into ML, which we
believe is a promising direction for obtaining improvements for ML.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.15953v1,"BLNet: A Fast Deep Learning Framework for Low-Light Image Enhancement
  with Noise Removal and Color Restoration","Images obtained in real-world low-light conditions are not only low in
brightness, but they also suffer from many other types of degradation, such as
color bias, unknown noise, detail loss and halo artifacts. In this paper, we
propose a very fast deep learning framework called Bringing the Lightness
(denoted as BLNet) that consists of two U-Nets with a series of well-designed
loss functions to tackle all of the above degradations. Based on Retinex
Theory, the decomposition net in our model can decompose low-light images into
reflectance and illumination and remove noise in the reflectance during the
decomposition phase. We propose a Noise and Color Bias Control module (NCBC
Module) that contains a convolutional neural network and two loss functions
(noise loss and color loss). This module is only used to calculate the loss
functions during the training phase, so our method is very fast during the test
phase. This module can smooth the reflectance to achieve the purpose of noise
removal while preserving details and edge information and controlling color
bias. We propose a network that can be trained to learn the mapping between
low-light and normal-light illumination and enhance the brightness of images
taken in low-light illumination. We train and evaluate the performance of our
proposed model over the real-world Low-Light (LOL) dataset), and we also test
our model over several other frequently used datasets (LIME, DICM and MEF
datasets). We conduct extensive experiments to demonstrate that our approach
achieves a promising effect with good rubustness and generalization and
outperforms many other state-of-the-art methods qualitatively and
quantitatively. Our method achieves high speed because we use loss functions
instead of introducing additional denoisers for noise removal and color
correction. The code and model are available at
https://github.com/weixinxu666/BLNet.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.04266v3,An objective function for order preserving hierarchical clustering,"We present an objective function for similarity based hierarchical clustering
of partially ordered data that preserves the partial order. That is, if $x \le
y$, and if $[x]$ and $[y]$ are the respective clusters of $x$ and $y$, then
there is an order relation $\le'$ on the clusters for which $[x] \le' |y]$. The
theory distinguishes itself from existing theories for clustering of ordered
data in that the order relation and the similarity are combined into a
bi-objective optimisation problem to obtain a hierarchical clustering seeking
to satisfy both. In particular, the order relation is weighted in the range
$[0,1]$, and if the similarity and the order relation are not aligned, then
order preservation may have to yield in favor of clustering. Finding an optimal
solution is NP-hard, so we provide a polynomial time approximation algorithm,
with a relative performance guarantee of $O\!\left(\log^{3/2} \!\!\, n
\right)$, based on successive applications of directed sparsest cut. We provide
a demonstration on a benchmark dataset, showing that our method outperforms
existing methods for order preserving hierarchical clustering with significant
margin. The theory is an extension of the Dasgupta cost function for divisive
hierarchical clustering.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0906.2446v2,"Ftklipse - Design and Implementation of an Extendable Computer Forensics
  Environment: Software Requirements Specification Document","The purpose behind this article is to describe the features of Ftklipse, an
extendable platform for computer forensics. This document designed to provide a
detailed specification for the developers of Ftklipse. Ftklipse is a
thick-client solution for forensics investigation. It is designed to collect
and preserve evidence, to analyze it and to report on it. It supports chain of
custody management, access control policies, and batch operation of its
included tools in order to facilitate and accelerate the investigation. The
environment itself and its tools are configurable as well and is based on
Eclipse.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.07478v1,"Stochastic equivalence for performance analysis of concurrent systems in
  dtsiPBC","We propose an extension with immediate multiactions of discrete time
stochastic Petri Box Calculus (dtsPBC), presented by I.V. Tarasyuk. The
resulting algebra dtsiPBC is a discrete time analogue of stochastic Petri Box
Calculus (sPBC) with immediate multiactions, designed by H. Maci\`a, V. Valero
et al. within a continuous time domain. The step operational semantics is
constructed via labeled probabilistic transition systems. The denotational
semantics is based on labeled discrete time stochastic Petri nets with
immediate transitions. To evaluate performance, the corresponding semi-Markov
chains are analyzed. We define step stochastic bisimulation equivalence of
expressions that is applied to reduce their transition systems and underlying
semi-Markov chains while preserving the functionality and performance
characteristics. We explain how this equivalence can be used to simplify
performance analysis of the algebraic processes. In a case study, a method of
modeling, performance evaluation and behaviour reduction for concurrent systems
is outlined and applied to the shared memory system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1406.0303v2,A Superposition Calculus for Abductive Reasoning,"We present a modification of the superposition calculus that is meant to
generate consequences of sets of first-order axioms. This approach is proven to
be sound and deductive-complete in the presence of redundancy elimination
rules, provided the considered consequences are built on a given finite set of
ground terms, represented by constant symbols. In contrast to other approaches,
most existing results about the termination of the superposition calculus can
be carried over to our procedure. This ensures in particular that the calculus
is terminating for many theories of interest to the SMT community.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0804.1707v1,Computation of unirational fields (extended abstract),"In this paper we present an algorithm for computing all algebraic
intermediate subfields in a separably generated unirational field extension
(which in particular includes the zero characteristic case). One of the main
tools is Groebner bases theory. Our algorithm also requires computing computing
primitive elements and factoring over algebraic extensions. Moreover, the
method can be extended to finitely generated K-algebras.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1212.5210v5,GNU epsilon - an extensible programming language,"Reductionism is a viable strategy for designing and implementing practical
programming languages, leading to solutions which are easier to extend,
experiment with and formally analyze. We formally specify and implement an
extensible programming language, based on a minimalistic first-order imperative
core language plus strong abstraction mechanisms, reflection and
self-modification features. The language can be extended to very high levels:
by using Lisp-style macros and code-to-code transforms which automatically
rewrite high-level expressions into core forms, we define closures and
first-class continuations on top of the core. Non-self-modifying programs can
be analyzed and formally reasoned upon, thanks to the language simple
semantics. We formally develop a static analysis and prove a soundness property
with respect to the dynamic semantics. We develop a parallel garbage collector
suitable to multi-core machines to permit efficient execution of parallel
programs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.01479v1,"LG4AV: Combining Language Models and Graph Neural Networks for Author
  Verification","The automatic verification of document authorships is important in various
settings. Researchers are for example judged and compared by the amount and
impact of their publications and public figures are confronted by their posts
on social media platforms. Therefore, it is important that authorship
information in frequently used web services and platforms is correct. The
question whether a given document is written by a given author is commonly
referred to as authorship verification (AV). While AV is a widely investigated
problem in general, only few works consider settings where the documents are
short and written in a rather uniform style. This makes most approaches
unpractical for online databases and knowledge graphs in the scholarly domain.
Here, authorships of scientific publications have to be verified, often with
just abstracts and titles available. To this point, we present our novel
approach LG4AV which combines language models and graph neural networks for
authorship verification. By directly feeding the available texts in a
pre-trained transformer architecture, our model does not need any hand-crafted
stylometric features that are not meaningful in scenarios where the writing
style is, at least to some extent, standardized. By the incorporation of a
graph neural network structure, our model can benefit from relations between
authors that are meaningful with respect to the verification process. For
example, scientific authors are more likely to write about topics that are
addressed by their co-authors and twitter users tend to post about the same
subjects as people they follow. We experimentally evaluate our model and study
to which extent the inclusion of co-authorships enhances verification decisions
in bibliometric environments.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1408.1127v1,"SADDLE: A Modular Design Automation Framework for Cluster Supercomputers
  and Data Centres","In this paper we present SADDLE, a modular framework for automated design of
cluster supercomputers and data centres. In contrast with commonly used
approaches that operate on logic gate level (Verilog, VHDL) or board level
(such as EDA tools), SADDLE works at a much higher level of abstraction: its
building blocks are ready-made servers, network switches, power supply systems
and so on. Modular approach provides the potential to include low-level tools
as elements of SADDLE's design workflow, moving towards the goal of electronic
system level (ESL) design automation. Designs produced by SADDLE include
project documentation items such as bills of materials and wiring diagrams,
providing a formal specification of a computer system and streamlining assembly
operations.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2010.15728v4,"Explainable Automated Coding of Clinical Notes using Hierarchical
  Label-wise Attention Networks and Label Embedding Initialisation","Diagnostic or procedural coding of clinical notes aims to derive a coded
summary of disease-related information about patients. Such coding is usually
done manually in hospitals but could potentially be automated to improve the
efficiency and accuracy of medical coding. Recent studies on deep learning for
automated medical coding achieved promising performances. However, the
explainability of these models is usually poor, preventing them to be used
confidently in supporting clinical practice. Another limitation is that these
models mostly assume independence among labels, ignoring the complex
correlation among medical codes which can potentially be exploited to improve
the performance. We propose a Hierarchical Label-wise Attention Network (HLAN),
which aimed to interpret the model by quantifying importance (as attention
weights) of words and sentences related to each of the labels. Secondly, we
propose to enhance the major deep learning models with a label embedding (LE)
initialisation approach, which learns a dense, continuous vector representation
and then injects the representation into the final layers and the label-wise
attention layers in the models. We evaluated the methods using three settings
on the MIMIC-III discharge summaries: full codes, top-50 codes, and the UK NHS
COVID-19 shielding codes. Experiments were conducted to compare HLAN and LE
initialisation to the state-of-the-art neural network based methods. HLAN
achieved the best Micro-level AUC and $F_1$ on the top-50 code prediction and
comparable results on the NHS COVID-19 shielding code prediction to other
models. By highlighting the most salient words and sentences for each label,
HLAN showed more meaningful and comprehensive model interpretation compared to
its downgraded baselines and the CNN-based models. LE initialisation
consistently boosted most deep learning models for automated medical coding.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1208.6137v1,Benchmarking recognition results on word image datasets,"We have benchmarked the maximum obtainable recognition accuracy on various
word image datasets using manual segmentation and a currently available
commercial OCR. We have developed a Matlab program, with graphical user
interface, for semi-automated pixel level segmentation of word images. We
discuss the advantages of pixel level annotation. We have covered five
databases adding up to over 3600 word images. These word images have been
cropped from camera captured scene, born-digital and street view images. We
recognize the segmented word image using the trial version of Nuance Omnipage
OCR. We also discuss, how the degradations introduced during acquisition or
inaccuracies introduced during creation of word images affect the recognition
of the word present in the image. Word images for different kinds of
degradations and correction for slant and curvy nature of words are also
discussed. The word recognition rates obtained on ICDAR 2003, Sign evaluation,
Street view, Born-digital and ICDAR 2011 datasets are 83.9%, 89.3%, 79.6%,
88.5% and 86.7% respectively.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1807.01577v1,"VideoKifu, or the automatic transcription of a Go game","In two previous papers [arXiv:1508.03269, arXiv:1701.05419] we described the
techniques we employed for reconstructing the whole move sequence of a Go game.
That task was at first accomplished by means of a series of photographs,
manually shot, as explained during the scientific conference held within the
LIX European Go Congress (Liberec, CZ). The photographs were subsequently
replaced by a possibly unattended video live stream (provided by webcams,
videocameras, smartphones and so on) or, were the live stream not available, by
means of a pre-recorded video of the game itself, on condition that the goban
and the stones were clearly visible more often than not. As we hinted in the
latter paper, in the last two years we have improved both the algorithms
employed for reconstructing the grid and detecting the stones, making extensive
usage of the multicore capabilities offered by modern CPUs. Those capabilities
prompted us to develop some asynchronous routines, capable of double-checking
the position of the grid and the number and colour of any stone previously
detected, in order to get rid of minor errors possibly occurred during the main
analysis, and that may pass undetected especially in the course of an
unattended live streaming. Those routines will be described in details, as they
address some problems that are of general interest when reconstructing the move
sequence, for example what to do when large movements of the whole goban occur
(deliberate or not) and how to deal with captures of dead stones $-$ that could
be wrongly detected and recorded as ""fresh"" moves if not promptly removed.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.12269v1,"Multiscale Modeling of Elasto-Plasticity in Heterogeneous Geomaterials
  Based on Continuum Micromechanics","In this paper, we investigate some micromechanical aspects of
elasto-plasticity in heterogeneous geomaterials. The aim is to upscale the
elasto-plastic behavior for a representative volume of the material which is
indeed a very challenging task due to the irreversible deformations involved.
Considering the plastic strains as eigen-strains allows us to employ the
powerful tools offered by Continuum Micromechanics which are mainly developed
for upscaling of eigen-stressed elastic media. The validity of such
eigen-strain based formulation of multiscale elasto-plasticity is herein
examined by comparing its predictions against Finite Element (FE) simulations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.07559v1,Inductive Analysis of the Internet Protocol TLS,"Internet browsers use security protocols to protect sensitive messages. An
inductive analysis of TLS (a descendant of SSL 3.0) has been performed using
the theorem prover Isabelle. Proofs are based on higher-order logic and make no
assumptions concerning beliefs or finiteness. All the obvious security goals
can be proved; session resumption appears to be secure even if old session keys
have been compromised. The proofs suggest minor changes to simplify the
analysis. TLS, even at an abstract level, is much more complicated than most
protocols that researchers have verified. Session keys are negotiated rather
than distributed, and the protocol has many optional parts. Nevertheless, the
resources needed to verify TLS are modest: six man-weeks of effort and three
minutes of processor time.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.07723v3,Evolutionary learning of interpretable decision trees,"Reinforcement learning techniques achieved human-level performance in several
tasks in the last decade. However, in recent years, the need for
interpretability emerged: we want to be able to understand how a system works
and the reasons behind its decisions. Not only we need interpretability to
assess the safety of the produced systems, we also need it to extract knowledge
about unknown problems. While some techniques that optimize decision trees for
reinforcement learning do exist, they usually employ greedy algorithms or they
do not exploit the rewards given by the environment. This means that these
techniques may easily get stuck in local optima. In this work, we propose a
novel approach to interpretable reinforcement learning that uses decision
trees. We present a two-level optimization scheme that combines the advantages
of evolutionary algorithms with the advantages of Q-learning. This way we
decompose the problem into two sub-problems: the problem of finding a
meaningful and useful decomposition of the state space, and the problem of
associating an action to each state. We test the proposed method on three
well-known reinforcement learning benchmarks, on which it results competitive
with respect to the state-of-the-art in both performance and interpretability.
Finally, we perform an ablation study that confirms that using the two-level
optimization scheme gives a boost in performance in non-trivial environments
with respect to a one-layer optimization technique.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.05105v3,"Assembling sequences of DNA using an on-line algorithm based on DeBruijn
  graphs","The problem of assembling DNA fragments starting from imperfect strings given
by a sequencer, classified as NP hard when trying to get perfect answers, has a
huge importance in several fields, because of its relation with the possibility
of detecting similarities between animals, dangerous pests in crops, and so on.
Some of the algorithms and data structures that have been created to solve this
problem are Needleman Wunsch algorithm, DeBruijn graphs and greedy algorithms
working on overlaps graphs; these try to work out the problem from different
approaches that give place to certain advantages and disadvantages to be
discussed.
  In this article we first expose a summary of the research done on already
created solutions for the DNA assembly problem, to present later an on-line
solution to the same matter, which, despite not considering mutations, would
have the capacity of using only the necessary amount of readings to assemble an
user specified amount of genes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1406.5429v2,"Playing with Duality: An Overview of Recent Primal-Dual Approaches for
  Solving Large-Scale Optimization Problems","Optimization methods are at the core of many problems in signal/image
processing, computer vision, and machine learning. For a long time, it has been
recognized that looking at the dual of an optimization problem may drastically
simplify its solution. Deriving efficient strategies which jointly brings into
play the primal and the dual problems is however a more recent idea which has
generated many important new contributions in the last years. These novel
developments are grounded on recent advances in convex analysis, discrete
optimization, parallel processing, and non-smooth optimization with emphasis on
sparsity issues. In this paper, we aim at presenting the principles of
primal-dual approaches, while giving an overview of numerical methods which
have been proposed in different contexts. We show the benefits which can be
drawn from primal-dual algorithms both for solving large-scale convex
optimization problems and discrete ones, and we provide various application
examples to illustrate their usefulness.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1312.4414v1,Small Universal Petri Nets with Inhibitor Arcs,"We investigate the problem of construction of small-size universal Petri nets
with inhibitor arcs. We consider four descriptional complexity parameters: the
number of places, transitions, inhibitor arcs, and the maximal degree of a
transition, each of which we try to minimize.
  We give six constructions having the following values of parameters (listed
in the above order): $(30,34,13,3)$, $(14, 31, 51, 8)$, $(11, 31, 79, 11)$,
$(21,25,13,5)$, $(67, 64, 8, 3)$, $(58, 55, 8, 5)$ that improve the few known
results on this topic. Our investigation also highlights several interesting
trade-offs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.07374v1,MatchKAT: An Algebraic Foundation For Match-Action,"We present MatchKAT, an algebraic language for modeling match-action packet
processing in network switches. Although the match-action paradigm has remained
a popular low-level programming model for specifying packet forwarding
behavior, little has been done towards giving it formal semantics. With
MatchKAT, we hope to embark on the first steps in exploring how network
programs compiled to match-action rules can be reasoned about formally in a
reliable, algebraic way. In this paper, we give details of MatchKAT and its
metatheory, as well as a formal treatment of match expressions on binary
strings that form the basis of ""match"" in match-action. Through a
correspondence with NetKAT, we show that MatchKAT's equational theory is sound
and complete with regards to a similar packet filtering semantics. We also
demonstrate the complexity of deciding equivalence in MatchKAT is
PSPACE-complete.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.01535v3,Predicting Propensity to Vote with Machine Learning,"We demonstrate that machine learning enables the capability to infer an
individual's propensity to vote from their past actions and attributes. This is
useful for microtargeting voter outreach, voter education and get-out-the-vote
(GOVT) campaigns. Political scientists developed increasingly sophisticated
techniques for estimating election outcomes since the late 1940s. Two prior
studies similarly used machine learning to predict individual future voting
behavior. We built a machine learning environment using TensorFlow, obtained
voting data from 2004 to 2018, and then ran three experiments. We show positive
results with a Matthews correlation coefficient of 0.39.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2006.00586v1,An Exploratory Characterization of Bugs in COVID-19 Software Projects,"Context: The dire consequences of the COVID-19 pandemic has influenced
development of COVID-19 software i.e., software used for analysis and
mitigation of COVID-19. Bugs in COVID-19 software can be consequential, as
COVID-19 software projects can impact public health policy and user data
privacy. Objective: The goal of this paper is to help practitioners and
researchers improve the quality of COVID-19 software through an empirical study
of open source software projects related to COVID-19. Methodology: We use 129
open source COVID-19 software projects hosted on GitHub to conduct our
empirical study. Next, we apply qualitative analysis on 550 bug reports from
the collected projects to identify bug categories. Findings: We identify 8 bug
categories, which include data bugs i.e., bugs that occur during mining and
storage of COVID-19 data. The identified bug categories appear for 7 categories
of software projects including (i) projects that use statistical modeling to
perform predictions related to COVID-19, and (ii) medical equipment software
that are used to design and implement medical equipment, such as ventilators.
Conclusion: Based on our findings, we advocate for robust statistical model
construction through better synergies between data science practitioners and
public health experts. Existence of security bugs in user tracking software
necessitates development of tools that will detect data privacy violations and
security weaknesses.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1802.01226v3,"Differential Equation Axiomatization: The Impressive Power of
  Differential Ghosts","We prove the completeness of an axiomatization for differential equation
invariants. First, we show that the differential equation axioms in
differential dynamic logic are complete for all algebraic invariants. Our proof
exploits differential ghosts, which introduce additional variables that can be
chosen to evolve freely along new differential equations. Cleverly chosen
differential ghosts are the proof-theoretical counterpart of dark matter. They
create new hypothetical state, whose relationship to the original state
variables satisfies invariants that did not exist before. The reflection of
these new invariants in the original system then enables its analysis.
  We then show that extending the axiomatization with existence and uniqueness
axioms makes it complete for all local progress properties, and further
extension with a real induction axiom makes it complete for all real arithmetic
invariants. This yields a parsimonious axiomatization, which serves as the
logical foundation for reasoning about invariants of differential equations.
Moreover, our results are purely axiomatic, and so the axiomatization is
suitable for sound implementation in foundational theorem provers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1710.07264v3,"Informational Neurobayesian Approach to Neural Networks Training.
  Opportunities and Prospects","A study of the classification problem in context of information theory is
presented in the paper. Current research in that field is focused on
optimisation and bayesian approach. Although that gives satisfying results,
they require a vast amount of data and computations to train on. Authors
propose a new concept named Informational Neurobayesian Approach (INA), which
allows to solve the same problems, but requires significantly less training
data as well as computational power. Experiments were conducted to compare its
performance with the traditional one and the results showed that capacity of
the INA is quite promising.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1902.11035v1,SAFE ML: Surrogate Assisted Feature Extraction for Model Learning,"Complex black-box predictive models may have high accuracy, but opacity
causes problems like lack of trust, lack of stability, sensitivity to concept
drift. On the other hand, interpretable models require more work related to
feature engineering, which is very time consuming. Can we train interpretable
and accurate models, without timeless feature engineering? In this article, we
show a method that uses elastic black-boxes as surrogate models to create a
simpler, less opaque, yet still accurate and interpretable glass-box models.
New models are created on newly engineered features extracted/learned with the
help of a surrogate model. We show applications of this method for model level
explanations and possible extensions for instance level explanations. We also
present an example implementation in Python and benchmark this method on a
number of tabular data sets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0803.1862v1,Exploring Human Factors in Spreadsheet Development,"In this paper we consider human factors and their impact on spreadsheet
development in strategic decision-making. This paper brings forward research
from many disciplines both directly related to spreadsheets and a broader
spectrum from psychology to industrial processing. We investigate how human
factors affect a simplified development cycle and what the potential
consequences are.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1
http://arxiv.org/abs/cs/0612120v2,Generalizing the Paige-Tarjan Algorithm by Abstract Interpretation,"The Paige and Tarjan algorithm (PT) for computing the coarsest refinement of
a state partition which is a bisimulation on some Kripke structure is well
known. It is also well known in model checking that bisimulation is equivalent
to strong preservation of CTL, or, equivalently, of Hennessy-Milner logic.
Drawing on these observations, we analyze the basic steps of the PT algorithm
from an abstract interpretation perspective, which allows us to reason on
strong preservation in the context of generic inductively defined (temporal)
languages and of possibly non-partitioning abstract models specified by
abstract interpretation. This leads us to design a generalized Paige-Tarjan
algorithm, called GPT, for computing the minimal refinement of an abstract
interpretation-based model that strongly preserves some given language. It
turns out that PT is a straight instance of GPT on the domain of state
partitions for the case of strong preservation of Hennessy-Milner logic. We
provide a number of examples showing that GPT is of general use. We first show
how a well-known efficient algorithm for computing stuttering equivalence can
be viewed as a simple instance of GPT. We then instantiate GPT in order to
design a new efficient algorithm for computing simulation equivalence that is
competitive with the best available algorithms. Finally, we show how GPT allows
to compute new strongly preserving abstract models by providing an efficient
algorithm that computes the coarsest refinement of a given partition that
strongly preserves the language generated by the reachability operator.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1310.1498v1,"Deeper Into the Folksonomy Graph: FolkRank Adaptations and Extensions
  for Improved Tag Recommendations","The information contained in social tagging systems is often modelled as a
graph of connections between users, items and tags. Recommendation algorithms
such as FolkRank, have the potential to leverage complex relationships in the
data, corresponding to multiple hops in the graph. We present an in-depth
analysis and evaluation of graph models for social tagging data and propose
novel adaptations and extensions of FolkRank to improve tag recommendations. We
highlight implicit assumptions made by the widely used folksonomy model, and
propose an alternative and more accurate graph-representation of the data. Our
extensions of FolkRank address the new item problem by incorporating content
data into the algorithm, and significantly improve prediction results on
unpruned datasets. Our adaptations address issues in the iterative weight
spreading calculation that potentially hinder FolkRank's ability to leverage
the deep graph as an information source. Moreover, we evaluate the benefit of
considering each deeper level of the graph, and present important insights
regarding the characteristics of social tagging data in general. Our results
suggest that the base assumption made by conventional weight propagation
methods, that closeness in the graph always implies a positive relationship,
does not hold for the social tagging domain.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.02729v1,"Facial Emotion Characterization and Detection using Fourier Transform
  and Machine Learning","We present a Fourier-based machine learning technique that characterizes and
detects facial emotions. The main challenging task in the development of
machine learning (ML) models for classifying facial emotions is the detection
of accurate emotional features from a set of training samples, and the
generation of feature vectors for constructing a meaningful feature space and
building ML models. In this paper, we hypothesis that the emotional features
are hidden in the frequency domain; hence, they can be captured by leveraging
the frequency domain and masking techniques. We also make use of the conjecture
that a facial emotions are convoluted with the normal facial features and the
other emotional features; however, they carry linearly separable spatial
frequencies (we call computational emotional frequencies). Hence, we propose a
technique by leveraging fast Fourier transform (FFT) and rectangular
narrow-band frequency kernels, and the widely used Yale-Faces image dataset. We
test the hypothesis using the performance scores of the random forest (RF) and
the artificial neural network (ANN) classifiers as the measures to validate the
effectiveness of the captured emotional frequencies. Our finding is that the
computational emotional frequencies discovered by the proposed approach
provides meaningful emotional features that help RF and ANN achieve a high
precision scores above 93%, on average.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1503.00454v1,Flexible and Robust Privacy-Preserving Implicit Authentication,"Implicit authentication consists of a server authenticating a user based on
the user's usage profile, instead of/in addition to relying on something the
user explicitly knows (passwords, private keys, etc.). While implicit
authentication makes identity theft by third parties more difficult, it
requires the server to learn and store the user's usage profile. Recently, the
first privacy-preserving implicit authentication system was presented, in which
the server does not learn the user's profile. It uses an ad hoc two-party
computation protocol to compare the user's fresh sampled features against an
encrypted stored user's profile. The protocol requires storing the usage
profile and comparing against it using two different cryptosystems, one of them
order-preserving; furthermore, features must be numerical. We present here a
simpler protocol based on set intersection that has the advantages of: i)
requiring only one cryptosystem; ii) not leaking the relative order of fresh
feature samples; iii) being able to deal with any type of features (numerical
or non-numerical).
  Keywords: Privacy-preserving implicit authentication, privacy-preserving set
intersection, implicit authentication, active authentication, transparent
authentication, risk mitigation, data brokers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2102.10952v1,"A Relational Tsetlin Machine with Applications to Natural Language
  Understanding","TMs are a pattern recognition approach that uses finite state machines for
learning and propositional logic to represent patterns. In addition to being
natively interpretable, they have provided competitive accuracy for various
tasks. In this paper, we increase the computing power of TMs by proposing a
first-order logic-based framework with Herbrand semantics. The resulting TM is
relational and can take advantage of logical structures appearing in natural
language, to learn rules that represent how actions and consequences are
related in the real world. The outcome is a logic program of Horn clauses,
bringing in a structured view of unstructured data. In closed-domain
question-answering, the first-order representation produces 10x more compact
KBs, along with an increase in answering accuracy from 94.83% to 99.48%. The
approach is further robust towards erroneous, missing, and superfluous
information, distilling the aspects of a text that are important for real-world
understanding.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1306.1901v1,Eventual Linear Ranking Functions,"Program termination is a hot research topic in program analysis. The last few
years have witnessed the development of termination analyzers for programming
languages such as C and Java with remarkable precision and performance. These
systems are largely based on techniques and tools coming from the field of
declarative constraint programming. In this paper, we first recall an algorithm
based on Farkas' Lemma for discovering linear ranking functions proving
termination of a certain class of loops. Then we propose an extension of this
method for showing the existence of eventual linear ranking functions, i.e.,
linear functions that become ranking functions after a finite unrolling of the
loop. We show correctness and completeness of this algorithm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.04958v2,Constraint Satisfaction Problems over Finite Structures,"We initiate a systematic study of the computational complexity of the
Constraint Satisfaction Problem (CSP) over finite structures that may contain
both relations and operations. We show the close connection between this
problem and a natural algebraic question: which finite algebras admit only
polynomially many homomorphisms into them? We give some sufficient and some
necessary conditions for a finite algebra to have this property. In particular,
we show that every finite equationally nontrivial algebra has this property
which gives us, as a simple consequence, a complete complexity classification
of CSPs over two-element structures, thus extending the classification for
two-element relational structures by Schaefer (STOC'78). We also present
examples of two-element structures that have bounded width but do not have
relational width (2,3), thus demonstrating that, from a descriptive complexity
perspective, allowing operations leads to a richer theory.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0305032v1,Robust Report Level Cluster-to-Track Fusion,"In this paper we develop a method for report level tracking based on
Dempster-Shafer clustering using Potts spin neural networks where clusters of
incoming reports are gradually fused into existing tracks, one cluster for each
track. Incoming reports are put into a cluster and continuous reclustering of
older reports is made in order to obtain maximum association fit within the
cluster and towards the track. Over time, the oldest reports of the cluster
leave the cluster for the fixed track at the same rate as new incoming reports
are put into it. Fusing reports to existing tracks in this fashion allows us to
take account of both existing tracks and the probable future of each track, as
represented by younger reports within the corresponding cluster. This gives us
a robust report-to-track association. Compared to clustering of all available
reports this approach is computationally faster and has a better
report-to-track association than simple step-by-step association.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0010002v1,Noise Effects in Fuzzy Modelling Systems,"Noise is source of ambiguity for fuzzy systems. Although being an important
aspect, the effects of noise in fuzzy modeling have been little investigated.
This paper presents a set of tests using three well-known fuzzy modeling
algorithms. These evaluate perturbations in the extracted rule-bases caused by
noise polluting the learning data, and the corresponding deformations in each
learned functional relation. We present results to show: 1) how these fuzzy
modeling systems deal with noise; 2) how the established fuzzy model structure
influences noise sensitivity of each algorithm; and 3) whose characteristics of
the learning algorithms are relevant to noise attenuation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.08142v1,On Doctrines and Cartesian Bicategories,"We study the relationship between cartesian bicategories and a specialisation
of Lawvere's hyperdoctrines, namely elementary existential doctrines. Both
provide different ways of abstracting the structural properties of logical
systems: the former in algebraic terms based on a string diagrammatic calculus,
the latter in universal terms using the fundamental notion of adjoint functor.
We prove that these two approaches are related by an adjunction, which can be
strengthened to an equivalence by imposing further constraints on doctrines.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1801.06782v2,How can we naturally order and organize graph Laplacian eigenvectors?,"When attempting to develop wavelet transforms for graphs and networks, some
researchers have used graph Laplacian eigenvalues and eigenvectors in place of
the frequencies and complex exponentials in the Fourier theory for regular
lattices in the Euclidean domains. This viewpoint, however, has a fundamental
flaw: on a general graph, the Laplacian eigenvalues cannot be interpreted as
the frequencies of the corresponding eigenvectors. In this paper, we discuss
this important problem further and propose a new method to organize those
eigenvectors by defining and measuring ""natural"" distances between eigenvectors
using the Ramified Optimal Transport Theory followed by embedding them into a
low-dimensional Euclidean domain. We demonstrate its effectiveness using a
synthetic graph as well as a dendritic tree of a retinal ganglion cell of a
mouse.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0505007v1,Adaptive Codes: A New Class of Non-standard Variable-length Codes,"We introduce a new class of non-standard variable-length codes, called
adaptive codes. This class of codes associates a variable-length codeword to
the symbol being encoded depending on the previous symbols in the input data
string. An efficient algorithm for constructing adaptive codes of order one is
presented. Then, we introduce a natural generalization of adaptive codes,
called GA codes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.09371v1,"Learning-based Bias Correction for Ultra-wideband Localization of
  Resource-constrained Mobile Robots","Accurate indoor localization is a crucial enabling technology for many
robotics applications, from warehouse management to monitoring tasks.
Ultra-wideband (UWB) ranging is a promising solution which is low-cost,
lightweight, and computationally inexpensive compared to alternative
state-of-the-art approaches such as simultaneous localization and mapping,
making it especially suited for resource-constrained aerial robots. Many
commercially-available ultra-wideband radios, however, provide inaccurate,
biased range measurements. In this article, we propose a bias correction
framework compatible with both two-way ranging and time difference of arrival
ultra-wideband localization. Our method comprises of two steps: (i) statistical
outlier rejection and (ii) a learning-based bias correction. This approach is
scalable and frugal enough to be deployed on-board a nano-quadcopter's
microcontroller. Previous research mostly focused on two-way ranging bias
correction and has not been implemented in closed-loop nor using
resource-constrained robots. Experimental results show that, using our
approach, the localization error is reduced by ~18.5% and 48% (for TWR and
TDoA, respectively), and a quadcopter can accurately track trajectories with
position information from UWB only.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0
http://arxiv.org/abs/1612.00100v1,Noise-Tolerant Life-Long Matrix Completion via Adaptive Sampling,"We study the problem of recovering an incomplete $m\times n$ matrix of rank
$r$ with columns arriving online over time. This is known as the problem of
life-long matrix completion, and is widely applied to recommendation system,
computer vision, system identification, etc. The challenge is to design
provable algorithms tolerant to a large amount of noises, with small sample
complexity. In this work, we give algorithms achieving strong guarantee under
two realistic noise models. In bounded deterministic noise, an adversary can
add any bounded yet unstructured noise to each column. For this problem, we
present an algorithm that returns a matrix of a small error, with sample
complexity almost as small as the best prior results in the noiseless case. For
sparse random noise, where the corrupted columns are sparse and drawn randomly,
we give an algorithm that exactly recovers an $\mu_0$-incoherent matrix by
probability at least $1-\delta$ with sample complexity as small as
$O\left(\mu_0rn\log (r/\delta)\right)$. This result advances the
state-of-the-art work and matches the lower bound in a worst case. We also
study the scenario where the hidden matrix lies on a mixture of subspaces and
show that the sample complexity can be even smaller. Our proposed algorithms
perform well experimentally in both synthetic and real-world datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1007.5188v1,Characterising Probabilistic Processes Logically,"In this paper we work on (bi)simulation semantics of processes that exhibit
both nondeterministic and probabilistic behaviour. We propose a probabilistic
extension of the modal mu-calculus and show how to derive characteristic
formulae for various simulation-like preorders over finite-state processes
without divergence. In addition, we show that even without the fixpoint
operators this probabilistic mu-calculus can be used to characterise these
behavioural relations in the sense that two states are equivalent if and only
if they satisfy the same set of formulae.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.02397v3,A Deep Learning Approach to Tongue Detection for Pediatric Population,"Children with severe disabilities and complex communication needs face
limitations in the usage of access technology (AT) devices. Conventional ATs
(e.g., mechanical switches) can be insufficient for nonverbal children and
those with limited voluntary motion control. Automatic techniques for the
detection of tongue gestures represent a promising pathway. Previous studies
have shown the robustness of tongue detection algorithms on adult participants,
but further research is needed to use these methods with children. In this
study, a network architecture for tongue-out gesture recognition was
implemented and evaluated on videos recorded in a naturalistic setting when
children were playing a video-game. A cascade object detector algorithm was
used to detect the participants' faces, and an automated classification scheme
for tongue gesture detection was developed using a convolutional neural network
(CNN). In evaluation experiments conducted, the network was trained using
adults and children's images. The network classification accuracy was evaluated
using leave-one-subject-out cross-validation. Preliminary classification
results obtained from the analysis of videos of five typically developing
children showed an accuracy of up to 99% in predicting tongue-out gestures.
Moreover, we demonstrated that using only children data for training the
classifier yielded better performance than adult's one supporting the need for
pediatric tongue gesture datasets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.03038v4,Lambek pregroups are Frobenius spiders in preorders,"""Spider"" is a nickname of special Frobenius algebras, a fundamental structure
from mathematics, physics, and computer science. Pregroups are a fundamental
structure from linguistics. Pregroups and spiders have been used together in
natural language processing: one for syntax, the other for semantics. It turns
out that pregroups themselves can be characterized as pointed spiders in the
category of preordered relations, where they naturally arise from grammars. The
other way around, preordered spider algebras in general can be characterized as
unions of pregroups. This extends the characterization of relational spider
algebras as disjoint unions of groups. The compositional framework that emerged
with the results suggests new ways to understand and apply the basis structures
in machine learning and data analysis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1611.09666v1,"Generic and Efficient Solution Solves the Shortest Paths Problem in
  Square Runtime","We study a group of new methods to solve an open problem that is the shortest
paths problem on a given fix-weighted instance. It is the real significance at
a considerable altitude to reach our aim to meet these qualities of generic,
efficiency, precision which we generally require to a methodology. Besides our
proof to guarantee our measures might work normally, we pay more interest to
root out the vital theory about calculation and logic in favor of our extension
to range over a wide field about decision, operator, economy, management,
robot, AI and etc.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1102.5451v1,Reduction of fuzzy automata by means of fuzzy quasi-orders,"In our recent paper we have established close relationships between state
reduction of a fuzzy recognizer and resolution of a particular system of fuzzy
relation equations. In that paper we have also studied reductions by means of
those solutions which are fuzzy equivalences. In this paper we will see that in
some cases better reductions can be obtained using the solutions of this system
that are fuzzy quasi-orders. Generally, fuzzy quasi-orders and fuzzy
equivalences are equally good in the state reduction, but we show that right
and left invariant fuzzy quasi-orders give better reductions than right and
left invariant fuzzy equivalences. We also show that alternate reductions by
means of fuzzy quasi-orders give better results than alternate reductions by
means of fuzzy equivalences. Furthermore we study a more general type of fuzzy
quasi-orders, weakly right and left invariant ones, and we show that they are
closely related to determinization of fuzzy recognizers. We also demonstrate
some applications of weakly left invariant fuzzy quasi-orders in conflict
analysis of fuzzy discrete event systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1401.2720v3,"A hierarchically blocked Jacobi SVD algorithm for single and multiple
  graphics processing units","We present a hierarchically blocked one-sided Jacobi algorithm for the
singular value decomposition (SVD), targeting both single and multiple graphics
processing units (GPUs). The blocking structure reflects the levels of GPU's
memory hierarchy. The algorithm may outperform MAGMA's dgesvd, while retaining
high relative accuracy. To this end, we developed a family of parallel pivot
strategies on GPU's shared address space, but applicable also to inter-GPU
communication. Unlike common hybrid approaches, our algorithm in a single GPU
setting needs a CPU for the controlling purposes only, while utilizing GPU's
resources to the fullest extent permitted by the hardware. When required by the
problem size, the algorithm, in principle, scales to an arbitrary number of GPU
nodes. The scalability is demonstrated by more than twofold speedup for
sufficiently large matrices on a Tesla S2050 system with four GPUs vs. a single
Fermi card.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1106.5723v2,The inverse moment problem for convex polytopes,"The goal of this paper is to present a general and novel approach for the
reconstruction of any convex d-dimensional polytope P, from knowledge of its
moments. In particular, we show that the vertices of an N-vertex polytope in
R^d can be reconstructed from the knowledge of O(DN) axial moments (w.r.t. to
an unknown polynomial measure od degree D) in d+1 distinct generic directions.
Our approach is based on the collection of moment formulas due to Brion,
Lawrence, Khovanskii-Pukhikov, and Barvinok that arise in the discrete geometry
of polytopes, and what variously known as Prony's method, or Vandermonde
factorization of finite rank Hankel matrices.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1504.08150v2,"Reward Processes and Performance Simulation in Supermarket Models with
  Different Servers","Supermarket models with different servers become a key in modeling resource
management of stochastic networks, such as, computer networks, manufacturing
systems and transportation networks. While these different servers always make
analysis of such a supermarket model more interesting, difficult and
challenging. This paper provides a new novel method for analyzing the
supermarket model with different servers through a multi-dimensional
continuous-time Markov reward processes. Firstly, the utility functions are
constructed for expressing a routine selection mechanism that depends on queue
lengths, on service rates, and on some probabilities of individual preference.
Then applying the continuous-time Markov reward processes, some segmented
stochastic integrals of the random reward function are established by means of
an event-driven technique. Based on this, the mean of the random reward
function in a finite time period is effectively computed by means of the state
jump points of the Markov reward process, and also the mean of the discounted
random reward function in an infinite time period can be calculated through the
same event-driven technique. Finally, some simulation experiments are given to
indicate how the expected queue length of each server depends on the main
parameters of this supermarket model.",0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1004.3745v1,An Algorithm for Odd Graceful Labeling of the Union of Paths and Cycles,"In 1991, Gnanajothi [4] proved that the path graph P_n with n vertex and n-1
edge is odd graceful, and the cycle graph C_m with m vertex and m edges is odd
graceful if and only if m even, she proved the cycle graph is not graceful if m
odd. In this paper, firstly, we studied the graph C_m $\cup$ P_m when m = 4,
6,8,10 and then we proved that the graph C_ $\cup$ P_n is odd graceful if m is
even. Finally, we described an algorithm to label the vertices and the edges of
the vertex set V(C_m $\cup$ P_n) and the edge set E(C_m $\cup$ P_n).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.14901v1,"Visualization of topology optimization designs with representative
  subset selection","An important new trend in additive manufacturing is the use of optimization
to automatically design industrial objects, such as beams, rudders or wings.
Topology optimization, as it is often called, computes the best configuration
of material over a 3D space, typically represented as a grid, in order to
satisfy or optimize physical parameters. Designers using these automated
systems often seek to understand the interaction of physical constraints with
the final design and its implications for other physical characteristics. Such
understanding is challenging because the space of designs is large and small
changes in parameters can result in radically different designs. We propose to
address these challenges using a visualization approach for exploring the space
of design solutions. The core of our novel approach is to summarize the space
(ensemble of solutions) by automatically selecting a set of examples and to
represent the complete set of solutions as combinations of these examples. The
representative examples create a meaningful parameterization of the design
space that can be explored using standard visualization techniques for
high-dimensional spaces. We present evaluations of our subset selection
technique and that the overall approach addresses the needs of expert
designers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1601.07962v1,"DoubleTake: Fast and Precise Error Detection via Evidence-Based Dynamic
  Analysis","This paper presents evidence-based dynamic analysis, an approach that enables
lightweight analyses--under 5% overhead for these bugs--making it practical for
the first time to perform these analyses in deployed settings. The key insight
of evidence-based dynamic analysis is that for a class of errors, it is
possible to ensure that evidence that they happened at some point in the past
remains for later detection. Evidence-based dynamic analysis allows execution
to proceed at nearly full speed until the end of an epoch (e.g., a heavyweight
system call). It then examines program state to check for evidence that an
error occurred at some time during that epoch. If so, it rolls back execution
and re-executes the code with instrumentation activated to pinpoint the error.
  We present DoubleTake, a prototype evidence-based dynamic analysis framework.
DoubleTake is practical and easy to deploy, requiring neither custom hardware,
compiler, nor operating system support. We demonstrate DoubleTake's generality
and efficiency by building dynamic analyses that find buffer overflows, memory
use-after-free errors, and memory leaks. Our evaluation shows that DoubleTake
is efficient, imposing just 4% overhead on average, making it the fastest such
system to date. It is also precise: DoubleTake pinpoints the location of these
errors to the exact line and memory addresses where they occur, providing
valuable debugging information to programmers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2012.14465v1,Classification of Pathological and Normal Gait: A Survey,"Gait recognition is a term commonly referred to as an identification problem
within the Computer Science field. There are a variety of methods and models
capable of identifying an individual based on their pattern of ambulatory
locomotion. By surveying the current literature on gait recognition, this paper
seeks to identify appropriate metrics, devices, and algorithms for collecting
and analyzing data regarding patterns and modes of ambulatory movement across
individuals. Furthermore, this survey seeks to motivate interest in a broader
scope of longitudinal analysis regarding the perturbations in gait across
states (i.e. physiological, emotive, and/or cognitive states). More broadly,
inferences to normal versus pathological gait patterns can be attributed, based
on both longitudinal and non-longitudinal forms of classification. This may
indicate promising research directions and experimental designs, such as
creating algorithmic metrics for the quantification of fatigue, or models for
forecasting episodic disorders. Furthermore, in conjunction with other
measurements of physiological and environmental conditions, pathological gait
classification might be applicable to inference for syndromic surveillance of
infectious disease states or cognitive impairment.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0902.2501v1,"The Forgiving Graph: A distributed data structure for low stretch under
  adversarial attack","We consider the problem of self-healing in peer-to-peer networks that are
under repeated attack by an omniscient adversary. We assume that, over a
sequence of rounds, an adversary either inserts a node with arbitrary
connections or deletes an arbitrary node from the network. The network responds
to each such change by quick ""repairs,"" which consist of adding or deleting a
small number of edges.
  These repairs essentially preserve closeness of nodes after adversarial
deletions, without increasing node degrees by too much, in the following sense.
At any point in the algorithm, nodes $v$ and $w$ whose distance would have been
$\ell$ in the graph formed by considering only the adversarial insertions (not
the adversarial deletions), will be at distance at most $\ell \log n$ in the
actual graph, where $n$ is the total number of vertices seen so far. Similarly,
at any point, a node $v$ whose degree would have been $d$ in the graph with
adversarial insertions only, will have degree at most 3d in the actual graph.
Our algorithm is completely distributed and has low latency and bandwidth
requirements.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1606.05490v3,Homogeneous Equations of Algebraic Petri Nets,"Algebraic Petri nets are a formalism for modeling distributed systems and
algorithms, describing control and data flow by combining Petri nets and
algebraic specification. One way to specify correctness of an algebraic Petri
net model $N$ is to specify a linear equation $E$ over the places of $N$ based
on term substitution, and coefficients from an abelian group $G$. Then, $E$ is
valid in $N$ iff $E$ is valid in each reachable marking of $N$ . Due to the
expressive power of Algebraic Petri nets, validity is generally undecidable.
Stable linear equations form a class of linear equations for which validity is
decidable. Place invariants yield a well-understood but incomplete
characterization of all stable linear equations. In this paper, we provide a
complete characterization of stability for the subclass of homogeneous linear
equations, by restricting ourselves to the interpretation of terms over the
Herbrand structure without considering further equality axioms. Based thereon,
we show that stability is decidable for homogeneous linear equations if $G$ is
a cyclic group.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.00316v1,"Training Medical Communication Skills with Virtual Patients: Literature
  Review and Directions for Future Research","Effective communication is a crucial skill for healthcare providers since it
leads to better patient health, satisfaction and avoids malpractice claims. In
standard medical education, students' communication skills are trained with
role-playing and Standardized Patients (SPs), i.e., actors. However, SPs are
difficult to standardize, and are very resource consuming. Virtual Patients
(VPs) are interactive computer-based systems that represent a valuable
alternative to SPs. VPs are capable of portraying patients in realistic
clinical scenarios and engage learners in realistic conversations. Approaching
medical communication skill training with VPs has been an active research area
in the last ten years. As a result, the number of works in this field has grown
significantly. The objective of this work is to survey the recent literature,
assessing the state of the art of this technology with a specific focus on the
instructional and technical design of VP simulations. After having classified
and analysed the VPs selected for our research, we identified several areas
that require further investigation, and we drafted practical recommendations
for VP developers on design aspects that, based on our findings, are pivotal to
create novel and effective VP simulations or improve existing ones.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1205.3981v5,kLog: A Language for Logical and Relational Learning with Kernels,"We introduce kLog, a novel approach to statistical relational learning.
Unlike standard approaches, kLog does not represent a probability distribution
directly. It is rather a language to perform kernel-based learning on
expressive logical and relational representations. kLog allows users to specify
learning problems declaratively. It builds on simple but powerful concepts:
learning from interpretations, entity/relationship data modeling, logic
programming, and deductive databases. Access by the kernel to the rich
representation is mediated by a technique we call graphicalization: the
relational representation is first transformed into a graph --- in particular,
a grounded entity/relationship diagram. Subsequently, a choice of graph kernel
defines the feature space. kLog supports mixed numerical and symbolic data, as
well as background knowledge in the form of Prolog or Datalog programs as in
inductive logic programming systems. The kLog framework can be applied to
tackle the same range of tasks that has made statistical relational learning so
popular, including classification, regression, multitask learning, and
collective classification. We also report about empirical comparisons, showing
that kLog can be either more accurate, or much faster at the same level of
accuracy, than Tilde and Alchemy. kLog is GPLv3 licensed and is available at
http://klog.dinfo.unifi.it along with tutorials.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1406.0242v3,Random Walks that Find Perfect Objects and the Lov√°sz Local Lemma,"We give an algorithmic local lemma by establishing a sufficient condition for
the uniform random walk on a directed graph to reach a sink quickly. Our work
is inspired by Moser's entropic method proof of the Lov\'{a}sz Local Lemma
(LLL) for satisfiability and completely bypasses the Probabilistic Method
formulation of the LLL. In particular, our method works when the underlying
state space is entirely unstructured. Similarly to Moser's argument, the key
point is that the inevitability of reaching a sink is established by bounding
the entropy of the walk as a function of time.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0506007v2,Defensive forecasting for linear protocols,"We consider a general class of forecasting protocols, called ""linear
protocols"", and discuss several important special cases, including multi-class
forecasting. Forecasting is formalized as a game between three players:
Reality, whose role is to generate observations; Forecaster, whose goal is to
predict the observations; and Skeptic, who tries to make money on any lack of
agreement between Forecaster's predictions and the actual observations. Our
main mathematical result is that for any continuous strategy for Skeptic in a
linear protocol there exists a strategy for Forecaster that does not allow
Skeptic's capital to grow. This result is a meta-theorem that allows one to
transform any continuous law of probability in a linear protocol into a
forecasting strategy whose predictions are guaranteed to satisfy this law. We
apply this meta-theorem to a weak law of large numbers in Hilbert spaces to
obtain a version of the K29 prediction algorithm for linear protocols and show
that this version also satisfies the attractive properties of proper
calibration and resolution under a suitable choice of its kernel parameter,
with no assumptions about the way the data is generated.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1011.1716v4,Least Squares Ranking on Graphs,"Given a set of alternatives to be ranked, and some pairwise comparison data,
ranking is a least squares computation on a graph. The vertices are the
alternatives, and the edge values comprise the comparison data. The basic idea
is very simple and old: come up with values on vertices such that their
differences match the given edge data. Since an exact match will usually be
impossible, one settles for matching in a least squares sense. This formulation
was first described by Leake in 1976 for rankingfootball teams and appears as
an example in Professor Gilbert Strang's classic linear algebra textbook. If
one is willing to look into the residual a little further, then the problem
really comes alive, as shown effectively by the remarkable recent paper of
Jiang et al. With or without this twist, the humble least squares problem on
graphs has far-reaching connections with many current areas ofresearch. These
connections are to theoretical computer science (spectral graph theory, and
multilevel methods for graph Laplacian systems); numerical analysis (algebraic
multigrid, and finite element exterior calculus); other mathematics (Hodge
decomposition, and random clique complexes); and applications (arbitrage, and
ranking of sports teams). Not all of these connections are explored in this
paper, but many are. The underlying ideas are easy to explain, requiring only
the four fundamental subspaces from elementary linear algebra. One of our aims
is to explain these basic ideas and connections, to get researchers in many
fields interested in this topic. Another aim is to use our numerical
experiments for guidance on selecting methods and exposing the need for further
development.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1912.00011v1,Heuristic Strategies in Uncertain Approval Voting Environments,"In many collective decision making situations, agents vote to choose an
alternative that best represents the preferences of the group. Agents may
manipulate the vote to achieve a better outcome by voting in a way that does
not reflect their true preferences. In real world voting scenarios, people
often do not have complete information about other voter preferences and it can
be computationally complex to identify a strategy that will maximize their
expected utility. In such situations, it is often assumed that voters will vote
truthfully rather than expending the effort to strategize. However, being
truthful is just one possible heuristic that may be used. In this paper, we
examine the effectiveness of heuristics in single winner and multi-winner
approval voting scenarios with missing votes. In particular, we look at
heuristics where a voter ignores information about other voting profiles and
makes their decisions based solely on how much they like each candidate. In a
behavioral experiment, we show that people vote truthfully in some situations
and prioritize high utility candidates in others. We examine when these
behaviors maximize expected utility and show how the structure of the voting
environment affects both how well each heuristic performs and how humans employ
these heuristics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1501.00334v5,Data-Discriminants of Likelihood Equations,"Maximum likelihood estimation (MLE) is a fundamental computational problem in
statistics. The problem is to maximize the likelihood function with respect to
given data on a statistical model. An algebraic approach to this problem is to
solve a very structured parameterized polynomial system called likelihood
equations. For general choices of data, the number of complex solutions to the
likelihood equations is finite and called the ML-degree of the model. The only
solutions to the likelihood equations that are statistically meaningful are the
real/positive solutions. However, the number of real/positive solutions is not
characterized by the ML-degree. We use discriminants to classify data according
to the number of real/positive solutions of the likelihood equations. We call
these discriminants data-discriminants (DD). We develop a probabilistic
algorithm for computing DDs. Experimental results show that, for the benchmarks
we have tried, the probabilistic algorithm is more efficient than the standard
elimination algorithm. Based on the computational results, we discuss the real
root classification problem for the 3 by 3 symmetric matrix~model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.00290v1,"A Hybrid Ensemble Feature Selection Design for Candidate Biomarkers
  Discovery from Transcriptome Profiles","The discovery of disease biomarkers from gene expression data has been
greatly advanced by feature selection (FS) methods, especially using ensemble
FS (EFS) strategies with perturbation at the data level (i.e., homogeneous,
Hom-EFS) or method level (i.e., heterogeneous, Het-EFS). Here we proposed a
Hybrid EFS (Hyb-EFS) design that explores both types of perturbation to improve
the stability and the predictive power of candidate biomarkers. With this,
Hyb-EFS aims to disrupt associations of good performance with a single dataset,
single algorithm, or a specific combination of both, which is particularly
interesting for better reproducibility of genomic biomarkers. We investigated
the adequacy of our approach for microarray data related to four types of
cancer, carrying out an extensive comparison with other ensemble and single FS
approaches. Five FS methods were used in our experiments: Wx, Symmetrical
Uncertainty (SU), Gain Ratio (GR), Characteristic Direction (GeoDE), and
ReliefF. We observed that the Hyb-EFS and Het-EFS approaches attenuated the
large performance variation observed for most single FS and Hom-EFS across
distinct datasets. Also, the Hyb-EFS improved upon the stability of the Het-EFS
within our domain. Comparing the Hyb-EFS and Het-EFS composed of the
top-performing selectors (Wx, GR, and SU), our hybrid approach surpassed the
equivalent heterogeneous design and the best Hom-EFS (Hom-Wx). Interestingly,
the rankings produced by our Hyb-EFS reached greater biological plausibility,
with a notably high enrichment for cancer-related genes and pathways. Thus, our
experiments suggest the potential of the proposed Hybrid EFS design in
discovering candidate biomarkers from microarray data. Finally, we provide an
open-source framework to support similar analyses in other domains, both as a
user-friendly application and a plain Python package.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1305.0876v1,Knowledge and Security,"Epistemic concepts, and in some cases epistemic logic, have been used in
security research to formalize security properties of systems. This survey
illustrates some of these uses by focusing on confidentiality in the context of
cryptographic protocols, and in the context of multi-level security systems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1412.2314v4,$\ell_p$ Testing and Learning of Discrete Distributions,"The classic problems of testing uniformity of and learning a discrete
distribution, given access to independent samples from it, are examined under
general $\ell_p$ metrics. The intuitions and results often contrast with the
classic $\ell_1$ case. For $p > 1$, we can learn and test with a number of
samples that is independent of the support size of the distribution: With an
$\ell_p$ tolerance $\epsilon$, $O(\max\{ \sqrt{1/\epsilon^q}, 1/\epsilon^2 \})$
samples suffice for testing uniformity and $O(\max\{ 1/\epsilon^q,
1/\epsilon^2\})$ samples suffice for learning, where $q=p/(p-1)$ is the
conjugate of $p$. As this parallels the intuition that $O(\sqrt{n})$ and $O(n)$
samples suffice for the $\ell_1$ case, it seems that $1/\epsilon^q$ acts as an
upper bound on the ""apparent"" support size.
  For some $\ell_p$ metrics, uniformity testing becomes easier over larger
supports: a 6-sided die requires fewer trials to test for fairness than a
2-sided coin, and a card-shuffler requires fewer trials than the die. In fact,
this inverse dependence on support size holds if and only if $p > \frac{4}{3}$.
The uniformity testing algorithm simply thresholds the number of ""collisions""
or ""coincidences"" and has an optimal sample complexity up to constant factors
for all $1 \leq p \leq 2$. Another algorithm gives order-optimal sample
complexity for $\ell_{\infty}$ uniformity testing. Meanwhile, the most natural
learning algorithm is shown to have order-optimal sample complexity for all
$\ell_p$ metrics.
  The author thanks Cl\'{e}ment Canonne for discussions and contributions to
this work.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1401.6523v1,"Strategic aspects of the probabilistic serial rule for the allocation of
  goods","The probabilistic serial (PS) rule is one of the most prominent randomized
rules for the assignment problem. It is well-known for its superior fairness
and welfare properties. However, PS is not immune to manipulative behaviour by
the agents. We examine computational and non-computational aspects of
strategising under the PS rule. Firstly, we study the computational complexity
of an agent manipulating the PS rule. We present polynomial-time algorithms for
optimal manipulation. Secondly, we show that expected utility best responses
can cycle. Thirdly, we examine the existence and computation of Nash
equilibrium profiles under the PS rule. We show that a pure Nash equilibrium is
guaranteed to exist under the PS rule. For two agents, we identify two
different types of preference profiles that are not only in Nash equilibrium
but can also be computed in linear time. Finally, we conduct experiments to
check the frequency of manipulability of the PS rule under different
combinations of the number of agents, objects, and utility functions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1307.1270v1,"A heterogeneous many-core platform for experiments on scalable custom
  interconnects and management of fault and critical events, applied to
  many-process applications: Vol. II, 2012 technical report","This is the second of a planned collection of four yearly volumes describing
the deployment of a heterogeneous many-core platform for experiments on
scalable custom interconnects and management of fault and critical events,
applied to many-process applications. This volume covers several topics, among
which: 1- a system for awareness of faults and critical events (named LO|FA|MO)
on experimental heterogeneous many-core hardware platforms; 2- the integration
and test of the experimental hardware heterogeneous many-core platform QUoNG,
based on the APEnet+ custom interconnect; 3- the design of a
Software-Programmable Distributed Network Processor architecture (DNP) using
ASIP technology; 4- the initial stages of design of a new DNP generation onto a
28nm FPGA. These developments were performed in the framework of the EURETILE
European Project under the Grant Agreement no. 247846.",0,0,0,0,0,0,0,0,1,0,1,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1108.6223v1,Towards Configuration of applied Web-based information system,"In the paper, combinatorial synthesis of structure for applied Web-based
systems is described. The problem is considered as a combination of selected
design alternatives for system parts/components into a resultant composite
decision (i.e., system configuration design). The solving framework is based on
Hierarchical Morphological Multicriteria Design (HMMD) approach: (i)
multicriteria selection of alternatives for system parts, (ii) composing the
selected alternatives into a resultant combination (while taking into account
ordinal quality of the alternatives above and their compatibility). A
lattice-based discrete space is used to evaluate (to integrate) quality of the
resultant combinations (i.e., composite system decisions or system
configurations). In addition, a simplified solving framework based on
multicriteria multiple choice problem is considered. A multistage design
process to obtain a system trajectory is described as well. The basic applied
example is targeted to an applied Web-based system for a communication service
provider. Two other applications are briefly described (corporate system and
information system for academic application).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1203.2886v1,"BitPath -- Label Order Constrained Reachability Queries over Large
  Graphs","In this paper we focus on the following constrained reachability problem over
edge-labeled graphs like RDF -- ""given source node x, destination node y, and a
sequence of edge labels (a, b, c, d), is there a path between the two nodes
such that the edge labels on the path satisfy a regular expression
""*a.*b.*c.*d.*"". A ""*"" before ""a"" allows any other edge label to appear on the
path before edge ""a"". ""a.*"" forces at least one edge with label ""a"". "".*"" after
""a"" allows zero or more edge labels after ""a"" and before ""b"". Our query
processing algorithm uses simple divide-and-conquer and greedy pruning
procedures to limit the search space. However, our graph indexing technique --
based on ""compressed bit-vectors"" -- allows indexing large graphs which
otherwise would have been infeasible. We have evaluated our approach on graphs
with more than 22 million edges and 6 million nodes -- much larger compared to
the datasets used in the contemporary work on path queries.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.08680v1,A Data-driven Approach for Furniture and Indoor Scene Colorization,"We present a data-driven approach that colorizes 3D furniture models and
indoor scenes by leveraging indoor images on the internet. Our approach is able
to colorize the furniture automatically according to an example image. The core
is to learn image-guided mesh segmentation to segment the model into different
parts according to the image object. Given an indoor scene, the system supports
colorization-by-example, and has the ability to recommend the colorization
scheme that is consistent with a user-desired color theme. The latter is
realized by formulating the problem as a Markov random field model that imposes
user input as an additional constraint. We contribute to the community a
hierarchically organized image-model database with correspondences between each
image and the corresponding model at the part-level. Our experiments and a user
study show that our system produces perceptually convincing results comparable
to those generated by interior designers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0611041v1,Groebner Bases Applied to Systems of Linear Difference Equations,"In this paper we consider systems of partial (multidimensional) linear
difference equations. Specifically, such systems arise in scientific computing
under discretization of linear partial differential equations and in
computational high energy physics as recurrence relations for multiloop Feynman
integrals. The most universal algorithmic tool for investigation of linear
difference systems is based on their transformation into an equivalent Groebner
basis form. We present an algorithm for this transformation implemented in
Maple. The algorithm and its implementation can be applied to automatic
generation of difference schemes for linear partial differential equations and
to reduction of Feynman integrals. Some illustrative examples are given.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0612097v1,"Error Exponents for Variable-length Block Codes with Feedback and Cost
  Constraints","Variable-length block-coding schemes are investigated for discrete memoryless
channels with ideal feedback under cost constraints. Upper and lower bounds are
found for the minimum achievable probability of decoding error $P_{e,\min}$ as
a function of constraints $R, \AV$, and $\bar \tau$ on the transmission rate,
average cost, and average block length respectively. For given $R$ and $\AV$,
the lower and upper bounds to the exponent $-(\ln P_{e,\min})/\bar \tau$ are
asymptotically equal as $\bar \tau \to \infty$. The resulting reliability
function, $\lim_{\bar \tau\to \infty} (-\ln P_{e,\min})/\bar \tau$, as a
function of $R$ and $\AV$, is concave in the pair $(R, \AV)$ and generalizes
the linear reliability function of Burnashev to include cost constraints. The
results are generalized to a class of discrete-time memoryless channels with
arbitrary alphabets, including additive Gaussian noise channels with amplitude
and power constraints.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1309.5142v1,Simulation of Two-Way Pushdown Automata Revisited,"The linear-time simulation of 2-way deterministic pushdown automata (2DPDA)
by the Cook and Jones constructions is revisited. Following the semantics-based
approach by Jones, an interpreter is given which, when extended with
random-access memory, performs a linear-time simulation of 2DPDA. The recursive
interpreter works without the dump list of the original constructions, which
makes Cook's insight into linear-time simulation of exponential-time automata
more intuitive and the complexity argument clearer. The simulation is then
extended to 2-way nondeterministic pushdown automata (2NPDA) to provide for a
cubic-time recognition of context-free languages. The time required to run the
final construction depends on the degree of nondeterminism. The key mechanism
that enables the polynomial-time simulations is the sharing of computations by
memoization.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1305.3828v2,"Exploiting non-constant safe memory in resilient algorithms and data
  structures","We extend the Faulty RAM model by Finocchi and Italiano (2008) by adding a
safe memory of arbitrary size $S$, and we then derive tradeoffs between the
performance of resilient algorithmic techniques and the size of the safe
memory. Let $\delta$ and $\alpha$ denote, respectively, the maximum amount of
faults which can happen during the execution of an algorithm and the actual
number of occurred faults, with $\alpha \leq \delta$. We propose a resilient
algorithm for sorting $n$ entries which requires $O\left(n\log n+\alpha
(\delta/S + \log S)\right)$ time and uses $\Theta(S)$ safe memory words. Our
algorithm outperforms previous resilient sorting algorithms which do not
exploit the available safe memory and require $O\left(n\log n+
\alpha\delta\right)$ time. Finally, we exploit our sorting algorithm for
deriving a resilient priority queue. Our implementation uses $\Theta(S)$ safe
memory words and $\Theta(n)$ faulty memory words for storing $n$ keys, and
requires $O\left(\log n + \delta/S\right)$ amortized time for each insert and
deletemin operation. Our resilient priority queue improves the $O\left(\log n +
\delta\right)$ amortized time required by the state of the art.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9905012v1,Linear and Order Statistics Combiners for Pattern Classification,"Several researchers have experimentally shown that substantial improvements
can be obtained in difficult pattern recognition problems by combining or
integrating the outputs of multiple classifiers. This chapter provides an
analytical framework to quantify the improvements in classification results due
to combining. The results apply to both linear combiners and order statistics
combiners. We first show that to a first order approximation, the error rate
obtained over and above the Bayes error rate, is directly proportional to the
variance of the actual decision boundaries around the Bayes optimum boundary.
Combining classifiers in output space reduces this variance, and hence reduces
the ""added"" error. If N unbiased classifiers are combined by simple averaging,
the added error rate can be reduced by a factor of N if the individual errors
in approximating the decision boundaries are uncorrelated. Expressions are then
derived for linear combiners which are biased or correlated, and the effect of
output correlations on ensemble performance is quantified. For order statistics
based non-linear combiners, we derive expressions that indicate how much the
median, the maximum and in general the ith order statistic can improve
classifier performance. The analysis presented here facilitates the
understanding of the relationships among error rates, classifier boundary
distributions, and combining in output space. Experimental results on several
public domain data sets are provided to illustrate the benefits of combining
and to support the analytical results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1404.1009v1,"You are What you Eat (and Drink): Identifying Cultural Boundaries by
  Analyzing Food & Drink Habits in Foursquare","Food and drink are two of the most basic needs of human beings. However, as
society evolved, food and drink became also a strong cultural aspect, being
able to describe strong differences among people. Traditional methods used to
analyze cross-cultural differences are mainly based on surveys and, for this
reason, they are very difficult to represent a significant statistical sample
at a global scale. In this paper, we propose a new methodology to identify
cultural boundaries and similarities across populations at different scales
based on the analysis of Foursquare check-ins. This approach might be useful
not only for economic purposes, but also to support existing and novel
marketing and social applications. Our methodology consists of the following
steps. First, we map food and drink related check-ins extracted from Foursquare
into users' cultural preferences. Second, we identify particular individual
preferences, such as the taste for a certain type of food or drink, e.g., pizza
or sake, as well as temporal habits, such as the time and day of the week when
an individual goes to a restaurant or a bar. Third, we show how to analyze this
information to assess the cultural distance between two countries, cities or
even areas of a city. Fourth, we apply a simple clustering technique, using
this cultural distance measure, to draw cultural boundaries across countries,
cities and regions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.08740v1,"The DEWCAD Project: Pushing Back the Doubly Exponential Wall of
  Cylindrical Algebraic Decomposition","This abstract seeks to introduce the ISSAC community to the DEWCAD project,
which is based at Coventry University and the University of Bath, in the United
Kingdom. The project seeks to push back the Doubly Exponential Wall of
Cylindrical Algebraic Decomposition, through the integration of SAT/SMT
technology, the extension of Lazard projection theory, and the development of
new algorithms based on CAD technology but without producing CADs themselves.
The project also seeks to develop applications of CAD and will focus on
applications in the domains of economics and bio-network analysis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.12505v1,A Pub/Sub SDN-Integrated Framework for IoT Traffic Orchestration,"The Internet of Things (IoT) is advancing and the adoption of
internet-connected devices in everyday use is constantly growing. This increase
not only affects the traffic from other sources in the network, but also the
communication quality requirements, like Quality of Service (QoS), for the IoT
devices and applications. With the rise of dynamic network management and
dynamic network programming technologies like Software-Defined Networking
(SDN), traffic management and communication quality requirements can be
tailored to fit niche use cases and characteristics. We propose a
publish/subscribe QoS-aware framework (PSIoT-SDN) that orchestrates IoT traffic
and mediates the allocation of network resources between IoT data aggregators
and pub/sub consumers. The PSIoT framework allows edge-level QoS control using
the features of publish/ subscribe orchestrator at IoT aggregators and, in
addition, allows network-level QoS control by incorporating SDN features
coupled with a bandwidth allocation model for networkwide IoT traffic
management. The integration of the framework with SDN allows it to dynamically
react to bandwidth sharing enabled by the SDN controller, resulting in better
bandwidth distribution and higher link utilization for IoT traffic.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1311.0378v1,"Comparative Performance Analysis of Intel Xeon Phi, GPU, and CPU","We investigate and characterize the performance of an important class of
operations on GPUs and Many Integrated Core (MIC) architectures. Our work is
motivated by applications that analyze low-dimensional spatial datasets
captured by high resolution sensors, such as image datasets obtained from whole
slide tissue specimens using microscopy image scanners. We identify the data
access and computation patterns of operations in object segmentation and
feature computation categories. We systematically implement and evaluate the
performance of these core operations on modern CPUs, GPUs, and MIC systems for
a microscopy image analysis application. Our results show that (1) the data
access pattern and parallelization strategy employed by the operations strongly
affect their performance. While the performance on a MIC of operations that
perform regular data access is comparable or sometimes better than that on a
GPU; (2) GPUs are significantly more efficient than MICs for operations and
algorithms that irregularly access data. This is a result of the low
performance of the latter when it comes to random data access; (3) adequate
coordinated execution on MICs and CPUs using a performance aware task
scheduling strategy improves about 1.29x over a first-come-first-served
strategy. The example application attained an efficiency of 84% in an execution
with of 192 nodes (3072 CPU cores and 192 MICs).",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1012.3697v4,Analysis of Agglomerative Clustering,"The diameter $k$-clustering problem is the problem of partitioning a finite
subset of $\mathbb{R}^d$ into $k$ subsets called clusters such that the maximum
diameter of the clusters is minimized. One early clustering algorithm that
computes a hierarchy of approximate solutions to this problem (for all values
of $k$) is the agglomerative clustering algorithm with the complete linkage
strategy. For decades, this algorithm has been widely used by practitioners.
However, it is not well studied theoretically. In this paper, we analyze the
agglomerative complete linkage clustering algorithm. Assuming that the
dimension $d$ is a constant, we show that for any $k$ the solution computed by
this algorithm is an $O(\log k)$-approximation to the diameter $k$-clustering
problem. Our analysis does not only hold for the Euclidean distance but for any
metric that is based on a norm. Furthermore, we analyze the closely related
$k$-center and discrete $k$-center problem. For the corresponding agglomerative
algorithms, we deduce an approximation factor of $O(\log k)$ as well.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1908.06093v1,"Additional key features required for different directives based porting
  approaches","This document is one of the deliverable reports created for the ESCAPE
project. ESCAPE stands for Energy-efficient Scalable Algorithms for Weather
Prediction at Exascale. The project develops world-class, extreme-scale
computing capabilities for European operational numerical weather prediction
and future climate models. This is done by identifying Weather & Climate dwarfs
which are key patterns in terms of computation and communication (in the spirit
of the Berkeley dwarfs). These dwarfs are then optimised for different hardware
architectures (single and multi-node) and alternative algorithms are explored.
Performance portability is addressed through the use of domain specific
languages.
  This report summarizes key features required for OpenMP and OpenACC
directives based on experience in the ESCAPE project. For OpenMP, the latest
public draft standard, 5.0, contains the deep copy and multi-level memory
features desired; for OpenACC, Technical Report 16 summarizes ongoing
discussions beyond the Standard version 2.6. This document includes a summary
of our recommendations on this approach.
  Additional work is also desirable in coordinating the runtime and debugging
when both OpenACC and OpenMP directives are used; in particular the
interoperability of the new OPDT debugging interface for OpenMP and its
semantics in a mixed-directive program.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.11524v1,Practical Utility PV Multilevel Inverter Solutions,"Multilevel inverters are used to improve powerquality and reduce component
stresses. This paper describesand compares two multilevel cascaded three phase
inverterimplementations with two different modulation techniques: PhaseShifted
Pulse Width Modulation, and Nearest Level Control.Further analysis will show
required number of inverter levelswith respect to modulation techniques to
provide desired powerand power quality to resistive load or grid. Cascaded
inverterwill be designed and simulated to draw power from PV cells.",0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1306.6428v1,"Internet Control Plane Event Identification using Model Based Change
  Point Detection Techniques","In the raise of many global organizations deploying their data centers and
content services in India, the prefix reachability performance study from
global destinations garners our attention. The events such as failures and
attacks occurring in the Internet topology have impact on Autonomous System
(AS) paths announced in the control plane and reachability of prefixes from
spatially distributed ASes. As a consequence the customer reachability to the
services in terms of increased latency and outages for a short or long time are
experienced. The challenge in control plane event detection is when the data
plane traffic is able to reach the intended destinations correctly. However
detection of such events are crucial for the operations of content and data
center industries. By monitoring the spatially distributed routing table
features like AS path length distributions, spatial prefix reachability
distribution and covering to overlap route ratio, we can detect the control
plane events. In our work, we study prefix AS paths from the publicly available
route-view data and analyze the global reachability as well as reachability to
Indian AS topology. To capture the spatial events in a single temporal pattern,
we propose a counting based measure using prefixes announced by x % of spatial
peers. Employing statistical characteristics change point detection and
temporal aberration algorithm on the time series of the proposed measure, we
identify the occurrence of long and stochastic control plane events. The impact
and duration of the events are also quantified. We validate the mechanisms over
the proposed measure using the SEA-Me-We4 cable cut event manifestations in the
control plane of Indian AS topology. The cable cut events occurred on 6th June
2012 (long term event) and 17th April 2012 (stochastic event) are considered
for validation.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1605.06327v1,Games from Basic Data Structures,"In this paper, we consider combinatorial game rulesets based on data
structures normally covered in an undergraduate Computer Science Data
Structures course: arrays, stacks, queues, priority queues, sets, linked lists,
and binary trees. We describe many rulesets as well as computational and
mathematical properties about them. Two of the rulesets, Tower Nim and Myopic
Col, are new. We show polynomial-time solutions to Tower Nim and to Myopic Col
on paths.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1208.4289v3,"A Quantitative Study of Social Organisation in Open Source Software
  Communities","The success of open source projects crucially depends on the voluntary
contributions of a sufficiently large community of users. Apart from the mere
size of the community, interesting questions arise when looking at the
evolution of structural features of collaborations between community members.
In this article, we discuss several network analytic proxies that can be used
to quantify different aspects of the social organisation in social
collaboration networks. We particularly focus on measures that can be related
to the cohesiveness of the communities, the distribution of responsibilities
and the resilience against turnover of community members. We present a
comparative analysis on a large-scale dataset that covers the full history of
collaborations between users of 14 major open source software communities. Our
analysis covers both aggregate and time-evolving measures and highlights
differences in the social organisation across communities. We argue that our
results are a promising step towards the definition of suitable, potentially
multi-dimensional, resilience and risk indicators for open source software
communities.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/cs/0508078v4,"Proceedings of the 15th Workshop on Logic-based methods in Programming
  Environments WLPE'05 -- October 5, 2005 -- Sitges (Barcelona), Spain","This volume contains papers presented at WLPE 2005, 15th International
Workshop on Logic-based methods in Programming Environments.
  The aim of the workshop is to provide an informal meeting for the researchers
working on logic-based tools for development and analysis of programs. This
year we emphasized two aspects: on one hand the presentation, pragmatics and
experiences of tools for logic programming environments; on the other one,
logic-based environmental tools for programming in general.
  The workshop took place in Sitges (Barcelona), Spain as a satellite workshop
of the 21th International Conference on Logic Programming (ICLP 2005). This
workshop continues the series of successful international workshops on logic
programming environments held in Ohio, USA (1989), Eilat, Israel (1990), Paris,
France (1991), Washington, USA (1992), Vancouver, Canada (1993), Santa
Margherita Ligure, Italy (1994), Portland, USA (1995), Leuven, Belgium and Port
Jefferson, USA (1997), Las Cruces, USA (1999), Paphos, Cyprus (2001),
Copenhagen, Denmark (2002), Mumbai, India (2003) and Saint Malo, France (2004).
  We have received eight submissions (2 from France, 2 Spain-US cooperations,
one Spain-Argentina cooperation, one from Japan, one from the United Kingdom
and one Sweden-France cooperation). Program committee has decided to accept
seven papers. This volume contains revised versions of the accepted papers.
  We are grateful to the authors of the papers, the reviewers and the members
of the Program Committee for the help and fruitful discussions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1110.3741v3,Multi-criteria Anomaly Detection using Pareto Depth Analysis,"We consider the problem of identifying patterns in a data set that exhibit
anomalous behavior, often referred to as anomaly detection. In most anomaly
detection algorithms, the dissimilarity between data samples is calculated by a
single criterion, such as Euclidean distance. However, in many cases there may
not exist a single dissimilarity measure that captures all possible anomalous
patterns. In such a case, multiple criteria can be defined, and one can test
for anomalies by scalarizing the multiple criteria using a linear combination
of them. If the importance of the different criteria are not known in advance,
the algorithm may need to be executed multiple times with different choices of
weights in the linear combination. In this paper, we introduce a novel
non-parametric multi-criteria anomaly detection method using Pareto depth
analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies
under multiple criteria without having to run an algorithm multiple times with
different choices of weights. The proposed PDA approach scales linearly in the
number of criteria and is provably better than linear combinations of the
criteria.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1911.09471v1,"TrueLearn: A Family of Bayesian Algorithms to Match Lifelong Learners to
  Open Educational Resources","The recent advances in computer-assisted learning systems and the
availability of open educational resources today promise a pathway to providing
cost-efficient, high-quality education to large masses of learners. One of the
most ambitious use cases of computer-assisted learning is to build a lifelong
learning recommendation system. Unlike short-term courses, lifelong learning
presents unique challenges, requiring sophisticated recommendation models that
account for a wide range of factors such as background knowledge of learners or
novelty of the material while effectively maintaining knowledge states of
masses of learners for significantly longer periods of time (ideally, a
lifetime). This work presents the foundations towards building a dynamic,
scalable and transparent recommendation system for education, modelling
learner's knowledge from implicit data in the form of engagement with open
educational resources. We i) use a text ontology based on Wikipedia to
automatically extract knowledge components of educational resources and, ii)
propose a set of online Bayesian strategies inspired by the well-known areas of
item response theory and knowledge tracing. Our proposal, TrueLearn, focuses on
recommendations for which the learner has enough background knowledge (so they
are able to understand and learn from the material), and the material has
enough novelty that would help the learner improve their knowledge about the
subject and keep them engaged. We further construct a large open educational
video lectures dataset and test the performance of the proposed algorithms,
which show clear promise towards building an effective educational
recommendation system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1307.1769v1,Ensemble Methods for Multi-label Classification,"Ensemble methods have been shown to be an effective tool for solving
multi-label classification tasks. In the RAndom k-labELsets (RAKEL) algorithm,
each member of the ensemble is associated with a small randomly-selected subset
of k labels. Then, a single label classifier is trained according to each
combination of elements in the subset. In this paper we adopt a similar
approach, however, instead of randomly choosing subsets, we select the minimum
required subsets of k labels that cover all labels and meet additional
constraints such as coverage of inter-label correlations. Construction of the
cover is achieved by formulating the subset selection as a minimum set covering
problem (SCP) and solving it by using approximation algorithms. Every cover
needs only to be prepared once by offline algorithms. Once prepared, a cover
may be applied to the classification of any given multi-label dataset whose
properties conform with those of the cover. The contribution of this paper is
two-fold. First, we introduce SCP as a general framework for constructing label
covers while allowing the user to incorporate cover construction constraints.
We demonstrate the effectiveness of this framework by proposing two
construction constraints whose enforcement produces covers that improve the
prediction performance of random selection. Second, we provide theoretical
bounds that quantify the probabilities of random selection to produce covers
that meet the proposed construction criteria. The experimental results indicate
that the proposed methods improve multi-label classification accuracy and
stability compared with the RAKEL algorithm and to other state-of-the-art
algorithms.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/0812.0893v2,"Linear-Time Algorithms for Geometric Graphs with Sublinearly Many Edge
  Crossings","We provide linear-time algorithms for geometric graphs with sublinearly many
crossings. That is, we provide algorithms running in O(n) time on connected
geometric graphs having n vertices and k crossings, where k is smaller than n
by an iterated logarithmic factor. Specific problems we study include Voronoi
diagrams and single-source shortest paths. Our algorithms all run in linear
time in the standard comparison-based computational model; hence, we make no
assumptions about the distribution or bit complexities of edge weights, nor do
we utilize unusual bit-level operations on memory words. Instead, our
algorithms are based on a planarization method that ""zeroes in"" on edge
crossings, together with methods for extending planar separator decompositions
to geometric graphs with sublinearly many crossings. Incidentally, our
planarization algorithm also solves an open computational geometry problem of
Chazelle for triangulating a self-intersecting polygonal chain having n
segments and k crossings in linear time, for the case when k is sublinear in n
by an iterated logarithmic factor.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1612.02192v2,Fast Adaptation in Generative Models with Generative Matching Networks,"Despite recent advances, the remaining bottlenecks in deep generative models
are necessity of extensive training and difficulties with generalization from
small number of training examples. We develop a new generative model called
Generative Matching Network which is inspired by the recently proposed matching
networks for one-shot learning in discriminative tasks. By conditioning on the
additional input dataset, our model can instantly learn new concepts that were
not available in the training data but conform to a similar generative process.
The proposed framework does not explicitly restrict diversity of the
conditioning data and also does not require an extensive inference procedure
for training or adaptation. Our experiments on the Omniglot dataset demonstrate
that Generative Matching Networks significantly improve predictive performance
on the fly as more additional data is available and outperform existing state
of the art conditional generative models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0212054v1,"Improved Compact Visibility Representation of Planar Graph via
  Schnyder's Realizer","Let $G$ be an $n$-node planar graph. In a visibility representation of $G$,
each node of $G$ is represented by a horizontal line segment such that the line
segments representing any two adjacent nodes of $G$ are vertically visible to
each other. In the present paper we give the best known compact visibility
representation of $G$. Given a canonical ordering of the triangulated $G$, our
algorithm draws the graph incrementally in a greedy manner. We show that one of
three canonical orderings obtained from Schnyder's realizer for the
triangulated $G$ yields a visibility representation of $G$ no wider than
$\frac{22n-40}{15}$. Our easy-to-implement O(n)-time algorithm bypasses the
complicated subroutines for four-connected components and four-block trees
required by the best previously known algorithm of Kant. Our result provides a
negative answer to Kant's open question about whether $\frac{3n-6}{2}$ is a
worst-case lower bound on the required width. Also, if $G$ has no degree-three
(respectively, degree-five) internal node, then our visibility representation
for $G$ is no wider than $\frac{4n-9}{3}$ (respectively, $\frac{4n-7}{3}$).
Moreover, if $G$ is four-connected, then our visibility representation for $G$
is no wider than $n-1$, matching the best known result of Kant and He. As a
by-product, we obtain a much simpler proof for a corollary of Wagner's Theorem
on realizers, due to Bonichon, Sa\""{e}c, and Mosbah.",0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0906.4837v1,"Advances in the Design and Implementation of a Multi-Tier Architecture
  in the GIPSY Environment","We present advances in the software engineering design and implementation of
the multi-tier run-time system for the General Intensional Programming System
(GIPSY) by further unifying the distributed technologies used to implement the
Demand Migration Framework (DMF) in order to streamline distributed execution
of hybrid intensional-imperative programs using Java.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.05902v1,On fast bounded locality sensitive hashing,"In this paper, we examine the hash functions expressed as scalar products,
i.e., $f(x)=<v,x>$, for some bounded random vector $v$. Such hash functions
have numerous applications, but often there is a need to optimize the choice of
the distribution of $v$. In the present work, we focus on so-called
anti-concentration bounds, i.e. the upper bounds of $\mathbb{P}\left[|<v,x>| <
\alpha \right]$. In many applications, $v$ is a vector of independent random
variables with standard normal distribution. In such case, the distribution of
$<v,x>$ is also normal and it is easy to approximate $\mathbb{P}\left[|<v,x>| <
\alpha \right]$. Here, we consider two bounded distributions in the context of
the anti-concentration bounds. Particularly, we analyze $v$ being a random
vector from the unit ball in $l_{\infty}$ and $v$ being a random vector from
the unit sphere in $l_{2}$. We show optimal up to a constant anti-concentration
measures for functions $f(x)=<v,x>$.
  As a consequence of our research, we obtain new best results for \newline
\textit{$c$-approximate nearest neighbors without false negatives} for $l_p$ in
high dimensional space for all $p\in[1,\infty]$, for
$c=\Omega(\max\{\sqrt{d},d^{1/p}\})$. These results improve over those
presented in [16]. Finally, our paper reports progress on answering the open
problem by Pagh~[17], who considered the nearest neighbor search without false
negatives for the Hamming distance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1602.08237v1,Machine Agency in Human-Machine Networks; Impacts and Trust Implications,"We live in an emerging hyper-connected era in which people are in contact and
interacting with an increasing number of other people and devices.
Increasingly, modern IT systems form networks of humans and machines that
interact with one another. As machines take a more active role in such
networks, they exert an in-creasing level of influence on other participants.
We review the existing literature on agency and propose a definition of agency
that is practical for describing the capabilities and impact human and machine
actors may have in a human-machine network. On this basis, we discuss and
demonstrate the impact and trust implica-tions for machine actors in
human-machine networks for emergency decision support, healthcare and future
smart homes. We maintain that machine agency not only facilitates human to
machine trust, but also interpersonal trust; and that trust must develop to be
able to seize the full potential of future technology.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/cs/9906001v1,On Bounded-Weight Error-Correcting Codes,"This paper computationally obtains optimal bounded-weight, binary,
error-correcting codes for a variety of distance bounds and dimensions. We
compare the sizes of our codes to the sizes of optimal constant-weight, binary,
error-correcting codes, and evaluate the differences.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0503026v1,On Generalized Computable Universal Priors and their Convergence,"Solomonoff unified Occam's razor and Epicurus' principle of multiple
explanations to one elegant, formal, universal theory of inductive inference,
which initiated the field of algorithmic information theory. His central result
is that the posterior of the universal semimeasure M converges rapidly to the
true sequence generating posterior mu, if the latter is computable. Hence, M is
eligible as a universal predictor in case of unknown mu. The first part of the
paper investigates the existence and convergence of computable universal
(semi)measures for a hierarchy of computability classes: recursive, estimable,
enumerable, and approximable. For instance, M is known to be enumerable, but
not estimable, and to dominate all enumerable semimeasures. We present proofs
for discrete and continuous semimeasures. The second part investigates more
closely the types of convergence, possibly implied by universality: in
difference and in ratio, with probability 1, in mean sum, and for Martin-Loef
random sequences. We introduce a generalized concept of randomness for
individual sequences and use it to exhibit difficulties regarding these issues.
In particular, we show that convergence fails (holds) on generalized-random
sequences in gappy (dense) Bernoulli classes.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1007.1268v1,"Application of Data Mining to Network Intrusion Detection: Classifier
  Selection Model","As network attacks have increased in number and severity over the past few
years, intrusion detection system (IDS) is increasingly becoming a critical
component to secure the network. Due to large volumes of security audit data as
well as complex and dynamic properties of intrusion behaviors, optimizing
performance of IDS becomes an important open problem that is receiving more and
more attention from the research community. The uncertainty to explore if
certain algorithms perform better for certain attack classes constitutes the
motivation for the reported herein. In this paper, we evaluate performance of a
comprehensive set of classifier algorithms using KDD99 dataset. Based on
evaluation results, best algorithms for each attack category is chosen and two
classifier algorithm selection models are proposed. The simulation result
comparison indicates that noticeable performance improvement and real-time
intrusion detection can be achieved as we apply the proposed models to detect
different kinds of network attacks.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2107.05101v1,"Machine Learning Challenges and Opportunities in the African
  Agricultural Sector -- A General Perspective","The improvement of computers' capacities, advancements in algorithmic
techniques, and the significant increase of available data have enabled the
recent developments of Artificial Intelligence (AI) technology. One of its
branches, called Machine Learning (ML), has shown strong capacities in
mimicking characteristics attributed to human intelligence, such as vision,
speech, and problem-solving. However, as previous technological revolutions
suggest, their most significant impacts could be mostly expected on other
sectors that were not traditional users of that technology. The agricultural
sector is vital for African economies; improving yields, mitigating losses, and
effective management of natural resources are crucial in a climate change era.
Machine Learning is a technology with an added value in making predictions,
hence the potential to reduce uncertainties and risk across sectors, in this
case, the agricultural sector. The purpose of this paper is to contextualize
and discuss barriers to ML-based solutions for African agriculture. In the
second section, we provided an overview of ML technology from a historical and
technical perspective and its main driving force. In the third section, we
provided a brief review of the current use of ML in agriculture. Finally, in
section 4, we discuss ML growing interest in Africa and the potential barriers
to creating and using ML-based solutions in the agricultural sector.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1706.04337v1,Anonymization of System Logs for Privacy and Storage Benefits,"System logs constitute valuable information for analysis and diagnosis of
system behavior. The size of parallel computing systems and the number of their
components steadily increase. The volume of generated logs by the system is in
proportion to this increase. Hence, long-term collection and storage of system
logs is challenging. The analysis of system logs requires advanced text
processing techniques. For very large volumes of logs, the analysis is highly
time-consuming and requires a high level of expertise. For many parallel
computing centers, outsourcing the analysis of system logs to third parties is
the only affordable option. The existence of sensitive data within system log
entries obstructs, however, the transmission of system logs to third parties.
Moreover, the analytical tools for processing system logs and the solutions
provided by such tools are highly system specific. Achieving a more general
solution is only possible through the access and analysis system of logs of
multiple computing systems. The privacy concerns impede, however, the sharing
of system logs across institutions as well as in the public domain. This work
proposes a new method for the anonymization of the information within system
logs that employs de-identification and encoding to provide sharable system
logs, with the highest possible data quality and of reduced size. The results
presented in this work indicate that apart from eliminating the sensitive data
within system logs and converting them into shareable data, the proposed
anonymization method provides 25% performance improvement in post-processing of
the anonymized system logs, and more than 50% reduction in their required
storage space.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2008.01719v1,Designing for Critical Algorithmic Literacies,"As pervasive data collection and powerful algorithms increasingly shape
children's experience of the world and each other, their ability to interrogate
computational algorithms has become crucially important. A growing body of work
has attempted to articulate a set of ""literacies"" to describe the intellectual
tools that children can use to understand, interrogate, and critique the
algorithmic systems that shape their lives. Unfortunately, because many
algorithms are invisible, only a small number of children develop the
literacies required to critique these systems. How might designers support the
development of critical algorithmic literacies? Based on our experience
designing two data programming systems, we present four design principles that
we argue can help children develop literacies that allow them to understand not
only how algorithms work, but also to critique and question them.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/1412.7532v1,"Toward Refactoring of DMARF and GIPSY Case Studies -- A Team XI
  SOEN6471-S14 Project Report","This report focuses on improving the internal structure of the Distributed
Modular Audio recognition Framework (DMARF) and the General Intensional
Programming System (GIPSY) case studies without affecting their original
behavior. At first, the general principles, and the working of DMARF and GIPSY
are understood by mainly stressing on the architecture of the systems by
looking at their frameworks and running them in the Eclipse environment. To
improve the quality of the structure of the code, a furtherance of
understanding of the architecture of the case studies and this is achieved by
analyzing the design patterns present in the code. The improvement is done by
the identification and removal of code smells in the code of the case studies.
Code smells are identified by analyzing the source code by using Logiscope and
JDeodorant. Some refactoring techniques are suggested, out of which the best
suited ones are implemented to improve the code. Finally, Test cases are
implemented to check if the behavior of the code has changed or not.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1910.07921v2,"FASHION: Functional and Attack graph Secured HybrId Optimization of
  virtualized Networks","Maintaining a resilient computer network is a delicate task with conflicting
priorities. Flows should be served while controlling risk due to attackers.
Upon publication of a vulnerability, administrators scramble to manually
mitigate risk before a patch is available. Tools exist to check network
reachability (Khurshid et al., NSDI 2013) and risk using (probabilistic) attack
graphs (Sheyner et al., IEEE S\&P 2002). These tools are not designed to
fashion configurations that simultaneously satisfy multiple properties.
  We introduce FASHION: a linear optimizer that \emph{fashions} network
configurations to balance functionality and security requirements. FASHION
formalizes functionality as a multi-commodity flow problem with
side-constraints. FASHION formulates security as the average of 1) the risk of
the connected component in the attack graph and 2) the highest probability path
in the attack graph. These measures approximate the risk in a probabilistic
attack graph (Wang et al., Network Security Metrics 2017). FASHION outputs a
set of software-defined networking rules consumable by a Frenetic controller
(Foster et al., ICFP 2011). The approximation linearly combines two measures.
One measure is the impact of the set of nodes the attacker can reach in the
attack graph (ignoring probability). The second is the maximum probability path
in the attack graph.
  FASHION is evaluated on data center networks with up to 649 devices, usually
outputting a solution in under 30 minutes. FASHION allows an enterprise to
automatically reconfigure their network upon a change in functionality (shift
in user demand) or security (publication or patching of a vulnerability).",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.07562v1,Better Automata through Process Algebra,"This paper shows how the use of Structural Operational Semantics (SOS) in the
style popularized by the process-algebra community can lead to a more succinct
and useful construction for building finite automata from regular expressions.
Such constructions have been known for decades, and form the basis for the
proofs of one direction of Kleene's Theorem. The purpose of the new
construction is, on the one hand, to show students how small automata can be
constructed, without the need for empty transitions, and on the other hand to
show how the construction method admits closure proofs of regular languages
with respect to other operators as well. These results, while not theoretically
surprising, point to an additional influence of process-algebraic research: in
addition to providing fundamental insights into the nature of concurrent
computation, it also sheds new light on old, well-known constructions in
automata theory.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.04253v2,Sketching Word Vectors Through Hashing,"We propose a new fast word embedding technique using hash functions. The
method is a derandomization of a new type of random projections: By
disregarding the classic constraint used in designing random projections (i.e.,
preserving pairwise distances in a particular normed space), our solution
exploits extremely sparse non-negative random projections. Our experiments show
that the proposed method can achieve competitive results, comparable to neural
embedding learning techniques, however, with only a fraction of the
computational complexity of these methods. While the proposed derandomization
enhances the computational and space complexity of our method, the possibility
of applying weighting methods such as positive pointwise mutual information
(PPMI) to our models after their construction (and at a reduced dimensionality)
imparts a high discriminatory power to the resulting embeddings. Obviously,
this method comes with other known benefits of random projection-based
techniques such as ease of update.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0212019v1,"Thinking, Learning, and Autonomous Problem Solving","Ever increasing computational power will require methods for automatic
programming. We present an alternative to genetic programming, based on a
general model of thinking and learning. The advantage is that evolution takes
place in the space of constructs and can thus exploit the mathematical
structures of this space. The model is formalized, and a macro language is
presented which allows for a formal yet intuitive description of the problem
under consideration. A prototype has been developed to implement the scheme in
PERL. This method will lead to a concentration on the analysis of problems, to
a more rapid prototyping, to the treatment of new problem classes, and to the
investigation of philosophical problems. We see fields of application in
nonlinear differential equations, pattern recognition, robotics, model
building, and animated pictures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1411.4814v2,"Optimal control of the convergence time in the Hegselmann--Krause
  dynamics","We study the optimal control problem of minimizing the convergence time in
the discrete Hegselmann--Krause model of opinion dynamics. The underlying model
is extended with a set of strategic agents that can freely place their opinion
at every time step. Indeed, if suitably coordinated, the strategic agents can
significantly lower the convergence time of an instance of the
Hegselmann--Krause model. We give several lower and upper worst-case bounds for
the convergence time of a Hegselmann--Krause system with a given number of
strategic agents, while still leaving some gaps for future research.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.09034v1,"Managing layers of risk: Uncertainty in large development programs
  combining agile software development and traditional project management","How risks are managed implicitly and explicitly at multiple levels of agile
projects has not been extensively studied and there is a need to investigate
how risk management can be used in large agile projects. This is the objective
of this exploratory study which investigates the following research question:
How does a large software/hardware development project using agile practices
manage uncertainty at project/subproject and work package levels?",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1812.09701v1,Nonlinear Robust Filtering of Sampled-Data Dynamical Systems,"This work is concerned with robust filtering of nonlinear sampled-data
systems with and without exact discrete-time models. A linear matrix inequality
(LMI) based approach is proposed for the design of robust $H_{\infty}$
observers for a class of Lipschitz nonlinear systems. Two type of systems are
considered, Lipschitz nonlinear discrete-time systems and Lipschitz nonlinear
sampled-data systems with Euler approximate discrete-time models. Observer
convergence when the exact discrete-time model of the system is available is
shown. Then, practical convergence of the proposed observer is proved using the
Euler approximate discrete-time model. As an additional feature, maximizing the
admissible Lipschitz constant, the solution of the proposed LMI optimization
problem guaranties robustness against some nonlinear uncertainty. The robust
H_infty observer synthesis problem is solved for both cases. The maximum
disturbance attenuation level is achieved through LMI optimization. At the end,
a path to extending the results to higher-order approximate discretizations is
provided.",0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9812017v1,"A reusable iterative optimization software library to solve
  combinatorial problems with approximate reasoning","Real world combinatorial optimization problems such as scheduling are
typically too complex to solve with exact methods. Additionally, the problems
often have to observe vaguely specified constraints of different importance,
the available data may be uncertain, and compromises between antagonistic
criteria may be necessary. We present a combination of approximate reasoning
based constraints and iterative optimization based heuristics that help to
model and solve such problems in a framework of C++ software libraries called
StarFLIP++. While initially developed to schedule continuous caster units in
steel plants, we present in this paper results from reusing the library
components in a shift scheduling system for the workforce of an industrial
production plant.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1311.3211v1,Stochastic inference with deterministic spiking neurons,"The seemingly stochastic transient dynamics of neocortical circuits observed
in vivo have been hypothesized to represent a signature of ongoing stochastic
inference. In vitro neurons, on the other hand, exhibit a highly deterministic
response to various types of stimulation. We show that an ensemble of
deterministic leaky integrate-and-fire neurons embedded in a spiking noisy
environment can attain the correct firing statistics in order to sample from a
well-defined target distribution. We provide an analytical derivation of the
activation function on the single cell level; for recurrent networks, we
examine convergence towards stationarity in computer simulations and
demonstrate sample-based Bayesian inference in a mixed graphical model. This
establishes a rigorous link between deterministic neuron models and functional
stochastic dynamics on the network level.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1005.4379v1,"A Meta-Programming Approach to Realizing Dependently Typed Logic
  Programming","Dependently typed lambda calculi such as the Logical Framework (LF) can
encode relationships between terms in types and can naturally capture
correspondences between formulas and their proofs. Such calculi can also be
given a logic programming interpretation: the Twelf system is based on such an
interpretation of LF. We consider here whether a conventional logic programming
language can provide the benefits of a Twelf-like system for encoding type and
proof-and-formula dependencies. In particular, we present a simple mapping from
LF specifications to a set of formulas in the higher-order hereditary Harrop
(hohh) language, that relates derivations and proof-search between the two
frameworks. We then show that this encoding can be improved by exploiting
knowledge of the well-formedness of the original LF specifications to elide
much redundant type-checking information. The resulting logic program has a
structure that closely resembles the original specification, thereby allowing
LF specifications to be viewed as hohh meta-programs. Using the Teyjus
implementation of lambdaProlog, we show that our translation provides an
efficient means for executing LF specifications, complementing the ability that
the Twelf system provides for reasoning about them.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0212037v1,The Management of Context-Sensitive Features: A Review of Strategies,"In this paper, we review five heuristic strategies for handling
context-sensitive features in supervised machine learning from examples. We
discuss two methods for recovering lost (implicit) contextual information. We
mention some evidence that hybrid strategies can have a synergetic effect. We
then show how the work of several machine learning researchers fits into this
framework. While we do not claim that these strategies exhaust the
possibilities, it appears that the framework includes all of the techniques
that can be found in the published literature on contextsensitive learning.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0708.2686v1,"The universal evolutionary computer based on super-recursive algorithms
  of evolvability","This work exposes which mechanisms and procesess in the Nature of evolution
compute a function not computable by Turing machine. The computer with
intelligence that is not higher than one bacteria population could have, but
with efficency to solve the problems that are non-computable by Turing machine
is represented. This theoretical construction is called Universal Evolutinary
Computer and it is based on the superecursive algorithms of evolvability.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0606046v1,Authorised Translations of Electronic Documents,"A concept is proposed to extend authorised translations of documents to
electronically signed, digital documents. Central element of the solution is an
electronic seal, embodied as an XML data structure, which attests to the
correctness of the translation and the authorisation of the translator. The
seal contains a digital signature binding together original and translated
document, thus enabling forensic inspection and therefore legal security in the
appropriation of the translation. Organisational aspects of possible
implementation variants of electronic authorised translations are discussed and
a realisation as a stand-alone web-service is presented.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1202.1163v2,D-iteration method or how to improve Gauss-Seidel method,"The aim of this paper is to present the recently proposed fluid diffusion
based algorithm in the general context of the matrix inversion problem
associated to the Gauss-Seidel method. We explain the simple intuitions that
are behind this diffusion method and how it can outperform existing methods.
Then we present some theoretical problems that are associated to this
representation as open research problems. We also illustrate some connected
problems such as the graph transformation and the PageRank problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0404050v1,"A General Framework For Lazy Functional Logic Programming With Algebraic
  Polymorphic Types","We propose a general framework for first-order functional logic programming,
supporting lazy functions, non-determinism and polymorphic datatypes whose data
constructors obey a set C of equational axioms. On top of a given C, we specify
a program as a set R of C-based conditional rewriting rules for defined
functions. We argue that equational logic does not supply the proper semantics
for such programs. Therefore, we present an alternative logic which includes
C-based rewriting calculi and a notion of model. We get soundness and
completeness for C-based rewriting w.r.t. models, existence of free models for
all programs, and type preservation results. As operational semantics, we
develop a sound and complete procedure for goal solving, which is based on the
combination of lazy narrowing with unification modulo C. Our framework is quite
expressive for many purposes, such as solving action and change problems, or
realizing the GAMMA computation model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1510.05512v3,Arithmetic for Rooted Trees,"We propose a new arithmetic for non-empty rooted unordered trees simply
called trees. After discussing tree representation and enumeration, we define
the operations of tree addition, multiplication and stretch, prove their
properties, and show that all trees can be generated from a starting tree of
one vertex. We then show how a given tree can be obtained as the sum or product
of two trees, thus defining prime trees with respect to addition and
multiplication. In both cases we show how primality can be decided in time
polynomial in the number of vertices and we prove that factorization is unique.
We then define negative trees and suggest dealing with tree equations, giving
some preliminary results. Finally we comment on how our arithmetic might be
useful, and discuss preceding studies that have some relations with our. To the
best of our knowledge our approach and results are completely new aside for an
earlier version of this work submitte as an arXiv manuscript.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1507.01101v4,"Utility Optimal Thread Assignment and Resource Allocation in
  Multi-Server Systems","Achieving high performance in many multi-server systems requires finding a
good assignment of worker threads to servers and also effectively allocating
each server's resources to its assigned threads. The assignment and allocation
components of this problem have been studied extensively but largely separately
in the literature. In this paper, we introduce the assign and allocate (AA)
problem, which seeks to simultaneously find an assignment and allocation that
maximizes the total utility of the threads. Assigning and allocating the
threads together can result in substantially better overall utility than
performing the steps separately, as is traditionally done. We model each thread
by a utility function giving its performance as a function of its assigned
resources. We first prove that the AA problem is NP-hard. We then present a $2
(\sqrt{2}-1) > 0.828$ factor approximation algorithm for concave utility
functions, which runs in $O(mn^2 + n (\log mC)^2)$ time for $n$ threads and $m$
servers with $C$ amount of resources each. We also give a faster algorithm with
the same approximation ratio and $O(n (\log mC)^2)$ time complexity. We then
extend the problem to two more general settings. First, we consider threads
with nonconcave utility functions, and give a 1/2 factor approximation
algorithm. Next, we give an algorithm for threads using multiple types of
resources, and show the algorithm achieves good empirical performance. We
conduct extensive experiments to test the performance of our algorithms on
threads with both synthetic and realistic utility functions, and find that they
achieve over 92\% of the optimal utility on average. We also compare our
algorithms with a number of practical heuristics, and find that our algorithms
achieve up to 9 times higher total utility.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.10402v1,Data-Driven Physical Face Inversion,"Facial animation is one of the most challenging problems in computer
graphics, and it is often solved using linear heuristics like blend-shape
rigging. More expressive approaches like physical simulation have emerged, but
these methods are very difficult to tune, especially when simulating a real
actor's face. We propose to use a simple finite element simulation approach for
face animation, and present a novel method for recovering the required
simulation parameters in order to best match a real actor's face motion. Our
method involves reconstructing a very small number of head poses of the actor
in 3D, where the head poses span different configurations of force directions
due to gravity. Our algorithm can then automatically recover both the
gravity-free rest shape of the face as well as the spatially-varying physical
material stiffness such that a forward simulation will match the captured
targets as closely as possible. As a result, our system can produce
actor-specific, physical parameters that can be immediately used in recent
physical simulation methods for faces. Furthermore, as the simulation results
depend heavily on the chosen spatial layout of material clusters, we analyze
and compare different spatial layouts.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.03090v2,Multilevel Monte Carlo for quantum mechanics on a lattice,"Monte Carlo simulations of quantum field theories on a lattice become
increasingly expensive as the continuum limit is approached since the cost per
independent sample grows with a high power of the inverse lattice spacing.
Simulations on fine lattices suffer from critical slowdown, the rapid growth of
autocorrelations in the Markov chain. This causes a strong increase in the
number of lattice configurations that have to be generated to obtain
statistically significant results. This paper discusses hierarchical sampling
methods to tame the growth in autocorrelations. Combined with multilevel
variance reduction, this significantly reduces the computational cost of
simulations for given tolerances $\epsilon_{\text{disc}}$ on the discretisation
error and $\epsilon_{\text{stat}}$ on the statistical error. For observables
with lattice errors of order $\alpha$ and integrated autocorrelation times that
grow like $\tau_{\mathrm{int}}\propto a^{-z}$, multilevel Monte Carlo (MLMC)
reduces the cost from
$\mathcal{O}(\epsilon_{\text{stat}}^{-2}\epsilon_{\text{disc}}^{-(1+z)/\alpha})$
to $\mathcal{O}(\epsilon_{\text{stat}}^{-2}\vert\log \epsilon_{\text{disc}}
\vert^2+\epsilon_{\text{disc}}^{-1/\alpha})$ or
$\mathcal{O}(\epsilon_{\text{stat}}^{-2}+\epsilon_{\text{disc}}^{-1/\alpha})$.
Higher gains are expected for simulations of quantum field theories in $D$
dimensions. The efficiency of the approach is demonstrated on two model
systems, including a topological oscillator that is badly affected by critical
slowdown from topological charge freezing. On fine lattices, the new methods
are orders of magnitude faster than standard Hybrid Monte Carlo sampling. For
high resolutions, MLMC can be used to accelerate even the cluster algorithm for
the topological oscillator. Performance is further improved through
perturbative matching which guarantees efficient coupling of theories on the
multilevel hierarchy.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1101.1680v1,Safe Register Token Transfer in a Ring,"A token ring is an arrangement of N processors that take turns engaging in an
activity which must be controlled. A token confers the right to engage in the
controlled activity. Processors communicate with neighbors in the ring to
obtain and release a token. The communication mechanism investigated in this
paper is the safe register abstraction, which may arbitrarily corrupt a value
that a processor reads when the operation reading a register is concurrent with
an write operation on that register by a neighboring processor. The main
results are simple protocols for quasi-atomic communication, constructed from
safe registers. A quasi-atomic register behaves atomically except that a
special undefined value may be returned in the case of concurrent read and
write operations. Under certain conditions that constrain the number of writes
and registers, quasi-atomic protocols are adequate substitutes for atomic
protocols. The paper demonstrates how quasi-atomic protocols can be used to
implement a self-stabilizing token ring, either by using two safe registers
between neighboring processors or by using O(lg N) safe registers between
neighbors, which lowers read complexity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1207.0437v5,"Ordinal and Cardinal Dendrograms Depicting Migration-Based
  Regionalization of 3,000 + U. S. Counties","We have obtained a ""hierarchical regionalization"" of 3,107 county-level units
of the United States based upon census-recorded 1995-2000 intercounty migration
flows. The methodology employed was the two-stage (double-standardization and
strong component [directed graph] hierarchical clustering) algorithm described
in the 2009 PNAS (106 [26], E66) letter (arXiv:0904.4863). Various features (e.
g., cosmopolitan vs. provincial aspects, and indices of isolation) of the
regionalization have been previously discussed in arXiv:0907.2393,
arXiv:0903.3623 and arXiv:0809.2768. However, due to the lengthy (38-page)
nature of the associated dendrogram, the detailed tree structure itself was not
readily available for inspection. Here, we do present this (county-searchable)
dendrogram--and invite readers to explore it, based on their particular
interests/locations. An ordinal scale--rather than the originally-derived
cardinal scale of the doubly-standardized values--in which groupings/features
were more immediately apparent, was originally presented. Now, we append the
cardinal-scale dendrogram.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1706.04405v1,"Memory Efficient Forwarding Information Base for Content-Centric
  Networking","Content-Centric Networking (CCN) is a new paradigm for the future Internet
where content is addressed by hierarchically organized names with the goal to
replace TCP/IP networks. Unlike IP addresses, names have arbitrary length and
are larger than the four bytes of IPv4 addresses. One important data structure
in CCN is the Forwarding Information Base (FIB) where prefixes of names are
stored together with the forwarding face. Long prefixes create problems for
memory constrained Internet of Things (IoT) devices. In this work, we derive
requirements for a FIB in the IoT and survey possible solutions. We
investigate, design and compare memory-efficient solutions for the FIB based on
hashes and Bloom-Filters. For large number of prefixes and an equal
distribution of prefixes to faces we recommend a FIB implementation based on
Bloom-Filters. In all other cases, we recommend an implementation of the FIB
with hashes.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.10646v1,Characterization of the Gittins index for sequential multistage jobs,"The optimal scheduling problem in single-server queueing systems is a classic
problem in queueing theory. The Gittins index policy is known to be the optimal
preemptive nonanticipating policy (both for the open version of the problem
with Poisson arrivals and the closed version without arrivals) minimizing the
expected holding costs. While the Gittins index is thoroughly characterized for
ordinary jobs whose state is described by the attained service, it is not at
all the case with jobs that have more complex structure. Recently, a class of
such jobs, the multistage jobs, were introduced, and it was shown that the
computation of Gittins index of a multistage job reduces into separable
computations for the individual stages. The characterization is, however,
indirect in the sense that it relies on the recursion for an auxiliary function
(so called SJP function) and not for the Gittins index itself. In this paper,
we answer the natural question: Is it possible to compute the Gittins index for
a multistage job more directly by recursively combining the Gittins indexes of
its individual stages? According to our results, it seems to be possible, at
least, for sequential multistage jobs that have a fixed (deterministic)
sequence of stages. We prove this for sequential two-stage jobs that have
monotonous hazard rates in both stages, but our numerical experiments give an
indication that the result could possibly be generalized to any sequential
multistage jobs. Our approach, in this paper, is based on the Whittle index
originally developed in the context of restless bandits.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1808.02444v1,Adapting website design for people with color-blindness,"The aim of the study is the description of problem of developing web design
for people with color blindness. The objectives of the study are familiarising
with the exiting algorithms of simulation color blindness and searching the
most appropriate color models to realize a filter of disputed colors. The
object of the study is the convertation of color models and algorithms of
filtration. The subject of the study are methods of recognition disputed
colors. In the study were investigated the problems of color blind people,
examined the basic concepts of trichromatic color vision theory, substantiated
the necessity of changing different types of color models, given formulas
convertation from RGB-color model to HSL-color model, systematized the
algorithms of imitation and filtration of colors for different types of
dichromacy: protanopia, deuteranopia and tritanopia. The results of the study
are planned using in development of adapting website design for people with
color blindness.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/2010.11762v2,Ghost Signals: Verifying Termination of Busy-Waiting (Extended Version),"Programs for multiprocessor machines commonly perform busy waiting for
synchronization. We propose the first separation logic for modularly verifying
termination of such programs under fair scheduling. Our logic requires the
proof author to associate a ghost signal with each busy-waiting loop and allows
such loops to iterate while their corresponding signal $s$ is not set. The
proof author further has to define a well-founded order on signals and to prove
that if the looping thread holds an obligation to set a signal $s^\prime$, then
$s^\prime$ is ordered above $s$. By using conventional shared state invariants
to associate the state of ghost signals with the state of data structures,
programs busy-waiting for arbitrary conditions over arbitrary data structures
can be verified.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1911.06894v3,"Integrality of Linearizations of Polynomials over Binary Variables using
  Additional Monomials","Polynomial optimization problems over binary variables can be expressed as
integer programs using a linearization with extra monomials in addition to
those arising in the given polynomial. We characterize when such a
linearization yields an integral relaxation polytope, generalizing work by Del
Pia and Khajavirad (SIAM Journal on Optimization, 2018) and Buchheim, Crama and
Rodr\'iguez-Heck (European Journal of Operations Research, 2019). We also
present an algorithm that finds these extra monomials for a given polynomial to
yield an integral relaxation polytope or determines that no such set of extra
monomials exists. In the former case, our approach yields an algorithm to solve
the given polynomial optimization problem as a compact LP, and we complement
this with a purely combinatorial algorithm.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.03457v1,Math-Aware Search Engines: Physics Applications and Overview,"Search engines for equations now exist, which return results matching the
query's mathematical meaning or structural presentation. Operating over
scientific papers, online encyclopedias, and math discussion forums, their
content includes physics, math, and other sciences. They enable physicists to
avoid jargon and more easily target mathematical content within and across
disciplines. As a natural extension of keyword-based search, they open up a new
world for discovering both exact and approximate mathematical solutions;
physical systems' analogues and alternative models; and physics' patterns.
  This review presents the existing math-aware search engines, discusses
methods for maximizing their search success, and overviews their math-matching
capabilities. Proposed applications to physics are also given, to contribute
towards developers' and physicists' exploration of the newly available search
horizons.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.01125v1,"Context-aware demand prediction in bike sharing systems: incorporating
  spatial, meteorological and calendrical context","Bike sharing demand is increasing in large cities worldwide. The proper
functioning of bike-sharing systems is, nevertheless, dependent on a balanced
geographical distribution of bicycles throughout a day. In this context,
understanding the spatiotemporal distribution of check-ins and check-outs is
key for station balancing and bike relocation initiatives. Still, recent
contributions from deep learning and distance-based predictors show limited
success on forecasting bike sharing demand. This consistent observation is
hypothesized to be driven by: i) the strong dependence between demand and the
meteorological and situational context of stations; and ii) the absence of
spatial awareness as most predictors are unable to model the effects of
high-low station load on nearby stations.
  This work proposes a comprehensive set of new principles to incorporate both
historical and prospective sources of spatial, meteorological, situational and
calendrical context in predictive models of station demand. To this end, a new
recurrent neural network layering composed by serial long-short term memory
(LSTM) components is proposed with two major contributions: i) the feeding of
multivariate time series masks produced from historical context data at the
input layer, and ii) the time-dependent regularization of the forecasted time
series using prospective context data. This work further assesses the impact of
incorporating different sources of context, showing the relevance of the
proposed principles for the community even though not all improvements from the
context-aware predictors yield statistical significance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1310.8478v2,"Distributed simulation of polychronous and plastic spiking neural
  networks: strong and weak scaling of a representative mini-application
  benchmark executed on a small-scale commodity cluster","We introduce a natively distributed mini-application benchmark representative
of plastic spiking neural network simulators. It can be used to measure
performances of existing computing platforms and to drive the development of
future parallel/distributed computing systems dedicated to the simulation of
plastic spiking networks. The mini-application is designed to generate spiking
behaviors and synaptic connectivity that do not change when the number of
hardware processing nodes is varied, simplifying the quantitative study of
scalability on commodity and custom architectures. Here, we present the strong
and weak scaling and the profiling of the computational/communication
components of the DPSNN-STDP benchmark (Distributed Simulation of Polychronous
Spiking Neural Network with synaptic Spike-Timing Dependent Plasticity). In
this first test, we used the benchmark to exercise a small-scale cluster of
commodity processors (varying the number of used physical cores from 1 to 128).
The cluster was interconnected through a commodity network. Bidimensional grids
of columns composed of Izhikevich neurons projected synapses locally and toward
first, second and third neighboring columns. The size of the simulated network
varied from 6.6 Giga synapses down to 200 K synapses. The code demonstrated to
be fast and scalable: 10 wall clock seconds were required to simulate one
second of activity and plasticity (per Hertz of average firing rate) of a
network composed by 3.2 G synapses running on 128 hardware cores clocked @ 2.4
GHz. The mini-application has been designed to be easily interfaced with
standard and custom software and hardware communication interfaces. It has been
designed from its foundation to be natively distributed and parallel, and
should not pose major obstacles against distribution and parallelization on
several platforms.",0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.05631v1,Finding normal binary floating-point factors in constant time,"Solving the floating-point equation $x \otimes y = z$, where $x$, $y$ and $z$
belong to floating-point intervals, is a common task in automated reasoning for
which no efficient algorithm is known in general. We show that it can be solved
by computing a constant number of floating-point factors, and give a
constant-time algorithm for computing successive normal floating-point factors
of normal floating-point numbers in radix 2. This leads to a constant-time
procedure for solving the given equation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1912.12189v2,LLOV: A Fast Static Data-Race Checker for OpenMP Programs,"In the era of Exascale computing, writing efficient parallel programs is
indispensable and at the same time, writing sound parallel programs is very
difficult. Specifying parallelism with frameworks such as OpenMP is relatively
easy, but data races in these programs are an important source of bugs. In this
paper, we propose LLOV, a fast, lightweight, language agnostic, and static data
race checker for OpenMP programs based on the LLVM compiler framework. We
compare LLOV with other state-of-the-art data race checkers on a variety of
well-established benchmarks. We show that the precision, accuracy, and the F1
score of LLOV is comparable to other checkers while being orders of magnitude
faster. To the best of our knowledge, LLOV is the only tool among the
state-of-the-art data race checkers that can verify a C/C++ or FORTRAN program
to be data race free.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.12946v2,Redy: Remote Dynamic Memory Cache,"Redy is a cloud service that provides high performance caches using
RDMA-accessible remote memory. An application can customize the performance of
each cache with a service level objective (SLO) for latency and throughput. By
using remote memory, it can leverage stranded memory and spot VM instances to
reduce the cost of its caches and improve data center resource utilization.
Redy automatically customizes the resource configuration for the given SLO,
handles the dynamics of remote memory regions, and recovers from failures. The
experimental evaluation shows that Redy can deliver its promised performance
and robustness under remote memory dynamics in the cloud. We augment a
production key-value store, FASTER, with a Redy cache. When the working set
exceeds local memory, using Redy is significantly faster than spilling to SSDs.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.03523v1,An optimal mode selection algorithm for scalable video coding,"Scalable video coding (SVC) is extended from its predecessor advanced video
coding (AVC) because of its flexible transmission to all type of gadgets.
However, SVC is more flexible and scalable than AVC, but it is more complex in
determining the computations than AVC. The traditional full search method in
the standard H.264 SVC consumes more encoding time for computation. This
complexity in computation need to be reduced and many fast mode decision (FMD)
algorithms were developed, but many fail to balance in all the three measures
such as peak signal to noise ratio (PSNR), encoding time and bit rate. In this
paper, the proposed optimal mode selection algorithm based on the orientation
of pixels achieves better time saving, good PSNR and coding efficiency. The
proposed algorithm is compared with the standard H.264 JSVM reference software
and found to be 57.44% time saving, 0.43 dB increments in PSNR and 0.23%
compression in bit rate.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1003.5865v1,"Offline Signature Identification by Fusion of Multiple Classifiers using
  Statistical Learning Theory","This paper uses Support Vector Machines (SVM) to fuse multiple classifiers
for an offline signature system. From the signature images, global and local
features are extracted and the signatures are verified with the help of
Gaussian empirical rule, Euclidean and Mahalanobis distance based classifiers.
SVM is used to fuse matching scores of these matchers. Finally, recognition of
query signatures is done by comparing it with all signatures of the database.
The proposed system is tested on a signature database contains 5400 offline
signatures of 600 individuals and the results are found to be promising.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1708.02323v2,Odd Multiway Cut in Directed Acyclic Graphs,"We investigate the odd multiway node (edge) cut problem where the input is a
graph with a specified collection of terminal nodes and the goal is to find a
smallest subset of nonterminal nodes (edges) to delete so that the terminal
nodes do not have an odd length path between them. In an earlier work,
Lokshtanov and Ramanujan showed that both odd multiway node cut and odd
multiway edge cut are fixed-parameter tractable (FPT) when parameterized by the
size of the solution in undirected graphs. In this work, we focus on directed
acyclic graphs (DAGs) and design a fixed-parameter algorithm. Our main
contribution is a broadening of the shadow-removal framework to address parity
problems in DAGs. We complement our FPT results with tight approximability as
well as polyhedral results for 2 terminals in DAGs. Additionally, we show
inapproximability results for odd multiway edge cut in undirected graphs even
for 2 terminals.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0102015v1,"Non-convex cost functionals in boosting algorithms and methods for panel
  selection","In this document we propose a new improvement for boosting techniques as
proposed in Friedman '99 by the use of non-convex cost functional. The idea is
to introduce a correlation term to better deal with forecasting of additive
time series. The problem is discussed in a theoretical way to prove the
existence of minimizing sequence, and in a numerical way to propose a new
""ArgMin"" algorithm. The model has been used to perform the touristic presence
forecast for the winter season 1999/2000 in Trentino (italian Alps).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.11909v2,"Immersive virtual reality methods in cognitive neuroscience and
  neuropsychology: Meeting the criteria of the National Academy of
  Neuropsychology and American Academy of Clinical Neuropsychology","Clinical tools involving immersive virtual reality (VR) may bring several
advantages to cognitive neuroscience and neuropsychology. However, there are
some technical and methodological pitfalls. The American Academy of Clinical
Neuropsychology (AACN) and the National Academy of Neuropsychology (NAN) raised
8 key issues pertaining to Computerized Neuropsychological Assessment Devices.
These issues pertain to: (1) the safety and effectivity; (2) the identity of
the end-user; (3) the technical hardware and software features; (4) privacy and
data security; (5) the psychometric properties; (6) examinee issues; (7) the
use of reporting services; and (8) the reliability of the responses and
results. The VR Everyday Assessment Lab (VR-EAL) is the first immersive VR
neuropsychological battery with enhanced ecological validity for the assessment
of everyday cognitive functions by offering a pleasant testing experience
without inducing cybersickness. The VR-EAL meets the criteria of the NAN and
AACN, addresses the methodological pitfalls, and brings advantages for
neuropsychological testing. However, there are still shortcomings of the
VR-EAL, which should be addressed. Future iterations should strive to improve
the embodiment illusion in VR-EAL and the creation of an open access VR
software library should be attempted. The discussed studies demonstrate the
utility of VR methods in cognitive neuroscience and neuropsychology.",0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1412.4480v2,"On Performance Debugging of Unnecessary Lock Contentions on Multicore
  Processors: A Replay-based Approach","Locks have been widely used as an effective synchronization mechanism among
processes and threads. However, we observe that a large number of false
inter-thread dependencies (i.e., unnecessary lock contentions) exist during the
program execution on multicore processors, thereby incurring significant
performance overhead. This paper presents a performance debugging framework,
PERFPLAY, to facilitate a comprehensive and in-depth understanding of the
performance impact of unnecessary lock contentions. The core technique of our
debugging framework is trace replay. Specifically, PERFPLAY records the program
execution trace, on the basis of which the unnecessary lock contentions can be
identified through trace analysis. We then propose a novel technique of trace
transformation to transform these identified unnecessary lock contentions in
the original trace into the correct pattern as a new trace free of unnecessary
lock contentions. Through replaying both traces, PERFPLAY can quantify the
performance impact of unnecessary lock contentions. To demonstrate the
effectiveness of our debugging framework, we study five real-world programs and
PARSEC benchmarks. Our experimental results demonstrate the significant
performance overhead of unnecessary lock contentions, and the effectiveness of
PERFPLAY in identifying the performance critical unnecessary lock contentions
in real applications.",0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1703.01041v2,Large-Scale Evolution of Image Classifiers,"Neural networks have proven effective at solving difficult problems but
designing their architectures can be challenging, even for image classification
problems alone. Our goal is to minimize human participation, so we employ
evolutionary algorithms to discover such networks automatically. Despite
significant computational requirements, we show that it is now possible to
evolve models with accuracies within the range of those published in the last
year. Specifically, we employ simple evolutionary techniques at unprecedented
scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting
from trivial initial conditions and reaching accuracies of 94.6% (95.6% for
ensemble) and 77.0%, respectively. To do this, we use novel and intuitive
mutation operators that navigate large search spaces; we stress that no human
participation is required once evolution starts and that the output is a
fully-trained model. Throughout this work, we place special emphasis on the
repeatability of results, the variability in the outcomes and the computational
requirements.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.04074v2,NoSQL Databases: Yearning for Disambiguation,"The demanding requirements of the new Big Data intensive era raised the need
for flexible storage systems capable of handling huge volumes of unstructured
data and of tackling the challenges that traditional databases were facing.
NoSQL Databases, in their heterogeneity, are a powerful and diverse set of
databases tailored to specific industrial and business needs. However, the lack
of theoretical background creates a lack of consensus even among experts about
many NoSQL concepts, leading to ambiguity and confusion. In this paper, we
present a survey of NoSQL databases and their classification by data model
type. We also conduct a benchmark in order to compare different NoSQL databases
and distinguish their characteristics. Additionally, we present the major areas
of ambiguity and confusion around NoSQL databases and their related concepts,
and attempt to disambiguate them.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1407.3360v1,Even faster integer multiplication,"We give a new proof of F\""urer's bound for the cost of multiplying n-bit
integers in the bit complexity model. Unlike F\""urer, our method does not
require constructing special coefficient rings with ""fast"" roots of unity.
Moreover, we prove the more explicit bound O(n log n K^(log^* n))$ with K = 8.
We show that an optimised variant of F\""urer's algorithm achieves only K = 16,
suggesting that the new algorithm is faster than F\""urer's by a factor of
2^(log^* n). Assuming standard conjectures about the distribution of Mersenne
primes, we give yet another algorithm that achieves K = 4.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0511064v1,"The consistency principle for a digitization procedure. An algorithm for
  building normal digital spaces of continuous n-dimensional objects","This paper considers conditions, which allow to preserve important
topological and geometric properties in the process of digitization. For this
purpose, we introduce a triplet {C,M,D} consisting of a continuous object C, an
intermediate model M, which is a collection of subregions whose union is C, a
digital model D, which is the intersection graph of M, and apply the
consistency principle and criteria of similarity to M in order to make its
mathematical structure consistent with the natural structure of D.
Specifically, this paper introduces a locally centered lump collection of
subregions and shows that for any locally centered lump cover of an
n-dimensional continuous manifold, the digital model of the manifold is a
digital normal n-dimensional space. In addition, we give examples of locally
centered lump tilings of two-manifolds. We propose an algorithm for
constructing normal digital models of continuous objects.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2111.13907v1,A Hierarchy-Aware Pose Representation for Deep Character Animation,"Data-driven character animation techniques rely on the existence of a
properly established model of motion, capable of describing its rich context.
However, commonly used motion representations often fail to accurately encode
the full articulation of motion, or present artifacts. In this work, we address
the fundamental problem of finding a robust pose representation for motion
modeling, suitable for deep character animation, one that can better constrain
poses and faithfully capture nuances correlated with skeletal characteristics.
Our representation is based on dual quaternions, the mathematical abstractions
with well-defined operations, which simultaneously encode rotational and
positional orientation, enabling a hierarchy-aware encoding, centered around
the root. We demonstrate that our representation overcomes common motion
artifacts, and assess its performance compared to other popular
representations. We conduct an ablation study to evaluate the impact of various
losses that can be incorporated during learning. Leveraging the fact that our
representation implicitly encodes skeletal motion attributes, we train a
network on a dataset comprising of skeletons with different proportions,
without the need to retarget them first to a universal skeleton, which causes
subtle motion elements to be missed. We show that smooth and natural poses can
be achieved, paving the way for fascinating applications.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1209.4479v1,"Beyond Cumulated Gain and Average Precision: Including Willingness and
  Expectation in the User Model","In this paper, we define a new metric family based on two concepts: The
definition of the stopping criterion and the notion of satisfaction, where the
former depends on the willingness and expectation of a user exploring search
results. Both concepts have been discussed so far in the IR literature, but we
argue in this paper that defining a proper single valued metric depends on
merging them into a single conceptual framework.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0711.3500v1,Secure Fractal Image Coding,"In recent work, various fractal image coding methods are reported, which
adopt the self-similarity of images to compress the size of images. However,
till now, no solutions for the security of fractal encoded images have been
provided. In this paper, a secure fractal image coding scheme is proposed and
evaluated, which encrypts some of the fractal parameters during fractal
encoding, and thus, produces the encrypted and encoded image. The encrypted
image can only be recovered by the correct key. To keep secure and efficient,
only the suitable parameters are selected and encrypted through in-vestigating
the properties of various fractal parameters, including parameter space,
parameter distribu-tion and parameter sensitivity. The encryption process does
not change the file format, keeps secure in perception, and costs little time
or computational resources. These properties make it suitable for secure image
encoding or transmission.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.02639v2,"More Reliable AI Solution: Breast Ultrasound Diagnosis Using Multi-AI
  Combination","Objective: Breast cancer screening is of great significance in contemporary
women's health prevention. The existing machines embedded in the AI system do
not reach the accuracy that clinicians hope. How to make intelligent systems
more reliable is a common problem. Methods: 1) Ultrasound image
super-resolution: the SRGAN super-resolution network reduces the unclearness of
ultrasound images caused by the device itself and improves the accuracy and
generalization of the detection model. 2) In response to the needs of medical
images, we have improved the YOLOv4 and the CenterNet models. 3) Multi-AI
model: based on the respective advantages of different AI models, we employ two
AI models to determine clinical resuls cross validation. And we accept the same
results and refuses others. Results: 1) With the help of the super-resolution
model, the YOLOv4 model and the CenterNet model both increased the mAP score by
9.6% and 13.8%. 2) Two methods for transforming the target model into a
classification model are proposed. And the unified output is in a specified
format to facilitate the call of the molti-AI model. 3) In the classification
evaluation experiment, concatenated by the YOLOv4 model (sensitivity 57.73%,
specificity 90.08%) and the CenterNet model (sensitivity 62.64%, specificity
92.54%), the multi-AI model will refuse to make judgments on 23.55% of the
input data. Correspondingly, the performance has been greatly improved to
95.91% for the sensitivity and 96.02% for the specificity. Conclusion: Our work
makes the AI model more reliable in medical image diagnosis. Significance: 1)
The proposed method makes the target detection model more suitable for
diagnosing breast ultrasound images. 2) It provides a new idea for artificial
intelligence in medical diagnosis, which can more conveniently introduce target
detection models from other fields to serve medical lesion screening.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.00361v3,Railgun: streaming windows for mission critical systems,"Some mission critical systems, such as fraud detection, require accurate,
real-time metrics over long time windows on applications that demand high
throughputs and low latencies. As these applications need to run ""forever"",
cope with large and spiky data loads, they further require to be run in a
distributed setting. Unsurprisingly, we are unaware of any distributed
streaming system that provides all those properties. Instead, existing systems
take large simplifications, such as implementing sliding windows as a fixed set
of partially overlapping windows, jeopardizing metric accuracy (violating
financial regulator rules) or latency (breaching service agreements).
  In this paper, we propose Railgun, a fault-tolerant, elastic, and distributed
streaming system supporting real-time sliding windows for scenarios requiring
high loads and millisecond-level latencies. We benchmarked an initial prototype
of Railgun using real data, showing significant lower latency than Flink, and
low memory usage, independent of window size.",0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2104.06443v1,Modeling Framing in Immigration Discourse on Social Media,"The framing of political issues can influence policy and public opinion. Even
though the public plays a key role in creating and spreading frames, little is
known about how ordinary people on social media frame political issues. By
creating a new dataset of immigration-related tweets labeled for multiple
framing typologies from political communication theory, we develop supervised
models to detect frames. We demonstrate how users' ideology and region impact
framing choices, and how a message's framing influences audience responses. We
find that the more commonly-used issue-generic frames obscure important
ideological and regional patterns that are only revealed by
immigration-specific frames. Furthermore, frames oriented towards human
interests, culture, and politics are associated with higher user engagement.
This large-scale analysis of a complex social and linguistic phenomenon
contributes to both NLP and social science research.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1105.5104v3,Simplicial Flat Norm with Scale,"We study the multiscale simplicial flat norm (MSFN) problem, which computes
flat norm at various scales of sets defined as oriented subcomplexes of finite
simplicial complexes in arbitrary dimensions. We show that the multiscale
simplicial flat norm is NP-complete when homology is defined over integers. We
cast the multiscale simplicial flat norm as an instance of integer linear
optimization. Following recent results on related problems, the multiscale
simplicial flat norm integer program can be solved in polynomial time by
solving its linear programming relaxation, when the simplicial complex
satisfies a simple topological condition (absence of relative torsion). Our
most significant contribution is the simplicial deformation theorem, which
states that one may approximate a general current with a simplicial current
while bounding the expansion of its mass. We present explicit bounds on the
quality of this approximation, which indicate that the simplicial current gets
closer to the original current as we make the simplicial complex finer. The
multiscale simplicial flat norm opens up the possibilities of using flat norm
to denoise or extract scale information of large data sets in arbitrary
dimensions. On the other hand, it allows one to employ the large body of
algorithmic results on simplicial complexes to address more general problems
related to currents.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1504.02597v1,A State Space Tool for Models Expressed In C++ (tool paper),"This publication introduces A State Space Exploration Tool that is based on
representing the model under verification as a piece of C++ code that obeys
certain conventions. Its name is ASSET. Model checking takes place by compiling
the model and the tool together, and executing the result. This approach
facilitates very fast execution of the transitions of the model. On the other
hand, the use of stubborn sets and symmetries requires that either the modeller
or a preprocessor tool analyses the model at a syntactic level and expresses
stubborn set obligation rules and the symmetry mapping as suitable C++
functions. The tool supports the detection of illegal deadlocks, safety errors,
and may progress errors. It also partially supports the detection of must
progress errors.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1306.6526v4,Inference of Field-Sensitive Reachability and Cyclicity,"In heap-based languages, knowing that a variable x points to an acyclic data
structure is useful for analyzing termination: this information guarantees that
the depth of the data structure to which x points is greater than the depth of
the structure pointed to by x.fld, and allows bounding the number of iterations
of a loop which traverses the data structure on fld. In general, proving
termination needs acyclicity, unless program-specific or non-automated
reasoning is performed. However, recent work could prove that certain loops
terminate even without inferring acyclicity, because they traverse data
structures ""acyclically"". Consider a double-linked list: if it is possible to
demonstrate that every cycle involves both the ""next"" and the ""prev"" field,
then a traversal on ""next"" terminates since no cycle will be traversed
completely. This paper develops a static analysis inferring field-sensitive
reachability and cyclicity information, which is more general than existing
approaches. Propositional formulae are computed, which describe which fields
may or may not be traversed by paths in the heap. Consider a tree with edges
""left"" and ""right"" to the left and right sub-trees, and ""parent"" to the parent
node: termination of a loop traversing leaf-up cannot be guaranteed by
state-of-the-art analyses. Instead, propositional formulae computed by this
analysis indicate that cycles must traverse ""parent"" and at least one between
""left"" and ""right"": termination is guaranteed as no cycle is traversed
completely. This paper defines the necessary abstract domains and builds an
abstract semantics on them. A prototypical implementation provides the expected
result on relevant examples.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1707.09904v3,Temporal Hierarchical Clustering,"We study hierarchical clusterings of metric spaces that change over time.
This is a natural geometric primitive for the analysis of dynamic data sets.
Specifically, we introduce and study the problem of finding a temporally
coherent sequence of hierarchical clusterings from a sequence of unlabeled
point sets. We encode the clustering objective by embedding each point set into
an ultrametric space, which naturally induces a hierarchical clustering of the
set of points. We enforce temporal coherence among the embeddings by finding
correspondences between successive pairs of ultrametric spaces which exhibit
small distortion in the Gromov-Hausdorff sense. We present both upper and lower
bounds on the approximability of the resulting optimization problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0801.4280v1,Spreadsheet Debugging,"Spreadsheet programs, artifacts developed by non-programmers, are used for a
variety of important tasks and decisions. Yet a significant proportion of them
have severe quality problems. To address this issue, our previous work
presented an interval-based testing methodology for spreadsheets.
Interval-based testing rests on the observation that spreadsheets are mainly
used for numerical computations. It also incorporates ideas from symbolic
testing and interval analysis. This paper addresses the issue of efficiently
debugging spreadsheets. Based on the interval-based testing methodology, this
paper presents a technique for tracing faults in spreadsheet programs. The
fault tracing technique proposed uses the dataflow information and cell marks
to identify the most influential faulty cell(s) for a given formula cell
containing a propagated fault.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1
http://arxiv.org/abs/1405.4021v1,A Framework for Bottom-Up Simulation of SLD-Resolution,"This paper introduces a framework for the bottom-up simulation of
SLD-resolution based on partial evaluation. The main idea is to use database
facts to represent a set of SLD goals. For deductive databases it is natural to
assume that the rules defining derived predicates are known at ""compile time"",
whereas the database predicates are known only later at runtime. The framework
is inspired by the author's own SLDMagic method, and a variant of Earley
deduction recently introduced by Heike Stephan and the author. However, it
opens a much broader perspective. [To appear in Theory and Practice of Logic
Programming (TPLP)]",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.08775v2,"Momentum-inspired Low-Rank Coordinate Descent for Diagonally Constrained
  SDPs","We present a novel, practical, and provable approach for solving diagonally
constrained semi-definite programming (SDP) problems at scale using accelerated
non-convex programming. Our algorithm non-trivially combines acceleration
motions from convex optimization with coordinate power iteration and matrix
factorization techniques. The algorithm is extremely simple to implement, and
adds only a single extra hyperparameter -- momentum. We prove that our method
admits local linear convergence in the neighborhood of the optimum and always
converges to a first-order critical point. Experimentally, we showcase the
merits of our method on three major application domains: MaxCut, MaxSAT, and
MIMO signal detection. In all cases, our methodology provides significant
speedups over non-convex and convex SDP solvers -- 5X faster than
state-of-the-art non-convex solvers, and 9 to 10^3 X faster than convex SDP
solvers -- with comparable or improved solution quality.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1409.6022v2,"Exact Analysis of k-Connectivity in Secure Sensor Networks with
  Unreliable Links","The Eschenauer--Gligor (EG) random key predistribution scheme has been widely
recognized as a typical approach to secure communications in wireless sensor
networks (WSNs). However, there is a lack of precise probability analysis on
the reliable connectivity of WSNs under the EG scheme. To address this, we
rigorously derive the asymptotically exact probability of $k$-connectivity in
WSNs employing the EG scheme with unreliable links represented by independent
on/off channels, where $k$-connectivity ensures that the network remains
connected despite the failure of any $(k-1)$ sensors or links. Our analytical
results are confirmed via numerical experiments, and they provide precise
guidelines for the design of secure WSNs that exhibit a desired level of
reliability against node and link failures.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0711.4924v1,Nonuniform Bribery,"We study the concept of bribery in the situation where voters are willing to
change their votes as we ask them, but where their prices depend on the nature
of the change we request. Our model is an extension of the one of Faliszewski
et al. [FHH06], where each voter has a single price for any change we may ask
for. We show polynomial-time algorithms for our version of bribery for a broad
range of voting protocols, including plurality, veto, approval, and utility
based voting. In addition to our polynomial-time algorithms we provide
NP-completeness results for a couple of our nonuniform bribery problems for
weighted voters, and a couple of approximation algorithms for NP-complete
bribery problems defined in [FHH06] (in particular, an FPTAS for
plurality-weighted-$bribery problem).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1202.2922v1,Tracing monadic computations and representing effects,"In functional programming, monads are supposed to encapsulate computations,
effectfully producing the final result, but keeping to themselves the means of
acquiring it. For various reasons, we sometimes want to reveal the internals of
a computation. To make that possible, in this paper we introduce monad
transformers that add the ability to automatically accumulate observations
about the course of execution as an effect. We discover that if we treat the
resulting trace as the actual result of the computation, we can find new
functionality in existing monads, notably when working with non-terminating
computations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1708.08729v1,"Discovering Gender Differences in Facial Emotion Recognition via
  Implicit Behavioral Cues","We examine the utility of implicit behavioral cues in the form of EEG brain
signals and eye movements for gender recognition (GR) and emotion recognition
(ER). Specifically, the examined cues are acquired via low-cost, off-the-shelf
sensors. We asked 28 viewers (14 female) to recognize emotions from unoccluded
(no mask) as well as partially occluded (eye and mouth masked) emotive faces.
Obtained experimental results reveal that (a) reliable GR and ER is achievable
with EEG and eye features, (b) differential cognitive processing especially for
negative emotions is observed for males and females and (c) some of these
cognitive differences manifest under partial face occlusion, as typified by the
eye and mouth mask conditions.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.12992v1,"SofaMyRoom: a fast and multiplatform ""shoebox"" room simulator for
  binaural room impulse response dataset generation","This paper introduces a shoebox room simulator able to systematically
generate synthetic datasets of binaural room impulse responses (BRIRs) given an
arbitrary set of head-related transfer functions (HRTFs). The evaluation of
machine hearing algorithms frequently requires BRIR datasets in order to
simulate the acoustics of any environment. However, currently available
solutions typically consider only HRTFs measured on dummy heads, which poorly
characterize the high variability in spatial sound perception. Our solution
allows to integrate a room impulse response (RIR) simulator with different HRTF
sets represented in Spatially Oriented Format for Acoustics (SOFA). The source
code and the compiled binaries for different operating systems allow to both
advanced and non-expert users to benefit from our toolbox, see
https://github.com/spatialaudiotools/sofamyroom/ .",0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0404033v1,The Persistent Buffer Tree : An I/O-efficient Index for Temporal Data,"In a variety of applications, we need to keep track of the development of a
data set over time. For maintaining and querying this multi version data
I/O-efficiently, external memory data structures are required. In this paper,
we present a probabilistic self-balancing persistent data structure in external
memory called the persistent buffer tree, which supports insertions, updates
and deletions of data items at the present version and range queries for any
version, past or present. The persistent buffer tree is I/O-optimal in the
sense that the expected amortized I/O performance bounds are asymptotically the
same as the deterministic amortized bounds of the (single version) buffer tree
in the worst case.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0106008v1,"Computing Functional and Relational Box Consistency by Structured
  Propagation in Atomic Constraint Systems","Box consistency has been observed to yield exponentially better performance
than chaotic constraint propagation in the interval constraint system obtained
by decomposing the original expression into primitive constraints. The claim
was made that the improvement is due to avoiding decomposition. In this paper
we argue that the improvement is due to replacing chaotic iteration by a more
structured alternative.
  To this end we distinguish the existing notion of box consistency from
relational box consistency. We show that from a computational point of view it
is important to maintain the functional structure in constraint systems that
are associated with a system of equations. So far, it has only been considered
computationally important that constraint propagation be fair. With the
additional structure of functional constraint systems, one can define and
implement computationally effective, structured, truncated constraint
propagations. The existing algorithm for box consistency is one such. Our
results suggest that there are others worth investigating.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1705.07051v1,Speeding up Memory-based Collaborative Filtering with Landmarks,"Recommender systems play an important role in many scenarios where users are
overwhelmed with too many choices to make. In this context, Collaborative
Filtering (CF) arises by providing a simple and widely used approach for
personalized recommendation. Memory-based CF algorithms mostly rely on
similarities between pairs of users or items, which are posteriorly employed in
classifiers like k-Nearest Neighbor (kNN) to generalize for unknown ratings. A
major issue regarding this approach is to build the similarity matrix.
Depending on the dimensionality of the rating matrix, the similarity
computations may become computationally intractable. To overcome this issue, we
propose to represent users by their distances to preselected users, namely
landmarks. This procedure allows to drastically reduce the computational cost
associated with the similarity matrix. We evaluated our proposal on two
distinct distinguishing databases, and the results showed our method has
consistently and considerably outperformed eight CF algorithms (including both
memory-based and model-based) in terms of computational performance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1701.01639v1,"Behavioural - based modelling and analysis of Navigation Patterns across
  Information Networks","Navigation behaviour can be considered as one of the most crucial aspects of
user behaviour in an electronic commerce environment, which is very good
indicator of user's interests either in the process of browsing or purchasing.
Revealing user navigation patterns is very helpful in finding out a way for
increasing sale, turning the most browsers into buyers, keeping costumer's
attention, loyalty, adjusting and improving the interface in order to boost the
user experience and interaction with the system. In this regard, this research
has identified the most common user navigation patterns across information
networks, illustrated through the example of an electronic bookstore. A
behavioural-based model that provides profound knowledge about the processes of
navigation is proposed, specifically examined for different types of users,
automatically identified and clustered into two clusters according to their
navigational behaviour. The developed model is based on stochastic modelling
using the concept of Generalized Stochastic Petri Nets which complex solution
relies on Continuous Time Markov Chain. As a result, calculation of several
performance measures is performed, such as: expected time spent in a transient
tangible marking, cumulative sojourn time spent in a transient tangible
marking, total number of visits in a transient tangible marking etc.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2008.04162v7,Navigating Human Language Models with Synthetic Agents,"Modern natural language models such as the GPT-2/GPT-3 contain tremendous
amounts of information about human belief in a consistently testable form. If
these models could be shown to accurately reflect the underlying beliefs of the
human beings that produced the data used to train these models, then such
models become a powerful sociological tool in ways that are distinct from
traditional methods, such as interviews and surveys. In this study, We train a
version of the GPT-2 on a corpora of historical chess games, and then ""launch""
clusters of synthetic agents into the model, using text strings to create
context and orientation. We compare the trajectories contained in the text
generated by the agents/model and compare that to the known ground truth of the
chess board, move legality, and historical patterns of play. We find that the
percentages of moves by piece using the model are substantially similar from
human patterns. We further find that the model creates an accurate latent
representation of the chessboard, and that it is possible to plot trajectories
of legal moves across the board using this knowledge.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0712.2063v1,An axiomatic approach to intrinsic dimension of a dataset,"We perform a deeper analysis of an axiomatic approach to the concept of
intrinsic dimension of a dataset proposed by us in the IJCNN'07 paper
(arXiv:cs/0703125). The main features of our approach are that a high intrinsic
dimension of a dataset reflects the presence of the curse of dimensionality (in
a certain mathematically precise sense), and that dimension of a discrete
i.i.d. sample of a low-dimensional manifold is, with high probability, close to
that of the manifold. At the same time, the intrinsic dimension of a sample is
easily corrupted by moderate high-dimensional noise (of the same amplitude as
the size of the manifold) and suffers from prohibitevely high computational
complexity (computing it is an $NP$-complete problem). We outline a possible
way to overcome these difficulties.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1801.02508v1,Spiking memristor logic gates are a type of time-variant perceptron,"Memristors are low-power memory-holding resistors thought to be useful for
neuromophic computing, which can compute via spike-interactions mediated
through the device's short-term memory. Using interacting spikes, it is
possible to build an AND gate that computes OR at the same time, similarly a
full adder can be built that computes the arithmetical sum of its inputs. Here
we show how these gates can be understood by modelling the memristors as a
novel type of perceptron: one which is sensitive to input order. The
memristor's memory can change the input weights for later inputs, and thus the
memristor gates cannot be accurately described by a single perceptron,
requiring either a network of time-invarient perceptrons or a complex
time-varying self-reprogrammable perceptron. This work demonstrates the high
functionality of memristor logic gates, and also that the addition of
theasholding could enable the creation of a standard perceptron in hardware,
which may have use in building neural net chips.",0,0,1,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1706.05082v1,Probabilistic Model Checking of Incomplete Models,"It is crucial for accurate model checking that the model be a complete and
faithful representation of the system. Unfortunately, this is not always
possible, mainly because of two reasons: (i) the model is still under
development and (ii) the correctness of implementation of some modules is not
established. In such circumstances, is it still possible to get correct answers
for some model checking queries?
  This paper is a step towards answering this question. We formulate this
problem for the Discrete Time Markov Chains (DTMC) modeling formalism and the
Probabilistic Computation Tree Logic (PCTL) query language. We then propose a
simple solution by modifying DTMC and PCTL to accommodate three valued logic.
The technique builds on existing model checking algorithms and tools, obviating
the need for new ones to account for three valued logic.
  One of the most useful and popular techniques for modeling complex systems is
through discrete event simulation. Discrete event simulators are essentially
code in some programming language. We show an application of our approach on a
piece of code that contains a module of unknown correctness.
  A preliminary version of this paper appears in the proceedings of Leveraging
Applications of Formal Methods, Verification and Validation: Foundational
Techniques (ISoLA 2016), LNCS 9952, Springer.
  Keywords: Probabilistic models, Probabilistic Model checking Three-valued
Logic, Discrete Time Markov Chain, Probabilistic Computation Tree Logic.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1605.07728v2,Small Representations of Big Kidney Exchange Graphs,"Kidney exchanges are organized markets where patients swap willing but
incompatible donors. In the last decade, kidney exchanges grew from small and
regional to large and national---and soon, international. This growth results
in more lives saved, but exacerbates the empirical hardness of the
$\mathcal{NP}$-complete problem of optimally matching patients to donors.
State-of-the-art matching engines use integer programming techniques to clear
fielded kidney exchanges, but these methods must be tailored to specific models
and objective functions, and may fail to scale to larger exchanges. In this
paper, we observe that if the kidney exchange compatibility graph can be
encoded by a constant number of patient and donor attributes, the clearing
problem is solvable in polynomial time. We give necessary and sufficient
conditions for losslessly shrinking the representation of an arbitrary
compatibility graph. Then, using real compatibility graphs from the UNOS
nationwide kidney exchange, we show how many attributes are needed to encode
real compatibility graphs. The experiments show that, indeed, small numbers of
attributes suffice.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1807.06924v1,Development of SageMath filter for Moodle,"Research goals: determine the characteristics of the development process,
installation, configuration and usage of the filter SageMath for learning
support system Moodle. Research objectives: to prove the feasibility of using
Moodle system as a tool to support the process of competency formation in
technical objects simulation of future bachelors in electromechanical
engineering; to analyze existing support tools of technical objects simulation
and to identify the ways of it's integration into Moodle; to describe the
structure and features of the software implementation of the new SageMath
filter for Moodle; to provide the guidance on installing and configuring
developed filter; to describe the examples of filter usage. Research subject:
text filter development process for learning support system Moodle to
processing the commands of computer mathematics system SageMath. Research
results. Designed SageMath filter allows to execute the Sage code on the
external SageMathCell public server, to view the execution results at the
Moodle pages without reloading by using AJAX technology, to stave off XSS
attacks and ready for use with Moodle. The main conclusions and
recommendations: 1. The perspective direction of learning environment
development for bachelors in electromechanical engineering is the integration
of learning support system Moodle and computer mathematics system SageMath. 2.
An effective tool for embedded a computer mathematics systems SageMath models
into Moodle is a text filter. The software engineering process for this filter
is presented in the article. 3. Promising area of future research is the use of
a developed filter in the process of bachelor's in electromechanical
engineering competencies in technical objects simulation by embedding into
Moodle learning courses the interactive labs programmed in Sage.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/cs/0609059v1,"Automatic annotation of multilingual text collections with a conceptual
  thesaurus","Automatic annotation of documents with controlled vocabulary terms
(descriptors) from a conceptual thesaurus is not only useful for document
indexing and retrieval. The mapping of texts onto the same thesaurus
furthermore allows to establish links between similar documents. This is also a
substantial requirement of the Semantic Web. This paper presents an almost
language-independent system that maps documents written in different languages
onto the same multilingual conceptual thesaurus, EUROVOC. Conceptual thesauri
differ from Natural Language Thesauri in that they consist of relatively small
controlled lists of words or phrases with a rather abstract meaning. To
automatically identify which thesaurus descriptors describe the contents of a
document best, we developed a statistical, associative system that is trained
on texts that have previously been indexed manually. In addition to describing
the large number of empirically optimised parameters of the fully functional
application, we present the performance of the software according to a human
evaluation by professional indexers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1004.0944v2,"The Automatic Synthesis of Linear Ranking Functions: The Complete
  Unabridged Version","The classical technique for proving termination of a generic sequential
computer program involves the synthesis of a ranking function for each loop of
the program. Linear ranking functions are particularly interesting because many
terminating loops admit one and algorithms exist to automatically synthesize
it. In this paper we present two such algorithms: one based on work dated 1991
by Sohn and Van Gelder; the other, due to Podelski and Rybalchenko, dated 2004.
Remarkably, while the two algorithms will synthesize a linear ranking function
under exactly the same set of conditions, the former is mostly unknown to the
community of termination analysis and its general applicability has never been
put forward before the present paper. In this paper we thoroughly justify both
algorithms, we prove their correctness, we compare their worst-case complexity
and experimentally evaluate their efficiency, and we present an open-source
implementation of them that will make it very easy to include
termination-analysis capabilities in automatic program verifiers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1203.3814v2,Tree-width for first order formulae,"We introduce tree-width for first order formulae \phi, fotw(\phi). We show
that computing fotw is fixed-parameter tractable with parameter fotw. Moreover,
we show that on classes of formulae of bounded fotw, model checking is fixed
parameter tractable, with parameter the length of the formula. This is done by
translating a formula \phi\ with fotw(\phi)<k into a formula of the k-variable
fragment L^k of first order logic. For fixed k, the question whether a given
first order formula is equivalent to an L^k formula is undecidable. In
contrast, the classes of first order formulae with bounded fotw are fragments
of first order logic for which the equivalence is decidable.
  Our notion of tree-width generalises tree-width of conjunctive queries to
arbitrary formulae of first order logic by taking into account the quantifier
interaction in a formula. Moreover, it is more powerful than the notion of
elimination-width of quantified constraint formulae, defined by Chen and Dalmau
(CSL 2005): for quantified constraint formulae, both bounded elimination-width
and bounded fotw allow for model checking in polynomial time. We prove that
fotw of a quantified constraint formula \phi\ is bounded by the
elimination-width of \phi, and we exhibit a class of quantified constraint
formulae with bounded fotw, that has unbounded elimination-width. A similar
comparison holds for strict tree-width of non-recursive stratified datalog as
defined by Flum, Frick, and Grohe (JACM 49, 2002).
  Finally, we show that fotw has a characterization in terms of a cops and
robbers game without monotonicity cost.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1306.3882v2,Chaining Test Cases for Reactive System Testing (extended version),"Testing of synchronous reactive systems is challenging because long input
sequences are often needed to drive them into a state at which a desired
feature can be tested. This is particularly problematic in on-target testing,
where a system is tested in its real-life application environment and the time
required for resetting is high. This paper presents an approach to discovering
a test case chain---a single software execution that covers a group of test
goals and minimises overall test execution time. Our technique targets the
scenario in which test goals for the requirements are given as safety
properties. We give conditions for the existence and minimality of a single
test case chain and minimise the number of test chains if a single test chain
is infeasible. We report experimental results with a prototype tool for C code
generated from Simulink models and compare it to state-of-the-art test suite
generators.",0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1311.3877v1,Optimal Networks from Error Correcting Codes,"To address growth challenges facing large Data Centers and supercomputing
clusters a new construction is presented for scalable, high throughput, low
latency networks. The resulting networks require 1.5-5 times fewer switches,
2-6 times fewer cables, have 1.2-2 times lower latency and correspondingly
lower congestion and packet losses than the best present or proposed networks
providing the same number of ports at the same total bisection. These advantage
ratios increase with network size. The key new ingredient is the exact
equivalence discovered between the problem of maximizing network bisection for
large classes of practically interesting Cayley graphs and the problem of
maximizing codeword distance for linear error correcting codes. Resulting
translation recipe converts existent optimal error correcting codes into
optimal throughput networks.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0007013v1,Applying Constraint Handling Rules to HPSG,"Constraint Handling Rules (CHR) have provided a realistic solution to an
over-arching problem in many fields that deal with constraint logic
programming: how to combine recursive functions or relations with constraints
while avoiding non-termination problems. This paper focuses on some other
benefits that CHR, specifically their implementation in SICStus Prolog, have
provided to computational linguists working on grammar design tools. CHR rules
are applied by means of a subsumption check and this check is made only when
their variables are instantiated or bound. The former functionality is at best
difficult to simulate using more primitive coroutining statements such as
SICStus when/2, and the latter simply did not exist in any form before CHR.
  For the sake of providing a case study in how these can be applied to grammar
development, we consider the Attribute Logic Engine (ALE), a Prolog
preprocessor for logic programming with typed feature structures, and its
extension to a complete grammar development system for Head-driven Phrase
Structure Grammar (HPSG), a popular constraint-based linguistic theory that
uses typed feature structures. In this context, CHR can be used not only to
extend the constraint language of feature structure descriptions to include
relations in a declarative way, but also to provide support for constraints
with complex antecedents and constraints on the co-occurrence of feature values
that are necessary to interpret the type system of HPSG properly.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.02769v2,Visibility Reasoning for Concurrent Snapshot Algorithms,"Visibility relations have been proposed by Henzinger et al. as an abstraction
for proving linearizability of concurrent algorithms that obtains modular and
reusable proofs. This is in contrast to the customary approach based on
exhibiting the algorithm's linearization points. In this paper we apply
visibility relations to develop modular proofs for three elegant concurrent
snapshot algorithms of Jayanti. The proofs are divided by signatures into
components of increasing level of abstraction; the components at higher
abstraction levels are shared, i.e., they apply to all three algorithms
simultaneously. Importantly, the interface properties mathematically capture
Jayanti's original intuitions that have previously been given only informally.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1304.0988v3,Average Case and Distributional Analysis of Dual-Pivot Quicksort,"In 2009, Oracle replaced the long-serving sorting algorithm in its Java 7
runtime library by a new dual-pivot Quicksort variant due to Vladimir
Yaroslavskiy. The decision was based on the strikingly good performance of
Yaroslavskiy's implementation in running time experiments. At that time, no
precise investigations of the algorithm were available to explain its superior
performance - on the contrary: Previous theoretical studies of other dual-pivot
Quicksort variants even discouraged the use of two pivots. Only in 2012, two of
the authors gave an average case analysis of a simplified version of
Yaroslavskiy's algorithm, proving that savings in the number of comparisons are
possible. However, Yaroslavskiy's algorithm needs more swaps, which renders the
analysis inconclusive.
  To force the issue, we herein extend our analysis to the fully detailed style
of Knuth: We determine the exact number of executed Java Bytecode instructions.
Surprisingly, Yaroslavskiy's algorithm needs sightly more Bytecode instructions
than a simple implementation of classic Quicksort - contradicting observed
running times. Like in Oracle's library implementation we incorporate the use
of Insertionsort on small subproblems and show that it indeed speeds up
Yaroslavskiy's Quicksort in terms of Bytecodes; but even with optimal
Insertionsort thresholds the new Quicksort variant needs slightly more Bytecode
instructions on average.
  Finally, we show that the (suitably normalized) costs of Yaroslavskiy's
algorithm converge to a random variable whose distribution is characterized by
a fixed-point equation. From that, we compute variances of costs and show that
for large n, costs are concentrated around their mean.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.00317v2,On SGD's Failure in Practice: Characterizing and Overcoming Stalling,"Stochastic Gradient Descent (SGD) is widely used in machine learning problems
to efficiently perform empirical risk minimization, yet, in practice, SGD is
known to stall before reaching the actual minimizer of the empirical risk. SGD
stalling has often been attributed to its sensitivity to the conditioning of
the problem; however, as we demonstrate, SGD will stall even when applied to a
simple linear regression problem with unity condition number for standard
learning rates. Thus, in this work, we numerically demonstrate and
mathematically argue that stalling is a crippling and generic limitation of SGD
and its variants in practice. Once we have established the problem of stalling,
we generalize an existing framework for hedging against its effects, which (1)
deters SGD and its variants from stalling, (2) still provides convergence
guarantees, and (3) makes SGD and its variants more practical methods for
minimization.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1211.6188v1,Automatic Function Annotations for Hoare Logic,"In systems verification we are often concerned with multiple, inter-dependent
properties that a program must satisfy. To prove that a program satisfies a
given property, the correctness of intermediate states of the program must be
characterized. However, this intermediate reasoning is not always phrased such
that it can be easily re-used in the proofs of subsequent properties. We
introduce a function annotation logic that extends Hoare logic in two important
ways: (1) when proving that a function satisfies a Hoare triple, intermediate
reasoning is automatically stored as function annotations, and (2) these
function annotations can be exploited in future Hoare logic proofs. This
reduces duplication of reasoning between the proofs of different properties,
whilst serving as a drop-in replacement for traditional Hoare logic to avoid
the costly process of proof refactoring. We explain how this was implemented in
Isabelle/HOL and applied to an experimental branch of the seL4 microkernel to
significantly reduce the size and complexity of existing proofs.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0509026v1,Sampling to estimate arbitrary subset sums,"Starting with a set of weighted items, we want to create a generic sample of
a certain size that we can later use to estimate the total weight of arbitrary
subsets. For this purpose, we propose priority sampling which tested on
Internet data performed better than previous methods by orders of magnitude.
  Priority sampling is simple to define and implement: we consider a steam of
items i=0,...,n-1 with weights w_i. For each item i, we generate a random
number r_i in (0,1) and create a priority q_i=w_i/r_i. The sample S consists of
the k highest priority items. Let t be the (k+1)th highest priority. Each
sampled item i in S gets a weight estimate W_i=max{w_i,t}, while non-sampled
items get weight estimate W_i=0.
  Magically, it turns out that the weight estimates are unbiased, that is,
E[W_i]=w_i, and by linearity of expectation, we get unbiased estimators over
any subset sum simply by adding the sampled weight estimates from the subset.
Also, we can estimate the variance of the estimates, and surpricingly, there is
no co-variance between different weight estimates W_i and W_j.
  We conjecture an extremely strong near-optimality; namely that for any weight
sequence, there exists no specialized scheme for sampling k items with unbiased
estimators that gets smaller total variance than priority sampling with k+1
items. Very recently Mario Szegedy has settled this conjecture.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1111.6116v1,AstroDAbis: Annotations and Cross-Matches for Remote Catalogues,"Astronomers are good at sharing data, but poorer at sharing knowledge.
  Almost all astronomical data ends up in open archives, and access to these is
being simplified by the development of the global Virtual Observatory (VO).
This is a great advance, but the fundamental problem remains that these
archives contain only basic observational data, whereas all the astrophysical
interpretation of that data -- which source is a quasar, which a low-mass star,
and which an image artefact -- is contained in journal papers, with very little
linkage back from the literature to the original data archives. It is therefore
currently impossible for an astronomer to pose a query like ""give me all
sources in this data archive that have been identified as quasars"" and this
limits the effective exploitation of these archives, as the user of an archive
has no direct means of taking advantage of the knowledge derived by its
previous users.
  The AstroDAbis service aims to address this, in a prototype service enabling
astronomers to record annotations and cross-identifications in the AstroDAbis
service, annotating objects in other catalogues. We have deployed two
interfaces to the annotations, namely one astronomy-specific one using the TAP
protocol}, and a second exploiting generic Linked Open Data (LOD) and RDF
techniques.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1708.01292v2,What your Facebook Profile Picture Reveals about your Personality,"People spend considerable effort managing the impressions they give others.
Social psychologists have shown that people manage these impressions
differently depending upon their personality. Facebook and other social media
provide a new forum for this fundamental process; hence, understanding people's
behaviour on social media could provide interesting insights on their
personality. In this paper we investigate automatic personality recognition
from Facebook profile pictures. We analyze the effectiveness of four families
of visual features and we discuss some human interpretable patterns that
explain the personality traits of the individuals. For example, extroverts and
agreeable individuals tend to have warm colored pictures and to exhibit many
faces in their portraits, mirroring their inclination to socialize; while
neurotic ones have a prevalence of pictures of indoor places. Then, we propose
a classification approach to automatically recognize personality traits from
these visual features. Finally, we compare the performance of our
classification approach to the one obtained by human raters and we show that
computer-based classifications are significantly more accurate than averaged
human-based classifications for Extraversion and Neuroticism.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1812.08032v2,Progressive Data Science: Potential and Challenges,"Data science requires time-consuming iterative manual activities. In
particular, activities such as data selection, preprocessing, transformation,
and mining, highly depend on iterative trial-and-error processes that could be
sped-up significantly by providing quick feedback on the impact of changes. The
idea of progressive data science is to compute the results of changes in a
progressive manner, returning a first approximation of results quickly and
allow iterative refinements until converging to a final result. Enabling the
user to interact with the intermediate results allows an early detection of
erroneous or suboptimal choices, the guided definition of modifications to the
pipeline and their quick assessment. In this paper, we discuss the
progressiveness challenges arising in different steps of the data science
pipeline. We describe how changes in each step of the pipeline impact the
subsequent steps and outline why progressive data science will help to make the
process more effective. Computing progressive approximations of outcomes
resulting from changes creates numerous research challenges, especially if the
changes are made in the early steps of the pipeline. We discuss these
challenges and outline first steps towards progressiveness, which, we argue,
will ultimately help to significantly speed-up the overall data science
process.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1011.6121v1,On Beamformer Design for Multiuser MIMO Interference Channels,"This paper considers several linear beamformer design paradigms for multiuser
time-invariant multiple-input multiple-output interference channels. Notably,
interference alignment and sum-rate based algorithms such as the maximum
signal-to-interference-plus noise (max-SINR) algorithm are considered. Optimal
linear beamforming under interference alignment consists of two layers; an
inner precoder and decoder (or receive filter) accomplish interference
alignment to eliminate inter-user interference, and an outer precoder and
decoder diagonalize the effective single-user channel resulting from the
interference alignment by the inner precoder and decoder. The relationship
between this two-layer beamforming and the max-SINR algorithm is established at
high signal-to-noise ratio. Also, the optimality of the max-SINR algorithm
within the class of linear beamforming algorithms, and its local convergence
with exponential rate, are established at high signal-to-noise ratio.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1312.0350v1,Class Diagram Restructuring with GROOVE,"This paper describes the GROOVE solution to the ""Class Diagram Restructuring""
case study of the Tool Transformation Contest 2013. We show that the visual
rule formalism enables the required restructuring to be formulated in a very
concise manner. Moreover, the GROOVE functionality for state space exploration
allows checking confluence. Performance-wise, however, the solution does not
scale well.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1507.05290v2,A concise parametrisation of affine transformation,"Good parametrisations of affine transformations are essential to
interpolation, deformation, and analysis of shape, motion, and animation. It
has been one of the central research topics in computer graphics. However,
there is no single perfect method and each one has both advantages and
disadvantages. In this paper, we propose a novel parametrisation of affine
transformations, which is a generalisation to or an improvement of existing
methods. Our method adds yet another choice to the existing toolbox and shows
better performance in some applications. A C++ implementation is available to
make our framework ready to use in various applications.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2005.09942v1,"Estimating volcanic ash emissions using retrieved satellite ash columns
  and inverse ash transport modelling","This paper describes the inversion procedure being used operationally at the
Norwegian Meteorological Institute for estimating ash emission rates from
retrieved satellite ash column amounts and a priori knowledge.
  The overall procedure consists of five stages:
  (1) generate a priori emission estimates;
  (2) run forward simulations with unit emissions;
  (3) collocate/match observations with emission simulations;
  (4) build system of linear equations; and
  (5) solve overdetermined system.
  We go through the mathematical foundations for the inversion procedure,
performance for synthetic cases, and performance for real-world cases. The
novelties of this paper includes pruning of the linear system of equations used
in the inversion and inclusion of observations of ash cloud top altitude.
  The source code used in this work is freely available under an open source
license, and is possible to use for other similar applications.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0711.4388v1,"Contextual Information Retrieval based on Algorithmic Information Theory
  and Statistical Outlier Detection","The main contribution of this paper is to design an Information Retrieval
(IR) technique based on Algorithmic Information Theory (using the Normalized
Compression Distance- NCD), statistical techniques (outliers), and novel
organization of data base structure. The paper shows how they can be integrated
to retrieve information from generic databases using long (text-based) queries.
Two important problems are analyzed in the paper. On the one hand, how to
detect ""false positives"" when the distance among the documents is very low and
there is actual similarity. On the other hand, we propose a way to structure a
document database which similarities distance estimation depends on the length
of the selected text. Finally, the experimental evaluations that have been
carried out to study previous problems are shown.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1106.6196v2,On the behaviours produced by instruction sequences under execution,"We study several aspects of the behaviours produced by instruction sequences
under execution in the setting of the algebraic theory of processes known as
ACP. We use ACP to describe the behaviours produced by instruction sequences
under execution and to describe two protocols implementing these behaviours in
the case where the processing of instructions takes place remotely. We also
show that all finite-state behaviours considered in ACP can be produced by
instruction sequences under execution.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1504.03476v1,"Quantitative Analysis of Probabilistic Models of Software Product Lines
  with Statistical Model Checking","We investigate the suitability of statistical model checking techniques for
analysing quantitative properties of software product line models with
probabilistic aspects. For this purpose, we enrich the feature-oriented
language FLan with action rates, which specify the likelihood of exhibiting
particular behaviour or of installing features at a specific moment or in a
specific order. The enriched language (called PFLan) allows us to specify
models of software product lines with probabilistic configurations and
behaviour, e.g. by considering a PFLan semantics based on discrete-time Markov
chains. The Maude implementation of PFLan is combined with the distributed
statistical model checker MultiVeStA to perform quantitative analyses of a
simple product line case study. The presented analyses include the likelihood
of certain behaviour of interest (e.g. product malfunctioning) and the expected
average cost of products.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1010.2511v6,"The use of machine learning with signal- and NLP processing of source
  code to fingerprint, detect, and classify vulnerabilities and weaknesses with
  MARFCAT","We present a machine learning approach to static code analysis and
fingerprinting for weaknesses related to security, software engineering, and
others using the open-source MARF framework and the MARFCAT application based
on it for the NIST's SATE2010 static analysis tool exposition workshop found at
http://samate.nist.gov/SATE2010Workshop.html",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1108.6304v1,Anisotropic k-Nearest Neighbor Search Using Covariance Quadtree,"We present a variant of the hyper-quadtree that divides a multidimensional
space according to the hyperplanes associated to the principal components of
the data in each hyperquadrant. Each of the $2^\lambda$ hyper-quadrants is a
data partition in a $\lambda$-dimension subspace, whose intrinsic
dimensionality $\lambda\leq d$ is reduced from the root dimensionality $d$ by
the principal components analysis, which discards the irrelevant eigenvalues of
the local covariance matrix. In the present method a component is irrelevant if
its length is smaller than, or comparable to, the local inter-data spacing.
Thus, the covariance hyper-quadtree is fully adaptive to the local
dimensionality. The proposed data-structure is used to compute the anisotropic
K nearest neighbors (kNN), supported by the Mahalanobis metric. As an
application, we used the present k nearest neighbors method to perform density
estimation over a noisy data distribution. Such estimation method can be
further incorporated to the smoothed particle hydrodynamics, allowing computer
simulations of anisotropic fluid flows.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9906020v1,Temporal Meaning Representations in a Natural Language Front-End,"Previous work in the context of natural language querying of temporal
databases has established a method to map automatically from a large subset of
English time-related questions to suitable expressions of a temporal logic-like
language, called TOP. An algorithm to translate from TOP to the TSQL2 temporal
database language has also been defined. This paper shows how TOP expressions
could be translated into a simpler logic-like language, called BOT. BOT is very
close to traditional first-order predicate logic (FOPL), and hence existing
methods to manipulate FOPL expressions can be exploited to interface to
time-sensitive applications other than TSQL2 databases, maintaining the
existing English-to-TOP mapping.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1302.4138v2,Multi-parameter Mechanisms with Implicit Payment Computation,"In this paper we show that payment computation essentially does not present
any obstacle in designing truthful mechanisms, even for multi-parameter
domains, and even when we can only call the allocation rule once. We present a
general reduction that takes any allocation rule which satisfies ""cyclic
monotonicity"" (a known necessary and sufficient condition for truthfulness) and
converts it to a truthful mechanism using a single call to the allocation rule,
with arbitrarily small loss to the expected social welfare.
  A prominent example for a multi-parameter setting in which an allocation rule
can only be called once arises in sponsored search auctions. These are
multi-parameter domains when each advertiser has multiple possible ads he may
display, each with a different value per click. Moreover, the mechanism
typically does not have complete knowledge of the click-realization or the
click-through rates (CTRs); it can only call the allocation rule a single time
and observe the click information for ads that were presented. % are not known.
On the negative side, we show that an allocation that is truthful for any
realization essentially cannot depend on the bids, and hence cannot do better
than random selection for one agent. We then consider a relaxed requirement of
truthfulness, only in expectation over the CTRs. Even for that relaxed version,
making any progress is challenging as standard techniques for construction of
truthful mechanisms (as using VCG or an MIDR allocation rule) cannot be used in
this setting. We design an allocation rule with non-trivial performance and
directly prove it is cyclic-monotone, and thus it can be used to create a
truthful mechanism using our general reduction.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2104.07146v1,"Identification of unknown parameters and prediction with hierarchical
  matrices","Statistical analysis of massive datasets very often implies expensive linear
algebra operations with large dense matrices. Typical tasks are an estimation
of unknown parameters of the underlying statistical model and prediction of
missing values. We developed the H-MLE procedure, which solves these tasks. The
unknown parameters can be estimated by maximizing the joint Gaussian
log-likelihood function, which depends on a covariance matrix. To decrease high
computational cost, we approximate the covariance matrix in the hierarchical
(H-) matrix format. The H-matrix technique allows us to work with inhomogeneous
covariance matrices and almost arbitrary locations. Especially, H-matrices can
be applied in cases when the matrices under consideration are dense and
unstructured.
  For validation purposes, we implemented three machine learning methods: the
k-nearest neighbors (kNN), random forest, and deep neural network. The best
results (for the given datasets) were obtained by the kNN method with three or
seven neighbors depending on the dataset. The results computed with the H-MLE
method were compared with the results obtained by the kNN method.
  The developed H-matrix code and all datasets are freely available online.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0511103v1,An Infeasibility Result for the Multiterminal Source-Coding Problem,"We prove a new outer bound on the rate-distortion region for the
multiterminal source-coding problem. This bound subsumes the best outer bound
in the literature and improves upon it strictly in some cases. The improved
bound enables us to obtain a new, conclusive result for the binary erasure
version of the ""CEO problem."" The bound recovers many of the converse results
that have been established for special cases of the problem, including the
recent one for the Gaussian version of the CEO problem.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1112.3788v1,Bijective Term Encodings,"We encode/decode Prolog terms as unique natural numbers. Our encodings have
the following properties: a) are bijective b) natural numbers always decode to
syntactically valid terms c) they work in low polynomial time in the bitsize of
the representations d) the bitsize of our encodings is within constant factor
of the syntactic representation of the input.
  We describe encodings of term algebras with finite signature as well as
algorithms that separate the ""structure"" of a term, a natural number encoding
of a list of balanced parenthesis, from its ""content"", a list of atomic terms
and Prolog variables. The paper is organized as a literate Prolog program
available from \url{http://logic.cse.unt.edu/tarau/research/2011/bijenc.pl}.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1811.03831v3,"Adaptive Regularization Algorithms with Inexact Evaluations for
  Nonconvex Optimization","A regularization algorithm using inexact function values and inexact
derivatives is proposed and its evaluation complexity analyzed. This algorithm
is applicable to unconstrained problems and to problems with inexpensive
constraints (that is constraints whose evaluation and enforcement has
negligible cost) under the assumption that the derivative of highest degree is
$\beta$-H\""{o}lder continuous. It features a very flexible adaptive mechanism
for determining the inexactness which is allowed, at each iteration, when
computing objective function values and derivatives. The complexity analysis
covers arbitrary optimality order and arbitrary degree of available approximate
derivatives. It extends results of Cartis, Gould and Toint (2018) on the
evaluation complexity to the inexact case: if a $q$th order minimizer is sought
using approximations to the first $p$ derivatives, it is proved that a suitable
approximate minimizer within $\epsilon$ is computed by the proposed algorithm
in at most $O(\epsilon^{-\frac{p+\beta}{p-q+\beta}})$ iterations and at most
$O(|\log(\epsilon)|\epsilon^{-\frac{p+\beta}{p-q+\beta}})$ approximate
evaluations. An algorithmic variant, although more rigid in practice, can be
proved to find such an approximate minimizer in
$O(|\log(\epsilon)|+\epsilon^{-\frac{p+\beta}{p-q+\beta}})$ evaluations.While
the proposed framework remains so far conceptual for high degrees and orders,
it is shown to yield simple and computationally realistic inexact methods when
specialized to the unconstrained and bound-constrained first- and second-order
cases. The deterministic complexity results are finally extended to the
stochastic context, yielding adaptive sample-size rules for subsampling methods
typical of machine learning.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2102.11623v1,"PIERES: A Playground for Network Interrupt Experiments on Real-Time
  Embedded Systems in the IoT","IoT devices have become an integral part of our lives and the industry. Many
of these devices run real-time systems or are used as part of them. As these
devices receive network packets over IP networks, the network interface informs
the CPU about their arrival using interrupts that might preempt critical
processes. Therefore, the question arises whether network interrupts pose a
threat to the real-timeness of these devices. However, there are few tools to
investigate this issue. We present a playground which enables researchers to
conduct experiments in the context of network interrupt simulation. The
playground comprises different network interface controller implementations,
load generators and timing utilities. It forms a flexible and easy to use
foundation for future network interrupt research. We conduct two verification
experiments and two real world examples. The latter give insight into the
impact of the interrupt handling strategy parameters and the influence of
different load types on the execution time with respect to these parameters.",0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.12188v3,"A High-Fidelity Flow Solver for Unstructured Meshes on
  Field-Programmable Gate Arrays","The impending termination of Moore's law motivates the search for new forms
of computing to continue the performance scaling we have grown accustomed to.
Among the many emerging Post-Moore computing candidates, perhaps none is as
salient as the Field-Programmable Gate Array (FPGA), which offers the means of
specializing and customizing the hardware to the computation at hand.
  In this work, we design a custom FPGA-based accelerator for a computational
fluid dynamics (CFD) code. Unlike prior work -- which often focuses on
accelerating small kernels -- we target the entire Poisson solver on
unstructured meshes based on the high-fidelity spectral element method (SEM)
used in modern state-of-the-art CFD systems. We model our accelerator using an
analytical performance model based on the I/O cost of the algorithm. We
empirically evaluate our accelerator on a state-of-the-art Intel Stratix 10
FPGA in terms of performance and power consumption and contrast it against
existing solutions on general-purpose processors (CPUs). Finally, we propose a
data movement-reducing technique where we compute geometric factors on the fly,
which yields significant (700+ Gflop/s) single-precision performance and an
upwards of 2x reduction in runtime for the local evaluation of the Laplace
operator.
  We end the paper by discussing the challenges and opportunities of using
reconfigurable architecture in the future, particularly in the light of
emerging (not yet available) technologies.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0801.1063v1,Modeling Online Reviews with Multi-grain Topic Models,"In this paper we present a novel framework for extracting the ratable aspects
of objects from online user reviews. Extracting such aspects is an important
challenge in automatically mining product opinions from the web and in
generating opinion-based summaries of user reviews. Our models are based on
extensions to standard topic modeling methods such as LDA and PLSA to induce
multi-grain topics. We argue that multi-grain models are more appropriate for
our task since standard models tend to produce topics that correspond to global
properties of objects (e.g., the brand of a product type) rather than the
aspects of an object that tend to be rated by a user. The models we present not
only extract ratable aspects, but also cluster them into coherent topics, e.g.,
`waitress' and `bartender' are part of the same topic `staff' for restaurants.
This differentiates it from much of the previous work which extracts aspects
through term frequency analysis with minimal clustering. We evaluate the
multi-grain models both qualitatively and quantitatively to show that they
improve significantly upon standard topic models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1301.5074v1,How Computers Work: Computational Thinking for Everyone,"What would you teach if you had only one course to help students grasp the
essence of computation and perhaps inspire a few of them to make computing a
subject of further study? Assume they have the standard college prep
background. This would include basic algebra, but not necessarily more advanced
mathematics. They would have written a few term papers, but would not have
written computer programs. They could surf and twitter, but could not
exclusive-or and nand. What about computers would interest them or help them
place their experience in context? This paper provides one possible answer to
this question by discussing a course that has completed its second iteration.
Grounded in classical logic, elucidated in digital circuits and computer
software, it expands into areas such as CPU components and massive databases.
The course has succeeded in garnering the enthusiastic attention of students
with a broad range of interests, exercising their problem solving skills, and
introducing them to computational thinking.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/cs/0008018v1,The Bisimulation Problem for equational graphs of finite out-degree,"The ""bisimulation problem"" for equational graphs of finite out-degree is
shown to be decidable. We reduce this problem to the bisimulation problem for
deterministic rational (vectors of) boolean series on the alphabet of a dpda M.
We then exhibit a complete formal system for deducing equivalent pairs of such
vectors.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1303.6775v2,"Dynamic Provisioning in Next-Generation Data Centers with On-site Power
  Production","The critical need for clean and economical sources of energy is transforming
data centers that are primarily energy consumers to also energy producers. We
focus on minimizing the operating costs of next-generation data centers that
can jointly optimize the energy supply from on-site generators and the power
grid, and the energy demand from servers as well as power conditioning and
cooling systems. We formulate the cost minimization problem and present an
offline optimal algorithm. For ""on-grid"" data centers that use only the grid,
we devise a deterministic online algorithm that achieves the best possible
competitive ratio of $2-\alpha_{s}$, where $\alpha_{s}$ is a normalized
look-ahead window size. For ""hybrid"" data centers that have on-site power
generation in addition to the grid, we develop an online algorithm that
achieves a competitive ratio of at most \textmd{\normalsize {\small
$\frac{P_{\max} (2-\alpha_{s})}{c_{o}+c_{m}/L}
[1+2\frac{P_{\max}-c_{o}}{P_{\max}(1+\alpha_{g})}]$}}, where $\alpha_{s}$ and
$\alpha_{g}$ are normalized look-ahead window sizes, $P_{\max}$ is the maximum
grid power price, and $L$, $c_{o}$, and $c_{m}$ are parameters of an on-site
generator.
  Using extensive workload traces from Akamai with the corresponding grid power
prices, we simulate our offline and online algorithms in a realistic setting.
Our offline (resp., online) algorithm achieves a cost reduction of 25.8%
(resp., 20.7%) for a hybrid data center and 12.3% (resp., 7.3%) for an on-grid
data center. The cost reductions are quite significant and make a strong case
for a joint optimization of energy supply and energy demand in a data center. A
hybrid data center provides about 13% additional cost reduction over an on-grid
data center representing the additional cost benefits that on-site power
generation provides over using the grid alone.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.06856v1,CAD Adjacency Computation Using Validated Numerics,"We present an algorithm for computation of cell adjacencies for well-based
cylindrical algebraic decomposition. Cell adjacency information can be used to
compute topological operations e.g. closure, boundary, connected components,
and topological properties e.g. homology groups. Other applications include
visualization and path planning. Our algorithm determines cell adjacency
information using validated numerical methods similar to those used in CAD
construction, thus computing CAD with adjacency information in time comparable
to that of computing CAD without adjacency information. We report on
implementation of the algorithm and present empirical data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1306.1345v2,A Note on Graphs of Linear Rank-Width 1,"We prove that a connected graph has linear rank-width 1 if and only if it is
a distance-hereditary graph and its split decomposition tree is a path. An
immediate consequence is that one can decide in linear time whether a graph has
linear rank-width at most 1, and give an obstruction if not. Other immediate
consequences are several characterisations of graphs of linear rank-width 1. In
particular a connected graph has linear rank-width 1 if and only if it is
locally equivalent to a caterpillar if and only if it is a vertex-minor of a
path [O-joung Kwon and Sang-il Oum, Graphs of small rank-width are pivot-minors
of graphs of small tree-width, arxiv:1203.3606] if and only if it does not
contain the co-K_2 graph, the Net graph and the 5-cycle graph as vertex-minors
[Isolde Adler, Arthur M. Farley and Andrzej Proskurowski, Obstructions for
linear rank-width at most 1, arxiv:1106.2533].",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1207.0872v1,"Differential Privacy for Relational Algebra: Improving the Sensitivity
  Bounds via Constraint Systems","Differential privacy is a modern approach in privacy-preserving data analysis
to control the amount of information that can be inferred about an individual
by querying a database. The most common techniques are based on the
introduction of probabilistic noise, often defined as a Laplacian parametric on
the sensitivity of the query. In order to maximize the utility of the query, it
is crucial to estimate the sensitivity as precisely as possible.
  In this paper we consider relational algebra, the classical language for
queries in relational databases, and we propose a method for computing a bound
on the sensitivity of queries in an intuitive and compositional way. We use
constraint-based techniques to accumulate the information on the possible
values for attributes provided by the various components of the query, thus
making it possible to compute tight bounds on the sensitivity.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2010.04827v1,"Towards Self-Regulating AI: Challenges and Opportunities of AI Model
  Governance in Financial Services","AI systems have found a wide range of application areas in financial
services. Their involvement in broader and increasingly critical decisions has
escalated the need for compliance and effective model governance. Current
governance practices have evolved from more traditional financial applications
and modeling frameworks. They often struggle with the fundamental differences
in AI characteristics such as uncertainty in the assumptions, and the lack of
explicit programming. AI model governance frequently involves complex review
flows and relies heavily on manual steps. As a result, it faces serious
challenges in effectiveness, cost, complexity, and speed. Furthermore, the
unprecedented rate of growth in the AI model complexity raises questions on the
sustainability of the current practices. This paper focuses on the challenges
of AI model governance in the financial services industry. As a part of the
outlook, we present a system-level framework towards increased self-regulation
for robustness and compliance. This approach aims to enable potential solution
opportunities through increased automation and the integration of monitoring,
management, and mitigation capabilities. The proposed framework also provides
model governance and risk management improved capabilities to manage model risk
during deployment.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0404057v1,Convergence of Discrete MDL for Sequential Prediction,"We study the properties of the Minimum Description Length principle for
sequence prediction, considering a two-part MDL estimator which is chosen from
a countable class of models. This applies in particular to the important case
of universal sequence prediction, where the model class corresponds to all
algorithms for some fixed universal Turing machine (this correspondence is by
enumerable semimeasures, hence the resulting models are stochastic). We prove
convergence theorems similar to Solomonoff's theorem of universal induction,
which also holds for general Bayes mixtures. The bound characterizing the
convergence speed for MDL predictions is exponentially larger as compared to
Bayes mixtures. We observe that there are at least three different ways of
using MDL for prediction. One of these has worse prediction properties, for
which predictions only converge if the MDL estimator stabilizes. We establish
sufficient conditions for this to occur. Finally, some immediate consequences
for complexity relations and randomness criteria are proven.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0303019v1,"An Effective Decision Procedure for Linear Arithmetic with Integer and
  Real Variables","This paper considers finite-automata based algorithms for handling linear
arithmetic with both real and integer variables. Previous work has shown that
this theory can be dealt with by using finite automata on infinite words, but
this involves some difficult and delicate to implement algorithms. The
contribution of this paper is to show, using topological arguments, that only a
restricted class of automata on infinite words are necessary for handling real
and integer linear arithmetic. This allows the use of substantially simpler
algorithms, which have been successfully implemented.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1201.2553v2,"A New Order-theoretic Characterisation of the Polytime Computable
  Functions","We propose a new order, the small polynomial path order (sPOP* for short).
The order sPOP* provides a characterisation of the class of polynomial time
computable function via term rewrite systems. Any polynomial time computable
function gives rise to a rewrite system that is compatible with sPOP*. On the
other hand any function defined by a rewrite system compatible with sPOP* is
polynomial time computable. Technically sPOP* is a tamed recursive path order
with product status. Its distinctive feature is the precise control provided.
For any rewrite system that is compatible with sPOP* that makes use of
recursion up to depth d, the (innermost) runtime complexity is bounded from
above by a polynomial of degree d.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9812004v1,Name Strategy: Its Existence and Implications,"It is argued that colour name strategy, object name strategy, and chunking
strategy in memory are all aspects of the same general phenomena, called
stereotyping. It is pointed out that the Berlin-Kay universal partial ordering
of colours and the frequency of traffic accidents classified by colour are
surprisingly similar. Some consequences of the existence of a name strategy for
the philosophy of language and mathematics are discussed. It is argued that
real valued quantities occur {\it ab initio}. The implication of real valued
truth quantities is that the {\bf Continuum Hypothesis} of pure mathematics is
side-stepped. The existence of name strategy shows that thought/sememes and
talk/phonemes can be separate, and this vindicates the assumption of thought
occurring before talk used in psycholinguistic speech production models.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0201011v1,A Backward Analysis for Constraint Logic Programs,"One recurring problem in program development is that of understanding how to
re-use code developed by a third party. In the context of (constraint) logic
programming, part of this problem reduces to figuring out how to query a
program. If the logic program does not come with any documentation, then the
programmer is forced to either experiment with queries in an ad hoc fashion or
trace the control-flow of the program (backward) to infer the modes in which a
predicate must be called so as to avoid an instantiation error. This paper
presents an abstract interpretation scheme that automates the latter technique.
The analysis presented in this paper can infer moding properties which if
satisfied by the initial query, come with the guarantee that the program and
query can never generate any moding or instantiation errors. Other applications
of the analysis are discussed. The paper explains how abstract domains with
certain computational properties (they condense) can be used to trace
control-flow backward (right-to-left) to infer useful properties of initial
queries. A correctness argument is presented and an implementation is reported.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0007024v1,"Many uses, many annotations for large speech corpora: Switchboard and
  TDT as case studies","This paper discusses the challenges that arise when large speech corpora
receive an ever-broadening range of diverse and distinct annotations. Two case
studies of this process are presented: the Switchboard Corpus of telephone
conversations and the TDT2 corpus of broadcast news. Switchboard has undergone
two independent transcriptions and various types of additional annotation, all
carried out as separate projects that were dispersed both geographically and
chronologically. The TDT2 corpus has also received a variety of annotations,
but all directly created or managed by a core group. In both cases, issues
arise involving the propagation of repairs, consistency of references, and the
ability to integrate annotations having different formats and levels of detail.
We describe a general framework whereby these issues can be addressed
successfully.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0405048v1,"Interactive visualization of higher dimensional data in a multiview
  environment","We develop multiple view visualization of higher dimensional data. Our work
was chiefly motivated by the need to extract insight from four dimensional
Quantum Chromodynamic (QCD) data. We develop visualization where multiple
views, generally views of 3D projections or slices of a higher dimensional
data, are tightly coupled not only by their specific order but also by a view
synchronizing interaction style, and an internally defined interaction
language. The tight coupling of the different views allows a fast and
well-coordinated exploration of the data. In particular, the visualization
allowed us to easily make consistency checks of the 4D QCD data and to infer
the correctness of particle properties calculations. The software developed was
also successfully applied in material studies, in particular studies of
meteorite properties. Our implementation uses the VTK API. To handle a large
number of views (slices/projections) and to still maintain good resolution, we
use IBM T221 display (3840 X 2400 pixels).",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1504.02651v2,Reachability analysis of first-order definable pushdown systems,"We study pushdown systems where control states, stack alphabet, and
transition relation, instead of being finite, are first-order definable in a
fixed countably-infinite structure. We show that the reachability analysis can
be addressed with the well-known saturation technique for the wide class of
oligomorphic structures. Moreover, for the more restrictive homogeneous
structures, we are able to give concrete complexity upper bounds. We show ample
applicability of our technique by presenting several concrete examples of
homogeneous structures, subsuming, with optimal complexity, known results from
the literature. We show that infinitely many such examples of homogeneous
structures can be obtained with the classical wreath product construction.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.07669v1,Evolution of the ROOT Tree I/O,"The ROOT TTree data format encodes hundreds of petabytes of High Energy and
Nuclear Physics events. Its columnar layout drives rapid analyses, as only
those parts (""branches"") that are really used in a given analysis need to be
read from storage. Its unique feature is the seamless C++ integration, which
allows users to directly store their event classes without explicitly defining
data schemas. In this contribution, we present the status and plans of the
future ROOT 7 event I/O. Along with the ROOT 7 interface modernization, we aim
for robust, where possible compile-time safe C++ interfaces to read and write
event data. On the performance side, we show first benchmarks using ROOT's new
experimental I/O subsystem that combines the best of TTrees with recent
advances in columnar data formats. A core ingredient is a strong separation of
the high-level logical data layout (C++ classes) from the low-level physical
data layout (storage backed nested vectors of simple types). We show how the
new, optimized physical data layout speeds up serialization and deserialization
and facilitates parallel, vectorized and bulk operations. This lets ROOT I/O
run optimally on the upcoming ultra-fast NVRAM storage devices, as well as
file-less storage systems such as object stores.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0306039v1,Bayesian Information Extraction Network,"Dynamic Bayesian networks (DBNs) offer an elegant way to integrate various
aspects of language in one model. Many existing algorithms developed for
learning and inference in DBNs are applicable to probabilistic language
modeling. To demonstrate the potential of DBNs for natural language processing,
we employ a DBN in an information extraction task. We show how to assemble
wealth of emerging linguistic instruments for shallow parsing, syntactic and
semantic tagging, morphological decomposition, named entity recognition etc. in
order to incrementally build a robust information extraction system. Our method
outperforms previously published results on an established benchmark domain.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.09957v3,"Reinforcement Learning-based Application Autoscaling in the Cloud: A
  Survey","Reinforcement Learning (RL) has demonstrated a great potential for
automatically solving decision-making problems in complex uncertain
environments. RL proposes a computational approach that allows learning through
interaction in an environment with stochastic behavior, where agents take
actions to maximize some cumulative short-term and long-term rewards. Some of
the most impressive results have been shown in Game Theory where agents
exhibited superhuman performance in games like Go or Starcraft 2, which led to
its gradual adoption in many other domains, including Cloud Computing.
Therefore, RL appears as a promising approach for Autoscaling in Cloud since it
is possible to learn transparent (with no human intervention), dynamic (no
static plans), and adaptable (constantly updated) resource management policies
to execute applications. These are three important distinctive aspects to
consider in comparison with other widely used autoscaling policies that are
defined in an ad-hoc way or statically computed as in solutions based on
meta-heuristics. Autoscaling exploits the Cloud elasticity to optimize the
execution of applications according to given optimization criteria, which
demands to decide when and how to scale-up/down computational resources, and
how to assign them to the upcoming processing workload. Such actions have to be
taken considering that the Cloud is a dynamic and uncertain environment.
Motivated by this, many works apply RL to the autoscaling problem in the Cloud.
In this work, we survey exhaustively those proposals from major venues, and
uniformly compare them based on a set of proposed taxonomies. We also discuss
open problems and prospective research in the area.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1811.06502v2,Verified Runtime Validation for Partially Observable Hybrid Systems,"Formal verification provides strong safety guarantees but only for models of
cyber-physical systems. Hybrid system models describe the required interplay of
computation and physical dynamics, which is crucial to guarantee what
computations lead to safe physical behavior (e.g., cars should not collide).
Control computations that affect physical dynamics must act in advance to avoid
possibly unsafe future circumstances. Formal verification then ensures that the
controllers correctly identify and provably avoid unsafe future situations
under a certain model of physics. But any model of physics necessarily deviates
from reality and, moreover, any observation with real sensors and manipulation
with real actuators is subject to uncertainty. This makes runtime validation a
crucial step to monitor whether the model assumptions hold for the real system
implementation.
  The key question is what property needs to be runtime-monitored and what a
satisfied runtime monitor entails about the safety of the system: the
observations of a runtime monitor only relate back to the safety of the system
if they are themselves accompanied by a proof of correctness! For an unbroken
chain of correctness guarantees, we, thus, synthesize runtime monitors in a
provably correct way from provably safe hybrid system models. This paper
addresses the inevitable challenge of making the synthesized monitoring
conditions robust to partial observability of sensor uncertainty and partial
controllability due to actuator disturbance. We show that the monitoring
conditions result in provable safety guarantees with fallback controllers that
react to monitor violation at runtime.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1907.04521v2,The complexity of the first-order theory of pure equality,"We will find a lower bound on the recognition complexity of the theories that
are nontrivial relative to some equivalence relation (this relation may be
equality), namely, each of these theories is consistent with the formula, whose
sense is that there exist two non-equivalent elements. However, at first, we
will obtain a lower bound on the computational complexity for the first-order
theory of Boolean algebra that has only two elements. For this purpose, we will
code the long-continued deterministic Turing machine computations by the
relatively short-length quantified Boolean formulae; the modified Stockmeyer
and Meyer method will appreciably be used for this simulation. Then, we will
transform the modeling formulae of the theory of this Boolean algebra to the
simulation ones of the first-order theory of the only equivalence relation in
polynomial time. Since the computational complexity of these theories is not
polynomial, we obtain that the class $\mathbf{P}$ is a proper subclass of
$\mathbf{PSPACE}$ (Polynomial Time is a proper subset of Polynomial Space).
  Keywords: Computational complexity, the theory of equality, the coding of
computations, simulation by means formulae, polynomial time, polynomial space,
lower complexity bound",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1812.03535v1,On balanced clustering with tree-like structures over clusters,"The article addresses balanced clustering problems with an additional
requirement as a tree-like structure over the obtained balanced clusters. This
kind of clustering problems can be useful in some applications (e.g., network
design, management and routing). Various types of the initial elements are
considered. Four basic greedy-like solving strategies (design framework) are
considered: balancing-spanning strategy, spanning-balancing strategy, direct
strategy, and design of layered structures with balancing. An extended
description of the spanning-balancing strategy is presented including four
solving schemes and an illustrative numerical example.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.04554v1,"Local approximate Gaussian process regression for data-driven
  constitutive laws: Development and comparison with neural networks","Hierarchical computational methods for multiscale mechanics such as the
FE$^2$ and FE-FFT methods are generally accompanied by high computational
costs. Data-driven approaches are able to speed the process up significantly by
enabling to incorporate the effective micromechanical response in macroscale
simulations without the need of performing additional computations at each
Gauss point explicitly. Traditionally artificial neural networks (ANNs) have
been the surrogate modeling technique of choice in the solid mechanics
community. However they suffer from severe drawbacks due to their parametric
nature and suboptimal training and inference properties for the investigated
datasets in a three dimensional setting. These problems can be avoided using
local approximate Gaussian process regression (laGPR). This method can allow
the prediction of stress outputs at particular strain space locations by
training local regression models based on Gaussian processes, using only a
subset of the data for each local model, offering better and more reliable
accuracy than ANNs. A modified Newton-Raphson approach is proposed to
accommodate for the local nature of the laGPR approximation when solving the
global structural problem in a FE setting. Hence, the presented work offers a
complete and general framework enabling multiscale calculations combining a
data-driven constitutive prediction using laGPR, and macroscopic calculations
using an FE scheme that we test for finite-strain three-dimensional
hyperelastic problems.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.00062v2,Deep Feature Space: A Geometrical Perspective,"One of the most prominent attributes of Neural Networks (NNs) constitutes
their capability of learning to extract robust and descriptive features from
high dimensional data, like images. Hence, such an ability renders their
exploitation as feature extractors particularly frequent in an abundant of
modern reasoning systems. Their application scope mainly includes complex
cascade tasks, like multi-modal recognition and deep Reinforcement Learning
(RL). However, NNs induce implicit biases that are difficult to avoid or to
deal with and are not met in traditional image descriptors. Moreover, the lack
of knowledge for describing the intra-layer properties -- and thus their
general behavior -- restricts the further applicability of the extracted
features. With the paper at hand, a novel way of visualizing and understanding
the vector space before the NNs' output layer is presented, aiming to enlighten
the deep feature vectors' properties under classification tasks. Main attention
is paid to the nature of overfitting in the feature space and its adverse
effect on further exploitation. We present the findings that can be derived
from our model's formulation, and we evaluate them on realistic recognition
scenarios, proving its prominence by improving the obtained results.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.00787v1,Distributed Approximation Algorithms for the Multiple Knapsack Problem,"We consider the distributed version of the Multiple Knapsack Problem (MKP),
where $m$ items are to be distributed amongst $n$ processors, each with a
knapsack. We propose different distributed approximation algorithms with a
tradeoff between time and message complexities. The algorithms are based on the
greedy approach of assigning the best item to the knapsack with the largest
capacity. These algorithms obtain a solution with a bound of $\frac{1}{n+1}$
times the optimum solution, with either $\mathcal{O}\left(m\log n\right)$ time
and $\mathcal{O}\left(m n\right)$ messages, or $\mathcal{O}\left(m\right)$ time
and $\mathcal{O}\left(mn^{2}\right)$ messages.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2007.05129v3,Senior Living Communities: Made Safer by AI,"There is a historically unprecedented shift in demographics towards seniors,
which will result in significant housing development over the coming decade.
This is an enormous opportunity for real-estate operators to innovate and
address the demand in this growing market. However, investments in this area
are fraught with risk. Seniors often have more health issues, and Covid-19 has
exposed just how vulnerable they are -- especially those living in close
proximity. Conventionally, most services for seniors are ""high-touch"",
requiring close physical contact with trained caregivers. Not only are trained
caregivers short in supply, but the pandemic has made it evident that
conventional high-touch approaches to senior care are high-cost and greater
risk. There are not enough caregivers to meet the needs of this emerging
demographic, and even fewer who want to undertake the additional training and
risk of working in a senior facility, especially given the current pandemic. In
this article, we rethink the design of senior living facilities to mitigate the
risks and costs using automation. With AI-enabled pervasive automation, we
claim there is an opportunity, if not an urgency, to go from high-touch to
almost ""no touch"" while dramatically reducing risk and cost. Although our
vision goes beyond the current reality, we cite measurements from Caspar
AI-enabled senior properties that show the potential benefit of this approach.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/1408.2948v1,"Efficient Data Compression with Error Bound Guarantee in Wireless Sensor
  Networks","We present a data compression and dimensionality reduction scheme for data
fusion and aggregation applications to prevent data congestion and reduce
energy consumption at network connecting points such as cluster heads and
gateways. Our in-network approach can be easily tuned to analyze the data
temporal or spatial correlation using an unsupervised neural network scheme,
namely the autoencoders. In particular, our algorithm extracts intrinsic data
features from previously collected historical samples to transform the raw data
into a low dimensional representation. Moreover, the proposed framework
provides an error bound guarantee mechanism. We evaluate the proposed solution
using real-world data sets and compare it with traditional methods for temporal
and spatial data compression. The experimental validation reveals that our
approach outperforms several existing wireless sensor network's data
compression methods in terms of compression efficiency and signal
reconstruction.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0806.1768v1,Local Read-Write Operations in Sensor Networks,"Designing protocols and formulating convenient programming units of
abstraction for sensor networks is challenging due to communication errors and
platform constraints. This paper investigates properties and implementation
reliability for a \emph{local read-write} abstraction. Local read-write is
inspired by the class of read-modify-write operations defined for shared-memory
multiprocessor architectures. The class of read-modify-write operations is
important in solving consensus and related synchronization problems for
concurrency control. Local read-write is shown to be an atomic abstraction for
synchronizing neighborhood states in sensor networks. The paper compares local
read-write to similar lightweight operations in wireless sensor networks, such
as read-all, write-all, and a transaction-based abstraction: for some
optimistic scenarios, local read-write is a more efficient neighborhood
operation. A partial implementation is described, which shows that three
outcomes characterize operation response: success, failure, and cancel. A
failure response indicates possible inconsistency for the operation result,
which is the result of a timeout event at the operation's initiator. The paper
presents experimental results on operation performance with different timeout
values and situations of no contention, with some tests also on various
neighborhood sizes.",0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.12469v2,"LaForge: Always-Correct and Fast Incremental Builds from Simple
  Specifications","Developers rely on build systems to generate software from code. At a
minimum, a build system should produce build targets from a clean copy of the
code. However, developers rarely work from clean checkouts. Instead, they
rebuild software repeatedly, sometimes hundreds of times a day. To keep
rebuilds fast, build systems run incrementally, executing commands only when
built state cannot be reused. Existing tools like make present users with a
tradeoff. Simple build specifications are easy to write, but limit incremental
work. More complex build specifications produce faster incremental builds, but
writing them is labor-intensive and error-prone. This work shows that no such
tradeoff is necessary; build specifications can be both simple and fast.
  We introduce LaForge, a novel build tool that eliminates the need to specify
dependencies or incremental build steps. LaForge builds are easy to specify;
developers write a simple script that runs a full build. Even a single command
like gcc src/*.c will suffice. LaForge traces the execution of the build and
generates a transcript in the TraceIR language. On later builds, LaForge
evaluates the TraceIR transcript to detect changes and perform an efficient
incremental rebuild that automatically captures all build dependencies.
  We evaluate LaForge by building 14 software packages, including LLVM and
memcached. Our results show that LaForge automatically generates efficient
builds from simple build specifications. Full builds with LaForge have a median
overhead of 16.1% compared to a project's default full build. LaForge's
incremental builds consistently run fewer commands, and most take less than
3.08s longer than manually-specified incremental builds. Finally, LaForge is
always correct.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0508023v3,"Software Libraries and Their Reuse: Entropy, Kolmogorov Complexity, and
  Zipf's Law","We analyze software reuse from the perspective of information theory and
Kolmogorov complexity, assessing our ability to ``compress'' programs by
expressing them in terms of software components reused from libraries. A common
theme in the software reuse literature is that if we can only get the right
environment in place-- the right tools, the right generalizations, economic
incentives, a ``culture of reuse'' -- then reuse of software will soar, with
consequent improvements in productivity and software quality. The analysis
developed in this paper paints a different picture: the extent to which
software reuse can occur is an intrinsic property of a problem domain, and
better tools and culture can have only marginal impact on reuse rates if the
domain is inherently resistant to reuse. We define an entropy parameter $H \in
[0,1]$ of problem domains that measures program diversity, and deduce from this
upper bounds on code reuse and the scale of components with which we may work.
For ``low entropy'' domains with $H$ near 0, programs are highly similar to one
another and the domain is amenable to the Component-Based Software Engineering
(CBSE) dream of programming by composing large-scale components. For problem
domains with $H$ near 1, programs require substantial quantities of new code,
with only a modest proportion of an application comprised of reused,
small-scale components. Preliminary empirical results from Unix platforms
support some of the predictions of our model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.00564v1,An Introduction to Johnson-Lindenstrauss Transforms,"Johnson--Lindenstrauss Transforms are powerful tools for reducing the
dimensionality of data while preserving key characteristics of that data, and
they have found use in many fields from machine learning to differential
privacy and more. This note explains what they are; it gives an overview of
their use and their development since they were introduced in the 1980s; and it
provides many references should the reader wish to explore these topics more
deeply.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1110.0725v1,A Survey of Distributed Data Aggregation Algorithms,"Distributed data aggregation is an important task, allowing the decentralized
determination of meaningful global properties, that can then be used to direct
the execution of other applications. The resulting values result from the
distributed computation of functions like COUNT, SUM and AVERAGE. Some
application examples can found to determine the network size, total storage
capacity, average load, majorities and many others.
  In the last decade, many different approaches have been proposed, with
different trade-offs in terms of accuracy, reliability, message and time
complexity. Due to the considerable amount and variety of aggregation
algorithms, it can be difficult and time consuming to determine which
techniques will be more appropriate to use in specific settings, justifying the
existence of a survey to aid in this task.
  This work reviews the state of the art on distributed data aggregation
algorithms, providing three main contributions. First, it formally defines the
concept of aggregation, characterizing the different types of aggregation
functions. Second, it succinctly describes the main aggregation techniques,
organizing them in a taxonomy. Finally, it provides some guidelines toward the
selection and use of the most relevant techniques, summarizing their principal
characteristics.",1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0708.0361v7,"Why the relational data model can be considered as a formal basis for
  group operations in object-oriented systems","Relational data model defines a specification of a type ""relation"". However,
its simplicity does not mean that the system implementing this model must
operate with structures having the same simplicity. We consider two principles
allowing create a system which combines object-oriented paradigm (OOP) and
relational data model (RDM) in one framework. The first principle -- ""complex
data in encapsulated domains"" -- is well known from The Third Manifesto by Date
and Darwen. The second principle --""data complexity in names""-- is the basis
for a system where data are described as complex objects and uniquely
represented as a set of relations. Names of these relations and names of their
attributes are combinations of names entered in specifications of the complex
objects. Below, we consider the main properties of such a system.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1502.01968v1,"Old Wine in New Skins? Revisiting the Software Architecture for IP
  Network Stacks on Constrained IoT Devices","In this paper, we argue that existing concepts for the design and
implementation of network stacks for constrained devices do not comply with the
requirements of current and upcoming Internet of Things (IoT) use cases. The
IoT requires not only a lightweight but also a modular network stack, based on
standards. We discuss functional and non-functional requirements for the
software architecture of the network stack on constrained IoT devices. Then,
revisiting concepts from the early Internet as well as current implementations,
we propose a future-proof alternative to existing IoT network stack
architectures, and provide an initial evaluation of this proposal based on its
implementation running on top of state-of-the-art IoT operating system and
hardware.",0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1306.1958v1,Software Testing Models Against Information Security Requirements,"An overview and classification of software testing models are done.
Recommendations on the choice of models are proposed",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0803.1443v4,"Lexical growth, entropy and the benefits of networking","If each node of an idealized network has an equal capacity to efficiently
exchange benefits, then the network's capacity to use energy is scaled by the
average amount of energy required to connect any two of its nodes. The scaling
factor equals \textit{e}, and the network's entropy is $\ln(n)$. Networking
emerges in consequence of nodes minimizing the ratio of their energy use to the
benefits obtained for such use, and their connectability. Networking leads to
nested hierarchical clustering, which multiplies a network's capacity to use
its energy to benefit its nodes. Network entropy multiplies a node's capacity.
For a real network in which the nodes have the capacity to exchange benefits,
network entropy may be estimated as $C \log_L(n)$, where the base of the log is
the path length $L$, and $C$ is the clustering coefficient. Since $n$, $L$ and
$C$ can be calculated for real networks, network entropy for real networks can
be calculated and can reveal aspects of emergence and also of economic,
biological, conceptual and other networks, such as the relationship between
rates of lexical growth and divergence, and the economic benefit of adding
customers to a commercial communications network. \textit{Entropy dating} can
help estimate the age of network processes, such as the growth of hierarchical
society and of language.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1906.05532v1,"Parametric Modelling Within Immersive Environments: Building a Bridge
  Between Existing Tools and Virtual Reality Headsets","Even though architectural modelling radically evolved over the course of its
history, the current integration of Augmented Reality (AR) and Virtual
Reality(VR) components in the corresponding design tasks is mostly limited to
enhancing visualisation. Little to none of these tools attempt to tackle the
challenge of modelling within immersive environments, that calls for new input
modalities in order to move away from the traditional mouse and keyboard
combination. In fact, relying on 2D devices for 3D manipulations does not seem
to be effective as it does not offer the same degrees of freedom. We therefore
present a solution that brings VR modelling capabilities to Grasshopper, a
popular parametric design tool. Together with its associated proof-of-concept
application, our extension offers a glimpse at new perspectives in that field.
By taking advantage of them,one can edit geometries with real-time feedback on
the generated models, without ever leaving the virtual environment. The
distinctive characteristics of VR applications provide a range of benefits
without obstructing design activities. The designer can indeed experience the
architectural models at full scale from a realistic point-of-view and truly
feels immersed right next to them.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1711.09756v1,Witnet: A Decentralized Oracle Network Protocol,"Witnet is a decentralized oracle network (DON) that connects smart contracts
to the outer world. Generally speaking, it allows any piece of software to
retrieve the contents published at any web address at a certain point in time,
with complete and verifiable proof of its integrity and without blindly
trusting any third party. Witnet runs on a blockchain with a native protocol
token (called Wit), which miners-called witnesses-earn by retrieving, attesting
and delivering web contents for clients. On the other hand, clients spend Wit
to pay witnesses for their Retrieve-Attest-Deliver (RAD) work. Witnesses also
compete to mine blocks with considerable rewards, but Witnet mining power is
proportional to their previous performance in terms of honesty and
trustworthiness-this is, their reputation as witnesses. This creates a powerful
incentive for witnesses to do their work honestly, protect their reputation and
not to deceive the network. The Witnet protocol is designed to assign the RAD
tasks to witnesses in a way that mitigates most attack vectors to the greatest
extent. At the same time, it includes a novel 'sharding' feature that (1)
guarantees the efficiency and scalability of the network, (2) keeps the price
of RAD tasks within reasonable bounds and (3) gives clients the freedom to
adjust certainty and price by letting them choose how many witnesses will work
on their RAD tasks. When coupled with a Decentralized Storage Network (DSN),
Witnet also gives us the possibility to build the Digital Knowledge Ark: a
decentralized, immutable, censorship-resistant and eternal archive of
humanity's most relevant digital data. A truth vault aimed to ensure that
knowledge will remain democratic and verifiable forever and to prevent history
from being written by the victors.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0909.2814v1,"Graph-based data clustering: a quadratic-vertex problem kernel for
  s-Plex Cluster Vertex Deletion","We introduce the s-Plex Cluster Vertex Deletion problem. Like the Cluster
Vertex Deletion problem, it is NP-hard and motivated by graph-based data
clustering. While the task in Cluster Vertex Deletion is to delete vertices
from a graph so that its connected components become cliques, the task in
s-Plex Cluster Vertex Deletion is to delete vertices from a graph so that its
connected components become s-plexes. An s-plex is a graph in which every
vertex is nonadjacent to at most s-1 other vertices; a clique is an 1-plex. In
contrast to Cluster Vertex Deletion, s-Plex Cluster Vertex Deletion allows to
balance the number of vertex deletions against the sizes and the density of the
resulting clusters, which are s-plexes instead of cliques. The focus of this
work is the development of provably efficient and effective data reduction
rules for s-Plex Cluster Vertex Deletion. In terms of fixed-parameter
algorithmics, these yield a so-called problem kernel. A similar problem, s-Plex
Editing, where the task is the insertion or the deletion of edges so that the
connected components of a graph become s-plexes, has also been studied in terms
of fixed-parameter algorithmics. Using the number of allowed graph
modifications as parameter, we expect typical parameter values for s-Plex
Cluster Vertex Deletion to be significantly lower than for s-Plex Editing,
because one vertex deletion can lead to a high number of edge deletions. This
holds out the prospect for faster fixed-parameter algorithms for s-Plex Cluster
Vertex Deletion.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2006.01673v1,Learning Opinion Dynamics From Social Traces,"Opinion dynamics - the research field dealing with how people's opinions form
and evolve in a social context - traditionally uses agent-based models to
validate the implications of sociological theories. These models encode the
causal mechanism that drives the opinion formation process, and have the
advantage of being easy to interpret. However, as they do not exploit the
availability of data, their predictive power is limited. Moreover, parameter
calibration and model selection are manual and difficult tasks.
  In this work we propose an inference mechanism for fitting a generative,
agent-like model of opinion dynamics to real-world social traces. Given a set
of observables (e.g., actions and interactions between agents), our model can
recover the most-likely latent opinion trajectories that are compatible with
the assumptions about the process dynamics. This type of model retains the
benefits of agent-based ones (i.e., causal interpretation), while adding the
ability to perform model selection and hypothesis testing on real data.
  We showcase our proposal by translating a classical agent-based model of
opinion dynamics into its generative counterpart. We then design an inference
algorithm based on online expectation maximization to learn the latent
parameters of the model. Such algorithm can recover the latent opinion
trajectories from traces generated by the classical agent-based model. In
addition, it can identify the most likely set of macro parameters used to
generate a data trace, thus allowing testing of sociological hypotheses.
Finally, we apply our model to real-world data from Reddit to explore the
long-standing question about the impact of backfire effect. Our results suggest
a low prominence of the effect in Reddit's political conversation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1909.04899v2,"High order transition elements: The xNy-element concept -- Part I:
  Statics","Advanced transition elements are of utmost importance in many applications of
the finite element method (FEM) where a local mesh refinement is required.
Considering problems that exhibit singularities in the solution, an adaptive
hp-refinement procedure must be applied. Even today, this is a very demanding
task especially if only quadrilateral/hexahedral elements are deployed and
consequently the hanging nodes problem is encountered. These element types,
are, however, favored in computational mechanics due to the improved accuracy
compared to triangular/tetrahedral elements. Therefore, we propose a compatible
transition element - xNy-element - which provides the capability of coupling
different element types. The adjacent elements can exhibit different element
sizes, shape function types, and polynomial orders. Thus, it is possible to
combine independently refined h- and p-meshes. The approach is based on the
transfinite mapping concept and constitutes an extension/generalization of the
pNh-element concept. By means of several numerical examples, the convergence
behavior is investigated in detail, and the asymptotic rates of convergence are
determined numerically. Overall, it is found that the proposed approach
provides very promising results for local mesh refinement procedures.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0508086v1,High-performance BWT-based Encoders,"In 1994, Burrows and Wheeler developed a data compression algorithm which
performs significantly better than Lempel-Ziv based algorithms. Since then, a
lot of work has been done in order to improve their algorithm, which is based
on a reversible transformation of the input string, called BWT (the
Burrows-Wheeler transformation). In this paper, we propose a compression scheme
based on BWT, MTF (move-to-front coding), and a version of the algorithms
presented in [Dragos Trinca, ITCC-2004].",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2002.00720v1,Introduction of Quantification in Frame Semantics,"Feature Structures (FSs) are a widespread tool used for decompositional
frameworks of Attribute-Value associations. Even though they thrive in simple
systems, they lack a way of representing higher-order entities and relations.
This is however needed in Frame Semantics, where semantic dependencies should
be able to connect groups of individuals and their properties, especially to
model quantification. To answer this issue, this master report introduces
wrappings as a way to envelop a sub-FS and treat it as a node. Following the
work of [Kallmeyer, Osswald 2013], we extend its syntax, semantics and some
properties (translation to FOL, subsumption, unification). We can then expand
the proposed pipeline. Lexical minimal model sets are generated from formulas.
They unify by FS value equations obtained by LTAG parsing to an underspecified
sentence representation. The syntactic approach of quantifiers allows us to use
existing methods to produce any possible reading. Finally, we give a
transcription to type-logical formulas to interact with the context in the view
of dynamic semantics. Supported by ideas of Frame Types, this system provides a
workable and tractable tool for higher-order relations with FS.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/9910019v1,"Consistent Checkpointing in Distributed Databases: Towards a Formal
  Approach","Whether it is for audit or for recovery purposes, data checkpointing is an
important problem of distributed database systems. Actually, transactions
establish dependence relations on data checkpoints taken by data object
managers. So, given an arbitrary set of data checkpoints (including at least a
single data checkpoint from a data manager, and at most a data checkpoint from
each data manager), an important question is the following one: ``Can these
data checkpoints be members of a same consistent global checkpoint?''. This
paper answers this question by providing a necessary and sufficient condition
suited for database systems. Moreover, to show the usefulness of this
condition, two {\em non-intrusive} data checkpointing protocols are derived
from this condition. It is also interesting to note that this paper, by
exhibiting ``correspondences'', establishes a bridge between the data
object/transaction model and the process/message-passing model.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1912.09357v1,LinCode -- computer classification of linear codes,"We present an algorithm for the classification of linear codes over finite
fields, based on lattice point enumeration. We validate a correct
implementation of our algorithm with known classification results from the
literature, which we partially extend to larger ranges of parameters.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1310.5850v1,Remote Control of Mobile Devices in Android Platform,"Remote control systems are a very useful element to control and monitor
devices quickly and easily. This paper proposes a new architecture for remote
control of Android mobile devices, analyzing the different alternatives and
seeking the optimal solution in each case. Although the area of remote control,
in case of mobile devices, is little explored, it may provide important
advantages for testing software and hardware developments in several real
devices. It can also allow an efficient management of various devices of
different types for performing different tasks, related for example to security
or forensic tasks.
  The main idea behind the proposed architecture was the design of a system to
use it as a platform which provides the services needed to perform remote
control of mobile devices. As a result of this research, a proof of concept was
implemented. An Android application running a group of server programs on the
device, connected to the network or USB interface, depending on availability.
This servers can be controlled through a small client written in Java and
runnable both on desktop and web systems.",0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1006.0392v1,"Computing the speed of convergence of ergodic averages and pseudorandom
  points in computable dynamical systems","A pseudorandom point in an ergodic dynamical system over a computable metric
space is a point which is computable but its dynamics has the same statistical
behavior as a typical point of the system.
  It was proved in [Avigad et al. 2010, Local stability of ergodic averages]
that in a system whose dynamics is computable the ergodic averages of
computable observables converge effectively. We give an alternative, simpler
proof of this result.
  This implies that if also the invariant measure is computable then the
pseudorandom points are a set which is dense (hence nonempty) on the support of
the invariant measure.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1806.03384v5,Data Synthesis based on Generative Adversarial Networks,"Privacy is an important concern for our society where sharing data with
partners or releasing data to the public is a frequent occurrence. Some of the
techniques that are being used to achieve privacy are to remove identifiers,
alter quasi-identifiers, and perturb values. Unfortunately, these approaches
suffer from two limitations. First, it has been shown that private information
can still be leaked if attackers possess some background knowledge or other
information sources. Second, they do not take into account the adverse impact
these methods will have on the utility of the released data. In this paper, we
propose a method that meets both requirements. Our method, called table-GAN,
uses generative adversarial networks (GANs) to synthesize fake tables that are
statistically similar to the original table yet do not incur information
leakage. We show that the machine learning models trained using our synthetic
tables exhibit performance that is similar to that of models trained using the
original table for unknown testing cases. We call this property model
compatibility. We believe that anonymization/perturbation/synthesis methods
without model compatibility are of little value. We used four real-world
datasets from four different domains for our experiments and conducted in-depth
comparisons with state-of-the-art anonymization, perturbation, and generation
techniques. Throughout our experiments, only our method consistently shows a
balance between privacy level and model compatibility.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/2103.07713v1,"The Effects of Diffusion of Information on Epidemic Spread -- A
  Multilayer Approach","In this work, the aim is to study the spread of a contagious disease and
information on a multilayer social system. The main idea is to find a criterion
under which the adoption of the spreading information blocks or suppresses the
epidemic spread. A two-layer network is the base of the model. The first layer
describes the direct contact interactions, while the second layer is the
information propagation layer. Both layers consist of the same nodes. The
society consists of five different categories of individuals: susceptibles,
infective, recovered, vaccinated and precautioned. Initially, only one infected
individual starts transmitting the infection. Direct contact interactions
spread the infection to the susceptibles. The information spreads through the
second layer. The SIR model is employed for the infection spread, while the
Bass equation models the adoption of information. The control parameters of the
competition between the spread of information and spread of disease are the
topology and the density of connectivity. The topology of the information layer
is a scale-free network with increasing density of edges. In the contact layer,
regular and scale-free networks with the same average degree per node are used
interchangeably. The observation is that increasing complexity of the contact
network reduces the role of individual awareness. If the contact layer consists
of networks with limited range connections, or the edges sparser than the
information network, spread of information plays a significant role in
controlling the epidemics.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.12912v1,Word Embeddings for Chemical Patent Natural Language Processing,"We evaluate chemical patent word embeddings against known biomedical
embeddings and show that they outperform the latter extrinsically and
intrinsically. We also show that using contextualized embeddings can induce
predictive models of reasonable performance for this domain over a relatively
small gold standard.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.08997v2,"Psoriasis Severity Assessment with a Similarity-Clustering Machine
  Learning Approach Reduces Intra- and Inter-observation variation","Psoriasis is a complex disease with many variations in genotype and
phenotype. General advancements in medicine has further complicated both
assessments and treatment for both physicians and dermatologist alike. Even
with all of our technological progress we still primarily use the assessment
tool Psoriasis Area and Severity Index (PASI) for severity assessments which
was developed in the 1970s. In this study we evaluate a method involving
digital images, a comparison web application and similarity clustering,
developed to improve the assessment tool in terms of intra- and inter-observer
variation. Images of patients was collected from a mobile device. Images were
captured of the same lesion area taken approximately 1 week apart. Five
dermatologists evaluated the severity of psoriasis by modified-PASI, absolute
scoring and a relative pairwise PASI scoring using similarity-clustering and
conducted using a web-program displaying two images at a time. mPASI scoring of
single photos by the same or different dermatologist showed mPASI ratings of
50% to 80%, respectively. Repeated mPASI comparison using similarity clustering
showed consistent mPASI ratings > 95%. Pearson correlation between absolute
scoring and pairwise scoring progression was 0.72.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0102002v1,On the Automated Classification of Web Sites,"In this paper we discuss several issues related to automated text
classification of web sites. We analyze the nature of web content and metadata
in relation to requirements for text features. We find that HTML metatags are a
good source of text features, but are not in wide use despite their role in
search engine rankings. We present an approach for targeted spidering including
metadata extraction and opportunistic crawling of specific semantic hyperlinks.
We describe a system for automatically classifying web sites into industry
categories and present performance results based on different combinations of
text features and training data. This system can serve as the basis for a
generalized framework for automated metadata creation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0712.3360v1,Compressed Text Indexes:From Theory to Practice!,"A compressed full-text self-index represents a text in a compressed form and
still answers queries efficiently. This technology represents a breakthrough
over the text indexing techniques of the previous decade, whose indexes
required several times the size of the text. Although it is relatively new,
this technology has matured up to a point where theoretical research is giving
way to practical developments. Nonetheless this requires significant
programming skills, a deep engineering effort, and a strong algorithmic
background to dig into the research results. To date only isolated
implementations and focused comparisons of compressed indexes have been
reported, and they missed a common API, which prevented their re-use or
deployment within other applications.
  The goal of this paper is to fill this gap. First, we present the existing
implementations of compressed indexes from a practitioner's point of view.
Second, we introduce the Pizza&Chili site, which offers tuned implementations
and a standardized API for the most successful compressed full-text
self-indexes, together with effective testbeds and scripts for their automatic
validation and test. Third, we show the results of our extensive experiments on
these codes with the aim of demonstrating the practical relevance of this novel
and exciting technology.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2103.16930v1,"Anomaly-Based Intrusion Detection by Machine Learning: A Case Study on
  Probing Attacks to an Institutional Network","Cyber attacks constitute a significant threat to organizations with
implications ranging from economic, reputational, and legal consequences. As
cybercriminals' techniques get sophisticated, information security
professionals face a more significant challenge to protecting information
systems. In today's interconnected realm of computer systems, each attack
vector has a network dimension. The present study investigates network
intrusion attempts with anomaly-based machine learning models to provide better
protection than the conventional misuse-based models. Two models, namely an
ensemble learning model and a convolutional neural network model, were built
and implemented on a data set gathered from a real-life, institutional
production environment. To demonstrate the models' reliability and validity,
they were applied to the UNSW-NB15 benchmarking data set. The type of attack
was limited to probing attacks to keep the scope of the study manageable. The
findings revealed high accuracy rates, the CNN model being slightly more
accurate.",0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0112024v1,Media Objects in Time - A Multimedia Streaming System,"The widespread availability of networked multimedia potentials embedded in an
infrastructure of qualitative superior kind gives rise to new approaches in the
areas of teleteaching and internet presentation: The distribution of
professionally styled multimedia streams has fallen in the realm of
possibility. This paper presents a prototype - both model and runtime
environment - of a time directed media system treating any kind of
presentational contribution as reusable media object components. The plug-in
free runtime system is based on a database and allows for a flexible support of
static media types as well as for easy extensions by streaming media servers.
The prototypic implementation includes a preliminary Web Authoring platform.",0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1804.03887v1,"A web service based on RESTful API and JSON Schema/JSON Meta Schema to
  construct knowledge graphs","Data visualisation assists domain experts in understanding their data and
helps them make critical decisions. Enhancing their cognitive insight
essentially relies on the capability of combining domain-specific semantic
information with concepts extracted out of the data and visualizing the
resulting networks. Data scientists have the challenge of providing tools able
to handle the overall network lifecycle. In this paper, we present how the
combination of two powerful technologies namely the REST architecture style and
JSON Schema/JSON Meta Schema enable data scientists to use a RESTful web
service that permits the construction of knowledge graphs, one of the preferred
representations of large and semantically rich networks.",0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2011.13925v1,Investigation on Research Ethics and Building a Benchmark,"When dealing with leading edge cyber security research, especially when
operating from the perspective of an attacker or a red team, it becomes
necessary for one to at times consider how ethics comes into play. There are
currently no cyber security-specific ethics standards, which in particular is
one reason more adversarial cyber security research lags behind in Japan. In
this research, using machine learning and manual methods we extracted best
practices for research ethics from past top conference papers. Using this
knowledge we constructed an ethics knowledge base for cyber security research.
Such a knowledge base can be used to properly distinguish grey-area research so
that it is not wrongly forbidden. Using a decision tree-style user interface
that we created for our knowledge base, researchers may be able to efficiently
identify which aspects of their research require ethical consideration. In this
work, as a preliminary step we focused on only a portion of the areas of
research covered by cyber security conferences, but our results are applicable
to any area of research.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/cs/0112008v1,Representation of Uncertainty for Limit Processes,"Many mathematical models utilize limit processes. Continuous functions and
the calculus, differential equations and topology, all are based on limits and
continuity. However, when we perform measurements and computations, we can
achieve only approximate results. In some cases, this discrepancy between
theoretical schemes and practical actions changes drastically outcomes of a
research and decision-making resulting in uncertainty of knowledge. In the
paper, a mathematical approach to such kind of uncertainty, which emerges in
computation and measurement, is suggested on the base of the concept of a fuzzy
limit. A mathematical technique is developed for differential models with
uncertainty. To take into account the intrinsic uncertainty of a model, it is
suggested to use fuzzy derivatives instead of conventional derivatives of
functions in this model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1406.0079v1,"Bridging the gap between Legal Practitioners and Knowledge Engineers
  using semi-formal KR","The use of Structured English as a computation independent knowledge
representation format for non-technical users in business rules representation
has been proposed in OMGs Semantics and Business Vocabulary Representation
(SBVR). In the legal domain we face a similar problem. Formal representation
languages, such as OASIS LegalRuleML and legal ontologies (LKIF, legal OWL2
ontologies etc.) support the technical knowledge engineer and the automated
reasoning. But, they can be hardly used directly by the legal domain experts
who do not have a computer science background. In this paper we adapt the SBVR
Structured English approach for the legal domain and implement a
proof-of-concept, called KR4IPLaw, which enables legal domain experts to
represent their knowledge in Structured English in a computational independent
and hence, for them, more usable way. The benefit of this approach is that the
underlying pre-defined semantics of the Structured English approach makes
transformations into formal languages such as OASIS LegalRuleML and OWL2
ontologies possible. We exemplify our approach in the domain of patent law.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1201.5529v1,"A simple block representation of reversible cellular automata with
  time-symmetry","Reversible Cellular Automata (RCA) are a physics-like model of computation
consisting of an array of identical cells, evolving in discrete time steps by
iterating a global evolution G. Further, G is required to be shift-invariant
(it acts the same everywhere), causal (information cannot be transmitted faster
than some fixed number of cells per time step), and reversible (it has an
inverse which verifies the same requirements). An important, though only
recently studied special case is that of Time-symmetric Cellular Automata
(TSCA), for which G and its inverse are related via a local operation. In this
note we revisit the question of the Block representation of RCA, i.e. we
provide a very simple proof of the existence of a reversible circuit
description implementing G. This operational, bottom-up description of G turns
out to be time-symmetric, suggesting interesting connections with TSCA. Indeed
we prove, using a similar technique, that a wide class of them admit an Exact
block representation (EBR), i.e. one which does not increase the state space.",0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2001.07649v6,"Integrating data science ethics into an undergraduate major: A case
  study","We present a programmatic approach to incorporating ethics into an
undergraduate major in statistical and data sciences. We discuss
departmental-level initiatives designed to meet the National Academy of
Sciences recommendation for integrating ethics into the curriculum from
top-to-bottom as our majors progress from our introductory courses to our
senior capstone course, as well as from side-to-side through co-curricular
programming. We also provide six examples of data science ethics modules used
in five different courses at our liberal arts college, each focusing on a
different ethical consideration. The modules are designed to be portable such
that they can be flexibly incorporated into existing courses at different
levels of instruction with minimal disruption to syllabi. We connect our
efforts to a growing body of literature on the teaching of data science ethics,
present assessments of our effectiveness, and conclude with next steps and
final thoughts.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0
http://arxiv.org/abs/1009.3306v1,"Proceedings Fourth International Workshop on Testing, Analysis and
  Verification of Web Software","This volume contains the papers presented at the fourth international
workshop on Testing, Analysis and Verification of Software, which was
associated with the 25th IEEE/ACM International Conference on Automated
Software Engineering (ASE 2010). The collection of papers includes research on
formal specification, model-checking, testing, and debugging of Web software.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.11611v1,"A Simple Methodology for Model-Driven Business Innovation and Low Code
  Implementation","Low Code platforms, according to Gartner Group, represent one of the more
disruptive technologies in the development and maintenance of enterprise
applications. The key factor is represented by the central involvement of
business people and domain expert, with a substantial disintermediation with
respect to technical people. In this paper we propose a methodology conceived
to support non-technical people in addressing business process innovation and
developing enterprise software application. The proposed methodology, called
EasInnova, is solidly rooted in Model-Driven Engineering and adopts a three
staged model of an innovation undertaking. The three stages are: AsIs that
models the existing business scenario; Transformation that consists in the
elaboration of the actual innovation; ToBe that concerns the modeling of new
business scenario. The core of EasInnova is represented by a matrix where
columns are the three innovation stages and the rows are the three Model-Driven
Architecture layers: CIM, PIM, PSM. The cells indicate the steps to be followed
in achieving the sought innovation. Finally, the produced models will be
transferred onto a BonitaSoft, the Low Code platform selected in our work. The
methodology is described by means of a simple example in the domain of home
food delivery.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.08211v2,An Efficient Implementation of Manacher's Algorithm,"Manacher's algorithm has been shown to be optimal to the longest palindromic
substring problem. Many of the existing implementations of this algorithm,
however, unanimously required in-memory construction of an augmented string
that is twice as long as the original string. Although it has found widespread
use, we found that this preprocessing is neither economic nor necessary. We
present a more efficient implementation of Manacher's algorithm based on index
mapping that makes the string augmentation process obsolete.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2110.01861v2,Social Co-OS: Cyber-Human Social Co-Operating System,"We propose the novel concept of a Cyber-Human Social System (CHSS) and a
diverse and pluralistic ""mixed-life society,"" wherein cyber and human societies
commit to each other. This concept enhances the Cyber-Physical System (CPS),
which is associated with the current Society 5.0, a social vision realized
through the fusion of cyber (virtual) and physical (real) spaces following
information society (Society 4.0 and Industry 4.0). Moreover, the CHSS enhances
the Human-CPS (HCPS), the Human-in-the-Loop CPS (HiLCPS), and the Cyber-Human
System (CHS) by intervening in individual behavior pro-socially and supporting
consensus building. As a form of architecture that embodies the CHSS concept,
the Cyber-Human Social Co-Operating System (Social Co-OS) that combines cyber
and human societies is shown. In this architecture, the cyber and human systems
cooperate through the fast loop (operation and administration) and slow loop
(consensus and politics). Furthermore, the technical content and current
implementation of the basic functions of the Social Co-OS are described. These
functions consist of individual behavioral diagnostics and interventions in the
fast loop and group decision diagnostics and consensus building in the slow
loop. Subsequently, this system will contribute to mutual aid communities and
platform cooperatives.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
http://arxiv.org/abs/2103.09586v1,An Inextensible Model for Robotic Simulations of Textiles,"We introduce a new isometric strain model for the study of the dynamics of
cloth garments in a moderate stress environment, such as robotic manipulation
in the neighborhood of humans. This model treats textiles as surfaces which are
inextensible, admitting only isometric motions. Inextensibility is imposed in a
continuous setting, prior to any discretization, which gives consistency with
respect to re-meshing and prevents the problem of locking even with coarse
meshes. The simulations of robotic manipulation using the model are compared to
the actual manipulation in the real world, finding that the error between the
simulated and real position of each point in the garment is lower than 1cm in
average, even when a coarse mesh is used. Aerodynamic contributions to motion
are incorporated to the model through the virtual uncoupling of the inertial
and gravitational mass of the garment. This approach results in an accurate, as
compared to reality, description of cloth motion incorporating aerodynamic
effects by using only two parameters.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0512016v2,"A linear-time algorithm for finding the longest segment which scores
  above a given threshold","This paper describes a linear-time algorithm that finds the longest stretch
in a sequence of real numbers (``scores'') in which the sum exceeds an input
parameter. The algorithm also solves the problem of finding the longest
interval in which the average of the scores is above a fixed threshold. The
problem originates from molecular sequence analysis: for instance, the
algorithm can be employed to identify long GC-rich regions in DNA sequences.
The algorithm can also be used to trim low-quality ends of shotgun sequences in
a preprocessing step of whole-genome assembly.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.1992v1,Structured Approach to Web Development,"In today's world of Web application development, programmers are commonly
called upon to use the Hypertext Markup Language (HTML) as a programming
language, something for which it was never intended and for which it is
woefully inadequate. HTML is a data language, nothing more. It lacks high level
programming constructions like procedures, conditions, and loops. Moreover it
provides no intrinsic mechanism to insert or associate dynamic application
data. Lastly, despite the visibly apparent structure of a web page when viewed
in a browser, the responsible HTML code bears little to no discernible
corresponding structure, making it very difficult to read, augment, and
maintain.
  This paper examines the various drawbacks inherent in HTML when used in Web
development and examines the various augmenting technologies available in the
industry today and their drawbacks. It then proposes an alternative, complete
with the necessary constructs, structure, and data associating facilities based
upon server-side, Extensible Stylesheet Language Transforms (XSLT). This
alternative approach gives rise to an entirely new, higher level, markup
language that can be readily used in web development.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1511.05585v1,Model-Driven Automatic Tiling with Cache Associativity Lattices,"Traditional compiler optimization theory distinguishes three separate classes
of cache miss -- Cold, Conflict and Capacity. Tiling for cache is typically
guided by capacity miss counts. Models of cache function have not been
effectively used to guide cache tiling optimizations due to model error and
expense. Instead, heuristic or empirical approaches are used to select tilings.
We argue that conflict misses, traditionally neglected or seen as a small
constant effect, are the only fundamentally important cache miss category, that
they form a solid basis by which caches can become modellable, and that models
leaning on cache associatvity analysis can be used to generate cache performant
tilings. We develop a mathematical framework that expresses potential and
actual cache misses in associative caches using Associativity Lattices. We show
these lattices to possess two theoretical advantages over rectangular tiles --
volume maximization and miss regularity. We also show that to generate such
lattice tiles requires, unlike rectangular tiling, no explicit, expensive
lattice point counting. We also describe an implementation of our lattice
tiling approach, show that it can be used to give speedups of over 10x versus
unoptimized code, and despite currently only tiling for one level of cache, can
already be competitive with the aggressive compiler optimizations used in
general purposes compares such as GCC and Intel's ICC. We also show that the
tiling approach can lead to reasonable automatic parallelism when compared to
existing auto-threading compilers.",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0003069v1,Proving Failure of Queries for Definite Logic Programs Using XSB-Prolog,"Proving failure of queries for definite logic programs can be done by
constructing a finite model of the program in which the query is false. A
general purpose model generator for first order logic can be used for this. A
recent paper presented at PLILP98 shows how the peculiarities of definite
programs can be exploited to obtain a better solution. There a procedure is
described which combines abduction with tabulation and uses a meta-interpreter
for heuristic control of the search. The current paper shows how similar
results can be obtained by direct execution under the standard tabulation of
the XSB-Prolog system. The loss of control is compensated for by better
intelligent backtracking and more accurate failure analysis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.08886v1,"Challenges and Solutions for Utilizing Earth Observations in the ""Big
  Data"" era","The ever-growing need of data preservation and their systematic analysis
contributing to sustainable development of the society spurred in the past
decade,numerous Big Data projects and initiatives are focusing on the Earth
Observation (EO). The number of Big Data EO applications has grown extremely
worldwide almost simultaneously with other scientific and technological areas
of the human knowledge due to the revolutionary technological progress in the
space and information technology sciences. The substantial contribution to this
development are the space programs of the renowned space agencies, such as
NASA, ESA,Roskosmos, JAXA, DLR, INPE, ISRO, CNES etc. A snap-shot of the
current Big Data sets from available satellite missions covering the Bulgarian
territory is also presented. This short overview of the geoscience Big Data
collection with a focus on EO will emphasize to the multiple Vs of EO in order
to provide a snapshot on the current state-of-the-art in EO data preservation
and manipulation. Main modern approaches for compressing, clustering and
modelling EO in the geoinformation science for Big Data analysis,
interpretation and visualization for a variety of applications are outlined.
Special attention is paid to the contemporary EO data modelling and
visualization systems.",0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1107.2382v2,"Computing the crosscap number of a knot using integer programming and
  normal surfaces","The crosscap number of a knot is an invariant describing the non-orientable
surface of smallest genus that the knot bounds. Unlike knot genus (its
orientable counterpart), crosscap numbers are difficult to compute and no
general algorithm is known. We present three methods for computing crosscap
number that offer varying trade-offs between precision and speed: (i) an
algorithm based on Hilbert basis enumeration and (ii) an algorithm based on
exact integer programming, both of which either compute the solution precisely
or reduce it to two possible values, and (iii) a fast but limited precision
integer programming algorithm that bounds the solution from above.
  The first two algorithms advance the theoretical state of the art, but remain
intractable for practical use. The third algorithm is fast and effective, which
we show in a practical setting by making significant improvements to the
current knowledge of crosscap numbers in knot tables. Our integer programming
framework is general, with the potential for further applications in
computational geometry and topology.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1110.3376v2,Faster and Low Power Twin Precision Multiplier,"In this work faster unsigned multiplication has been achieved by using a
combination of High Performance Multiplication [HPM] column reduction technique
and implementing a N-bit multiplier using 4 N/2-bit multipliers (recursive
multiplication) and acceleration of the final addition using a hybrid adder.
Low power has been achieved by using clock gating technique. Based on the
proposed technique 16 and 32-bit multipliers are developed. The performance of
the proposed multiplier is analyzed by evaluating the delay, area and power,
with TCBNPHP 90 nm process technology on interconnect and layout using Cadence
NC launch, RTL compiler and ENCOUNTER tools. The results show that the 32-bit
proposed multiplier is as much as 22% faster, occupies only 3% more area and
consumes 30% lesser power with respect to the recently reported twin precision
multiplier.",0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2010.01582v1,"DNS Covert Channel Detection via Behavioral Analysis: a Machine Learning
  Approach","Detecting covert channels among legitimate traffic represents a severe
challenge due to the high heterogeneity of networks. Therefore, we propose an
effective covert channel detection method, based on the analysis of DNS network
data passively extracted from a network monitoring system. The framework is
based on a machine learning module and on the extraction of specific anomaly
indicators able to describe the problem at hand. The contribution of this paper
is two-fold: (i) the machine learning models encompass network profiles
tailored to the network users, and not to the single query events, hence
allowing for the creation of behavioral profiles and spotting possible
deviations from the normal baseline; (ii) models are created in an unsupervised
mode, thus allowing for the identification of zero-days attacks and avoiding
the requirement of signatures or heuristics for new variants. The proposed
solution has been evaluated over a 15-day-long experimental session with the
injection of traffic that covers the most relevant exfiltration and tunneling
attacks: all the malicious variants were detected, while producing a low
false-positive rate during the same period.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1306.5381v1,RFID Technology Based Attendance Management System,"RFID is a nascent technology, deeply rooted by its early developments in
using radar1 as a harbinger of adversary planes during World War II. A plethora
of industries have leveraged the benefits of RFID technology for enhancements
in sectors like military, sports, security, airline, animal farms, healthcare
and other areas. Industry specific key applications of this technology include
vehicle tracking, automated inventory management, animal monitoring, secure
store checkouts, supply chain management, automatic payment, sport timing
technologies, etc. This paper introduces the distinctive components of RFID
technology and focuses on its core competencies: scalability and security. It
will be then supplemented by a detailed synopsis of an investigation conducted
to test the feasibility and practicality of RFID technology.",0,1,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0906.2635v1,"Bayesian History Reconstruction of Complex Human Gene Clusters on a
  Phylogeny","Clusters of genes that have evolved by repeated segmental duplication present
difficult challenges throughout genomic analysis, from sequence assembly to
functional analysis. Improved understanding of these clusters is of utmost
importance, since they have been shown to be the source of evolutionary
innovation, and have been linked to multiple diseases, including HIV and a
variety of cancers. Previously, Zhang et al. (2008) developed an algorithm for
reconstructing parsimonious evolutionary histories of such gene clusters, using
only human genomic sequence data. In this paper, we propose a probabilistic
model for the evolution of gene clusters on a phylogeny, and an MCMC algorithm
for reconstruction of duplication histories from genomic sequences in multiple
species. Several projects are underway to obtain high quality BAC-based
assemblies of duplicated clusters in multiple species, and we anticipate that
our method will be useful in analyzing these valuable new data sets.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.13363v1,Coding with Purpose: Learning AI in Rural California,"We use an autoethnographic case study of a Latinx high school student from an
agricultural community in California to highlight how AI is learned outside
classrooms and how her personal background influenced her social-justice
oriented applications of AI technologies. Applying the concept of learning
pathways from the learning sciences, we argue that redesigning AI education to
be more inclusive with respect to socioeconomic status, ethnoracial identity,
and gender is important in the development of computational projects that
address social-injustice. We also learn about the role of institutions, power
structures, and community as they relate to her journey of learning and
applying AI. The future of AI, its potential to address issues of social
injustice and limiting the negative consequences of its use, will depend on the
participation and voice of students from the most vulnerable communities.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0
http://arxiv.org/abs/0710.0937v1,Multichannel algorithm based on generalized positional numeration system,"This report is devoted to introduction in multichannel algorithm based on
generalized numeration notations (GPN). The internal, external and mixed
account are entered. The concept of the GPN and its classification as
decomposition of an integer on composed of integers is discussed. Realization
of multichannel algorithm on the basis of GPN is introduced. In particular,
some properties of Fibonacci multichannel algorithm are discussed.",0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1403.3533v3,Quantum linear network coding as one-way quantum computation,"Network coding is a technique to maximize communication rates within a
network, in communication protocols for simultaneous multi-party transmission
of information. Linear network codes are examples of such protocols in which
the local computations performed at the nodes in the network are limited to
linear transformations of their input data (represented as elements of a ring,
such as the integers modulo 2). The quantum linear network coding protocols of
Kobayashi et al [arXiv:0908.1457 and arXiv:1012.4583] coherently simulate
classical linear network codes, using supplemental classical communication. We
demonstrate that these protocols correspond in a natural way to
measurement-based quantum computations with graph states over over qudits
[arXiv:quant-ph/0301052, arXiv:quant-ph/0603226, and arXiv:0704.1263] having a
structure directly related to the network.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2112.08315v1,Nirikshak: An Autonomous Testing Framework,"Quality Assurance (QA) is an important part of any product. But even with
automated methods of software testing, QA is mostly a set of repetitive tasks.
A lot of time, energy, and resources are spent in writing tests than in
realizing the bugs themselves and the traditional process does not scale well
to changes in the software. With advances in data science, it is theoretically
possible to have an autonomous testing framework. Such a framework would need
minimal user input and will be able to perform the complete testing process by
itself. This project is an effort to make such a framework.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1607.08363v4,Automata for Specifying and Orchestrating Service Contracts,"An approach to the formal description of service contracts is presented in
terms of automata. We focus on the basic property of guaranteeing that in the
multi-party composition of principals each of them gets his requests satisfied,
so that the overall composition reaches its goal. Depending on whether requests
are satisfied synchronously or asynchronously, we construct an orchestrator
that at static time either yields composed services enjoying the required
properties or detects the principals responsible for possible violations. To do
that in the asynchronous case we resort to Linear Programming techniques. We
also relate our automata with two logically based methods for specifying
contracts.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1704.02641v1,Quantized Innovations Bayesian Filtering,"The paper provides simple formulas of Bayesian filtering for the exact
recursive computation of state conditional probability density functions given
quantized innovations signal measurements of a linear stochastic system. This
is a topic of current interest because the innovations signal should be white
and therefore efficient in its use of channel capacity and in the design and
optimization of the quantizer. Earlier approaches, which we reexamine and
characterize here, have relied on assumptions concerning densities or
approximations to yield recursive solutions, which include the
sign-of-innovations Kalman filter and a Particle filtering technique. Our
approach uses the Kalman filter innovations at the transmitter side and
provides a point of comparison for the other methods, since it is based on the
Bayesian filter. Computational examples are provided.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1207.2017v1,dup -- Explicit un-sharing in Haskell,"We propose two operations to prevent sharing in Haskell that do not require
modifying the data generating code, demonstrate their use and usefulness, and
compare them to other approaches to preventing sharing. Our claims are
supported by a formal semantics and a prototype implementation.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.10759v1,Astronomical Pipeline Provenance: A Use Case Evaluation,"In this decade astronomy is undergoing a paradigm shift to handle data from
next generation observatories such as the Square Kilometre Array (SKA) or the
Vera C. Rubin Observatory (LSST). Producing real time data streams of up to 10
TB/s and data products of the order of 600 Pbytes/year, the SKA will be the
biggest civil data producing machine of the world that demands novel solutions
on how these data volumes can be stored and analysed. Through the use of
complex, automated pipelines the provenance of this real time data processing
is key to establish confidence within the system, its final data products, and
ultimately its scientific results.
  The intention of this paper is to lay the foundation for making an automated
provenance generation tool for astronomical/data-processing pipelines. We
therefore present a use case analysis, specific to the astronomical needs which
addresses the issues of trust and reproducibility as well as other ulterior use
cases which are of interest to astronomers. This analysis is subsequently used
as the basis to discuss the requirements, challenges, and opportunities
involved in designing both the tool and the associated provenance model.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1504.01780v1,Welfare Maximization with Limited Interaction,"We continue the study of welfare maximization in unit-demand (matching)
markets, in a distributed information model where agent's valuations are
unknown to the central planner, and therefore communication is required to
determine an efficient allocation. Dobzinski, Nisan and Oren (STOC'14) showed
that if the market size is $n$, then $r$ rounds of interaction (with
logarithmic bandwidth) suffice to obtain an $n^{1/(r+1)}$-approximation to the
optimal social welfare. In particular, this implies that such markets converge
to a stable state (constant approximation) in time logarithmic in the market
size.
  We obtain the first multi-round lower bound for this setup. We show that even
if the allowable per-round bandwidth of each agent is $n^{\epsilon(r)}$, the
approximation ratio of any $r$-round (randomized) protocol is no better than
$\Omega(n^{1/5^{r+1}})$, implying an $\Omega(\log \log n)$ lower bound on the
rate of convergence of the market to equilibrium.
  Our construction and technique may be of interest to round-communication
tradeoffs in the more general setting of combinatorial auctions, for which the
only known lower bound is for simultaneous ($r=1$) protocols [DNO14].",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2108.05450v1,Speed Control of DC Motor Using Fuzzy PID Controller,"In this project, we designed a DC motor whose speed can be controlled by a
PID controller. The proportional, integral and derivative gains (KP, KI, KD) of
the PID controller are adjusted according to Fuzzy logic. First of all, the
fuzzy logic controller is designed according to rules so that the systems is
basically robust. There are 25 rules for the auto-tuning of each parameter of
the PID controller. The FLC (fuzzy logic controller) has two inputs. The first
is the motor speed error between the reference (setpoint) and the actual speed.
The second is the variation of the speed error (derivative of the speed error).
Secondly the output of the FLC is the parameters of the PID controller which
are used to control the speed of the DC motor. The study shows that both the
precise characters of PID controllers and the flexible characters of fuzzy
controllers are present in the fuzzy self-tuning PID controller. The fuzzy
auto-tuning approach implemented on a conventional PID structure was able to
control the speed of the DC motor. It also improved the dynamic and static
response of the system. The comparison between the conventional response and
the fuzzy self-tuning response was performed based on the simulation result
obtained by MATLAB/SIMULINK.
  The simulation results show that the designed self-adaptive PID controller
achieves good dynamic behavior of the DC motor, perfect speed tracking with
short rise and settling times, zero overshoot and steady state error and thus
gives better performance compared to the conventional PID controller. We then
model the fuzzy PID using simple code on Arduino IDE and perform a practical
experiment, to confirm our theorical results.",0,1,0,0,1,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1209.4854v2,Geometric simulation of locally optimal tool paths in three-axis milling,"The most important aim in tool path generation methods is to increase the
machining efficiency by minimizing the total length of tool paths while the
error is kept under a prescribed tolerance. This can be achieved by determining
the moving direction of the cutting tool such that the machined stripe is the
widest. From a technical point of view it is recommended that the angle between
the tool axis and the surface normal does not change too much along the tool
path in order to ensure even abrasion of the tool. In this paper a mathematical
method for tool path generation in 3-axis milling is presented, which considers
these requirements by combining the features of isophotic curves and principal
curvatures. It calculates the proposed moving direction of the tool at each
point of the surface. The proposed direction depends on the measurement of the
tool and on the curvature values of the surface. For triangulated surfaces a
new local offset computation method is presented, which is suitable also for
detecting tool collision with the target surface and self intersection in the
offset mesh.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1708.05357v2,"Efficient Use of Limited-Memory Accelerators for Linear Learning on
  Heterogeneous Systems","We propose a generic algorithmic building block to accelerate training of
machine learning models on heterogeneous compute systems. Our scheme allows to
efficiently employ compute accelerators such as GPUs and FPGAs for the training
of large-scale machine learning models, when the training data exceeds their
memory capacity. Also, it provides adaptivity to any system's memory hierarchy
in terms of size and processing speed. Our technique is built upon novel
theoretical insights regarding primal-dual coordinate methods, and uses duality
gap information to dynamically decide which part of the data should be made
available for fast processing. To illustrate the power of our approach we
demonstrate its performance for training of generalized linear models on a
large-scale dataset exceeding the memory size of a modern GPU, showing an
order-of-magnitude speedup over existing approaches.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.05781v1,The Semantic Brand Score,"The Semantic Brand Score (SBS) is a new measure of brand importance
calculated on text data, combining methods of social network and semantic
analysis. This metric is flexible as it can be used in different contexts and
across products, markets and languages. It is applicable not only to brands,
but also to multiple sets of words. The SBS, described together with its three
dimensions of brand prevalence, diversity and connectivity, represents a
contribution to the research on brand equity and on word co-occurrence
networks. It can be used to support decision-making processes within companies;
for example, it can be applied to forecast a company's stock price or to assess
brand importance with respect to competitors. On the one side, the SBS relates
to familiar constructs of brand equity, on the other, it offers new
perspectives for effective strategic management of brands in the era of big
data.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.03631v2,Renovo: A Sensor-Based Therapeutic System for Brachial Monoplegia,"$Introduction:$ Patients with Brachial Monoplegia, paralysis of the upper
limb following various neurological disorders, require therapeutic
interventions with proper assessment for rehabilitation. State-of-the-art
assessment protocols, the majority of which are qualitative, often cause biased
assessment. Very few therapeutic systems aim to assist the physiotherapists in
this regard. $Methods:$ We designed a sensor-based therapeutic system for
assisting the physiotherapists with real-time visualization of Performance
Metrics and a reliable quantitative assessment. 5 healthy subjects
(Mean=24.4$\pm$2.4 years, 80% Male), 16 patients with Brachial Monoplegia
(Mean=39.56$\pm$16.4 years, 76.92% Male), and 5 physiotherapists volunteered
with informed consent. Each patient was evaluated by both the system and the
physiotherapists in 3 sessions. $Results:$ Insignificant difference between the
mean (t(15)=1.39, p=.184) and the variance (F(1, 15)=1.05, p=.460) of system
evaluation (Mean=6.19, 95% CI, [4.52, 7.86]) and that of the physiotherapists
(Mean=6.38, 95% CI, [4.75, 8.01]) having a strong, positive correlation,
r=.9885 was observed. The system showed good reliability from the Cronbach's
Alpha test, with $\alpha$=.8499. $Conclusions:$ To ensure effective
rehabilitation, proper assessment is obligatory. Sensor-based measurement of
performance metrics ensures reliability, reducing the chances of human error.
Real-time visualization informs about patient's progress. Automated
quantitative assessment reduces the possibility of bias, allowing optimization
of rehabilitation scheme.",0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1708.01925v1,"Designing Autonomous Vehicles: Evaluating the Role of Human Emotions and
  Social Norms","Humans are going to delegate the rights of driving to the autonomous vehicles
in near future. However, to fulfill this complicated task, there is a need for
a mechanism, which enforces the autonomous vehicles to obey the road and social
rules that have been practiced by well-behaved drivers. This task can be
achieved by introducing social norms compliance mechanism in the autonomous
vehicles. This research paper is proposing an artificial society of autonomous
vehicles as an analogy of human social society. Each AV has been assigned a
social personality having different social influence. Social norms have been
introduced which help the AVs in making the decisions, influenced by emotions,
regarding road collision avoidance. Furthermore, social norms compliance
mechanism, by artificial social AVs, has been proposed using prospect based
emotion i.e. fear, which is conceived from OCC model. Fuzzy logic has been
employed to compute the emotions quantitatively. Then, using SimConnect
approach, fuzzy values of fear has been provided to the Netlogo simulation
environment to simulate artificial society of AVs. Extensive testing has been
performed using the behavior space tool to find out the performance of the
proposed approach in terms of the number of collisions. For comparison, the
random-walk model based artificial society of AVs has been proposed as well. A
comparative study with a random walk, prove that proposed approach provides a
better option to tailor the autopilots of future AVS, Which will be more
socially acceptable and trustworthy by their riders in terms of safe road
travel.",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1202.6158v1,Optimized on-line computation of PageRank algorithm,"In this paper we present new ideas to accelerate the computation of the
eigenvector of the transition matrix associated to the PageRank algorithm. New
ideas are based on the decomposition of the matrix-vector product that can be
seen as a fluid diffusion model, associated to new algebraic equations. We show
through experiments on synthetic data and on real data-sets how much this
approach can improve the computation efficiency.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1912.09871v2,Convergence Rate Abstractions for Weakly-Hard Real-Time Control,"Classically, a control loop is designed to be executed strictly periodically.
This is, however, difficult to achieve in many scenarios, for example, when
overload or packet loss cannot be entirely avoided. Here, weakly-hard real-time
control systems are a common approach which relaxes timing constraints and
leverages the inherent robustness of controllers. Yet, their analysis is often
hampered by the complexity arising from the system dimension and the vast
number of possible timing sequences. In this paper, we present the novel
concept of convergence rate abstractions that provide a sound yet simple
one-dimensional system description. This approach simplifies the stability
analysis of weakly-hard real-time control systems. At the same time, our
abstractions facilitate efficient computation of bounds on the worst-case
system state at run-time and thus the implementation of adaptation mechanisms.",0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0106014v1,L.T.Kuzin: Research Program,"Lev T. Kuzin (1928--1997) is one of the founders of modern cybernetics and
information science in Russia. He was awarded and honored the USSR State Prize
for inspiring vision into the future of technical cybernetics and his invention
and innovation of key technologies.
  The last years he interested in the computational models of geometrical and
algebraic nature and their applications in various branches of computer science
and information technologies. In the recent years the interest in computation
models based on object notion has grown tremendously stimulating an interest to
Kuzin's ideas. This year of 50th Anniversary of Cybernetics and on the occasion
of his 70th birthday on September 12, 1998 seems especially appropriate for
discussing Kuzin's Research Program.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1701.05221v5,"Parsimonious Inference on Convolutional Neural Networks: Learning and
  applying on-line kernel activation rules","A new, radical CNN design approach is presented in this paper, considering
the reduction of the total computational load during inference. This is
achieved by a new holistic intervention on both the CNN architecture and the
training procedure, which targets to the parsimonious inference by learning to
exploit or remove the redundant capacity of a CNN architecture. This is
accomplished, by the introduction of a new structural element that can be
inserted as an add-on to any contemporary CNN architecture, whilst preserving
or even improving its recognition accuracy. Our approach formulates a
systematic and data-driven method for developing CNNs that are trained to
eventually change size and form in real-time during inference, targeting to the
smaller possible computational footprint. Results are provided for the optimal
implementation on a few modern, high-end mobile computing platforms indicating
a significant speed-up of up to x3 times.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0
http://arxiv.org/abs/2103.16977v1,"Solving Heterogeneous General Equilibrium Economic Models with Deep
  Reinforcement Learning","General equilibrium macroeconomic models are a core tool used by policymakers
to understand a nation's economy. They represent the economy as a collection of
forward-looking actors whose behaviours combine, possibly with stochastic
effects, to determine global variables (such as prices) in a dynamic
equilibrium. However, standard semi-analytical techniques for solving these
models make it difficult to include the important effects of heterogeneous
economic actors. The COVID-19 pandemic has further highlighted the importance
of heterogeneity, for example in age and sector of employment, in macroeconomic
outcomes and the need for models that can more easily incorporate it. We use
techniques from reinforcement learning to solve such models incorporating
heterogeneous agents in a way that is simple, extensible, and computationally
efficient. We demonstrate the method's accuracy and stability on a toy problem
for which there is a known analytical solution, its versatility by solving a
general equilibrium problem that includes global stochasticity, and its
flexibility by solving a combined macroeconomic and epidemiological model to
explore the economic and health implications of a pandemic. The latter
successfully captures plausible economic behaviours induced by differential
health risks by age.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1107.4617v4,Constant-time filtering using shiftable kernels,"It was recently demonstrated in [5] that the non-linear bilateral filter [14]
can be efficiently implemented using a constant-time or O(1) algorithm. At the
heart of this algorithm was the idea of approximating the Gaussian range kernel
of the bilateral filter using trigonometric functions. In this letter, we
explain how the idea in [5] can be extended to few other linear and non-linear
filters [14, 17, 2]. While some of these filters have received a lot of
attention in recent years, they are known to be computationally intensive. To
extend the idea in [5], we identify a central property of trigonometric
functions, called shiftability, that allows us to exploit the redundancy
inherent in the filtering operations. In particular, using shiftable kernels,
we show how certain complex filtering can be reduced to simply that of
computing the moving sum of a stack of images. Each image in the stack is
obtained through an elementary pointwise transform of the input image. This has
a two-fold advantage. First, we can use fast recursive algorithms for computing
the moving sum [15, 6], and, secondly, we can use parallel computation to
further speed up the computation. We also show how shiftable kernels can also
be used to approximate the (non-shiftable) Gaussian kernel that is ubiquitously
used in image filtering.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1609.09000v1,StruClus: Structural Clustering of Large-Scale Graph Databases,"We present a structural clustering algorithm for large-scale datasets of
small labeled graphs, utilizing a frequent subgraph sampling strategy. A set of
representatives provides an intuitive description of each cluster, supports the
clustering process, and helps to interpret the clustering results. The
projection-based nature of the clustering approach allows us to bypass
dimensionality and feature extraction problems that arise in the context of
graph datasets reduced to pairwise distances or feature vectors. While
achieving high quality and (human) interpretable clusterings, the runtime of
the algorithm only grows linearly with the number of graphs. Furthermore, the
approach is easy to parallelize and therefore suitable for very large datasets.
Our extensive experimental evaluation on synthetic and real world datasets
demonstrates the superiority of our approach over existing structural and
subspace clustering algorithms, both, from a runtime and quality point of view.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1203.4380v4,Analyzing closed frequent itemsets with convex polytopes,"Frequent itemsets form a polytope and can be found and analyzed with Linear
Programming.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1003.5192v1,"wiki.openmath.org - how it works, how you can participate","At http://wiki.openmath.org, the OpenMath 2 and 3 Content Dictionaries are
accessible via a semantic wiki interface, powered by the SWiM system. We
shortly introduce the inner workings of the system, then describe how to use
it, and conclude with first experiences gained from OpenMath society members
working with the system and an outlook to further development plans.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1306.1772v1,"Are Happy Developers more Productive? The Correlation of Affective
  States of Software Developers and their self-assessed Productivity","For decades now, it has been claimed that a way to improve software
developers' productivity is to focus on people. Indeed, while human factors
have been recognized in Software Engineering research, few empirical
investigations have attempted to verify the claim. Development tasks are
undertaken through cognitive processing abilities. Affective states - emotions,
moods, and feelings - have an impact on work-related behaviors, cognitive
processing activities, and the productivity of individuals. In this paper, we
report an empirical study on the impact of affective states on software
developers' performance while programming. Two affective states dimensions are
positively correlated with self-assessed productivity. We demonstrate the value
of applying psychometrics in Software Engineering studies and echo a call to
valorize the human, individualized aspects of software developers. We introduce
and validate a measurement instrument and a linear mixed-effects model to study
the correlation of affective states and the productivity of software
developers.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/cs/0211033v1,Propositional satisfiability in declarative programming,"Answer-set programming (ASP) paradigm is a way of using logic to solve search
problems. Given a search problem, to solve it one designs a theory in the logic
so that models of this theory represent problem solutions. To compute a
solution to a problem one needs to compute a model of the corresponding theory.
Several answer-set programming formalisms have been developed on the basis of
logic programming with the semantics of stable models. In this paper we show
that also the logic of predicate calculus gives rise to effective
implementations of the ASP paradigm, similar in spirit to logic programming
with stable model semantics and with a similar scope of applicability.
Specifically, we propose two logics based on predicate calculus as formalisms
for encoding search problems. We show that the expressive power of these logics
is given by the class NP-search. We demonstrate how to use them in programming
and develop computational tools for model finding. In the case of one of the
logics our techniques reduce the problem to that of propositional
satisfiability and allow one to use off-the-shelf satisfiability solvers. The
language of the other logic has more complex syntax and provides explicit means
to model some high-level constraints. For theories in this logic, we designed
our own solver that takes advantage of the expanded syntax. We present
experimental results demonstrating computational effectiveness of the overall
approach.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1612.01638v1,An EMOF-Compliant Abstract Syntax for Bigraphs,"Bigraphs are an emerging modeling formalism for structures in ubiquitous
computing. Besides an algebraic notation, which can be adopted to provide an
algebraic syntax for bigraphs, the bigraphical theory introduces a visual
concrete syntax which is intuitive and unambiguous at the same time; the
standard visual notation can be customized and thus tailored to domain-specific
requirements. However, in contrast to modeling standards based on the
Meta-Object Facility (MOF) and domain-specific languages typically used in
model-driven engineering (MDE), the bigraphical theory lacks a precise
definition of an abstract syntax for bigraphical modeling languages. As a
consequence, available modeling and analysis tools use proprietary formats for
representing bigraphs internally and persistently, which hampers the exchange
of models across tool boundaries. Moreover, tools can be hardly integrated with
standard MDE technologies in order to build sophisticated tool chains and
modeling environments, as required for systematic engineering of large systems
or fostering experimental work to evaluate the bigraphical theory in real-world
applications. To overcome this situation, we propose an abstract syntax for
bigraphs which is compliant to the Essential MOF (EMOF) standard defined by the
Object Management Group (OMG). We use typed graphs as a formal underpinning of
EMOF-based models and present a canonical mapping which maps bigraphs to typed
graphs in a natural way. We also discuss application-specific variation points
in the graph-based representation of bigraphs. Following standard techniques
from software product line engineering, we present a framework to customize the
graph-based representation to support a variety of application scenarios.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1203.3434v1,"On the Impact of Information Technologies on Society: an Historical
  Perspective through the Game of Chess","The game of chess as always been viewed as an iconic representation of
intellectual prowess. Since the very beginning of computer science, the
challenge of being able to program a computer capable of playing chess and
beating humans has been alive and used both as a mark to measure
hardware/software progresses and as an ongoing programming challenge leading to
numerous discoveries. In the early days of computer science it was a topic for
specialists. But as computers were democratized, and the strength of chess
engines began to increase, chess players started to appropriate to themselves
these new tools. We show how these interactions between the world of chess and
information technologies have been herald of broader social impacts of
information technologies. The game of chess, and more broadly the world of
chess (chess players, literature, computer softwares and websites dedicated to
chess, etc.), turns out to be a surprisingly and particularly sharp indicator
of the changes induced in our everyday life by the information technologies.
Moreover, in the same way that chess is a modelization of war that captures the
raw features of strategic thinking, chess world can be seen as small society
making the study of the information technologies impact easier to analyze and
to grasp.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
http://arxiv.org/abs/1511.04691v1,"Optimization of the Block-level Bit Allocation in Perceptual Video
  Coding based on MINMAX","In video coding, it is expected that the encoder could adaptively select the
encoding parameters (e.g., quantization parameter) to optimize the bit
allocation to different sources under the given constraint. However, in hybrid
video coding, the dependency between sources brings high complexity for the bit
allocation optimization, especially in the block-level, and existing
optimization methods mostly focus on frame-level bit allocation. In this paper,
we propose a macroblock (MB) level bit allocation method based on the minimum
maximum (MINMAX) criterion, which has acceptable encoding complexity for
offline applications. An iterative-based algorithm, namely maximum distortion
descend (MDD), is developed to reduce quality fluctuation among MBs within a
frame, where the Structure SIMilarity (SSIM) index is used to measure the
perceptual distortion of MBs. Our extensive experimental results on benchmark
video sequences show that the proposed method can greatly enhance the encoding
performance in terms of both bits saving and perceptual quality improvement.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.03181v1,Can ML predict the solution value for a difficult combinatorial problem?,"We look at whether machine learning can predict the final objective function
value of a difficult combinatorial optimisation problem from the input. Our
context is the pattern reduction problem, one industrially important but
difficult aspect of the cutting stock problem. Machine learning appears to have
higher prediction accuracy than a na\""ive model, reducing mean absolute
percentage error (MAPE) from 12.0% to 8.7%.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.02270v1,"Encryption and Real Time Decryption for protecting Machine Learning
  models in Android Applications","With the Increasing use of Machine Learning in Android applications, more
research and efforts are being put into developing better-performing machine
learning algorithms with a vast amount of data. Along with machine learning for
mobile phones, the threat of extraction of trained machine learning models from
application packages (APK) through reverse engineering exists. Currently, there
are ways to protect models in mobile applications such as name obfuscation,
cloud deployment, last layer isolation. Still, they offer less security, and
their implementation requires more effort. This paper gives an algorithm to
protect trained machine learning models inside android applications with high
security and low efforts to implement it. The algorithm ensures security by
encrypting the model and real-time decrypting it with 256-bit Advanced
Encryption Standard (AES) inside the running application. It works efficiently
with big model files without interrupting the User interface (UI) Thread. As
compared to other methods, it is fast, more secure, and involves fewer efforts.
This algorithm provides the developers and researchers a way to secure their
actions and making the results available to all without any concern.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1708.08189v1,"An IoT Real-Time Biometric Authentication System Based on ECG Fiducial
  Extracted Features Using Discrete Cosine Transform","The conventional authentication technologies, like RFID tags and
authentication cards/badges, suffer from different weaknesses, therefore a
prompt replacement to use biometric method of authentication should be applied
instead. Biometrics, such as fingerprints, voices, and ECG signals, are unique
human characters that can be used for authentication processing. In this work,
we present an IoT real-time authentication system based on using extracted ECG
features to identify the unknown persons. The Discrete Cosine Transform (DCT)
is used as an ECG feature extraction, where it has better characteristics for
real-time system implementations. There are a substantial number of researches
with a high accuracy of authentication, but most of them ignore the real-time
capability of authenticating individuals. With the accuracy rate of 97.78% at
around 1.21 seconds of processing time, the proposed system is more suitable
for use in many applications that require fast and reliable authentication
processing demands.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0
http://arxiv.org/abs/1803.09356v1,"Neural Nets via Forward State Transformation and Backward Loss
  Transformation","This article studies (multilayer perceptron) neural networks with an emphasis
on the transformations involved --- both forward and backward --- in order to
develop a semantical/logical perspective that is in line with standard program
semantics. The common two-pass neural network training algorithms make this
viewpoint particularly fitting. In the forward direction, neural networks act
as state transformers. In the reverse direction, however, neural networks
change losses of outputs to losses of inputs, thereby acting like a
(real-valued) predicate transformer. In this way, backpropagation is functorial
by construction, as shown earlier in recent other work. We illustrate this
perspective by training a simple instance of a neural network.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1702.05376v1,Towards a Unified Taxonomy of Biclustering Methods,"Being an unsupervised machine learning and data mining technique,
biclustering and its multimodal extensions are becoming popular tools for
analysing object-attribute data in different domains. Apart from conventional
clustering techniques, biclustering is searching for homogeneous groups of
objects while keeping their common description, e.g., in binary setting, their
shared attributes. In bioinformatics, biclustering is used to find genes, which
are active in a subset of situations, thus being candidates for biomarkers.
However, the authors of those biclustering techniques that are popular in gene
expression analysis, may overlook the existing methods. For instance, BiMax
algorithm is aimed at finding biclusters, which are well-known for decades as
formal concepts. Moreover, even if bioinformatics classify the biclustering
methods according to reasonable domain-driven criteria, their classification
taxonomies may be different from survey to survey and not full as well. So, in
this paper we propose to use concept lattices as a tool for taxonomy building
(in the biclustering domain) and attribute exploration as means for
cross-domain taxonomy completion.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/cs/0601018v1,A comparison between two logical formalisms for rewriting,"Meseguer's rewriting logic and the rewriting logic CRWL are two well-known
approaches to rewriting as logical deduction that, despite some clear
similarities, were designed with different objectives. Here we study the
relationships between them, both at a syntactic and at a semantic level. Even
though it is not possible to establish an entailment system map between them,
both can be naturally simulated in each other. Semantically, there is no
embedding between the corresponding institutions. Along the way, the notions of
entailment and satisfaction in Meseguer's rewriting logic are generalized. We
also use the syntactic results to prove reflective properties of CRWL.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0903.4961v2,"Global Clock, Physical Time Order and Pending Period Analysis in
  Multiprocessor Systems","In multiprocessor systems, various problems are treated with Lamport's
logical clock and the resultant logical time orders between operations.
However, one often needs to face the high complexities caused by the lack of
logical time order information in practice. In this paper, we utilize the
\emph{global clock} to infuse the so-called \emph{pending period} to each
operation in a multiprocessor system, where the pending period is a time
interval that contains the performed time of the operation. Further, we define
the \emph{physical time order} for any two operations with disjoint pending
periods. The physical time order is obeyed by any real execution in
multiprocessor systems due to that it is part of the truly happened operation
orders restricted by global clock, and it is then proven to be independent and
consistent with traditional logical time orders. The above novel yet
fundamental concepts enables new effective approaches for analyzing
multiprocessor systems, which are named \emph{pending period analysis} as a
whole. As a consequence of pending period analysis, many important problems of
multiprocessor systems can be tackled effectively. As a significant application
example, complete memory consistency verification, which was known as an
NP-hard problem, can be solved with the complexity of $O(n^2)$ (where $n$ is
the number of operations). Moreover, the two event ordering problems, which
were proven to be Co-NP-Hard and NP-hard respectively, can both be solved with
the time complexity of O(n) if restricted by pending period information.",0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1005.0990v1,Polynomial integration on regions defined by a triangle and a conic,"We present an efficient solution to the following problem, of relevance in a
numerical optimization scheme: calculation of integrals of the type \[\iint_{T
\cap \{f\ge0\}} \phi_1\phi_2 \, dx\,dy\] for quadratic polynomials
$f,\phi_1,\phi_2$ on a plane triangle $T$. The naive approach would involve
consideration of the many possible shapes of $T\cap\{f\geq0\}$ (possibly after
a convenient transformation) and parameterizing its border, in order to
integrate the variables separately. Our solution involves partitioning the
triangle into smaller triangles on which integration is much simpler.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2106.07488v1,"User-Guided Personalized Image Aesthetic Assessment based on Deep
  Reinforcement Learning","Personalized image aesthetic assessment (PIAA) has recently become a hot
topic due to its usefulness in a wide variety of applications such as
photography, film and television, e-commerce, fashion design and so on. This
task is more seriously affected by subjective factors and samples provided by
users. In order to acquire precise personalized aesthetic distribution by small
amount of samples, we propose a novel user-guided personalized image aesthetic
assessment framework. This framework leverages user interactions to retouch and
rank images for aesthetic assessment based on deep reinforcement learning
(DRL), and generates personalized aesthetic distribution that is more in line
with the aesthetic preferences of different users. It mainly consists of two
stages. In the first stage, personalized aesthetic ranking is generated by
interactive image enhancement and manual ranking, meanwhile two policy networks
will be trained. The images will be pushed to the user for manual retouching
and simultaneously to the enhancement policy network. The enhancement network
utilizes the manual retouching results as the optimization goals of DRL. After
that, the ranking process performs the similar operations like the retouching
mentioned before. These two networks will be trained iteratively and
alternatively to help to complete the final personalized aesthetic assessment
automatically. In the second stage, these modified images are labeled with
aesthetic attributes by one style-specific classifier, and then the
personalized aesthetic distribution is generated based on the multiple
aesthetic attributes of these images, which conforms to the aesthetic
preference of users better.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1812.08342v5,"A Survey of Safety and Trustworthiness of Deep Neural Networks:
  Verification, Testing, Adversarial Attack and Defence, and Interpretability","In the past few years, significant progress has been made on deep neural
networks (DNNs) in achieving human-level performance on several long-standing
tasks. With the broader deployment of DNNs on various applications, the
concerns over their safety and trustworthiness have been raised in public,
especially after the widely reported fatal incidents involving self-driving
cars. Research to address these concerns is particularly active, with a
significant number of papers released in the past few years. This survey paper
conducts a review of the current research effort into making DNNs safe and
trustworthy, by focusing on four aspects: verification, testing, adversarial
attack and defence, and interpretability. In total, we survey 202 papers, most
of which were published after 2017.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1707.03201v3,"Functional approach to the error control in adaptive IgA schemes for
  elliptic boundary value problems","This work presents a numerical study of functional type a posteriori error
estimates for IgA approximation schemes in the context of elliptic
boundary-value problems. Along with the detailed discussion of the most crucial
properties of such estimates, we present the algorithm of a reliable solution
approximation together with the scheme of efficient a posteriori error bound
generation-based on solving an auxiliary problem with respect to an introduced
vector-valued variable. In this approach, we take advantage of B-(THB-)spline's
high smoothness for the auxiliary vector function reconstruction, which, at the
same time, allows to use much coarser meshes and decrease the number of
unknowns substantially. The most representative numerical results, obtained
during a systematic testing of error estimates, are presented in the second
part of the paper. The efficiency of the obtained error bounds is analysed from
both the error estimation (indication) and the computational expenses points of
view. Several examples illustrate that functional error estimates
(alternatively referred to as the majorants and minorants of deviation from an
exact solution) perform a much sharper error control than, for instance,
residual-based error estimates. Simultaneously, assembling and solving the
routines for an auxiliary variable reconstruction which generate the majorant
of an error can be executed several times faster than the routines for a primal
unknown.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2009.06833v1,Compositional Models for Power Systems,"The problem of integrating multiple overlapping models and data is pervasive
in engineering, though often implicit. We consider this issue of model
management in the context of the electrical power grid as it transitions
towards a modern 'Smart Grid.' We present a methodology for specifying,
managing, and reasoning within multiple models of distributed energy resources
(DERs), entities which produce, consume, or store power, using categorical
databases and symmetric monoidal categories. Considering the problem of
distributing power on the grid in the presence of DERs, we show how to connect
a generic problem specification with implementation-specific numerical solvers
using the paradigm of categorical databases.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2109.11406v1,"Named Entity Recognition and Classification on Historical Documents: A
  Survey","After decades of massive digitisation, an unprecedented amount of historical
documents is available in digital format, along with their machine-readable
texts. While this represents a major step forward with respect to preservation
and accessibility, it also opens up new opportunities in terms of content
mining and the next fundamental challenge is to develop appropriate
technologies to efficiently search, retrieve and explore information from this
'big data of the past'. Among semantic indexing opportunities, the recognition
and classification of named entities are in great demand among humanities
scholars. Yet, named entity recognition (NER) systems are heavily challenged
with diverse, historical and noisy inputs. In this survey, we present the array
of challenges posed by historical documents to NER, inventory existing
resources, describe the main approaches deployed so far, and identify key
priorities for future developments.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1212.0703v2,The Cost of Address Translation,"Modern computers are not random access machines (RAMs). They have a memory
hierarchy, multiple cores, and virtual memory. In this paper, we address the
computational cost of address translation in virtual memory. Starting point for
our work is the observation that the analysis of some simple algorithms (random
scan of an array, binary search, heapsort) in either the RAM model or the EM
model (external memory model) does not correctly predict growth rates of actual
running times. We propose the VAT model (virtual address translation) to
account for the cost of address translations and analyze the algorithms
mentioned above and others in the model. The predictions agree with the
measurements. We also analyze the VAT-cost of cache-oblivious algorithms.",0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2003.08767v2,"A Review of Computational Approaches for Evaluation of Rehabilitation
  Exercises","Recent advances in data analytics and computer-aided diagnostics stimulate
the vision of patient-centric precision healthcare, where treatment plans are
customized based on the health records and needs of every patient. In physical
rehabilitation, the progress in machine learning and the advent of affordable
and reliable motion capture sensors have been conducive to the development of
approaches for automated assessment of patient performance and progress toward
functional recovery. The presented study reviews computational approaches for
evaluating patient performance in rehabilitation programs using motion capture
systems. Such approaches will play an important role in supplementing
traditional rehabilitation assessment performed by trained clinicians, and in
assisting patients participating in home-based rehabilitation. The reviewed
computational methods for exercise evaluation are grouped into three main
categories: discrete movement score, rule-based, and template-based approaches.
The review places an emphasis on the application of machine learning methods
for movement evaluation in rehabilitation. Related work in the literature on
data representation, feature engineering, movement segmentation, and scoring
functions is presented. The study also reviews existing sensors for capturing
rehabilitation movements and provides an informative listing of pertinent
benchmark datasets. The significance of this paper is in being the first to
provide a comprehensive review of computational methods for evaluation of
patient performance in rehabilitation programs.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1504.02578v1,Blade: A Data Center Garbage Collector,"An increasing number of high-performance distributed systems are written in
garbage collected languages. This removes a large class of harmful bugs from
these systems. However, it also introduces high tail-latency do to garbage
collection pause times. We address this problem through a new technique of
garbage collection avoidance which we call Blade. Blade is an API between the
collector and application developer that allows developers to leverage existing
failure recovery mechanisms in distributed systems to coordinate collection and
bound the latency impact. We describe Blade and implement it for the Go
programming language. We also investigate two different systems that utilize
Blade, a HTTP load-balancer and the Raft consensus algorithm. For the
load-balancer, we eliminate any latency introduced by the garbage collector,
for Raft, we bound the latency impact to a single network round-trip, (48
{\mu}s in our setup). In both cases, latency at the tail using Blade is up to
three orders of magnitude better.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2105.05445v2,Snipuzz: Black-box Fuzzing of IoT Firmware via Message Snippet Inference,"The proliferation of Internet of Things (IoT) devices has made people's lives
more convenient, but it has also raised many security concerns. Due to the
difficulty of obtaining and emulating IoT firmware, the black-box fuzzing of
IoT devices has become a viable option. However, existing black-box fuzzers
cannot form effective mutation optimization mechanisms to guide their testing
processes, mainly due to the lack of feedback. It is difficult or even
impossible to apply existing grammar-based fuzzing strategies. Therefore, an
efficient fuzzing approach with syntax inference is required in the IoT fuzzing
domain. To address these critical problems, we propose a novel automatic
black-box fuzzing for IoT firmware, termed Snipuzz. Snipuzz runs as a client
communicating with the devices and infers message snippets for mutation based
on the responses. Each snippet refers to a block of consecutive bytes that
reflect the approximate code coverage in fuzzing. This mutation strategy based
on message snippets considerably narrows down the search space to change the
probing messages. We compared Snipuzz with four state-of-the-art IoT fuzzing
approaches, i.e., IoTFuzzer, BooFuzz, Doona, and Nemesys. Snipuzz not only
inherits the advantages of app-based fuzzing (e.g., IoTFuzzer, but also
utilizes communication responses to perform efficient mutation. Furthermore,
Snipuzz is lightweight as its execution does not rely on any prerequisite
operations, such as reverse engineering of apps. We also evaluated Snipuzz on
20 popular real-world IoT devices. Our results show that Snipuzz could identify
5 zero-day vulnerabilities, and 3 of them could be exposed only by Snipuzz. All
the newly discovered vulnerabilities have been confirmed by their vendors.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
http://arxiv.org/abs/1405.6326v1,"Second Life: game, simulator, or serious game?","This article reports on an exploratory case study conducted to examine the
viability of Second Life (SL) as an environment for physical simulations and
microworlds. It begins by discussing specific features of the SL environment
relevant to its use as a support for microworlds and simulations as well as a
few differences found between SL and traditional simulators such as Modellus,
along with their implications to simulations, as a support for subsequent
analysis. Afterwards, we will use Narayanasamy et al. and Johnston and
Whitehead criteria to analyze the SL environment and determine into which of
training simulators, games, simulation games, or serious games categories SL
fits best. We conclude that SL shows itself as a huge and sophisticated
simulator of an entire Earthlike world used by thousands of users to simulate
real life in some sense and a viable and flexible platform for microworlds and
simulations.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1104.2547v5,"C-Codes: Cyclic Lowest-Density MDS Array Codes Constructed Using
  Starters for RAID 6","The distance-3 cyclic lowest-density MDS array code (called the C-Code) is a
good candidate for RAID 6 because of its optimal storage efficiency, optimal
update complexity, optimal length, and cyclic symmetry. In this paper, the
underlying connections between C-Codes (or quasi-C-Codes) and starters in group
theory are revealed. It is shown that each C-Code (or quasi-C-Code) of length
$2n$ can be constructed using an even starter (or even multi-starter) in
$(Z_{2n},+)$. It is also shown that each C-Code (or quasi-C-Code) has a twin
C-Code (or quasi-C-Code). Then, four infinite families (three of which are new)
of C-Codes of length $p-1$ are constructed, where $p$ is a prime. Besides the
family of length $p-1$, C-Codes for some sporadic even lengths are also
presented. Even so, there are still some even lengths (such as 8) for which
C-Codes do not exist. To cover this limitation, two infinite families (one of
which is new) of quasi-C-Codes of length $2(p-1)$ are constructed for these
even lengths.",0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1612.06115v2,"Complex Network Tools to Understand the Behavior of Criminality in Urban
  Areas","Complex networks are nowadays employed in several applications. Modeling
urban street networks is one of them, and in particular to analyze criminal
aspects of a city. Several research groups have focused on such application,
but until now, there is a lack of a well-defined methodology for employing
complex networks in a whole crime analysis process, i.e. from data preparation
to a deep analysis of criminal communities. Furthermore, the ""toolset""
available for those works is not complete enough, also lacking techniques to
maintain up-to-date, complete crime datasets and proper assessment measures. In
this sense, we propose a threefold methodology for employing complex networks
in the detection of highly criminal areas within a city. Our methodology
comprises three tasks: (i) Mapping of Urban Crimes; (ii) Criminal Community
Identification; and (iii) Crime Analysis. Moreover, it provides a proper set of
assessment measures for analyzing intrinsic criminality of communities,
especially when considering different crime types. We show our methodology by
applying it to a real crime dataset from the city of San Francisco - CA, USA.
The results confirm its effectiveness to identify and analyze high criminality
areas within a city. Hence, our contributions provide a basis for further
developments on complex networks applied to crime analysis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/2101.01229v2,A Survey on Embedding Dynamic Graphs,"Embedding static graphs in low-dimensional vector spaces plays a key role in
network analytics and inference, supporting applications like node
classification, link prediction, and graph visualization. However, many
real-world networks present dynamic behavior, including topological evolution,
feature evolution, and diffusion. Therefore, several methods for embedding
dynamic graphs have been proposed to learn network representations over time,
facing novel challenges, such as time-domain modeling, temporal features to be
captured, and the temporal granularity to be embedded. In this survey, we
overview dynamic graph embedding, discussing its fundamentals and the recent
advances developed so far. We introduce the formal definition of dynamic graph
embedding, focusing on the problem setting and introducing a novel taxonomy for
dynamic graph embedding input and output. We further explore different dynamic
behaviors that may be encompassed by embeddings, classifying by topological
evolution, feature evolution, and processes on networks. Afterward, we describe
existing techniques and propose a taxonomy for dynamic graph embedding
techniques based on algorithmic approaches, from matrix and tensor
factorization to deep learning, random walks, and temporal point processes. We
also elucidate main applications, including dynamic link prediction, anomaly
detection, and diffusion prediction, and we further state some promising
research directions in the area.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1603.06914v4,SimOutUtils - Utilities for analyzing time series simulation output,"SimOutUtils is a suite of MATLAB/Octave functions for studying and analyzing
time series-like output from stochastic simulation models. More specifically,
SimOutUtils allows modelers to study and visualize simulation output dynamics,
perform distributional analysis of output statistical summaries, as well as
compare these summaries in order to assert the statistical equivalence of two
or more model implementations. Additionally, the provided functions are able to
produce publication quality figures and tables showcasing results from the
specified simulation output studies.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1309.7665v1,Group-theoretic structure of linear phase multirate filter banks,"Unique lifting factorization results for group lifting structures are used to
characterize the group-theoretic structure of two-channel linear phase FIR
perfect reconstruction filter bank groups. For D-invariant, order-increasing
group lifting structures, it is shown that the associated lifting cascade group
C is isomorphic to the free product of the upper and lower triangular lifting
matrix groups. Under the same hypotheses, the associated scaled lifting group S
is the semidirect product of C by the diagonal gain scaling matrix group D.
These results apply to the group lifting structures for the two principal
classes of linear phase perfect reconstruction filter banks, the whole- and
half-sample symmetric classes. Since the unimodular whole-sample symmetric
class forms a group, W, that is in fact equal to its own scaled lifting group,
W=S_W, the results of this paper characterize the group-theoretic structure of
W up to isomorphism. Although the half-sample symmetric class H does not form a
group, it can be partitioned into cosets of its lifting cascade group, C_H, or,
alternatively, into cosets of its scaled lifting group, S_H. Homomorphic
comparisons reveal that scaled lifting groups covered by the results in this
paper have a structure analogous to a ""noncommutative vector space.""",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/1404.2267v2,Transparallel mind: Classical computing with quantum power,"Inspired by the extraordinary computing power promised by quantum computers,
the quantum mind hypothesis postulated that quantum mechanical phenomena are
the source of neuronal synchronization, which, in turn, might underlie
consciousness. Here, I present an alternative inspired by a classical computing
method with quantum power. This method relies on special distributed
representations called hyperstrings. Hyperstrings are superpositions of up to
an exponential number of strings, which -- by a single-processor classical
computer -- can be evaluated in a transparallel fashion, that is,
simultaneously as if only one string were concerned. Building on a neurally
plausible model of human visual perceptual organization, in which hyperstrings
are formal counterparts of transient neural assemblies, I postulate that
synchronization in such assemblies is a manifestation of transparallel
information processing. This accounts for the high combinatorial capacity and
speed of human visual perceptual organization and strengthens ideas that
self-organizing cognitive architecture bridges the gap between neurons and
consciousness.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
http://arxiv.org/abs/0805.2681v2,Canonical polygon Queries on the plane: a New Approach,"The polygon retrieval problem on points is the problem of preprocessing a set
of $n$ points on the plane, so that given a polygon query, the subset of points
lying inside it can be reported efficiently.
  It is of great interest in areas such as Computer Graphics, CAD applications,
Spatial Databases and GIS developing tasks. In this paper we study the problem
of canonical $k$-vertex polygon queries on the plane. A canonical $k$-vertex
polygon query always meets the following specific property: a point retrieval
query can be transformed into a linear number (with respect to the number of
vertices) of point retrievals for orthogonal objects such as rectangles and
triangles (throughout this work we call a triangle orthogonal iff two of its
edges are axis-parallel).
  We present two new algorithms for this problem. The first one requires
$O(n\log^2{n})$ space and $O(k\frac{log^3n}{loglogn}+A)$ query time. A simple
modification scheme on first algorithm lead us to a second solution, which
consumes $O(n^2)$ space and $O(k \frac{logn}{loglogn}+A)$ query time, where $A$
denotes the size of the answer and $k$ is the number of vertices.
  The best previous solution for the general polygon retrieval problem uses
$O(n^2)$ space and answers a query in $O(k\log{n}+A)$ time, where $k$ is the
number of vertices. It is also very complicated and difficult to be implemented
in a standard imperative programming language such as C or C++.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
